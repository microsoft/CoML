[{"id": "openml-steel-plates-fault-9967", "name": "steel-plates-fault", "description": "The dataset is called \"steel-plates-fault\". It contains 2 classes, 1941 instances, 34 features, 33 numeric features, 1 categorical features. The majority class size is 1268 and the minority class size is 673."}, {"id": "openml-collins-3567", "name": "collins", "description": "The dataset is called \"collins\". It contains 2 classes, 500 instances, 22 features, 19 numeric features, 3 categorical features. The majority class size is 80 and the minority class size is 6."}, {"id": "openml-cardiotocography-9979", "name": "cardiotocography", "description": "The dataset is called \"cardiotocography\". It contains 2 classes, 2126 instances, 36 features, 35 numeric features, 1 categorical features. The majority class size is 579 and the minority class size is 53."}, {"id": "openml-monks-problems-2-3493", "name": "monks-problems-2", "description": "The dataset is called \"monks-problems-2\". It contains 2 classes, 601 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 395 and the minority class size is 206."}, {"id": "openml-monks-problems-3-3494", "name": "monks-problems-3", "description": "The dataset is called \"monks-problems-3\". It contains 2 classes, 554 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 288 and the minority class size is 266."}, {"id": "openml-irish-3543", "name": "irish", "description": "The dataset is called \"irish\". It contains 2 classes, 500 instances, 6 features, 2 numeric features, 4 categorical features. The majority class size is 278 and the minority class size is 222."}, {"id": "openml-banknote-authentication-10093", "name": "banknote-authentication", "description": "The dataset is called \"banknote-authentication\". It contains 2 classes, 1372 instances, 5 features, 4 numeric features, 1 categorical features. The majority class size is 762 and the minority class size is 610."}, {"id": "openml-wilt-9914", "name": "wilt", "description": "The dataset is called \"wilt\". It contains 2 classes, 4839 instances, 6 features, 5 numeric features, 1 categorical features. The majority class size is 4578 and the minority class size is 261."}, {"id": "openml-car-21", "name": "car", "description": "The dataset is called \"car\". It contains 2 classes, 1728 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 1210 and the minority class size is 65."}, {"id": "openml-monks-problems-1-3492", "name": "monks-problems-1", "description": "The dataset is called \"monks-problems-1\". It contains 2 classes, 556 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 278 and the minority class size is 278."}, {"id": "openml-segment-36", "name": "segment", "description": "The dataset is called \"segment\". It contains 2 classes, 2310 instances, 20 features, 19 numeric features, 1 categorical features. The majority class size is 330 and the minority class size is 330."}, {"id": "openml-breast-w-15", "name": "breast-w", "description": "The dataset is called \"breast-w\". It contains 2 classes, 699 instances, 10 features, 9 numeric features, 1 categorical features. The majority class size is 458 and the minority class size is 241."}, {"id": "openml-anneal-2", "name": "anneal", "description": "The dataset is called \"anneal\". It contains 2 classes, 898 instances, 39 features, 6 numeric features, 33 categorical features. The majority class size is 684 and the minority class size is 8."}, {"id": "openml-soybean-41", "name": "soybean", "description": "The dataset is called \"soybean\". It contains 2 classes, 683 instances, 36 features, 0 numeric features, 36 categorical features. The majority class size is 92 and the minority class size is 8."}, {"id": "openml-climate-model-simulation-crashes-9980", "name": "climate-model-simulation-crashes", "description": "The dataset is called \"climate-model-simulation-crashes\". It contains 2 classes, 540 instances, 21 features, 20 numeric features, 1 categorical features. The majority class size is 494 and the minority class size is 46."}, {"id": "openml-synthetic-control-3512", "name": "synthetic control", "description": "The dataset is called \"synthetic_control\". It contains 2 classes, 600 instances, 61 features, 60 numeric features, 1 categorical features. The majority class size is 100 and the minority class size is 100."}, {"id": "openml-pc4-3902", "name": "pc4", "description": "The dataset is called \"pc4\". It contains 2 classes, 1458 instances, 38 features, 37 numeric features, 1 categorical features. The majority class size is 1280 and the minority class size is 178."}, {"id": "openml-pc3-3903", "name": "pc3", "description": "The dataset is called \"pc3\". It contains 2 classes, 1563 instances, 38 features, 37 numeric features, 1 categorical features. The majority class size is 1403 and the minority class size is 160."}, {"id": "openml-phoneme-9952", "name": "phoneme", "description": "The dataset is called \"phoneme\". It contains 2 classes, 5404 instances, 6 features, 5 numeric features, 1 categorical features. The majority class size is 3818 and the minority class size is 1586."}, {"id": "openml-credit-approval-29", "name": "credit-approval", "description": "The dataset is called \"credit-approval\". It contains 2 classes, 690 instances, 16 features, 6 numeric features, 10 categorical features. The majority class size is 383 and the minority class size is 307."}, {"id": "openml-kc1-3917", "name": "kc1", "description": "The dataset is called \"kc1\". It contains 2 classes, 2109 instances, 22 features, 21 numeric features, 1 categorical features. The majority class size is 1783 and the minority class size is 326."}, {"id": "openml-kc2-3913", "name": "kc2", "description": "The dataset is called \"kc2\". It contains 2 classes, 522 instances, 22 features, 21 numeric features, 1 categorical features. The majority class size is 415 and the minority class size is 107."}, {"id": "openml-vowel-3022", "name": "vowel", "description": "The dataset is called \"vowel\". It contains 2 classes, 990 instances, 13 features, 10 numeric features, 3 categorical features. The majority class size is 90 and the minority class size is 90."}, {"id": "openml-qsar-biodeg-9957", "name": "qsar-biodeg", "description": "The dataset is called \"qsar-biodeg\". It contains 2 classes, 1055 instances, 42 features, 41 numeric features, 1 categorical features. The majority class size is 699 and the minority class size is 356."}, {"id": "openml-balance-scale-11", "name": "balance-scale", "description": "The dataset is called \"balance-scale\". It contains 2 classes, 625 instances, 5 features, 4 numeric features, 1 categorical features. The majority class size is 288 and the minority class size is 49."}, {"id": "openml-blood-transfusion-service-center-10101", "name": "blood-transfusion-service-center", "description": "The dataset is called \"blood-transfusion-service-center\". It contains 2 classes, 748 instances, 5 features, 4 numeric features, 1 categorical features. The majority class size is 570 and the minority class size is 178."}, {"id": "openml-artificial-characters-14964", "name": "artificial-characters", "description": "The dataset is called \"artificial-characters\". It contains 2 classes, 10218 instances, 8 features, 7 numeric features, 1 categorical features. The majority class size is 1416 and the minority class size is 600."}, {"id": "openml-diabetes-37", "name": "diabetes", "description": "The dataset is called \"diabetes\". It contains 2 classes, 768 instances, 9 features, 8 numeric features, 1 categorical features. The majority class size is 500 and the minority class size is 268."}, {"id": "openml-cylinder-bands-14968", "name": "cylinder-bands", "description": "The dataset is called \"cylinder-bands\". It contains 2 classes, 540 instances, 40 features, 18 numeric features, 22 categorical features. The majority class size is 312 and the minority class size is 228."}, {"id": "openml-mfeat-morphological-18", "name": "mfeat-morphological", "description": "The dataset is called \"mfeat-morphological\". It contains 2 classes, 2000 instances, 7 features, 6 numeric features, 1 categorical features. The majority class size is 200 and the minority class size is 200."}, {"id": "openml-ilpd-9971", "name": "ilpd", "description": "The dataset is called \"ilpd\". It contains 2 classes, 583 instances, 11 features, 9 numeric features, 2 categorical features. The majority class size is 416 and the minority class size is 167."}, {"id": "openml-profb-3561", "name": "profb", "description": "The dataset is called \"profb\". It contains 2 classes, 672 instances, 10 features, 5 numeric features, 5 categorical features. The majority class size is 448 and the minority class size is 224."}, {"id": "openml-vehicle-53", "name": "vehicle", "description": "The dataset is called \"vehicle\". It contains 2 classes, 846 instances, 19 features, 18 numeric features, 1 categorical features. The majority class size is 218 and the minority class size is 199."}, {"id": "openml-eucalyptus-2079", "name": "eucalyptus", "description": "The dataset is called \"eucalyptus\". It contains 2 classes, 736 instances, 20 features, 14 numeric features, 6 categorical features. The majority class size is 214 and the minority class size is 105."}, {"id": "openml-autouniv-au7-700-9905", "name": "autoUniv-au7-700", "description": "The dataset is called \"autoUniv-au7-700\". It contains 2 classes, 700 instances, 13 features, 8 numeric features, 5 categorical features. The majority class size is 245 and the minority class size is 214."}, {"id": "openml-analcatdata-dmft-3560", "name": "analcatdata dmft", "description": "The dataset is called \"analcatdata_dmft\". It contains 2 classes, 797 instances, 5 features, 0 numeric features, 5 categorical features. The majority class size is 155 and the minority class size is 123."}, {"id": "openml-musk-3950", "name": "musk", "description": "The dataset is called \"musk\". It contains 2 classes, 6598 instances, 168 features, 166 numeric features, 2 categorical features. The majority class size is 5581 and the minority class size is 1017."}, {"id": "openml-musk-146082", "name": "musk", "description": "The dataset is called \"musk\". It contains 2 classes, 6598 instances, 168 features, 166 numeric features, 2 categorical features. The majority class size is 5581 and the minority class size is 1017."}, {"id": "openml-banknote-authentication-145834", "name": "banknote-authentication", "description": "The dataset is called \"banknote-authentication\". It contains 2 classes, 1372 instances, 5 features, 4 numeric features, 1 categorical features. The majority class size is 762 and the minority class size is 610."}, {"id": "openml-tic-tac-toe-49", "name": "tic-tac-toe", "description": "The dataset is called \"tic-tac-toe\". It contains 2 classes, 958 instances, 10 features, 0 numeric features, 10 categorical features. The majority class size is 626 and the minority class size is 332."}, {"id": "openml-kr-vs-kp-145953", "name": "kr-vs-kp", "description": "The dataset is called \"kr-vs-kp\". It contains 2 classes, 3196 instances, 37 features, 0 numeric features, 37 categorical features. The majority class size is 1669 and the minority class size is 1527."}, {"id": "openml-kr-vs-kp-3", "name": "kr-vs-kp", "description": "The dataset is called \"kr-vs-kp\". It contains 2 classes, 3196 instances, 37 features, 0 numeric features, 37 categorical features. The majority class size is 1669 and the minority class size is 1527."}, {"id": "openml-sylva-agnostic-3889", "name": "sylva agnostic", "description": "The dataset is called \"sylva_agnostic\". It contains 2 classes, 14395 instances, 217 features, 216 numeric features, 1 categorical features. The majority class size is 13509 and the minority class size is 886."}, {"id": "openml-scene-3485", "name": "scene", "description": "The dataset is called \"scene\". It contains 2 classes, 2407 instances, 300 features, 294 numeric features, 6 categorical features. The majority class size is 1976 and the minority class size is 431."}, {"id": "openml-monks-problems-3-146066", "name": "monks-problems-3", "description": "The dataset is called \"monks-problems-3\". It contains 2 classes, 554 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 288 and the minority class size is 266."}, {"id": "openml-wilt-9889", "name": "wilt", "description": "The dataset is called \"wilt\". It contains 2 classes, 4839 instances, 6 features, 5 numeric features, 1 categorical features. The majority class size is 4578 and the minority class size is 261."}, {"id": "openml-wdbc-145878", "name": "wdbc", "description": "The dataset is called \"wdbc\". It contains 2 classes, 569 instances, 31 features, 30 numeric features, 1 categorical features. The majority class size is 357 and the minority class size is 212."}, {"id": "openml-nomao-145854", "name": "nomao", "description": "The dataset is called \"nomao\". It contains 2 classes, 34465 instances, 119 features, 89 numeric features, 30 categorical features. The majority class size is 24621 and the minority class size is 9844."}, {"id": "openml-nomao-9977", "name": "nomao", "description": "The dataset is called \"nomao\". It contains 2 classes, 34465 instances, 119 features, 89 numeric features, 30 categorical features. The majority class size is 24621 and the minority class size is 9844."}, {"id": "openml-ozone-level-8hr-145855", "name": "ozone-level-8hr", "description": "The dataset is called \"ozone-level-8hr\". It contains 2 classes, 2534 instances, 73 features, 72 numeric features, 1 categorical features. The majority class size is 2374 and the minority class size is 160."}, {"id": "openml-ozone-level-8hr-9978", "name": "ozone-level-8hr", "description": "The dataset is called \"ozone-level-8hr\". It contains 2 classes, 2534 instances, 73 features, 72 numeric features, 1 categorical features. The majority class size is 2374 and the minority class size is 160."}, {"id": "openml-pc1-3918", "name": "pc1", "description": "The dataset is called \"pc1\". It contains 2 classes, 1109 instances, 22 features, 21 numeric features, 1 categorical features. The majority class size is 1032 and the minority class size is 77."}, {"id": "openml-spambase-43", "name": "spambase", "description": "The dataset is called \"spambase\". It contains 2 classes, 4601 instances, 58 features, 57 numeric features, 1 categorical features. The majority class size is 2788 and the minority class size is 1813."}, {"id": "openml-spambase-145979", "name": "spambase", "description": "The dataset is called \"spambase\". It contains 2 classes, 4601 instances, 58 features, 57 numeric features, 1 categorical features. The majority class size is 2788 and the minority class size is 1813."}, {"id": "openml-gina-agnostic-3891", "name": "gina agnostic", "description": "The dataset is called \"gina_agnostic\". It contains 2 classes, 3468 instances, 971 features, 970 numeric features, 1 categorical features. The majority class size is 1763 and the minority class size is 1705."}, {"id": "openml-mozilla4-3899", "name": "mozilla4", "description": "The dataset is called \"mozilla4\". It contains 2 classes, 15545 instances, 6 features, 5 numeric features, 1 categorical features. The majority class size is 10437 and the minority class size is 5108."}, {"id": "openml-bank-marketing-145833", "name": "bank-marketing", "description": "The dataset is called \"bank-marketing\". It contains 2 classes, 45211 instances, 17 features, 7 numeric features, 10 categorical features. The majority class size is 39922 and the minority class size is 5289."}, {"id": "openml-bank-marketing-14965", "name": "bank-marketing", "description": "The dataset is called \"bank-marketing\". It contains 2 classes, 45211 instances, 17 features, 7 numeric features, 10 categorical features. The majority class size is 39922 and the minority class size is 5289."}, {"id": "openml-eeg-eye-state-14951", "name": "eeg-eye-state", "description": "The dataset is called \"eeg-eye-state\". It contains 2 classes, 14980 instances, 15 features, 14 numeric features, 1 categorical features. The majority class size is 8257 and the minority class size is 6723."}, {"id": "openml-phoneme-145857", "name": "phoneme", "description": "The dataset is called \"phoneme\". It contains 2 classes, 5404 instances, 6 features, 5 numeric features, 1 categorical features. The majority class size is 3818 and the minority class size is 1586."}, {"id": "openml-qsar-biodeg-145862", "name": "qsar-biodeg", "description": "The dataset is called \"qsar-biodeg\". It contains 2 classes, 1055 instances, 42 features, 41 numeric features, 1 categorical features. The majority class size is 699 and the minority class size is 356."}, {"id": "openml-magictelescope-3954", "name": "MagicTelescope", "description": "The dataset is called \"MagicTelescope\". It contains 2 classes, 19020 instances, 12 features, 11 numeric features, 1 categorical features. The majority class size is 12332 and the minority class size is 6688."}, {"id": "openml-electricity-219", "name": "electricity", "description": "The dataset is called \"electricity\". It contains 2 classes, 45312 instances, 9 features, 7 numeric features, 2 categorical features. The majority class size is 26075 and the minority class size is 19237."}, {"id": "openml-ada-agnostic-3896", "name": "ada agnostic", "description": "The dataset is called \"ada_agnostic\". It contains 2 classes, 4562 instances, 49 features, 48 numeric features, 1 categorical features. The majority class size is 3430 and the minority class size is 1132."}, {"id": "openml-electricity-146012", "name": "electricity", "description": "The dataset is called \"electricity\". It contains 2 classes, 45312 instances, 9 features, 7 numeric features, 2 categorical features. The majority class size is 26075 and the minority class size is 19237."}, {"id": "openml-click-prediction-small-7295", "name": "Click prediction small", "description": "The dataset is called \"Click_prediction_small\". It contains 2 classes, 39948 instances, 10 features, 9 numeric features, 1 categorical features. The majority class size is 33220 and the minority class size is 6728."}, {"id": "openml-click-prediction-small-14971", "name": "Click prediction small", "description": "The dataset is called \"Click_prediction_small\". It contains 2 classes, 39948 instances, 10 features, 9 numeric features, 1 categorical features. The majority class size is 33220 and the minority class size is 6728."}, {"id": "openml-blood-transfusion-service-center-145836", "name": "blood-transfusion-service-center", "description": "The dataset is called \"blood-transfusion-service-center\". It contains 2 classes, 748 instances, 5 features, 4 numeric features, 1 categorical features. The majority class size is 570 and the minority class size is 178."}, {"id": "openml-diabetes-145976", "name": "diabetes", "description": "The dataset is called \"diabetes\". It contains 2 classes, 768 instances, 9 features, 8 numeric features, 1 categorical features. The majority class size is 500 and the minority class size is 268."}, {"id": "openml-credit-g-145972", "name": "credit-g", "description": "The dataset is called \"credit-g\". It contains 2 classes, 1000 instances, 21 features, 7 numeric features, 14 categorical features. The majority class size is 700 and the minority class size is 300."}, {"id": "openml-hill-valley-9970", "name": "hill-valley", "description": "The dataset is called \"hill-valley\". It contains 2 classes, 1212 instances, 101 features, 100 numeric features, 1 categorical features. The majority class size is 606 and the minority class size is 606."}, {"id": "openml-hill-valley-145847", "name": "hill-valley", "description": "The dataset is called \"hill-valley\". It contains 2 classes, 1212 instances, 101 features, 100 numeric features, 1 categorical features. The majority class size is 606 and the minority class size is 606."}, {"id": "openml-madelon-9976", "name": "madelon", "description": "The dataset is called \"madelon\". It contains 2 classes, 2600 instances, 501 features, 500 numeric features, 1 categorical features. The majority class size is 1300 and the minority class size is 1300."}, {"id": "openml-madelon-145853", "name": "madelon", "description": "The dataset is called \"madelon\". It contains 2 classes, 2600 instances, 501 features, 500 numeric features, 1 categorical features. The majority class size is 1300 and the minority class size is 1300."}, {"id": "openml-steel-plates-fault-145872", "name": "steel-plates-fault", "description": "The dataset is called \"steel-plates-fault\". It contains 2 classes, 1941 instances, 34 features, 33 numeric features, 1 categorical features. The majority class size is 1268 and the minority class size is 673."}, {"id": "openml-wdbc-9946", "name": "wdbc", "description": "The dataset is called \"wdbc\". It contains 2 classes, 569 instances, 31 features, 30 numeric features, 1 categorical features. The majority class size is 357 and the minority class size is 212."}, {"id": "openml-climate-model-simulation-crashes-145839", "name": "climate-model-simulation-crashes", "description": "The dataset is called \"climate-model-simulation-crashes\". It contains 2 classes, 540 instances, 21 features, 20 numeric features, 1 categorical features. The majority class size is 494 and the minority class size is 46."}, {"id": "openml-eeg-eye-state-9983", "name": "eeg-eye-state", "description": "The dataset is called \"eeg-eye-state\". It contains 2 classes, 14980 instances, 15 features, 14 numeric features, 1 categorical features. The majority class size is 8257 and the minority class size is 6723."}, {"id": "openml-heart-statlog-282", "name": "heart-statlog", "description": "The dataset is called \"heart-statlog\". It contains 2 classes, 270 instances, 14 features, 13 numeric features, 1 categorical features. The majority class size is 150 and the minority class size is 120."}, {"id": "openml-haberman-272", "name": "haberman", "description": "The dataset is called \"haberman\". It contains 2 classes, 306 instances, 4 features, 2 numeric features, 2 categorical features. The majority class size is 225 and the minority class size is 81."}, {"id": "openml-ilpd-145848", "name": "ilpd", "description": "The dataset is called \"ilpd\". It contains 2 classes, 583 instances, 11 features, 9 numeric features, 2 categorical features. The majority class size is 416 and the minority class size is 167."}, {"id": "openml-amazon-employee-access-9911", "name": "Amazon employee access", "description": "The dataset is called \"Amazon_employee_access\". It contains 2 classes, 32769 instances, 10 features, 0 numeric features, 10 categorical features. The majority class size is 30872 and the minority class size is 1897."}, {"id": "openml-amazon-employee-access-34539", "name": "Amazon employee access", "description": "The dataset is called \"Amazon_employee_access\". It contains 2 classes, 32769 instances, 10 features, 0 numeric features, 10 categorical features. The majority class size is 30872 and the minority class size is 1897."}, {"id": "openml-phishingwebsites-14952", "name": "PhishingWebsites", "description": "The dataset is called \"PhishingWebsites\". It contains 2 classes, 11055 instances, 31 features, 0 numeric features, 31 categorical features. The majority class size is 6157 and the minority class size is 4898."}, {"id": "openml-monks-problems-1-146064", "name": "monks-problems-1", "description": "The dataset is called \"monks-problems-1\". It contains 2 classes, 556 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 278 and the minority class size is 278."}, {"id": "openml-monks-problems-2-146065", "name": "monks-problems-2", "description": "The dataset is called \"monks-problems-2\". It contains 2 classes, 601 instances, 7 features, 0 numeric features, 7 categorical features. The majority class size is 395 and the minority class size is 206."}, {"id": "openml-bioresponse-14966", "name": "Bioresponse", "description": "The dataset is called \"Bioresponse\". It contains 2 classes, 3751 instances, 1777 features, 1776 numeric features, 1 categorical features. The majority class size is 2034 and the minority class size is 1717."}, {"id": "openml-bioresponse-145677", "name": "Bioresponse", "description": "The dataset is called \"Bioresponse\". It contains 2 classes, 3751 instances, 1777 features, 1776 numeric features, 1 categorical features. The majority class size is 2034 and the minority class size is 1717."}, {"id": "openml-australian-125923", "name": "Australian", "description": "The dataset is called \"Australian\". It contains 2 classes, 690 instances, 15 features, 14 numeric features, 1 categorical features. The majority class size is 383 and the minority class size is 307."}, {"id": "openml-phishingwebsites-34537", "name": "PhishingWebsites", "description": "The dataset is called \"PhishingWebsites\". It contains 2 classes, 11055 instances, 31 features, 0 numeric features, 31 categorical features. The majority class size is 6157 and the minority class size is 4898."}, {"id": "openml-bioresponse-9910", "name": "Bioresponse", "description": "The dataset is called \"Bioresponse\". It contains 2 classes, 3751 instances, 1777 features, 1776 numeric features, 1 categorical features. The majority class size is 2034 and the minority class size is 1717."}, {"id": "openml-credit-g-31", "name": "credit-g", "description": "The dataset is called \"credit-g\". It contains 2 classes, 1000 instances, 21 features, 7 numeric features, 14 categorical features. The majority class size is 700 and the minority class size is 300."}, {"id": "openml-tic-tac-toe-145804", "name": "tic-tac-toe", "description": "The dataset is called \"tic-tac-toe\". It contains 2 classes, 958 instances, 10 features, 0 numeric features, 10 categorical features. The majority class size is 626 and the minority class size is 332."}, {"id": "openml-credit-g-146803", "name": "credit-g", "description": "The dataset is called \"credit-g\". It contains 2 classes, 1000 instances, 21 features, 7 numeric features, 14 categorical features. The majority class size is 700 and the minority class size is 300."}, {"id": "packit", "name": "PackIt", "description": "The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. This ability is referred to as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. PackIt is a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space."}, {"id": "youtube-vis-2021-validation-video-instance-segmentation-on-youtube-vis-2021-validation", "name": "YouTube-VIS 2021 validation (Video Instance Segmentation on YouTube-VIS 2021 validation)", "description": "3,859 high-resolution YouTube videos, 2,985 training videos, 421 validation videos and 453 test videos. An improved 40-category label set by merging eagle and owl into bird, ape into monkey, deleting hands, and adding flying disc, squirrel and whale 8,171 unique video instances 232k high-quality manual annotations"}, {"id": "jrdb-jackrabbot-dataset-and-benchmark", "name": "JRDB (JackRabbot Dataset and Benchmark)", "description": "A novel egocentric dataset collected from social mobile manipulator JackRabbot. The dataset includes 64 minutes of annotated multimodal sensor data including stereo cylindrical 360 degrees RGB video at 15 fps, 3D point clouds from two Velodyne 16 Lidars, line 3D point clouds from two Sick Lidars, audio signal, RGB-D video at 30 fps, 360 degrees spherical image from a fisheye camera and encoder values from the robot's wheels."}, {"id": "salinas-salinas-scene", "name": "Salinas (Salinas Scene)", "description": "Salinas Scene is a hyperspectral dataset collected by the 224-band AVIRIS sensor over Salinas Valley, California, and is characterized by high spatial resolution (3.7-meter pixels). The area covered comprises 512 lines by 217 samples. 20 water absorption bands were discarder: [108-112], [154-167], 224. This image was available only as at-sensor radiance data. It includes vegetables, bare soils, and vineyard fields. Salinas groundtruth contains 16 classes."}, {"id": "mslr-web10k", "name": "MSLR-WEB10K", "description": "The MSLR-WEB10K dataset consists of 10,000 search queries over the documents from search results. The data also contains the values of 136 features and a corresponding user-labeled relevance factor on a scale of one to five with respect to each query-document pair. It is a subset of the MSLR-WEB30K dataset."}, {"id": "ap-10k", "name": "AP-10K", "description": "AP-10K is the first large-scale benchmark for general animal pose estimation, to facilitate the research in animal pose estimation. AP-10K consists of 10,015 images collected and filtered from 23 animal families and 60 species following the taxonomic rank and high-quality keypoint annotations labeled and checked manually."}, {"id": "mdbd-multicue-dataset-for-edge-detection", "name": "MDBD (Multicue Dataset for Edge Detection)", "description": "In order to study the interaction of several early visual cues (luminance, color, stereo, motion) during boundary detection in challenging natural scenes, we have built a multi-cue video dataset composed of short binocular video sequences of natural scenes using a consumer-grade Fujifilm stereo camera (M\u00e9ly, Kim, McGill, Guo and Serre, 2016). We considered a variety of places (from university campuses to street scenes and parks) and seasons to minimize possible biases. We attempted to capture more challenging scenes for boundary detection by framing a few dominant objects in each shot under a variety of appearances. Representative sample keyframes are shown on the figure below. The dataset contains 100 scenes, each consisting of a left and right view short (10-frame) color sequence. Each sequence was sampled at a rate of 30 frames per second. Each frame has a resolution of 1280 by 720 pixels."}, {"id": "couch", "name": "COUCH", "description": "COUCH is a large human-chair interaction dataset with clean annotations. The dataset consists of 3 hours and over 500 sequences of motion capture (MoCap) on human-chair interactions."}, {"id": "drivingstereo", "name": "DrivingStereo", "description": "DrivingStereo contains over 180k images covering a diverse set of driving scenarios, which is hundreds of times larger than the KITTI Stereo dataset. High-quality labels of disparity are produced by a model-guided filtering strategy from multi-frame LiDAR points. "}, {"id": "dcm", "name": "DCM", "description": "The DCM dataset is composed of 772 annotated images from 27 golden age comic books. We freely collected them from the free public domain collection of digitized comic books Digital Comics Museum. One album per available publisher was selected to get as many different styles as possible. We made ground-truth bounding boxes of all panels, all characters (body + faces), small or big, human-like or animal-like."}, {"id": "tut-urban-acoustic-scenes-2018", "name": "TUT Urban Acoustic Scenes 2018", "description": "The dataset for this task is the TUT Urban Acoustic Scenes 2018 dataset, consisting of recordings from various acoustic scenes. The dataset was recorded in six large european cities, in different locations for each scene class. For each recording location there are 5-6 minutes of audio. The original recordings were split into segments with a length of 10 seconds that are provided in individual files. Available information about the recordings include the following: acoustic scene class, city, and recording location."}, {"id": "spacenet-1-spacenet-1-building-detection-v1", "name": "SpaceNet 1 (SpaceNet 1: Building Detection v1)", "description": "SpaceNet 1: Building Detection v1 is a dataset for building footprint detection. The data is comprised of 382,534 building footprints, covering an area of 2,544 sq. km of 3/8 band WorldView-2 imagery (0.5 m pixel res.) across the city of Rio de Janeiro, Brazil. The images are processed as 200m\u00d7200m tiles with associated building footprint vectors for training."}, {"id": "trajnet", "name": "TrajNet", "description": "The TrajNet Challenge represents a large multi-scenario forecasting benchmark. The challenge consists on  predicting 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) i.e., t\u22127,t\u22126,\u2026,t, in world plane coordinates (the so-called world plane Human-Human protocol) and forecasting the following 12 (4.8 seconds), i.e., t+1,\u2026,t+12. The 8-12-value protocol is consistent with the most trajectory forecasting approaches, usually focused on the 5-dataset ETH-univ + ETH-hotel + UCY-zara01 + UCY-zara02 + UCY-univ. Trajnet extends substantially the 5-dataset scenario by diversifying the training data, thus stressing the flexibility and generalization one approach has to exhibit when it comes to unseen scenery/situations. In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel (orthogonal bird\u2019s eye flight view, moving people), 2) Crowds UCY (3 datasets, tilted bird\u2019s eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS (multisensor, different human activities) and 4) Stanford Drone Dataset (8 scenes, high orthogonal bird\u2019s eye flight view, different agents as people, cars etc. ), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants)."}, {"id": "tracking-the-trackers", "name": "Tracking the Trackers", "description": "Tracking the Trackers is a large-scale analysis of third-party trackers on the World Wide Web. We extract third-party embeddings from more than 3.5 billion web pages of the CommonCrawl 2012 corpus, and aggregate those to a dataset containing more than 140 million third-party embeddings in over 41 million domains."}, {"id": "coauthor", "name": "CoAuthor", "description": "CoAuthor is a dataset designed for revealing GPT-3's capabilities in assisting creative and argumentative writing. CoAuthor captures rich interactions between 63 writers and four instances of GPT-3 across 1445 writing sessions."}, {"id": "bam-behance-artistic-media", "name": "BAM! (Behance Artistic Media)", "description": "The Behance Artistic Media dataset (BAM!) is a large-scale dataset of contemporary artwork from Behance, a website containing millions of portfolios from professional and commercial artists. We annotate Behance imagery with rich attribute labels for content, emotions, and artistic media. We believe our Behance Artistic Media dataset will be a good starting point for researchers wishing to study artistic imagery and relevant problems."}, {"id": "peta-pedestrian-attribute", "name": "PETA (Pedestrian Attribute)", "description": "The PEdesTrian Attribute dataset (PETA) is a dataset fore recognizing pedestrian attributes, such as gender and clothing style, at a far distance. It is of interest in video surveillance scenarios where face and body close-shots and hardly available. It consists of 19,000 pedestrian images with 65 attributes (61 binary and 4 multi-class). Those images contain 8705 persons."}, {"id": "p-destre", "name": "P-DESTRE", "description": "Provides consistent ID annotations across multiple days, making it suitable for the extremely challenging problem of person search, i.e., where no clothing information can be reliably used. Apart this feature, the P-DESTRE annotations enable the research on UAV-based pedestrian detection, tracking, re-identification and soft biometric solutions."}, {"id": "open-pi", "name": "Open PI", "description": "Open PI is the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. The dataset comprises 29,928 state changes over 4,050 sentences from 810 procedural real-world paragraphs from WikiHow.com. The state tracking task assumes new formulation in which just the text is provided, from which a set of state changes (entity, attribute, before, after) is generated for each step, where the entity, attribute, and values must all be predicted from an open vocabulary."}, {"id": "funcom", "name": "Funcom", "description": "Funcom is a collection of ~2.1 million Java methods and their associated Javadoc comments. This data set was derived from a set of 51 million Java methods and only includes methods that have an associated comment, comments that are in the English language, and has had auto-generated files removed. Each method/comment pair also has an associated method_uid and project_uid so that it is easy to group methods by their parent project."}, {"id": "raider", "name": "Raider", "description": "The Raider dataset collects fMRI recordings of 1000 voxels from the ventral temporal cortex, for 10 healthy adult participants passively watching the full-length movie \u201cRaiders of the Lost Ark\u201d."}, {"id": "friendster", "name": "Friendster", "description": "Friendster is an on-line gaming network. Before re-launching as a game website, Friendster was a social networking site where users can form friendship edge each other. Friendster social network also allows users form a group which other members can then join. The Friendster dataset consist of ground-truth communities (based on user-defined groups) and the social network from induced subgraph of the nodes that either belong to at least one community or are connected to other nodes that belong to at least one community."}, {"id": "casual-conversations", "name": "Casual Conversations", "description": "Casual Conversations dataset is designed to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of age, genders, apparent skin tones and ambient lighting conditions."}, {"id": "korquad-the-korean-question-answering-dataset", "name": "KorQuAD (The Korean Question Answering Dataset)", "description": "KorQuAD is a large-scale question-and-answer dataset constructed for Korean machine reading comprehension, and investigate the dataset to understand the distribution of answers and the types of reasoning required to answer the question. This dataset benchmarks the data generating process of SQuAD to meet the standard."}, {"id": "ut-interaction", "name": "UT-Interaction", "description": "The UT-Interaction dataset contains videos of continuous executions of 6 classes of human-human interactions: shake-hands, point, hug, push, kick and punch. Ground truth labels for these interactions are provided, including time intervals and bounding boxes. There is a total of 20 video sequences whose lengths are around 1 minute. Each video contains at least one execution per interaction, resulting in 8 executions of human activities per video on average. Several participants with more than 15 different clothing conditions appear in the videos. The videos are taken with the resolution of 720*480, 30fps, and the height of a person in the video is about 200 pixels."}, {"id": "20newsgroup-10-tasks", "name": "20Newsgroup (10 tasks)", "description": "This dataset has 20 classes and each class has about 1000 documents. The data split for train/validation/test is 1600/200/200. We created 10 tasks, 2 classes per task.  Since this is topic-based text classification data, the classes are very different and have little shared knowledge. As mentioned above, this application (and dataset) is mainly used to show a CL model's ability to overcome forgetting. Detailed statistics please on page https://github.com/ZixuanKe/PyContinual"}, {"id": "kitti-c", "name": "KITTI-C", "description": "KITTI-C is an evaluation benchmark heading toward robust and reliable 3D object detection in autonomous driving. With it, we probe the robustness of 3D detectors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:"}, {"id": "actionet", "name": "ActioNet", "description": "ActioNet is a video task-based dataset collected in a synthetic 3D environment. It contains 3,038 annotated videos and hierarchical task structures over 65 individual household tasks from 120 different scenes. Each task is annotated across three to five different scenes by 10 different annotators. The tasks can be broken down into four categories: living room, bedroom, bathroom, kitchen."}, {"id": "c4-colossal-clean-crawled-corpus", "name": "C4 (Colossal Clean Crawled Corpus)", "description": "C4 is a colossal, cleaned version of Common Crawl's web crawl corpus. It was based on Common Crawl dataset: https://commoncrawl.org. It was used to train the T5 text-to-text Transformer models."}, {"id": "sume-a-dataset-towards-summarizing-biomedical-mechanisms", "name": "SuMe (A Dataset Towards Summarizing Biomedical Mechanisms)", "description": "Can language models read biomedical texts and explain the biomedical mechanisms discussed? In this work we introduce a biomedical mechanism summarization task. Biomedical studies often investigate the mechanisms behind how one entity (e.g., a protein or a chemical) affects another in a biological context. The abstracts of these publications often include a focused set of sentences that present relevant supporting statements regarding such relationships, associated experimental evidence, and a concluding sentence that summarizes the mechanism underlying the relationship. We leverage this structure and create a summarization task, where the input is a collection of sentences and the main entities in an abstract, and the output includes the relationship and a sentence that summarizes the mechanism. Using a small amount of manually labeled mechanism sentences, we train a mechanism sentence classifier to filter a large biomedical abstract collection and create a summarization dataset with 22k instances. We also introduce conclusion sentence generation as a pretraining task with 611k instances. We benchmark the performance of large bio-domain language models. We find that while the pretraining task help improves performance, the best model produces acceptable mechanism outputs in only 32% of the instances, which shows the task presents significant challenges in biomedical language understanding and summarization."}, {"id": "wildreceipt", "name": "WildReceipt", "description": "WildReceipt is a collection  of receipts.  It contains, for each photo, of a list of OCRs - with bounding box, text, and class. "}, {"id": "intel-tau", "name": "INTEL-TAU", "description": "A new large dataset for illumination estimation. This dataset, called INTEL-TAU, contains 7022 images in total, which makes it the largest available high-resolution dataset for illumination estimation research. "}, {"id": "k-sportssum", "name": "K-SportsSum", "description": "K-SportsSum is a sports game summarization dataset with two characteristics: (1) K-SportsSum collects a large amount of data from massive games. It has 7,854 commentary-news pairs. To improve the quality, K-SportsSum employs a manual cleaning process; (2) Different from existing datasets, to narrow the knowledge gap, K-SportsSum further provides a large-scale knowledge corpus that contains the information of 523 sports teams and 14,724 sports players."}, {"id": "mrnet", "name": "MRNet", "description": "The MRNet dataset consists of 1,370 knee MRI exams performed at Stanford University Medical Center. The dataset contains 1,104 (80.6%) abnormal exams, with 319 (23.3%) ACL tears and 508 (37.1%) meniscal tears; labels were obtained through manual extraction from clinical reports. "}, {"id": "musicbrainz20k", "name": "MusicBrainz20K", "description": "The MusicBrainz20K dataset for entity resolution and entity clustering is based on real records about songs from the MusicBrainz database. Each record is described with the following attributes: artist, title, album, year and length. The records have been modified with the DAPO [1] data generator. The generated dataset consists of five sources and approximately 20K records describing 10K unique song entities. It contains duplicates for 50% of the original records in two to five sources which are generated with a high degree of corruption to stress-test the entity resolution and clustering approaches."}, {"id": "dialogre", "name": "DialogRE", "description": "DialogRE is the first human-annotated dialogue-based relation extraction dataset, containing 1,788 dialogues originating from the complete transcripts of a famous American television situation comedy Friends. The are annotations for all occurrences of 36 possible relation types that exist between an argument pair in a dialogue. DialogRE is available in English and Chinese."}, {"id": "carnatic", "name": "Carnatic", "description": "This dataset includes music time information i.e. Beat, Bar, and meter annotations of the Indian Carnatic music dataset. The dataset is gathered by A. Srinivasamurthy and X. Serra [1]."}, {"id": "a-ground-truth-dataset-to-identify-bots-in-github", "name": "A ground-truth dataset to identify bots in GitHub", "description": "This dataset is a ground truth dataset that is used to identify bots. Each account in this dataset is rated by at least 3 raters with a high interrater agreement."}, {"id": "nombank", "name": "NomBank", "description": "NomBank is an annotation project at New York University that is related to the PropBank project at the University of Colorado.  The goal is to mark the sets of arguments that cooccur with nouns in the PropBank Corpus (the Wall Street Journal Corpus of the Penn Treebank), just as PropBank records such information for verbs.  As a side effect of the annotation process, the authors are producing a number of other resources including various dictionaries, as well as PropBank style lexical entries called frame files. These resources help the user label the various arguments and adjuncts of the head nouns with roles (sets of argument labels for each sense of each noun).  NYU and U of Colorado are making a coordinated effort to insure that, when possible, role definitions are consistent across parts of speech. For example, PropBank's frame file for the verb \"decide\" was used in the annotation of the noun \"decision\"."}, {"id": "dyml-animal-dynamic-metric-learning-animal", "name": "DyML-Animal (Dynamic Metric Learning Animal)", "description": "DyML-Animal is based on animal images selected from ImageNet-5K [1]. It has 5 semantic scales (i.e., classes, order, family, genus, species) according to biological taxonomy. Specifically, there are 611 \u201cspecies\u201d for the fine level, 47 categories corresponding to \u201corder\u201d, \u201cfamily\u201d or \u201cgenus\u201d for the middle level, and 5 \u201cclasses\u201d for the coarse level. We note some animals have contradiction between visual perception and biological taxonomy, e.g., whale in \u201cmammal\u201d actually looks more similar to fish. Annotating the whale images as belonging to mammal would cause confusion to visual recognition. So we take a detailed check on potential contradictions and intentionally leave out those animals."}, {"id": "mars-motion-analysis-and-re-identification-set", "name": "MARS (Motion Analysis and Re-identification Set)", "description": "MARS (Motion Analysis and Re-identification Set) is a large scale video based person reidentification dataset, an extension of the Market-1501 dataset. It has been collected from six near-synchronized cameras. It consists of 1,261 different pedestrians, who are captured by at least 2 cameras. The variations in poses, colors and illuminations of pedestrians, as well as the poor image quality, make it very difficult to yield high matching accuracy. Moreover, the dataset contains 3,248 distractors in order to make it more realistic. Deformable Part Model and GMMCP tracker were used to automatically generate the tracklets (mostly 25-50 frames long)."}, {"id": "openlane", "name": "OpenLane", "description": "OpenLane is the first real-world and the largest scaled 3D lane dataset to date. The dataset collects valuable contents from public perception dataset Waymo Open Dataset and provides lane&closest-in-path object(CIPO) annotation for 1000 segments. In short, OpenLane owns 200K frames and over 880K carefully annotated lanes. The OpenLane Dataset is publicly released to aid the research community in making advancements in 3D perception and autonomous driving technology."}, {"id": "videonavqa", "name": "VideoNavQA", "description": "The VideoNavQA dataset contains pairs of questions and videos generated in the House3D environment. The goal of this dataset is to assess question-answering performance from nearly-ideal navigation paths, while considering a much more complete variety of questions than current instantiations of the Embodied Question Answering (EQA) task."}, {"id": "ucf101-24", "name": "UCF101-24", "description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled)."}, {"id": "landcover-ai-dataset-for-automatic-mapping-of-buildings-woodlands-water-and-roads-from-aerial-imagery", "name": "LandCover.ai (Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery)", "description": "The LandCover.ai (Land Cover from Aerial Imagery) dataset is a dataset for automatic mapping of buildings, woodlands, water and roads from aerial images. "}, {"id": "cb-tof-cornell-box-time-of-flight-dataset", "name": "CB-ToF (Cornell-Box Time-of-Flight Dataset)", "description": "The CornellBox Dataset can be downloaded from this URL"}, {"id": "l3cubemahasent", "name": "L3CubeMahaSent", "description": "L3CubeMahaSent  is a large publicly available Marathi Sentiment Analysis dataset. It consists of marathi tweets which are manually labelled."}, {"id": "fma-free-music-archive", "name": "FMA (Free Music Archive)", "description": "The Free Music Archive (FMA) is a large-scale dataset for evaluating several tasks in Music Information Retrieval. It consists of 343 days of audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies."}, {"id": "casia-v2", "name": "CASIA V2", "description": "CASIA V2 is a dataset for forgery classification. It contains 4795 images, 1701 authentic and 3274 forged."}, {"id": "hover", "name": "HoVer", "description": "Is a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is Supported or Not-Supported by the facts. In HoVer, the claims require evidence to be extracted from as many as four English Wikipedia articles and embody reasoning graphs of diverse shapes."}, {"id": "wikitext-103", "name": "WikiText-103", "description": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License."}, {"id": "musan", "name": "MUSAN", "description": "MUSAN is a corpus of music, speech and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises."}, {"id": "guesswhat", "name": "GuessWhat?!", "description": "GuessWhat?! is a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images."}, {"id": "didi-dataset-the-didi-dataset-digital-ink-diagram-data", "name": "DIDI Dataset (The DIDI dataset: Digital Ink Diagram data)", "description": "The dataset contains digital ink drawings of diagrams with dynamic drawing information. The dataset aims to foster research in interactive graphical symbolic understanding. The dataset was obtained using a prompted data collection effort."}, {"id": "vi-fi-multi-modal-dataset", "name": "Vi-Fi Multi-modal Dataset", "description": "A large-scale multi-modal dataset to facilitate research and studies that concentrate on vision-wireless systems. The Vi-Fi dataset is a large-scale multi-modal dataset that consists of vision, wireless and smartphone motion sensor data of multiple participants and passer-by pedestrians in both indoor and outdoor scenarios. In Vi-Fi, vision modality includes RGB-D video from a mounted camera. Wireless modality comprises smartphone data from participants including WiFi FTM and IMU measurements."}, {"id": "talk2car", "name": "Talk2Car", "description": "The Talk2Car dataset finds itself at the intersection of various research domains, promoting the development of cross-disciplinary solutions for improving the state-of-the-art in grounding natural language into visual space. The annotations were gathered with the following aspects in mind: Free-form high quality natural language commands, that stimulate the development of solutions that can operate in the wild. A realistic task setting. Specifically, the authors consider an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle by giving commands in natural language. The Talk2Car dataset was build on top of the nuScenes dataset to include an extensive suite of sensor modalities, i.e. semantic maps, GPS, LIDAR, RADAR and 360-degree RGB images annotated with 3D bounding boxes. Such variety of input modalities sets the object referral task on the Talk2Car dataset apart from related challenges, where additional sensor modalities are generally missing."}, {"id": "undd-urban-night-driving-dataset", "name": "UNDD (Urban Night Driving Dataset)", "description": "UNDD consists of 7125 unlabelled day and night images; additionally, it has 75 night images with pixel-level annotations having classes equivalent to Cityscapes dataset."}, {"id": "ecoset", "name": "ecoset", "description": "Ecoset, an ecologically motivated image dataset, is a large-scale image dataset designed for human visual neuroscience, which consists of over 1.5 million images from 565 basic-level categories. Category selection was based on English nouns that most frequently occur in spoken language (estimated on a set of 51 million words obtained from American television and film subtitles) and concreteness ratings from human observers. Ecoset consists of basic-level categories (including human categories man, woman, and child) that describe physical things in the world (rather than abstract concepts) that are important to humans."}, {"id": "visevent", "name": "VisEvent", "description": "VisEvent (Visible-Event benchmark) is a dataset constructed for the evaluation of tracking by combing visible and event cameras. VisEvent is featured in: "}, {"id": "live1-live-public-domain-subjective-image-quality-database", "name": "LIVE1 (LIVE Public-Domain Subjective Image Quality Database)", "description": "Quality Assessment research strongly depends upon subjective experiments to provide calibration data as well as a testing mechanism. After all, the goal of all QA research is to make quality predictions that are in agreement with subjective opinion of human observers. In order to calibrate QA algorithms and test their performance, a data set of images and videos whose quality has been ranked by human subjects is required. The QA algorithm may be trained on part of this data set, and tested on the rest. "}, {"id": "eyediap", "name": "EYEDIAP", "description": "The EYEDIAP dataset is a dataset for gaze estimation from remote RGB, and RGB-D (standard vision and depth), cameras. The recording methodology was designed by systematically including, and isolating, most of the variables which affect the remote gaze estimation algorithms:"}, {"id": "biasbios-bias-in-bios", "name": "BiasBios (Bias in Bios)", "description": "The purpose of this dataset was to study gender bias in occupations. Online biographies, written in English, were collected to find the names, pronouns, and occupations. Twenty-eight most frequent occupations were identified based on their appearances.  The resulting dataset consists of 397,340 biographies spanning twenty-eight different occupations. Of these occupations, the professor is the most frequent, with 118,400 biographies, while the rapper is the least frequent, with 1,406 biographies.  Important information about the biographies: 1. The longest biography is 194 tokens, while the shortest is eighteen; the median biography length is seventy-two tokens. 2. It should be noted that the demographics of online biographies\u2019 subjects differ from those of the overall workforce and that this dataset does not contain all biographies on the Internet."}, {"id": "senteval", "name": "SentEval", "description": "SentEval is a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders."}, {"id": "nabirds-north-america-birds", "name": "NABirds (North America Birds)", "description": "NABirds V1 is a collection of 48,000 annotated photographs of the 400 species of birds that are commonly observed in North America. More than 100 photographs are available for each species, including separate annotations for males, females and juveniles that comprise 700 visual categories. This dataset is to be used for fine-grained visual categorization experiments."}, {"id": "scenenet", "name": "SceneNet", "description": "SceneNet is a dataset of labelled synthetic indoor scenes. There are several labeled indoor scenes, including:"}, {"id": "rwc-real-world-computing-music-database", "name": "RWC (Real World Computing Music Database)", "description": "The RWC (Real World Computing) Music Database is a copyright-cleared music database (DB) that is available to researchers as a common foundation for research. It contains around 100 complete songs with manually labeled section boundaries. For the 50 instruments, individual sounds at half-tone intervals were captured with several variations of playing styles, dynamics, instrument manufacturers and musicians."}, {"id": "wilddeepfake", "name": "WildDeepfake", "description": "WildDeepfake is a dataset for real-world deepfakes detection which consists of 7,314 face sequences extracted from 707 deepfake videos that are collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop more effective detectors against real-world deepfakes."}, {"id": "phrasecut", "name": "PhraseCut", "description": "PhraseCut is a dataset consisting of 77,262 images and 345,486 phrase-region pairs. The dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated."}, {"id": "masri-headset", "name": "MASRI-HEADSET", "description": "MASRI-HEADSET is a corpus that was developed by the MASRI project at the University of Malta. It consists of 8 hours of speech paired with text, recorded by using short text snippets in a laboratory environment. The speakers were recruited from different geographical locations all over the Maltese islands, and were roughly evenly distributed by gender. "}, {"id": "rellis-3d", "name": "RELLIS-3D", "description": "RELLIS-3D is a multi-modal dataset for off-road robotics. It was collected in an off-road environment containing annotations for 13,556 LiDAR scans and 6,235 images. The data was collected on the Rellis Campus of Texas A&M University and presents challenges to existing algorithms related to class imbalance and environmental topography. The dataset also provides full-stack sensor data in ROS bag format, including RGB camera images, LiDAR point clouds, a pair of stereo images, high-precision GPS measurement, and IMU data."}, {"id": "ifcnet", "name": "IFCNet", "description": "The full IFCNet dataset currently consists of 19,000 CAD models distributed over 65 classes according to the taxonomy of the Industry Foundation Classes (IFC) standard. The IFC standard provides an open data exchange format for projects in the Architecture, Engineering and Construction (AEC) domain. Due to high imbalances with respect to the number of objects in each class, a subset of 8,000 objects from 20 classes is selected to form the IFCNetCore dataset, providing a more balanced distribution. Apart from the geometric information of the CAD model, most objects also have semantic information in the form of key-value pairs, enums or lists, which are relevant to different stages of the construction process."}, {"id": "4dmatch", "name": "4DMatch", "description": "A benchmark for matching and registration of partial point clouds with time-varying geometry. It is constructed using randomly selected 1761 sequences from DeformingThings4D."}, {"id": "graphine", "name": "Graphine", "description": "The Graphine dataset contains 2,010,648 terminology definition pairs organized in 227 directed acyclic graphs. Each node in the graph is associated with a terminology and its definition. Terminologies are organized from coarse-grained ones to fine-grained ones in each graph."}, {"id": "sachs-sachs-protein-dataset", "name": "Sachs (Sachs Protein Dataset)", "description": "Sachs dataset measures the expression level of different proteins and phospholipids in human cells. It includes the simultaneous measurements of 11 phosphorylated proteins and phospholipids derived from thousands of individual primary immune system cells, subjected to both general and specific molecular interventions."}, {"id": "hku-is", "name": "HKU-IS", "description": "HKU-IS is a visual saliency prediction dataset which contains 4447 challenging images, most of which have either low contrast or multiple salient objects."}, {"id": "sutd-trafficqa", "name": "SUTD-TrafficQA", "description": "SUTD-TrafficQA (Singapore University of Technology and Design - Traffic Question Answering) is a dataset which takes the form of video QA based on 10,080 in-the-wild videos and annotated 62,535 QA pairs, for benchmarking the cognitive capability of causal inference and event understanding models in complex traffic scenarios. Specifically, the dataset proposes 6 challenging reasoning tasks corresponding to various traffic scenarios, so as to evaluate the reasoning capability over different kinds of complex yet practical traffic events."}, {"id": "dark-machines-anomaly-score", "name": "Dark Machines Anomaly Score", "description": "This dataset is the outcome of a data challenge conducted as part of the Dark Machines Initiative and the Les Houches 2019 workshop on Physics at TeV colliders. The challenge aims at detecting signals of new physics at the LHC using unsupervised machine learning algorithms. "}, {"id": "multext-east", "name": "MULTEXT-East", "description": "The MULTEXT-East resources are a multilingual dataset for language engineering research and development. It consists of the (1) MULTEXT-East morphosyntactic specifications, defining categories (parts-of-speech), their morphosyntactic features (attributes and values), and the compact MSD tagset representations; (2) morphosyntactic lexica, (3) the annotated parallel \"1984\" corpus; and (4) some comparable text and speech corpora. The specifications are available for the following macrolanguages, languages and language varieties: Albanian, Bulgarian, Chechen, Czech, Damaskini, English, Estonian, Hungarian, Macedonian, Persian, Polish, Resian, Romanian, Russian, Serbo-Croatian, Slovak, Slovene, Torlak, and Ukrainian, while the other resources are available for a subset of these languages."}, {"id": "meva-multiview-extended-video-with-activities", "name": "MEVA (Multiview Extended Video with Activities)", "description": "Large-scale dataset for human activity recognition. Existing security datasets either focus on activity counts by aggregating public video disseminated due to its content, which typically excludes same-scene background video, or they achieve persistence by observing public areas and thus cannot control for activity content. The dataset is over 9300 hours of untrimmed, continuous video, scripted to include diverse, simultaneous activities, along with spontaneous background activity."}, {"id": "vmrd-visual-manipulation-relationship-dataset", "name": "VMRD (Visual Manipulation Relationship Dataset)", "description": "VMRD is a multi-object grasp dataset. It has been collected and labeled using hundreds of objects coming from 31 categories. There are totally 5,185 images including 17,688 object instances and 51,530 manipulation relationships."}, {"id": "pull-request-descriptions", "name": "Pull Request Descriptions", "description": "This is a dataset of over 333K Pull Requests, used for automatic pull request description generation."}, {"id": "o-haze", "name": "O-HAZE", "description": "The O-Haze dataset contains 35 hazy images (size 2833\u00d74657 pixels) for training. It has 5 hazy images for validation along with their corresponding ground truth images."}, {"id": "cufs-cuhk-face-sketch-database", "name": "CUFS (CUHK Face Sketch Database)", "description": "CUHK Face Sketch database (CUFS) is for research on face sketch synthesis and face sketch recognition. It includes 188 faces from the Chinese University of Hong Kong (CUHK) student database, 123 faces from the AR database [1], and 295 faces from the XM2VTS database [2]. There are 606 faces in total. For each face, there is a sketch drawn by an artist based on a photo taken in a frontal pose, under normal lighting condition, and with a neutral expression."}, {"id": "autofr-dataset", "name": "AutoFR Dataset", "description": "AutoFR Dataset is broken down by each site that we crawl within a zip file. It contains multiple different experiments that we conducted in our paper. The overall dataset contains 1042 sites that we crawled where we detected ads within the Top-5K."}, {"id": "animals-10", "name": "Animals-10", "description": "It contains about 28K medium quality animal images belonging to 10 categories: dog, cat, horse, spyder, butterfly, chicken, sheep, cow, squirrel, and elephant."}, {"id": "conll-2003", "name": "CoNLL-2003", "description": "CoNLL-2003 is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition. The data consists of eight files covering two languages: English and German. For each of the languages there is a training file, a development file, a test file and a large file with unannotated data."}, {"id": "ave-audio-visual-event-localization", "name": "AVE (Audio-Visual Event Localization)", "description": "To investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization."}, {"id": "xl-wic", "name": "XL-WiC", "description": "A large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer."}, {"id": "opv2v", "name": "OPV2V", "description": "OPV2V is a large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles."}, {"id": "wikisem500", "name": "WikiSem500", "description": "The WikiSem500 dataset contains around 500 per-language cluster groups for English, Spanish, German, Chinese, and Japanese (a total of 13,314 test cases)."}, {"id": "faust-partial", "name": "FAUST-partial", "description": "FAUST-partial is a 3D registration benchmark dataset created to address the lack of data variability in the existing 3D registration benchmarks such as: 3DMatch, ETH, KITTI."}, {"id": "botnet", "name": "BotNet", "description": "The BotNet dataset is a set of topological botnet detection datasets forgraph neural networks."}, {"id": "wikihowqa", "name": "WikiHowQA", "description": "WikiHowQA is a Community-based Question Answering dataset, which can be used for both answer selection and abstractive summarization tasks. It contains 76,687 questions in the train set, 8,000 in the development set and 22,354 in the test set."}, {"id": "mit-adobe-fivek", "name": "MIT-Adobe FiveK", "description": "The MIT-Adobe FiveK dataset consists of 5,000 photographs taken with SLR cameras by a set of different photographers. They are all in RAW format; that is, all the information recorded by the camera sensor is preserved. We made sure that these photographs cover a broad range of scenes, subjects, and lighting conditions. We then hired five photography students in an art school to adjust the tone of the photos. Each of them retouched all the 5,000 photos using a software dedicated to photo adjustment (Adobe Lightroom) on which they were extensively trained. We asked the retouchers to achieve visually pleasing renditions, akin to a postcard. The retouchers were compensated for their work."}, {"id": "coverage-copy-move-forgery-database-with-similar-but-genuine-objects", "name": "COVERAGE (Copy-Move Forgery Database with Similar but Genuine Objects)", "description": "COVERAGE contains copymove forged (CMFD) images and their originals with similar but genuine objects (SGOs). COVERAGE is designed to highlight and address tamper detection ambiguity of popular methods, caused by self-similarity within natural images. In COVERAGE, forged\u2013original pairs are annotated with (i) the duplicated and forged region masks, and (ii) the tampering factor/similarity metric. For benchmarking, forgery quality is evaluated using (i) computer vision-based methods, and (ii) human detection performance."}, {"id": "sixray", "name": "SIXray", "description": "The SIXray dataset is constructed by the Pattern Recognition and Intelligent System Development Laboratory, University of Chinese Academy of Sciences. It contains 1,059,231 X-ray images which are collected from some several subway stations. There are six common categories of prohibited items, namely, gun, knife, wrench, pliers, scissors and hammer. It has three subsets called SIXray10, SIXray100 and SIXray1000, There are image-level annotations provided by human security inspectors for the whole dataset. In addition the images in the test set are annotated with a bounding-box for each prohibited item to evaluate the performance of object localization."}, {"id": "openml-cc18", "name": "OpenML-CC18", "description": "We advocate the use of curated, comprehensive benchmark suites of machine learning datasets, backed by standardized OpenML-based interfaces and complementary software toolkits written in Python, Java and R. We demonstrate how to easily execute comprehensive benchmarking studies using standardized OpenML-based benchmarking suites and complementary software toolkits written in Python, Java and R. Major distinguishing features of OpenML benchmark suites are (i) ease of use through standardized data formats, APIs, and existing client libraries; (ii) machine-readable meta-information regarding the contents of the suite; and (iii) online sharing of results, enabling large scale comparisons. As a first such suite, we propose the OpenML-CC18, a machine learning benchmark suite of 72 classification datasets carefully curated from the thousands of datasets on OpenML."}, {"id": "occludedpascal3d", "name": "OccludedPASCAL3D+", "description": "The OccludedPASCAL3D+ is a dataset is designed to evaluate the robustness to occlusion for a number of computer vision tasks, such as object detection, keypoint detection and pose estimation. In the OccludedPASCAL3D+ dataset, we simulate partial occlusion by superimposing objects cropped from the MS-COCO dataset on top of objects from the PASCAL3D+ dataset. We only use ImageNet subset in PASCAL3D+, which has 10812 testing images."}, {"id": "uavid", "name": "UAVid", "description": "UAVid is a high-resolution UAV semantic segmentation dataset as a complement, which brings new challenges, including large scale variation, moving object recognition and temporal consistency preservation. The UAV dataset consists of 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes for the semantic labeling task. "}, {"id": "argoverse-2", "name": "Argoverse 2", "description": "Argoverse 2 (AV2) is a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions be- tween the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for \u201cscored actors\" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry \u2014 sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license."}, {"id": "synbols", "name": "Synbols", "description": "Synbols is a dataset generator designed for probing the behavior of learning algorithms. By defining the distribution over latent factors one can craft a dataset specifically tailored to answer specific questions about a given algorithm."}, {"id": "implicit-hate", "name": "Implicit Hate", "description": "The Implicit Hate corpus is a dataset for hate speech detection with fine-grained labels for each message and its implication. This dataset contains 22,056 tweets from the most prominent extremist groups in the United States; 6,346 of these tweets contain implicit hate speech."}, {"id": "florence-4d", "name": "Florence 4D", "description": "Florence 4D is a dataset that consists of dynamic sequences of 3D face models, where a combination of synthetic and real identities exhibit an unprecedented variety of 4D facial expressions, with variations that include the classical neutral-apex transition, but generalize to expression-to-expression. It is designed for research in 4D facial analysis, with a particular focus on dynamic expressions."}, {"id": "es-imagenet", "name": "ES-ImageNet", "description": "ES-ImageNet is a large-scale event-stream dataset for SNNs and neuromorphic vision. It consists of about 1.3 M samples converted from ILSVRC2012 in 1000 different categories. ES-ImageNet is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software"}, {"id": "dureader", "name": "DuReader", "description": "DuReader is a large-scale open-domain Chinese machine reading comprehension dataset. The dataset consists of 200K questions, 420K answers and 1M documents. The questions and documents are based on Baidu Search and Baidu Zhidao. The answers are manually generated. The dataset additionally provides question type annotations \u2013 each question was manually annotated as either Entity, Description or YesNo and one of Fact or Opinion."}, {"id": "tssb-time-series-segmentation-benchmark", "name": "TSSB (Time Series Segmentation Benchmark)", "description": "The time series segmentation benchmark (TSSB) currently contains 75 annotated time series (TS) with 1-9 segments. Each TS is constructed from one of the UEA & UCR time series classification datasets. We group TS by label and concatenate them to create segments with distinctive temporal patterns and statistical properties. We annotate the offsets at which we concatenated the segments as change points (CPs). Addtionally, we apply resampling to control the dataset resolution and add approximate, hand-selected window sizes that are able to capture temporal patterns."}, {"id": "isarcasmeval", "name": "iSarcasmEval", "description": "iSarcasmEval is the first shared task to target intended sarcasm detection: the data for this task was provided and labelled by the authors of the texts themselves. Such an approach minimises the downfalls of other methods to collect sarcasm data, which rely on distant supervision or third-party annotations. The shared task contains two languages, English and Arabic, and three subtasks: sarcasm detection, sarcasm category classification, and pairwise sarcasm identification given a sarcastic sentence and its non-sarcastic rephrase. The task received submissions from 60 different teams, with the sarcasm detection task being the most popular. Most of the participating teams utilised pre-trained language models. In this paper, we provide an overview of the task, data, and participating teams."}, {"id": "funsd-form-understanding-in-noisy-scanned-documents", "name": "FUNSD (Form Understanding in Noisy Scanned Documents)", "description": "Form Understanding in Noisy Scanned Documents (FUNSD) comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking."}, {"id": "common-crawl", "name": "Common Crawl", "description": "The Common Crawl corpus contains petabytes of data collected over 12 years of web crawling. The corpus contains raw web page data, metadata extracts and text extracts. Common Crawl data is stored on Amazon Web Services\u2019 Public Data Sets and on multiple academic cloud platforms across the world."}, {"id": "scidocs", "name": "SciDocs", "description": "SciDocs evaluation framework consists of a suite of evaluation tasks designed for document-level tasks."}, {"id": "florence3d", "name": "Florence3D", "description": "The dataset collected at the University of Florence during 2012, has been captured using a Kinect camera. It includes 9 activities: wave, drink from a bottle, answer phone,clap, tight lace, sit down, stand up, read watch, bow. During acquisition, 10 subjects were asked to perform the above actions for 2/3 times. This resulted in a total of 215 activity samples."}, {"id": "ksof-the-kassel-state-of-fluency-dataset-a-therapy-centered-dataset-of-stuttering", "name": "KSoF (The Kassel State of Fluency Dataset \u2013 A Therapy Centered Dataset of Stuttering)", "description": "Stuttering is a complex speech disorder that negatively affects an individual\u2019s ability to communicate effectively. Persons who stutter (PWS) often suffer considerably under the condition and seek help through therapy. Fluency shaping is a therapy approach where PWSs learn to modify their speech to help them to overcome their stutter. Mastering such speech techniques takes time and practice, even after therapy. Shortly after therapy, success is evaluated highly, but relapse rates are high. To be able to monitor speech behavior over a long time, the ability to detect stuttering events and modifications in speech could help PWSs and speech pathologists to track the level of fluency. Monitoring could create the ability to intervene early by detecting lapses in fluency. To the best of our knowledge, no public dataset is available that contains speech from people who underwent stuttering therapy that changed the style of speaking. This work introduces the Kassel State of Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The clips were labeled with six stuttering-related event types: blocks, prolongations, sound repetitions, word repetitions, interjections, and \u2013 specific to therapy \u2013 speech modifications. The audio was recorded during therapy sessions at the Institut der Kasseler Stottertherapie."}, {"id": "scenenn", "name": "SceneNN", "description": "SceneNN is an RGB-D scene dataset consisting of more than 100 indoor scenes. The scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc., from University of Massachusetts Boston and Singapore University of Technology and Design. All scenes are reconstructed into triangle meshes and have per-vertex and per-pixel annotation. The dataset is additionally enriched with fine-grained information such as axis-aligned bounding boxes, oriented bounding boxes, and object poses."}, {"id": "how2r", "name": "How2R", "description": "Amazon Mechanical Turk (AMT) is used to collect annotations on HowTo100M videos. 30k 60-second clips are randomly sampled from 9,421 videos and present each clip to the turkers, who are asked to select a video segment containing a single, self-contained scene. After this segment selection step, another group of workers are asked to write descriptions for each displayed segment. Narrations are not provided to the workers to ensure that their written queries are based on visual content only. These final video segments are 10-20 seconds long on average, and the length of queries ranges from 8 to 20 words. From this process, 51,390 queries are collected for 24k 60-second clips from 9,371 videos in HowTo100M, on average 2-3 queries per clip. The video clips and its associated queries are split into 80% train, 10% val and 10% test."}, {"id": "asd-annotated-semantic-dataset", "name": "ASD (Annotated Semantic Dataset)", "description": "The Annotated Semantic Dataset is composed of $11$ videos, divided in $3$ activity categories: Biking; Driving and Walking, according to their amount of semantic information. The classes are: $0p$, which represents the videos with approximately no semantic information; $25p$, for the videos containing relevant semantic information in \u223c$25%$ of its frames ; the same ideia for the classes $50p$ and $75p$, The videos were record using a GoPro Hero 3 camera mounted in a helmet for the Biking and Walking videos and attached to a head strap for the Driving videos."}, {"id": "bugswarm", "name": "BugSwarm", "description": "BugSwarm is a dataset of reproducible faults and fixes to perform experimental evaluation of approaches to software quality. The BugSwarm toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers."}, {"id": "sun397", "name": "SUN397", "description": "The Scene UNderstanding (SUN) database contains 899 categories and 130,519 images. There are 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition."}, {"id": "salicon-salicency-in-context", "name": "SALICON (Salicency in Context)", "description": "The SALIency in CONtext (SALICON) dataset contains 10,000 training images, 5,000 validation images and 5,000 test images for saliency prediction. This dataset has been created by annotating saliency in images from MS COCO. The ground-truth saliency annotations include fixations generated from mouse trajectories. To improve the data quality, isolated fixations with low local density have been excluded. The training and validation sets, provided with ground truth, contain the following data fields: image, resolution and gaze. The testing data contains only the image and resolution fields."}, {"id": "wizard-of-wikipedia", "name": "Wizard of Wikipedia", "description": "Wizard of Wikipedia is a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. It is used to train and evaluate dialogue systems for knowledgeable open dialogue with clear grounding"}, {"id": "send-stanford-emotional-narratives-dataset", "name": "SEND (Stanford Emotional Narratives Dataset)", "description": "SEND (Stanford Emotional Narratives Dataset) is a set of rich, multimodal videos of self-paced, unscripted emotional narratives, annotated for emotional valence over time. The complex narratives and naturalistic expressions in this dataset provide a challenging test for contemporary time-series emotion recognition models."}, {"id": "jaad-joint-attention-in-autonomous-driving", "name": "JAAD (Joint Attention in Autonomous Driving)", "description": "JAAD is a dataset for studying joint attention in the context of autonomous driving. The focus is on pedestrian and driver behaviors at the point of crossing and factors that influence them. To this end, JAAD dataset provides a richly annotated collection of 346 short video clips (5-10 sec long) extracted from over 240 hours of driving footage. These videos filmed in several locations in North America and Eastern Europe represent scenes typical for everyday urban driving in various weather conditions."}, {"id": "tat-qa", "name": "TAT-QA", "description": "TAT-QA (Tabular And Textual dataset for Question Answering) is a large-scale QA dataset, aiming to stimulate progress of QA research over more complex and realistic tabular and textual data, especially those requiring numerical reasoning."}, {"id": "youtubevis", "name": "YouTubeVIS", "description": "YouTubeVIS is a new dataset tailored for tasks like simultaneous detection, segmentation and tracking of object instances in videos and is collected based on the current largest video object segmentation dataset YouTubeVOS."}, {"id": "auto-kws", "name": "Auto-KWS", "description": "Auto-KWS is a dataset for customized keyword spotting, the task of detecting spoken keywords. The dataset closely resembles real world scenarios, as each recorder is assigned with an unique wake-up word and can choose their recording environment and familiar dialect freely."}, {"id": "star", "name": "STAR", "description": "A schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog."}, {"id": "aave-sae-paired-dataset", "name": "AAVE/SAE Paired Dataset", "description": "AAVE/SAE Paired Dataset contains 2019 intent-equivalent AAVE/SAE pairs. The AAVE (African-American Vernacular English) samples are sampled from Blodgett et. al. (2016)'s TwitterAAE, with their corresponding SAE (Standard American English) samples annotated by Amazon MTurk."}, {"id": "quasar-t-question-answering-by-search-and-reading-trivia", "name": "QUASAR-T (QUestion Answering by Search And Reading \u2013 Trivia)", "description": "QUASAR-T is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 43,013 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. The answers to these questions are free-form spans of text, though most are noun phrases."}, {"id": "tau-nigens-spatial-sound-events-2021", "name": "TAU-NIGENS Spatial Sound Events 2021", "description": "The TAU-NIGENS Spatial Sound Events 2021 dataset contains multiple spatial sound-scene recordings, consisting of sound events of distinct categories integrated into a variety of acoustical spaces, and from multiple source directions and distances as seen from the recording position. The spatialization of all sound events is based on filtering through real spatial room impulse responses (RIRs), captured in multiple rooms of various shapes, sizes, and acoustical absorption properties. Furthermore, each scene recording is delivered in two spatial recording formats, a microphone array one (MIC), and first-order Ambisonics one (FOA). The sound events are spatialized as either stationary sound sources in the room, or moving sound sources, in which case time-variant RIRs are used. Each sound event in the sound scene is associated with a single direction-of-arrival (DoA) if static, a trajectory DoAs if moving, and a temporal onset and offset time. The isolated sound event recordings used for the synthesis of the sound scenes are obtained from the NIGENS general sound events database. These recordings serve as the development dataset for the DCASE 2021 Sound Event Localization and Detection Task of the DCASE 2021 Challenge."}, {"id": "orbit", "name": "ORBIT", "description": "ORBIT is a real-world few-shot dataset and benchmark grounded in a real-world application of teachable object recognizers for people who are blind/low vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones, and the benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions."}, {"id": "ci-mnist-correlated-and-imbalanced-mnist", "name": "CI-MNIST (Correlated and Imbalanced MNIST)", "description": "CI-MNIST (Correlated and Imbalanced MNIST) is a variant of MNIST dataset with introduced different types of correlations between attributes, dataset features, and an artificial eligibility criterion. For an input image $x$, the label $y \\in \\{1, 0\\}$ indicates eligibility or ineligibility, respectively, given that $x$ is even or odd. The dataset defines the background colors as the protected or sensitive attribute $s \\in \\{0, 1\\}$, where blue denotes the unprivileged group and red denotes the privileged group. The dataset was designed in order to evaluate bias-mitigation approaches in challenging setups and be capable of controlling different dataset configurations."}, {"id": "wisdom-warehouse-instance-segmentation-dataset-for-object-manipulation", "name": "WISDOM (Warehouse Instance Segmentation Dataset for Object Manipulation)", "description": "Synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. "}, {"id": "drcd-delta-reading-comprehension-dataset", "name": "DRCD (Delta Reading Comprehension Dataset)", "description": "Delta Reading Comprehension Dataset (DRCD) is an open domain traditional Chinese machine reading comprehension (MRC) dataset. This dataset aimed to be a standard Chinese machine reading comprehension dataset, which can be a source dataset in transfer learning. The dataset contains 10,014 paragraphs from 2,108 Wikipedia articles and 30,000+ questions generated by annotators."}, {"id": "nh-haze", "name": "NH-HAZE", "description": "NN-HAZE is an image dehazing dataset. Since in many real cases haze is not uniformly distributed NH-HAZE, a non-homogeneous realistic dataset with pairs of real hazy and corresponding haze-free images. This is the first non-homogeneous image dehazing dataset and contains 55 outdoor scenes. The non-homogeneous haze has been introduced in the scene using a professional haze generator that imitates the real conditions of hazy scenes."}, {"id": "indonlu-benchmark", "name": "IndoNLU Benchmark", "description": "The IndoNLU benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems for Bahasa Indonesia. It is a joint venture from many Indonesia NLP enthusiasts from different institutions such as Gojek, Institut Teknologi Bandung, HKUST, Universitas Multimedia Nusantara, Prosa.ai, and Universitas Indonesia."}, {"id": "diamante", "name": "Diamante", "description": "Diamante is a novel and efficient framework consisting of a data collection strategy and a learning method to boost the performance of pre-trained dialogue models. Two kinds of human feedback are collected and leveraged in Diamante, including explicit demonstration and implicit preference. The Diamante dataset is publicly available at the LUGE platform."}, {"id": "sports-1m", "name": "Sports-1M", "description": "The Sports-1M dataset consists of over a million videos from YouTube. The videos in the dataset can be obtained through the YouTube URL specified by the authors. Approximately 7% (as of 2016) of the videos have been removed by the YouTube uploaders since the dataset was compiled. However, there are still over a million videos in the dataset with 487 sports-related categories with 1,000 to 3,000 videos per category. The videos are automatically labelled with 487 sports classes using the YouTube Topics API by analyzing the text metadata associated with the videos (e.g. tags, descriptions). Approximately 5% of the videos are annotated with more than one class."}, {"id": "nell-never-ending-language-learning", "name": "NELL (Never Ending Language Learning)", "description": "NELL is a dataset built from the Web via an intelligent agent called Never-Ending Language Learner. This agent attempts to learn over time to read the web. NELL has accumulated over 50 million candidate beliefs by reading the web, and it is considering these at different levels of confidence. NELL has high confidence in 2,810,379 of these beliefs."}, {"id": "3d-future", "name": "3D-FUTURE", "description": "3D-FUTURE (3D FUrniture shape with TextURE) is a 3D dataset that contains 20,240 photo-realistic synthetic images captured in 5,000 diverse scenes, and 9,992 involved unique industrial 3D CAD shapes of furniture with high-resolution informative textures developed by professional designers."}, {"id": "kvasir-sessile-dataset-sessile-polyps-from-kvasir-seg", "name": "Kvasir-Sessile dataset (Sessile polyps from Kvasir-SEG)", "description": "The Kvasir-SEG dataset includes 196 polyps smaller than 10 mm classified as Paris class 1 sessile or Paris class IIa. We have selected it with the help of expert gastroenterologists. We have released this dataset separately as a subset of Kvasir-SEG. We call this subset Kvasir-Sessile."}, {"id": "off-hard-sequential", "name": "Off_Hard_sequential", "description": "SMAC+ offensive hard scenario with sequential episodic buffer"}, {"id": "serial-speakers", "name": "Serial Speakers", "description": "An annotated dataset of 161 episodes from three popular American TV serials: Breaking Bad, Game of Thrones and House of Cards. Serial Speakers is suitable both for investigating multimedia retrieval in realistic use case scenarios, and for addressing lower level speech related tasks in especially challenging conditions."}, {"id": "subedits", "name": "SubEdits", "description": "SubEdits is a human-annnoated post-editing dataset of neural machine translation outputs, compiled from in-house NMT outputs and human post-edits of subtitles form Rakuten Viki. It is collected from English-German annotations and contains 160k triplets."}, {"id": "carl-database", "name": "CARL Database", "description": "Visible and thermal images have been acquired using a thermographic camera TESTO 880-3, equipped with an uncooled detector with a spectral sensitivity range from 8 to 14 \u03bcm and provided with a germanium optical lens, and an approximate cost of 8.000 EUR. For the NIR a customized Logitech Quickcam messenger E2500 has been used, provided with a Silicon based CMOS image sensor with a sensibility to the overall visible spectrum and the half part of the NIR (until 1.000 nm approximately) with a cost of approx. 30 EUR. We have replaced the default optical filter of this camera by a couple of Kodak daylight filters for IR interspersed between optical and sensor. They both have similar spectrum responses and are coded as wratten filter 87 and 87C, respectively. In addition, we have used a special purpose printed circuit board (PCB) with a set of 16 infrared leds (IRED) with a range of emission from 820 to 1.000 nm in order to provide the required illumination."}, {"id": "webquestions", "name": "WebQuestions", "description": "The WebQuestions dataset is a question answering dataset using Freebase as the knowledge base and contains 6,642 question-answer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. The original split uses 3,778 examples for training and 2,032 for testing. All answers are defined as Freebase entities."}, {"id": "creative-flow-dataset", "name": "Creative Flow+ Dataset", "description": "Includes 3000 animated sequences rendered using styles randomly selected from 40 textured line styles and 38 shading styles, spanning the range between flat cartoon fill and wildly sketchy shading. The dataset includes 124K+ train set frames and 10K test set frames rendered at 1500x1500 resolution, far surpassing the largest available optical flow datasets in size."}, {"id": "casia-webface", "name": "CASIA-WebFace", "description": "The CASIA-WebFace dataset is used for face verification and face identification tasks. The dataset contains 494,414 face images of 10,575 real identities collected from the web."}, {"id": "wikiclir", "name": "WikiCLIR", "description": "WikiCLIR is a large-scale (German-English) retrieval data set for Cross-Language Information Retrieval (CLIR). It contains a total of 245,294 German single-sentence queries with 3,200,393 automatically extracted relevance judgments for 1,226,741 English Wikipedia articles as documents. Queries are well-formed natural language sentences that allow large-scale training of (translation-based) ranking models."}, {"id": "mixatis", "name": "MixATIS", "description": "Dataset is constructed from single intent dataset ATIS. "}, {"id": "eyeq", "name": "EyeQ", "description": "Dataset with 28,792 retinal images from the EyePACS dataset, based on a three-level quality grading system (i.e., Good',Usable' and `Reject') for evaluating RIQA methods. "}, {"id": "o3-odd-one-out-dataset", "name": "O3 (Odd-One-Out Dataset)", "description": "A set of realistic odd-one-out stimuli gathered \"in the wild\". Each image in the Odd-One-Out (O3) dataset depicts a scene with multiple objects similar to each other in appearance (distractors) and a singleton (target) distinct in one or more feature dimensions (e.g. color, shape, size). All images are resized so that the larger dimension is 1024px. Targets represent approx. 400 common object types such as flowers, sweets, chicken eggs, leaves, tiles and birds. Pixelwise masks are provided for targets and distractors. Annotations are generated using CVAT."}, {"id": "brixia-brixia-covid-19", "name": "BrixIA (BrixIA Covid-19)", "description": "BrixIA Covid-19 is a large dataset of CXR images corresponding to the entire amount of images taken for both triage and patient monitoring in sub-intensive and intensive care units during one month (between March 4th and April 4th 2020) of pandemic peak at the ASST Spedali Civili di Brescia, and contains all the variability originating from a real clinical scenario. It includes 4,707 CXR images of COVID-19 subjects, acquired with both CR and DX modalities, in AP or PA projection, and retrieved from the facility RIS-PACS system."}, {"id": "framenet", "name": "FrameNet", "description": "FrameNet is a linguistic knowledge graph containing information about lexical and predicate argument semantics of the English language. FrameNet contains two distinct entity classes: frames and lexical units, where a frame is a meaning and a lexical unit is a single meaning for a word."}, {"id": "shapenet", "name": "ShapeNet", "description": "ShapeNet is a large scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University and the Toyota Technological Institute at Chicago, USA. The repository contains over 300M models with 220,000 classified into 3,135 classes arranged using WordNet hypernym-hyponym relationships. ShapeNet Parts subset contains 31,693 meshes categorised into 16 common object classes (i.e. table, chair, plane etc.). Each shapes ground truth contains 2-5 parts (with a total of 50 part classes)."}, {"id": "digits-optical-recognition-of-handwritten-digits", "name": "Digits (Optical Recognition of Handwritten Digits)", "description": "The DIGITS dataset consists of 1797 8\u00d78 grayscale images (1439 for training and 360 for testing) of handwritten digits."}, {"id": "objectron", "name": "Objectron", "description": "The Objectron dataset is a collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. In each video, the camera moves around the object, capturing it from different angles. The data also contain manually annotated 3D bounding boxes for each object, which describe the object\u2019s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images in the following categories: bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes. To ensure geo-diversity, the dataset is collected from 10 countries across five continents."}, {"id": "duc-2004", "name": "DUC 2004", "description": "The DUC2004 dataset is a dataset for document summarization. Is designed and used for testing only. It consists of 500 news articles, each paired with four human written summaries. Specifically it consists of 50 clusters of Text REtrieval Conference (TREC) documents, from the following collections: AP newswire, 1998-2000; New York Times newswire, 1998-2000; Xinhua News Agency (English version), 1996-2000. Each cluster contained on average 10 documents."}, {"id": "facebook-pages", "name": "Facebook Pages", "description": "We collected data about Facebook pages (November 2017). These datasets represent blue verified Facebook page networks of different categories. Nodes represent the pages and edges are mutual likes among them. We reindexed the nodes in order to achieve a certain level of anonimity. The csv files contain the edges -- nodes are indexed from 0. We included 8 different distinct types of pages. These are listed below. For each dataset we listed the number of nodes an edges."}, {"id": "ur-funny", "name": "UR-FUNNY", "description": "For understanding multimodal language used in expressing humor."}, {"id": "visual7w", "name": "Visual7W", "description": "Visual7W is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories."}, {"id": "fpl-first-person-locomotion", "name": "FPL (First-Person Locomotion)", "description": "Supports new task that predicts future locations of people observed in first-person videos."}, {"id": "wnut-2020-wnut-2020-task-1-overview-extracting-entities-and-relations-from-wet-lab-protocols", "name": "WNUT 2020 (WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols)", "description": "The training and development dataset for our task was taken from previous work on wet lab corpus (Kulkarni et al., 2018) that consists of from the 623 protocols. We excluded the eight duplicate protocols from this dataset and then re-annotated the 615 unique protocols in BRAT (Stenetorp et al., 2012)."}, {"id": "atari-grand-challenge", "name": "Atari Grand Challenge", "description": "The Atari Grand Challenge dataset is a large dataset of human Atari 2600 replays. It consists of replays for 5 different games: * Space Invaders (445 episodes, 2M frames) * Q*bert (659 episodes, 1.6M frames) * Ms.Pacman (384 episodes, 1.7M frames) * Video Pinball (211 episodes, 1.5M frames) * Montezuma\u2019s revenge (668 episodes, 2.7M frames)"}, {"id": "aristo-v4-aristo-tuple-kb-version-4", "name": "Aristo-v4 (Aristo Tuple KB Version 4)", "description": "The Aristo Tuple KB contains a collection of high-precision, domain-targeted (subject,relation,object) tuples extracted from text using a high-precision extraction pipeline, and guided by domain vocabulary constraints.  The dataset was introduced by the paper Domain-Targeted, High Precision Knowledge Extraction."}, {"id": "ms-fimu-mobility-scenario-fimu", "name": "MS-FIMU (Mobility Scenario FIMU)", "description": "Open Dataset: Mobility Scenario FIMU"}, {"id": "metalwoz-meta-learning-wizard-of-oz", "name": "MetaLWOz (Meta-Learning Wizard-of-Oz)", "description": "Collected by leveraging background knowledge from a larger, more highly represented dialogue source."}, {"id": "clothing1m", "name": "Clothing1M", "description": "Clothing1M contains 1M clothing images in 14 classes. It is a dataset with noisy labels, since the data is collected from several online shopping websites and include many mislabelled samples. This dataset also contains 50k, 14k, and 10k images with clean labels for training, validation, and testing, respectively."}, {"id": "uvo-unidentified-video-objects-a-benchmark-for-dense-open-world-segmentation", "name": "UVO (Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation)", "description": "UVO is a new benchmark for open-world class-agnostic object segmentation in videos. Besides shifting the problem focus to the open-world setup, UVO is significantly larger, providing approximately 8 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VOS and YouTube-VIS. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. Some highlights of the dataset include:"}, {"id": "brats-2015", "name": "BraTS 2015", "description": "The BraTS 2015 dataset is a dataset for brain tumor image segmentation. It consists of 220 high grade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs. The four MRI modalities are T1, T1c, T2, and T2FLAIR. Segmented \u201cground truth\u201d is provide about four intra-tumoral classes, viz. edema, enhancing tumor, non-enhancing tumor, and necrosis."}, {"id": "poc-points-of-correspondence", "name": "PoC (Points of correspondence)", "description": "A dataset containing the documents, source and fusion sentences, and human annotations of points of correspondence between sentences. The dataset bridges the gap between coreference resolution and summarization."}, {"id": "uestc-rgb-d-uestc-rgb-d-varying-view-action-database", "name": "UESTC RGB-D (UESTC RGB-D Varying-view action database)", "description": "UESTC RGB-D Varying-view action database contains 40 categories of aerobic exercise. We utilized 2 Kinect V2 cameras in 8 fixed directions and 1 round direction to capture these actions with the data modalities of RGB video, 3D skeleton sequences and depth map sequences."}, {"id": "semantickitti-c", "name": "SemanticKITTI-C", "description": "SemanticKITTI-C is an evaluation benchmark heading toward robust and reliable 3D semantic segmentation in autonomous driving. With it, we probe the robustness of 3D segmentors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:"}, {"id": "wec-eng", "name": "WEC-Eng", "description": "WEC-eng is a cross-document event coreference resolution dataset extracted from English Wikipedia. Coreference links are not restricted within predefined topics. The training set includes 40,529 mentions distributed into 7,042 coreference clusters."}, {"id": "word2word", "name": "word2word", "description": "word2word contains easy-to-use word translations for 3,564 language pairs."}, {"id": "wikisql", "name": "WikiSQL", "description": "WikiSQL consists of a corpus of 87,726 hand-annotated SQL query and natural language question pairs. These SQL queries are further split into training (61,297 examples), development (9,145 examples) and test sets (17,284 examples). It can be used for natural language inference tasks related to relational databases."}, {"id": "german-credit-dataset", "name": "German Credit Dataset", "description": "Two datasets are provided. the original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file \"german.data\". "}, {"id": "multidoc2dial-multidoc2dial-modeling-dialogues-grounded-in-multiple-documents", "name": "MultiDoc2Dial (MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents)", "description": "MultiDoc2Dial is a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as a machine reading comprehension task based on a single given document or passage. We aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents."}, {"id": "instafake", "name": "InstaFake", "description": "Includes two datasets published for the detection of fake and automated accounts. "}, {"id": "starss22-sony-tau-realistic-spatial-soundscapes-2022", "name": "STARSS22 (Sony-TAu Realistic Spatial Soundscapes 2022)", "description": "The Sony-TAu Realistic Spatial Soundscapes 2022(STARSS22) dataset consists of recordings of real scenes captured with high channel-count spherical microphone array (SMA). The recordings are conducted from two different teams at two different sites, Tampere University in Tammere, Finland, and Sony facilities in Tokyo, Japan. Recordings at both sites share the same capturing and annotation process, and a similar organization. They are organized in sessions, corresponding to distinct rooms, human participants, and sound making props with a few exceptions."}, {"id": "opendd", "name": "openDD", "description": "Annotated using images taken by a drone in 501 separate flights, totalling in over 62 hours of trajectory data. As of today, openDD is by far the largest publicly available trajectory dataset recorded from a drone perspective, while comparable datasets span 17 hours at most."}, {"id": "concadia", "name": "Concadia", "description": "Concadia is a publicly available Wikipedia-based corpus, which consists of 96,918 images with corresponding English-language descriptions, captions, and surrounding context."}, {"id": "segtrack-v2", "name": "SegTrack-v2", "description": "SegTrack v2 is a video segmentation dataset with full pixel-level annotations on multiple objects at each frame within each video."}, {"id": "trecvid-avs17-iacc-3", "name": "TRECVID-AVS17 (IACC.3)", "description": "Internet Archive videos (IACC.3) under Creative Commons licenses. The test video collection for TRECVID-AVS2016-TRECVID-AVS2018 contains 335,944 web video clips (600hr)."}, {"id": "360-sod", "name": "360-SOD", "description": "360-SOD contains 500 high-resolution equirectangular images."}, {"id": "action-camera-parking", "name": "Action-Camera Parking", "description": "The Action-Camera Parking Dataset contains 293 images captured at a roughly 10-meter height using a GoPro Hero 6 camera. It can be used for training machine learning models that perform image-based parking space occupancy classification."}, {"id": "nyt11-hrl", "name": "NYT11-HRL", "description": "Preprocessed version of NYT11."}, {"id": "androzoo", "name": "AndroZoo", "description": "AndroZoo is a growing collection of Android apps collected from several sources, including the official Google Play app market and a growing collection of various metadata of those collected apps aiming at facilitating the Android-relevant research works. "}, {"id": "whoi-plankton", "name": "WHOI-Plankton", "description": "WHOI-Plankton is a collection of annotated plankton images. It contains > 3.5 million images of microscopic marine plankton, organized according to category labels provided by researchers at the Woods Hole Oceanographic Institution (WHOI). The images are currently placed into one of 103 categories."}, {"id": "xsum", "name": "XSum", "description": "The Extreme Summarization (XSum) dataset is a dataset for evaluation of abstractive single-document summarization systems. The goal is to create a short, one-sentence new summary answering the question \u201cWhat is the article about?\u201d. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts). The official random split contains 204,045 (90%), 11,332 (5%) and 11,334 (5) documents in training, validation and test sets, respectively."}, {"id": "prox", "name": "PROX", "description": "A dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. "}, {"id": "amazon-fashion", "name": "Amazon Fashion", "description": "This datasets is a subset of the Amazon reviews dataset which contain Fashion related products"}, {"id": "knowit-vqa", "name": "KnowIT VQA", "description": "KnowIT VQA is a video dataset with 24,282 human-generated question-answer pairs about The Big Bang Theory. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered."}, {"id": "gazeta", "name": "Gazeta", "description": "Gazeta is a dataset for automatic summarization of Russian news. The dataset consists of 63,435 text-summary pairs. To form training, validation, and test datasets, these pairs were sorted by time and the first 52,400 pairs are used as the training dataset, the proceeding 5,265 pairs as the validation dataset, and the remaining 5,770 pairs as the test dataset."}, {"id": "humanact12", "name": "HumanAct12", "description": "HumanAct12 is a new 3D human motion dataset adopted from the polar image and 3D pose dataset PHSPD, with proper temporal cropping and action annotating. Statistically, there are 1191 3D motion clips(and 90,099 poses in total) which are categorized into 12 action classes, and 34 fine-grained sub-classes. The action types includes daily actions such as walk, run, sit down, jump up, warm up, etc. Fine-grained action types contain more specific information like Warm up by bowing left side, Warm up by pressing left leg, etc. "}, {"id": "msu-based-msu-based-video-deblurring-dataset-and-benchmark", "name": "MSU BASED (MSU BASED Video Deblurring Dataset and Benchmark)", "description": "Qualitative dataset with real blurred videos, created by using beam-splitter setup in lab environment"}, {"id": "apricot", "name": "APRICOT", "description": "APRICOT is a collection of over 1,000 annotated photographs of printed adversarial patches in public locations. The patches target several object categories for three COCO-trained detection models, and the photos represent natural variation in position, distance, lighting conditions, and viewing angle. "}, {"id": "a-collection-of-x-ray-projections-of-131-pieces-of-modeling-clay-containing-stones-for-machine-learning-driven-object-detection", "name": "A collection of X-ray projections of 131 pieces of modeling clay containing stones for machine learning-driven object detection", "description": "This dataset contains a collection of 235800 X-ray projections of 131 pieces of modeling clay (Play-Doh) with various numbers of stones inserted. The dataset is intended as an extensive and easy-to-use training dataset for supervised machine learning driven object detection. The ground truth locations of the stones are included."}, {"id": "mcmd-multi-programming-language-commit-message-dataset", "name": "MCMD (Multi-programming-language Commit Message Dataset)", "description": "A large-scale dataset in multi-programming languages and with rich information."}, {"id": "vqa-ce-vqa-counterexamples", "name": "VQA-CE (VQA Counterexamples)", "description": "This dataset provides a new split of VQA v2 (similarly to VQA-CP v2), which is built of questions that are hard to answer for biased models."}, {"id": "msu-shot-boundary-detection-benchmark", "name": "MSU Shot Boundary Detection Benchmark", "description": "This is a dataset for a shot boundary detection task. The dataset contains 2 existing datasets and 19 manually marked up open source videos with a total length of more than 1200 minutes and 10000 scene transitions. The dataset includes different types of videos with different resolutions from 360\u00d7288 to 1920\u00d71080 in MP4 and MKV formats. Videos include samples in RGB scale or in grayscale with FPS from 23 to 60."}, {"id": "proofwriter", "name": "ProofWriter", "description": "The ProofWriter dataset contains many small rulebases of facts and rules, expressed in English. Each rulebase also has a set of questions (English statements) which can either be proven true or false using proofs of various depths, or the answer is \u201cUnknown\u201d (in open-world setting, OWA) or assumed negative (in closed-world setting, CWA)."}, {"id": "mldoc-multilingual-document-classification-corpus", "name": "MLDoc (Multilingual Document Classification Corpus)", "description": "Multilingual Document Classification Corpus (MLDoc) is a cross-lingual document classification dataset covering English, German, French, Spanish, Italian, Russian, Japanese and Chinese. It is a subset of the Reuters Corpus Volume 2 selected according to the following design choices:"}, {"id": "github-python", "name": "GitHub-Python", "description": "Repair AST parse (syntax) errors in Python code"}, {"id": "italian-crime-news-from-gazzetta-di-modena-2011-2021", "name": "Italian Crime News (from Gazzetta di Modena [2011-2021])", "description": "The dataset contains the main components of the news articles published online by the newspaper named Gazzetta di Modena: url of the web page, title, sub-title, text, date of publication, crime category assigned to each news article by the author."}, {"id": "hls4ml-lhc-jet-dataset-hls4ml-lhc-jet-dataset-100-particles", "name": "hls4ml LHC Jet dataset (hls4ml LHC Jet dataset (100 particles))", "description": "Dataset of high-pT jets from simulations of LHC proton-proton collisions"}, {"id": "thirdtofirst", "name": "ThirdToFirst", "description": "Two datasets (synthetic and natural/real) containing simultaneously recorded egocentric and exocentric videos."}, {"id": "utkface", "name": "UTKFace", "description": "The UTKFace dataset is a large-scale face dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The images cover large variation in pose, facial expression, illumination, occlusion, resolution, etc. This dataset could be used on a variety of tasks, e.g., face detection, age estimation, age progression/regression, landmark localization, etc."}, {"id": "notebookcdg", "name": "notebookcdg", "description": "Inspired by Wang et al. 2021, we decided to utilize the top-voted and well-documented Kaggle notebooks to construct the notebookCDGdataset"}, {"id": "nsynth", "name": "NSynth", "description": "NSynth is a dataset of one shot instrumental notes, containing 305,979 musical notes with unique pitch, timbre and envelope. The sounds were collected from 1006 instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. Four second monophonic 16kHz audio snippets were generated (notes) for the instruments."}, {"id": "subjective-discourse", "name": "Subjective Discourse", "description": "This is a discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents.  The dataset consists of witness testimonials in U.S. congressional hearings."}, {"id": "gid-gaofen-image-dataset", "name": "GID (Gaofen Image Dataset)", "description": "GID is large-scale land-cover dataset with Gaofen-2 (GF-2) satellite images. This new dataset, which is named as Gaofen Image Dataset (GID), has superiorities over the existing land-cover dataset because of its large coverage, wide distribution, and high spatial resolution. GID consists of two parts: a large-scale classification set and a fine land-cover classification set. The large-scale classification set contains 150 pixel-level annotated GF-2 images, and the fine classification set is composed of 30,000 multi-scale image patches coupled with 10 pixel-level annotated GF-2 images. The training and validation data with 15 categories is collected and re-labeled based on the training and validation images with 5 categories, respectively."}, {"id": "muserc-russian-multi-sentence-reading-comprehension", "name": "MuSeRC (Russian Multi-Sentence Reading Comprehension)", "description": "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills."}, {"id": "learn2reg", "name": "Learn2Reg", "description": "Learn2Reg is a dataset for medical image registration. Learn2Reg covers a wide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound, CT, MR), availability of annotations, as well as intra- and inter-patient registration evaluation."}, {"id": "semantic-scholar", "name": "Semantic Scholar", "description": "The Semantic Scholar corpus (S2) is composed of titles from scientific papers published in machine learning conferences and journals from 1985 to 2017, split by year (33 timesteps)."}, {"id": "cellcycle-funcat", "name": "Cellcycle Funcat", "description": "Hierarchical multi-label classification dataset for functional genomics"}, {"id": "composition-1k", "name": "Composition-1K", "description": "Composition-1K is a large-scale image matting dataset including 49300 training images and 1000 testing images."}, {"id": "pmdata", "name": "PMData", "description": "The PMData dataset aims to combine the traditional lifelogging with sports activity logging."}, {"id": "desiredb", "name": "DesireDB", "description": "Includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context."}, {"id": "procgen", "name": "ProcGen", "description": "Procgen Benchmark includes 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills."}, {"id": "vitaminc-vitaminc-robust-fact-verification", "name": "VitaminC (VitaminC Robust Fact Verification)", "description": "The VitaminC dataset contains more than 450,000 claim-evidence pairs for fact verification and factual consistent generation. Based on over 100,000 revisions to popular Wikipedia pages, and additional \"synthetic\" revisions."}, {"id": "pcam-patchcamelyon", "name": "PCam (PatchCamelyon)", "description": "PatchCamelyon is an image classification dataset. It consists of 327.680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue. PCam provides a new benchmark for machine learning models: bigger than CIFAR10, smaller than ImageNet, trainable on a single GPU."}, {"id": "changeit", "name": "ChangeIt", "description": "ChangeIt dataset with more than 2600 hours of video with state-changing actions published at CVPR 2022."}, {"id": "voxceleb1", "name": "VoxCeleb1", "description": "VoxCeleb1 is an audio dataset containing over 100,000 utterances for 1,251 celebrities, extracted from videos uploaded to YouTube."}, {"id": "dyml-product-dynamic-metric-learning-product", "name": "DyML-Product (Dynamic Metric Learning Product)", "description": "DyML-Product is derived from iMaterialist-2019, a hierarchical online product dataset. The original iMaterialist-2019 offers up to 4 levels of hierarchical annotations. We remove the coarsest level and maintain 3 levels for DyML-Product."}, {"id": "zest", "name": "ZEST", "description": "A new English language dataset structured for task-oriented evaluation on unseen tasks. "}, {"id": "nih3t3", "name": "NIH3T3", "description": "The archive contains original images from NIH3T3 cells stained with Hoechst 33342 as PNG files. It also contains images (as Photoshop and GIMP files) showing hand-segmentation of the Hoechst images into regions containing single nuclei."}, {"id": "real-life-violence-situations-dataset", "name": "Real Life Violence Situations Dataset", "description": "This dataset has the following citation: M. Soliman, M. Kamal, M. Nashed, Y. Mostafa, B. Chawky, D. Khattab, \u201c Violence Recognition from Videos using Deep Learning Techniques\u201d, Proc. 9th International Conference on Intelligent Computing and Information Systems (ICICIS'19), Cairo, pp. 79-84, 2019. please use it in case of using the dataset in research or engineering purpose ) when we start our Graduation Project Violence Recognition from Videos we found that there is shortage in available datasets related to violence between individuals so we decide to create new big dataset with variety of scenes"}, {"id": "medal", "name": "MeDAL", "description": "The  Medical Dataset for Abbreviation Disambiguation for Natural Language Understanding (MeDAL) is a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. It was published at the ClinicalNLP workshop at EMNLP."}, {"id": "camus-cardiac-acquisitions-for-multi-structure-ultrasound-segmentation", "name": "CAMUS (Cardiac Acquisitions for Multi-structure Ultrasound Segmentation)", "description": "The goal of this project is to provide all the materials to the community to resolve the problem of echocardiographic image segmentation and volume estimation from 2D ultrasound sequences (both two and four-chamber views). To this aim, the following solutions were set-up"}, {"id": "wikiwiki", "name": "WikiWiki", "description": "WikiWiki is a dataset for understanding entities and their place in a taxonomy of knowledge\u2014their types. It consists of entities and passages from 10M Wikipedia articles linked to the Wikidata knowledge graph with 41K types."}, {"id": "eli5", "name": "ELI5", "description": "ELI5 is a dataset for long-form question answering. It contains 270K complex, diverse questions that require explanatory multi-sentence answers. Web search results are used as evidence documents to answer each question."}, {"id": "sbu-captions-dataset", "name": "SBU Captions Dataset", "description": "A collection that allows researchers to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results."}, {"id": "usf-human-id-gait-challenge-dataset", "name": "USF (Human ID Gait Challenge Dataset)", "description": "The USF Human ID Gait Challenge Dataset is a dataset of videos for gait recognition. It has videos from 122 subjects in up to 32 possible combinations of variations in factors."}, {"id": "aim-500-automatic-image-matting-500", "name": "AIM-500 (Automatic Image Matting-500)", "description": "AIM-500 is the first natural image matting test set, contains 500 high-resolution real-world natural images from three types of images (salient opaque foregrounds, salient transparent/meticulous foregrounds, non-salient foregrounds), and multiple categories. The amount of each category is shown in the following table."}, {"id": "deep-fashion3d", "name": "Deep Fashion3D", "description": "A novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations."}, {"id": "hc-stvg2", "name": "HC-STVG2", "description": "We have added data and cleaned the labels in HC-STVG to build the HC-STVG2.0. While the original database contained 5660 videos, the new database has been re-annotated and modified and now contains 16,000 + videos for this challenge."}, {"id": "hq-wmca-high-quality-wide-multi-channel-attack-database", "name": "HQ-WMCA (High-Quality Wide Multi-Channel Attack database)", "description": "The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database consists of 2904 short multi-modal video recordings of both bona-fide and presentation attacks. There are 555 bonafide presentations from 51 participants and the remaining 2349 are presentation attacks. The data is recorded from several channels including color, depth, thermal, infrared (spectra), and short-wave infrared (spectra)."}, {"id": "nats-bench", "name": "NATS-Bench", "description": "A unified benchmark on searching for both topology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. "}, {"id": "quality-question-answering-with-long-input-texts-yes", "name": "QuALITY (Question Answering with Long Input Texts, Yes!)", "description": "QuALITY (Question Answering with Long Input Texts, Yes!) is a multiple-choice question answering dataset for long document comprehension. The dataset consists of context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, the questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts."}, {"id": "cmu-book-summary-dataset", "name": "CMU Book Summary Dataset", "description": "This dataset contains plot summaries for 16,559 books extracted from Wikipedia, along with aligned metadata from Freebase, including book author, title, and genre."}, {"id": "business-scene-dialogue", "name": "Business Scene Dialogue", "description": "The Japanese-English business conversation corpus, namely Business Scene Dialogue corpus, was constructed in 3 steps:"}, {"id": "casia-surf", "name": "CASIA-SURF", "description": "Dataset for face anti-spoofing in terms of both subjects and modalities. Specifically, it consists of  subjects with  videos and each sample has  modalities (i.e., RGB, Depth and IR). "}, {"id": "yago3-10-yet-another-great-ontology-3-10", "name": "YAGO3-10 (Yet Another Great Ontology 3-10)", "description": "YAGO3-10 is benchmark dataset for knowledge base completion. It is a subset of YAGO3 (which itself is an extension of YAGO) that contains entities associated with at least ten different relations. In total, YAGO3-10 has 123,182 entities and 37 relations and 1,179,040 triples, and most of the triples describe attributes of persons such as citizenship, gender, and profession."}, {"id": "2d-moving-clusters", "name": "2D Moving Clusters", "description": "Contains $10^7$ points, sampled from 20 clusters, with incremental concept drift - On each batch (of size 1000) the mean of each of the clusters moves a random (small) length in some random direction, the means move independently of each other. This dataset should be used sequentially, in batches of $1000$."}, {"id": "jsrt-japanese-society-of-radiological-technology-database", "name": "JSRT (Japanese Society of Radiological Technology Database)", "description": "The standard digital image database with and without chest lung nodules (JSRT database) was created(*1) by the Japanese Society of Radiological Technology (JSRT) in cooperation with the Japanese Radiological Society (JRS) in 1998. Since then, the JSRT database has been used by a number of researchers in the world for various research purposes such as image processing, image compression, evaluation of image display, computer-aided diagnosis (CAD), picture archiving and communication system (PACS), and for training and testing."}, {"id": "screen2words", "name": "Screen2Words", "description": "Screen2Words is a large-scale screen summarization dataset annotated by human workers. The dataset contains more than 112k language summarization across 22k unique UI screens. This dataset can be used for Mobile User Interface Summarization, which is a task where a model generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen."}, {"id": "afad-asian-face-age-dataset", "name": "AFAD (Asian Face Age Dataset)", "description": "The Asian Face Age Dataset (AFAD) is a new dataset proposed for evaluating the performance of age estimation, which contains more than 160K facial images and the corresponding age and gender labels. This dataset is oriented to age estimation on Asian faces, so all the facial images are for Asian faces. It is noted that the AFAD is the biggest dataset for age estimation to date. It is well suited to evaluate how deep learning methods can be adopted for age estimation."}, {"id": "wikisplit", "name": "WikiSplit", "description": "Contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task."}, {"id": "fmfcc-a", "name": "FMFCC-A", "description": "FMFCC-A is a large publicly-available Mandarin dataset for synthetic speech detection, which contains 40,000 synthesized Mandarin utterances that generated by 11 Mandarin TTS systems and two Mandarin VC systems, and 10,000 genuine Mandarin utterance collected from 58 speakers. The FMFCCA dataset is divided into the training, development and evaluation sets, which are used for the research of detection of synthesised Mandarin speech under various previously unknown speech synthesis systems or audio post-processing operations."}, {"id": "tvqa", "name": "TVQA+", "description": "TVQA+ contains 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers."}, {"id": "humanml3d", "name": "HumanML3D", "description": "HumanML3D is a 3D human motion-language dataset that originates from a combination of HumanAct12 and Amass dataset. It covers a broad range of human actions such as daily activities (e.g., 'walking', 'jumping'), sports (e.g., 'swimming', 'playing golf'), acrobatics (e.g., 'cartwheel') and artistry (e.g., 'dancing'). Overall, HumanML3D dataset consists of 14,616 motions and 44,970 descriptions composed by 5,371 distinct words. The total length of motions amounts to 28.59 hours. The average motion length is 7.1 seconds, while average description length is 12 words."}, {"id": "tsp-hcp-benchmark-set", "name": "TSP/HCP Benchmark set", "description": "This is a benchmark set for Traveling salesman problem (TSP) with characteristics that are different from the existing benchmark sets. In particular, it focuses on small instances which prove to be challenging for one or more state-of-the-art TSP algorithms. These instances are based on difficult instances of Hamiltonian cycle problem (HCP). This includes instances from literature, specially modified randomly generated instances, and instances arising from the conversion of other difficult problems to HCP."}, {"id": "v4v-vision-for-vitals", "name": "V4V (Vision for Vitals)", "description": "Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the Vision for Vitals dataset was introduced. It is a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population."}, {"id": "holstep", "name": "HolStep", "description": "HolStep is a dataset based on higher-order logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. "}, {"id": "dailydialog", "name": "DailyDialog", "description": "DailyDialog is a high-quality multi-turn open-domain English dialog dataset. It contains 13,118 dialogues split into a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. On average there are around 8 speaker turns per dialogue with around 15 tokens per turn."}, {"id": "winogrande", "name": "WinoGrande", "description": "WinoGrande is a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations."}, {"id": "commongen", "name": "CommonGen", "description": "CommonGen is constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. "}, {"id": "roadtext-1k", "name": "RoadText-1K", "description": "A dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. The dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. "}, {"id": "semantickitti", "name": "SemanticKITTI", "description": "SemanticKITTI is a large-scale outdoor-scene dataset for point cloud semantic segmentation. It is derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR. The dataset consists of 22 sequences. Overall, the dataset provides 23201 point clouds for training and 20351 for testing."}, {"id": "schwerin", "name": "Schwerin", "description": "Schwerin contains handwritten texts written in medieval German. Train sample consists of 793 lines, validation - 68 lines and test - 196 lines."}, {"id": "off-distant-parallel-smac-off-distant-parallel-20", "name": "Off_Distant_parallel (SMAC+_Off_Distant_parallel_20)", "description": "SMAC+ offense distant scenario."}, {"id": "urbanscene3d", "name": "UrbanScene3D", "description": "UrbanScene3D is a large scale urban scene dataset associated with a handy simulator based on Unreal Engine 4 and AirSim, which consists of both man-made and real-world reconstruction scenes in different scales, referred to as UrbanScene3D. The manually made scene models have compact structures, which are carefully constructed/designed by professional modelers according to the images and maps of target areas. In contrast, UrbanScene3D also offers dense, detailed scene models reconstructed by aerial images through multi-view stereo (MVS) techniques. These scenes have realistic textures and meticulous structures. The release also includes the originally captured aerial images that have been used to reconstruct the 3D scene models, as well as a set of 4K video sequences that would facilitate designing algorithms, such SLAM and MVS."}, {"id": "oxford-iiit-pet-dataset", "name": "Oxford-IIIT Pet Dataset", "description": "The Oxford-IIIT Pet Dataset has 37 categories with roughly 200 images for each class. The images have a large variations in scale, pose and lighting. All images have an associated ground truth annotation of breed, head ROI, and pixel level trimap segmentation."}, {"id": "multilingual-reuters-multilingual-reuters-collection", "name": "Multilingual Reuters (Multilingual Reuters Collection)", "description": "The Multilingual Reuters Collection dataset comprises over 11,000 articles from six classes in five languages, i.e., English (E), French (F), German (G), Italian (I), and Spanish (S)."}, {"id": "fewrel-2-0", "name": "FewRel 2.0", "description": "A more challenging task to investigate two aspects of few-shot relation classification models: (1) Can they adapt to a new domain with only a handful of instances? (2) Can they detect none-of-the-above (NOTA) relations?"}, {"id": "eeg-motor-movement-imagery-dataset", "name": "EEG Motor Movement/Imagery Dataset", "description": "This data set consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers."}, {"id": "ali-ccp-alibaba-click-and-conversion-prediction", "name": "Ali-CCP (Alibaba Click and Conversion Prediction)", "description": "This data set is provided by Alimama"}, {"id": "proteins", "name": "PROTEINS", "description": "PROTEINS is a dataset of proteins that are classified as enzymes or non-enzymes. Nodes represent the amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart."}, {"id": "kth-kth-action-dataset", "name": "KTH (KTH Action dataset)", "description": "The efforts to create a non-trivial and publicly available dataset for action recognition was initiated at the KTH Royal Institute of Technology in 2004. The KTH dataset is one of the most standard datasets, which contains six actions: walk, jog, run, box, hand-wave, and hand clap. To account for performance nuance, each action is performed by 25 different individuals, and the setting is systematically altered for each action per actor. Setting variations include: outdoor (s1), outdoor with scale variation (s2), outdoor with different clothes (s3), and indoor (s4). These variations test the ability of each algorithm to identify actions independent of the background, appearance of the actors, and the scale of the actors."}, {"id": "swiss3dcities", "name": "Swiss3DCities", "description": "Swiss3DCities is a dataset that is manually annotated for semantic segmentation with per-point labels, and is built using photogrammetry from images acquired by multirotors equipped with high-resolution cameras."}, {"id": "tweebank", "name": "Tweebank", "description": "Briefly describe the dataset. Provide:"}, {"id": "mrda-icsi-meeting-recorder-dialog-act-corpus", "name": "MRDA (ICSI Meeting Recorder Dialog Act Corpus)", "description": "The MRDA corpus consists of about 75 hours of speech from 75 naturally-occurring meetings among 53 speakers. The tagset used for labeling is a modified version of the SWBD-DAMSL tagset. It is annotated with three types of information: marking of the dialogue act segment boundaries, marking of the dialogue acts and marking of correspondences between dialogue acts."}, {"id": "rvl-cdip", "name": "RVL-CDIP", "description": "The RVL-CDIP dataset consists of scanned document images belonging to 16 classes such as letter, form, email, resume, memo, etc. The dataset has 320,000 training, 40,000 validation and 40,000 test images. The images are characterized by low quality, noise, and low resolution, typically 100 dpi."}, {"id": "visda-2017", "name": "VisDA-2017", "description": "VisDA-2017 is a simulation-to-real dataset for domain adaptation with over 280,000 images across 12 categories in the training, validation and testing domains. The training images are generated from the same object under different circumstances, while the validation images are collected from MSCOCO.."}, {"id": "semcat", "name": "SEMCAT", "description": "Contains more than 6500 words semantically grouped under 110 categories."}, {"id": "ocnli-original-chinese-natural-language-inference", "name": "OCNLI (Original Chinese Natural Language Inference)", "description": "OCNLI stands for Original Chinese Natural Language Inference. It is corpus for Chinese Natural Language Inference, collected following closely the procedures of MNLI, but with enhanced strategies aiming for more challenging inference pairs. No human/machine translation is used in creating the dataset, and thus the Chinese texts are original and not translated."}, {"id": "megaage", "name": "MegaAge", "description": "MegaAge is a large dataset that consists of 41,941 faces annotated with age posterior distributions."}, {"id": "mutagenicity", "name": "Mutagenicity", "description": "Mutagenicity is a chemical compound dataset of drugs, which can be categorized into two classes: mutagen and non-mutagen."}, {"id": "incidents", "name": "Incidents", "description": "Contains 446,684 images annotated by humans that cover 43 incidents across a variety of scenes. "}, {"id": "indosum", "name": "IndoSum", "description": "The IndoSum dataset is a benchmark dataset for Indonesian text summarization. The dataset consists of news articles and manually constructed summaries."}, {"id": "oak-objects-around-krishna", "name": "OAK (Objects Around Krishna)", "description": "OAK is a dataset for online continual object detection benchmark with an egocentric video dataset. OAK adopts the KrishnaCam videos, an ego-centric video stream collected over nine months by a graduate student. OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5 hours) for 105 object categories in outdoor scenes."}, {"id": "cubicasa5k", "name": "CubiCasa5K", "description": "CubiCasa5K is a large-scale floorplan image dataset containing 5000 samples annotated into over 80 floorplan object categories. The dataset annotations are performed in a dense and versatile manner by using polygons for separating the different objects."}, {"id": "cora-48-32-20-fixed-splits", "name": "Cora (48%/32%/20% fixed splits)", "description": "Node classification on Cora with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "golfdb", "name": "GolfDB", "description": "GolfDB is a high-quality video dataset created for general recognition applications in the sport of golf, and specifically for the task of golf swing sequencing."}, {"id": "bridge-data", "name": "Bridge Data", "description": "Bridge Data is a large multi-domain and multi-task dataset, with 7,200 demonstrations constituting 71 tasks across 10 environments. The dataset is collected using a low-cost yet versatile 6-DoF WidowX250 robot arm and contains 7,200 demonstrations of a robot performing 71 kitchen tasks across 10 environments with varying lighting, robot positions, and backgrounds. It can be used to boosting generalization of robotic skills and empirically study how it can improve the learning of new tasks in new environments. "}, {"id": "headlines-dataset", "name": "Headlines dataset", "description": "The Headlines dataset for sarcasm detection is collected from two news website. TheOnion aims at producing sarcastic versions of current events. The dataset includes all the headlines from News in Brief and News in Photos categories (which are sarcastic) and real (and non-sarcastic) news headlines from HuffPost. This dataset has following advantages over the existing Twitter datasets:"}, {"id": "prost-physical-reasoning-about-objects-through-space-and-time", "name": "PROST (Physical Reasoning about Objects Through Space and Time)", "description": "The PROST (Physical Reasoning about Objects Through Space and Time) dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting."}, {"id": "rxr-room-across-room", "name": "RxR (Room-across-Room)", "description": "Room-Across-Room (RxR) is a multilingual dataset for Vision-and-Language Navigation (VLN) for Matterport3D environments. In contrast to related datasets such as Room-to-Room (R2R), RxR is 10x larger, multilingual (English, Hindi and Telugu), with longer and more variable paths, and it includes and fine-grained visual groundings that relate each word to pixels/surfaces in the environment."}, {"id": "haze4k", "name": "Haze4k", "description": "Haze4k is a synthesized dataset with 4,000 hazy images, in which each hazy image has the associate ground truths of a latent clean image, a transmission map, and an atmospheric light ma"}, {"id": "pattr-patent-translation-resource", "name": "PatTR (Patent Translation Resource)", "description": "PatTR is a sentence-parallel corpus extracted from the MAREC patent collection. The current version contains more than 22 million German-English and 18 million French-English parallel sentences collected from all patent text sections as well as 5 million German-French sentence pairs from patent titles, abstracts and claims."}, {"id": "brca-m2c", "name": "BRCA-M2C", "description": "Dataset for multi-class cell classification in breast cancer H\\&E images using dot annotations . The labelled cell classes are lymphocytes, tumor or epithelial cells, and stromal cells."}, {"id": "amazonqa", "name": "AmazonQA", "description": "AmazonQA consists of 923k questions, 3.6M answers and 14M reviews across 156k products. Building on the well-known Amazon dataset, additional annotations are collected, marking each question as either answerable or unanswerable based on the available reviews. "}, {"id": "sen12ms-cr", "name": "SEN12MS-CR", "description": "SEN12MS-CR is a multi-modal and mono-temporal data set for cloud removal. It contains observations covering 175 globally distributed Regions of Interest recorded in one of four seasons throughout the year of 2018. For each region, paired and co-registered synthetic aperture radar (SAR) Sentinel-1 measurements as well as cloudy and cloud-free optical multi-spectral Sentinel-2 observations from European Space Agency's Copernicus mission are provided. The Sentinel satellites provide public access data and are among the most prominent satellites in Earth observation. "}, {"id": "nab-numenta-anomaly-benchmark", "name": "NAB (Numenta Anomaly Benchmark)", "description": "The First Temporal Benchmark Designed to Evaluate  Real-time Anomaly Detectors Benchmark"}, {"id": "dbates-database-of-audio-features-text-and-visual-expressions-in-competitive-debate-speeches", "name": "DBATES (DataBase of Audio features, Text and visual Expressions in competitive debate Speeches)", "description": "DBATES is a database of multimodal communication features extracted from debate speeches in the 2019 North American Universities Debate Championships (NAUDC)."}, {"id": "ut-zappos50k", "name": "UT Zappos50K", "description": "UT Zappos50K is a large shoe dataset consisting of 50,025 catalog images collected from Zappos.com. The images are divided into 4 major categories \u2014 shoes, sandals, slippers, and boots \u2014 followed by functional types and individual brands. The shoes are centered on a white background and pictured in the same orientation for convenient analysis."}, {"id": "qa-srl-bank-2-0", "name": "QA-SRL Bank 2.0", "description": "QA-SRL Bank 2.0 is a large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations. The corpus consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that was shown to have high precision and good recall at modest cost."}, {"id": "figer-fine-grained-entity-recognition", "name": "FIGER (Fine-Grained Entity Recognition)", "description": "The FIGER dataset is an entity recognition dataset where entities are labelled using fine-grained system 112 tags, such as person/doctor, art/written_work and building/hotel. The tags are derivied from Freebase types. The training set consists of Wikipedia articles automatically annotated with distant supervision approach that utilizes the information encoded in anchor links. The test set was annotated manually."}, {"id": "snopes", "name": "Snopes", "description": "Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from snopes.com."}, {"id": "baai-vanjee", "name": "BAAI-VANJEE", "description": "BAAI-VANJEE is a dataset for benchmarking and training various computer vision tasks such as 2D/3D object detection and multi-sensor fusion. The BAAI-VANJEE roadside dataset consists of LiDAR data and RGB images collected by VANJEE smart base station placed on the roadside about 4.5m high. This dataset contains 2500 frames of LiDAR data, 5000 frames of RGB images, including 20% collected at the same time. It also contains 12 classes of objects, 74K 3D object annotations and 105K 2D object annotations."}, {"id": "hurricaneemo", "name": "HurricaneEmo", "description": "HurricaneEmo is an emotion dataset that contains 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria."}, {"id": "ced-color-event-camera-dataset", "name": "CED (Color Event Camera Dataset)", "description": "Contains 50 minutes of footage with both color frames and events. CED features a wide variety of indoor and outdoor scenes."}, {"id": "ece", "name": "ECE", "description": "The ECE dataset (Gui et al., 2016a) is collected from SINA city news and contains 2105 instances. Its document has only one emotion word and one or more emotion causes."}, {"id": "decoco", "name": "DeCOCO", "description": "DeCOCO is a bilingual (English-German) corpus of image descriptions, where the English part is extracted from the COCO dataset, and the German part are translations by a native German speaker."}, {"id": "isprs-vaihingen-2d-semantic-labeling-vaihingen-data", "name": "ISPRS Vaihingen (2D Semantic Labeling - Vaihingen data)", "description": "The data set contains 33 patches (of different sizes), each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic."}, {"id": "cumulo", "name": "Cumulo", "description": "A benchmark dataset for training and evaluating global cloud classification models. It consists of one year of 1km resolution MODIS hyperspectral imagery merged with pixel-width 'tracks' of CloudSat cloud labels. "}, {"id": "mebal", "name": "mEBAL", "description": "A multimodal database for eye blink detection and attention level estimation. "}, {"id": "clip-clip-a-dataset-for-extracting-action-items-for-physicians-from-hospital-discharge-notes", "name": "CLIP (CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes)", "description": "We created a dataset of clinical action items annotated over MIMIC-III. This dataset, which we call CLIP, is annotated by physicians and covers 718 discharge summaries, representing 107,494 sentences. Annotations were collected as character-level spans to discharge summaries after applying surrogate generation to fill in the anonymized templates from MIMIC-III text with faked data. We release these spans, their aggregation into sentence-level labels, and the sentence tokenizer used to aggregate the spans and label sentences. We also release the surrogate data generator, and the document IDs used for training, validation, and test splits, to enable reproduction. The spans are annotated with 0 or more labels of 7 different types, representing the different actions that may need to be taken: Appointment, Lab, Procedure, Medication, Imaging, Patient Instructions, and Other. We encourage the community to use this dataset to develop methods for automatically extracting clinical action items from discharge summaries."}, {"id": "active-terahertz", "name": "Active Terahertz", "description": "This is a public dataset for evaluating multi-object detection algorithms in active Terahertz imaging resolution 5 mm by 5 mm."}, {"id": "aesi-athens-emotional-states-inventory", "name": "AESI (Athens Emotional States Inventory)", "description": "The development of ecologically valid procedures for collecting reliable and unbiased emotional data towards computer interfaces with social and affective intelligence targeting patients with mental disorders. Following its development, presented with, the Athens Emotional States Inventory (AESI) proposes the design, recording and validation of an audiovisual database for five emotional states: anger, fear, joy, sadness and neutral. The items of the AESI consist of sentences each having content indicative of the corresponding emotion. Emotional content was assessed through a survey of 40 young participants with a questionnaire following the Latin square design. The emotional sentences that were correctly identified by 85% of the participants were recorded in a soundproof room with microphones and cameras. A preliminary validation of AESI is performed through automatic emotion recognition experiments from speech. The resulting database contains 696 recorded utterances in Greek language by 20 native speakers and has a total duration of approximately 28 min. Speech classification results yield accuracy up to 75.15% for automatically recognizing the emotions in AESI. These results indicate the usefulness of our approach for collecting emotional data with reliable content, balanced across classes and with reduced environmental variability."}, {"id": "citesum", "name": "CiteSum", "description": "CiteSum is a large-scale scientific extreme summarization benchmark."}, {"id": "nlpr", "name": "NLPR", "description": "The NLPR dataset for salient object detection consists of 1,000 image pairs captured by a standard Microsoft Kinect with a resolution of 640\u00d7480. The images include indoor and outdoor scenes (e.g., offices, campuses, streets and supermarkets)."}, {"id": "s3e", "name": "S3E", "description": "S3E is a novel large-scale multimodal dataset captured by a fleet of unmanned ground vehicles along four designed collaborative trajectory paradigms. S3E consists of 7 outdoor and 5 indoor scenes that each exceed 200 seconds, consisting of well synchronized and calibrated high-quality stereo camera, LiDAR, and high-frequency IMU data."}, {"id": "gebiocorpus", "name": "GeBioCorpus", "description": "A high-quality dataset for machine translation evaluation that aims at being one of the first non-synthetic gender-balanced test datasets."}, {"id": "an-extension-of-xnli", "name": "An Extension of XNLI", "description": "https://github.com/salesforce/xnli_extension"}, {"id": "coughvid", "name": "COUGHVID", "description": "The COUGHVID dataset provides over 20,000 crowdsourced cough recordings representing a wide range of subject ages, genders, geographic locations, and COVID-19 statuses. First, the dataset was filtered using an open-sourced cough detection algorithm. Second, experienced pulmonologists labeled more than 2,000 recordings to diagnose medical abnormalities present in the coughs, thereby contributing one of the largest expert-labeled cough datasets in existence that can be used for a plethora of cough audio classification tasks."}, {"id": "tencent-ml-images", "name": "Tencent ML-Images", "description": "Tencent ML-Images is a large open-source multi-label image database, including 17,609,752 training and 88,739 validation image URLs, which are annotated with up to 11,166 categories."}, {"id": "rsscn7", "name": "RSSCN7", "description": "he RSSCN7 dataset contains satellite images acquired from Google Earth, which is originally collected for remote sensing scene classification. We conduct image synthesis on RSSCN7 to make it capable of the image inpainting task. It has seven classes: grassland, farmland, industrial and commercial regions, river and lake, forest field, residential region, and parking lot. Each class has 400 images, so there are total 2,800 images in the RSSCN7 dataset."}, {"id": "booktest", "name": "BookTest", "description": "BookTest is a new dataset similar to the popular Children\u2019s Book Test (CBT), however more than 60 times larger."}, {"id": "icartoonface", "name": "iCartoonFace", "description": "The iCartoonFace dataset is a large-scale dataset that can be used for two different tasks: cartoon face detection and cartoon face recognition."}, {"id": "numersense", "name": "NumerSense", "description": "Contains 13.6k masked-word-prediction probes, 10.5k for fine-tuning and 3.1k for testing."}, {"id": "scitsr", "name": "SciTSR", "description": "SciTSR is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format and their corresponding structure labels obtained from LaTeX source files."}, {"id": "metfaces", "name": "MetFaces", "description": "MetFaces is an image dataset of human faces extracted from works of art. The dataset consists of 1336 high-quality PNG images at 1024\u00d71024 resolution. The images were downloaded via the Metropolitan Museum of Art Collection API, and automatically aligned and cropped using dlib. Various automatic filters were used to prune the set."}, {"id": "housekeep", "name": "Housekeep", "description": "Housekeep a benchmark to evaluate common sense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. The dataset contains where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms."}, {"id": "rdd-2020-road-damage-dataset-2020", "name": "RDD-2020 (Road Damage Dataset 2020)", "description": "The Road Damage Dataset 2020 (RDD-2020) Secondly is a large-scale heterogeneous dataset comprising 26620 images collected from multiple countries using smartphones. The images are collected from roads in India, Japan and the Czech Republic."}, {"id": "ccmatrix", "name": "CCMatrix", "description": "CCMatrix uses ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences. "}, {"id": "emowoz", "name": "EmoWOZ", "description": "EmoWOZ is the first large-scale open-source dataset for emotion recognition in task-oriented dialogues. It contains emotion annotations for user utterances in the entire MultiWOZ (10k+ human-human dialogues) and DialMAGE (1k human-machine dialogues collected from our human trial). Overall, there are 83k user utterances annotated. In addition, the emotion annotation scheme is tailored to task-oriented dialogues and considers the valence, the elicitor, and the conduct of the user emotion."}, {"id": "climate-claims", "name": "Climate Claims", "description": "The Climate Change Claims dataset for generating fact checking summaries contains claims broadly related to climate change and global warming from climatefeedback.org. It contains 1k documents from 104 different claims from 97 different domains."}, {"id": "ddad-dense-depth-for-autonomous-driving", "name": "DDAD (Dense Depth for Autonomous Driving)", "description": "DDAD is a new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions. It contains monocular videos and accurate ground-truth depth (across a full 360 degree field of view) generated from high-density LiDARs mounted on a fleet of self-driving cars operating in a cross-continental setting. DDAD contains scenes from urban settings in the United States (San Francisco, Bay Area, Cambridge, Detroit, Ann Arbor) and Japan (Tokyo, Odaiba)."}, {"id": "uav-gesture", "name": "UAV-GESTURE", "description": "UAV-GESTURE is a dataset for UAV control and gesture recognition. It is an outdoor recorded video dataset for UAV commanding signals with 13 gestures suitable for basic UAV navigation and command from general aircraft handling and helicopter handling signals. It contains 119 high-definition video clips consisting of 37,151 frames. "}, {"id": "danfever", "name": "DanFEVER", "description": "We present a dataset, DANFEVER, intended for claim verification in Danish. The dataset builds upon the task framing of the FEVER fact extraction and verification challenge. DANFEVER can be used for creating models for detecting mis- & disinformation in Danish as well as for verification in multilingual settings."}, {"id": "fdst-fudan-shanghaitech", "name": "FDST (Fudan-ShanghaiTech)", "description": "The Fudan-ShanghaiTech dataset (FDST) is a dataset for video crowd counting. It contains 15K frames with about 394K annotated heads captured from 13 different scenes"}, {"id": "epic-kitchens-100", "name": "EPIC-KITCHENS-100", "description": "This paper introduces the pipeline to scale the largest dataset in egocentric vision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection of 100 hours, 20M frames, 90K actions in 700 variable-length videos, capturing long-term unscripted activities in 45 environments, using head-mounted cameras. Compared to its previous version (EPIC-KITCHENS-55), EPIC-KITCHENS-100 has been annotated using a novel pipeline that allows denser (54% more actions per minute) and more complete annotations of fine-grained actions (+128% more action segments). This collection also enables evaluating the \"test of time\" - i.e. whether models trained on data collected in 2018 can generalise to new footage collected under the same hypotheses albeit \"two years on\". The dataset is aligned with 6 challenges: action recognition (full and weak supervision), action detection, action anticipation, cross-modal retrieval (from captions), as well as unsupervised domain adaptation for action recognition. For each challenge, we define the task, provide baselines and evaluation metrics."}, {"id": "qvhighlights-query-based-video-highlights", "name": "QVHighlights (Query-based Video Highlights)", "description": "The Query-based Video Highlights (QVHighlights) dataset is a dataset for detecting customized moments and highlights from videos given natural language (NL). It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips."}, {"id": "alsat-2b", "name": "Alsat-2B", "description": "Alsat-2B is a remote sensing dataset of low and high spatial resolution images (10m and 2.5m respectively) for the single-image super-resolution task. The high-resolution images are obtained through pan-sharpening. The dataset has been created from 13 images captured by the Alsat-2B Earth observation satellite, where the image cover 13 different cities."}, {"id": "flickr1024", "name": "Flickr1024", "description": "Contains 1024 pairs of high-quality images and covers diverse scenarios."}, {"id": "sndzoo-the-softwarised-network-data-zoo", "name": "SNDZoo (The Softwarised Network Data Zoo)", "description": "The softwarised network data zoo (SNDZoo) is an open collection of software networking data sets aiming to streamline and ease machine learning research in the software networking domain. Most of the published data sets focus on, but are not limited to, the performance of virtualised network functions (VNFs). The data is collected using fully automated NFV benchmarking frameworks, such as tng-bench, developed by us or third party solutions like Gym. The collection of the presented data sets follows the general VNF benchmarking methodology described in."}, {"id": "chb-mit-chb-mit-scalp-eeg", "name": "CHB-MIT (CHB-MIT Scalp EEG)", "description": "The CHB-MIT dataset is a dataset of EEG recordings from pediatric subjects with intractable seizures. Subjects were monitored for up to several days following withdrawal of anti-seizure mediation in order to characterize their seizures and assess their candidacy for surgical intervention. The dataset contains 23 patients divided among 24 cases (a patient has 2 recordings, 1.5 years apart). The dataset consists of 969 Hours of scalp EEG recordings with 173 seizures. There exist various types of seizures in the dataset (clonic, atonic, tonic). The diversity of patients (Male, Female, 10-22 years old) and different types of seizures contained in the datasets are ideal for assessing the performance of automatic seizure detection methods in realistic settings."}, {"id": "wiki-squirrel-wikipedia-squirrel", "name": "Wiki Squirrel (Wikipedia Squirrel)", "description": "The data was collected from the English Wikipedia (December 2018). These datasets represent page-page networks on specific topics (chameleons, crocodiles and squirrels). Nodes represent articles and edges are mutual links between them. The edges csv files contain the edges - nodes are indexed from 0. The features json files contain the features of articles - each key is a page id, and node features are given as lists. The presence of a feature in the feature list means that an informative noun appeared in the text of the Wikipedia article. The target csv contains the node identifiers and the average monthly traffic between October 2017 and November 2018 for each page. For each page-page network we listed the number of nodes an edges with some other descriptive statistics."}, {"id": "cmrc-2018-chinese-machine-reading-comprehension-2018", "name": "CMRC 2018 (Chinese Machine Reading Comprehension 2018)", "description": "CMRC 2018 is a dataset for Chinese Machine Reading Comprehension. Specifically, it is a span-extraction reading comprehension dataset that is similar to SQuAD."}, {"id": "bdd100k-subsets", "name": "BDD100K-Subsets", "description": "Subsets of BDD100K Dataset that are used in Object Detection Under Rainy Conditions for Autonomous Vehicles: A Review of State-of-the-Art and Emerging Techniques"}, {"id": "mslr-web30k-microsoft-learning-to-rank-datasets-30k", "name": "MSLR WEB30K (Microsoft Learning to Rank Datasets-30k)", "description": "The datasets are machine learning data, in which queries and urls are represented by IDs. The datasets consist of feature vectors extracted from query-url pairs along with relevance judgment labels:"}, {"id": "pass-pictures-without-humans-for-self-supervision", "name": "PASS (Pictures without humAns for Self-Supervision)", "description": "PASS is a large-scale image dataset, containing 1.4 million images, that does not include any humans and which can be used for high-quality pretraining while significantly reducing privacy concerns."}, {"id": "pubmed-pico-element-detection-dataset", "name": "PubMed PICO Element Detection Dataset", "description": "PICO is a framework to formulate a well-defined focused clinical question. This framework identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). The PubMed PICO Element Detection dataset is a dataset for evaluating models that automatically detect PICO elements."}, {"id": "tcia-brain-tumor-progression", "name": "TCIA Brain-Tumor-Progression", "description": "This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy.  Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention).  All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images).  The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent.  All of the series are co-registered with the T1+C images.  The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression."}, {"id": "amazon-fraud-multi-relational-graph-dataset-for-amazon-fraudulent-account-detection", "name": "Amazon-Fraud (Multi-relational Graph Dataset for Amazon Fraudulent Account Detection)", "description": "Amazon-Fraud is a multi-relational graph dataset built upon the Amazon review dataset, which can be used in evaluating graph-based node classification, fraud detection, and anomaly detection models."}, {"id": "hope-image-household-objects-for-pose-estimation", "name": "HOPE-Image (Household Objects for Pose Estimation)", "description": "The NVIDIA HOPE datasets consist of RGBD images and video sequences with labeled 6-DoF poses for 28 toy grocery objects. The toy grocery objects are readily available for purchase and have ideal size and weight for robotic manipulation. 3D textured meshes for generating synthetic training data are provided."}, {"id": "idd-indian-driving-dataset", "name": "IDD (Indian Driving Dataset)", "description": "IDD is a dataset for road scene understanding in unstructured environments used for semantic segmentation and object detection for autonomous driving. It consists of 10,004 images, finely annotated with 34 classes collected from 182 drive sequences on Indian roads. "}, {"id": "ecg-heartbeat-categorization-dataset", "name": "ECG Heartbeat Categorization Dataset", "description": "This dataset consists of a series of CSV files. Each of these CSV files contain a matrix, with each row representing an example in that portion of the dataset. The final element of each row denotes the class to which that example belongs."}, {"id": "drealsr-diverse-real-world-image-super-resolution", "name": "DRealSR (Diverse Real-world image Super-Resolution)", "description": "DRealSR establishes a Super Resolution (SR) benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. "}, {"id": "nyctaxi", "name": "NYCTaxi", "description": "Taxi flow data of New York City with grid 20x10."}, {"id": "rareplanes-dataset", "name": "RarePlanes Dataset", "description": "The dataset specifically focuses on the value of synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700 hand-annotated aircraft. "}, {"id": "arch", "name": "ARCH", "description": "ARCH is a computational pathology (CP) multiple instance captioning dataset to facilitate dense supervision of CP tasks. Existing CP datasets focus on narrow tasks; ARCH on the other hand contains dense diagnostic and morphological descriptions for a range of stains, tissue types and pathologies. "}, {"id": "clinc-single-domain-oos", "name": "CLINC-Single-Domain-OOS", "description": "A dataset with two separate domains, i.e., the  \"Banking''  domain and the \"Credit cards''  domain with both general Out-of-Scope (OOD-OOS) queries and In-Domain but Out-of-Scope (ID-OOS) queries, where ID-OOS queries are semantically similar intents/queries with in-scope intents. Each domain in CLINC150 originally includes 15 intents. Each domain includes ten in-scope intents in this dataset, and the ID-OOS queries are built up based on five held-out in-scope intents."}, {"id": "amazon-review", "name": "Amazon Review", "description": "Amazon Review is a dataset to tackle the task of identifying whether the sentiment of a product review is positive or negative. This dataset includes reviews from four different merchandise categories: Books (B) (2834 samples), DVDs (D) (1199 samples), Electronics (E) (1883 samples), and Kitchen and housewares (K) (1755 samples)."}, {"id": "webquestionssp-webquestions-semantic-parses-dataset", "name": "WebQuestionsSP (WebQuestions Semantic Parses Dataset)", "description": "The WebQuestionsSP dataset is released as part of our ACL-2016 paper \u201cThe Value of Semantic Parse Labeling for Knowledge Base Question Answering\u201d [Yih, Richardson, Meek, Chang & Suh, 2016], in which we evaluated the value of gathering semantic parses, vs. answers, for a set of questions that originally comes from WebQuestions [Berant et al., 2013]. The WebQuestionsSP dataset contains full semantic parses in SPARQL queries for 4,737 questions, and \u201cpartial\u201d annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer. This release also includes an evaluation script and the output of the STAGG semantic parsing system when trained using the full semantic parses. More detail can be found in the document and labeling instructions included in this release, as well as the paper."}, {"id": "omd-oxford-multimotion-dataset", "name": "OMD (Oxford Multimotion Dataset)", "description": "The Oxford Multimotion Dataset (OMD) provides a number of multimotion estimation problems of varying complexity. It includes both complex problems that challenge existing algorithms as well as a number of simpler problems to support development. These include observations from both static and dynamic sensors, a varying number of moving bodies, and a variety of different 3D motions. It also provides a number of experiments designed to isolate specific challenges of the multimotion problem, including rotation about the optical axis and occlusion. In total, the Oxford Multimotion Dataset contains over 110 minutes of multimotion data consisting of stereo and RGB-D camera images, IMU data, and Vicon ground-truth trajectories. The dataset culminates in a complex toy car segment representative of many challenging real-world scenarios."}, {"id": "moroco-moldavian-and-romanian-dialectal-corpus", "name": "MOROCO (MOldavian and ROmanian Dialectal COrpus)", "description": "The MOldavian and ROmanian Dialectal COrpus (MOROCO) is a corpus that contains 33,564 samples of text (with over 10 million tokens) collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports and tech. The data set is divided into 21,719 samples for training, 5,921 samples for validation and another 5,924 samples for testing. "}, {"id": "expmrc", "name": "ExpMRC", "description": "ExpMRC is a benchmark for the Explainability evaluation of Machine Reading Comprehension. ExpMRC contains four subsets of popular MRC datasets with additionally annotated evidences, including SQuAD, CMRC 2018, RACE+ (similar to RACE), and C3, covering span-extraction and multiple-choice questions MRC tasks in both English and Chinese."}, {"id": "next-qa", "name": "NExT-QA", "description": "NExT-QA is a VideoQA benchmark targeting the explanation of video contents. It challenges QA models to reason about the causal and temporal actions and understand the rich object interactions in daily activities. Contains both multi-choice and open-ended QA tasks."}, {"id": "soundingearth", "name": "SoundingEarth", "description": "SoundingEarth consists of co-located aerial imagery and audio samples all around the world."}, {"id": "gem-generation-evaluation-and-metrics", "name": "GEM (Generation, Evaluation, and Metrics)", "description": "Generation, Evaluation, and Metrics (GEM) is a benchmark environment for Natural Language Generation with a focus on its Evaluation, both through human annotations and automated Metrics."}, {"id": "icews", "name": "ICEWS", "description": "A repository that contains political events with a specific timestamp. These political events relate entities (e.g. countries, presidents...) to a number of other entities via logical predicates (e.g. \u2019Make a visit\u2019 or \u2019Express intent to meet or negotiate\u2019)."}, {"id": "spotify-podcast", "name": "Spotify Podcast", "description": "A set of approximately 100K podcast episodes comprised of raw audio files along with accompanying ASR transcripts. This represents over 47,000 hours of transcribed audio, and is an order of magnitude larger than previous speech-to-text corpora. "}, {"id": "clipshots", "name": "ClipShots", "description": "ClipShots is a large-scale dataset for shot boundary detection collected from Youtube and Weibo covering more than 20 categories, including sports, TV shows, animals, etc. In contrast to previous shot boundary detection datasets, e.g. TRECVID and RAI, which only consist of documentaries or talk shows where the frames are relatively static, ClipShots contains moslty short videos from Youtube and Weibo. Many short videos are home-made, with more challenges, e.g. hand-held vibrations and large occlusion. The types of these videos are various, including movie spotlights, competition highlights, family videos recorded by mobile phones etc. Each video has a length of 1-20 minutes. The gradual transitions in the dataset include dissolve, fade in fade out, and sliding in sliding out."}, {"id": "reintel", "name": "ReINTEL", "description": "10,000 news collected from a social network in Vietnam."}, {"id": "scholars-on-twitter", "name": "Scholars on Twitter", "description": "This is a dataset of paired OpenAlex author_ids (https://docs.openalex.org/about-the-data/author) and tweeter_id."}, {"id": "wod-c", "name": "WOD-C", "description": "WOD-C is an evaluation benchmark heading toward robust and reliable 3D perception in autonomous driving. With it, we probe the robustness of 3D detectors and segmentors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:"}, {"id": "hateful-memes-challenge", "name": "Hateful Memes Challenge", "description": "A new challenge set for multimodal classification, focusing on detecting hate speech in multimodal memes."}, {"id": "coveo-data-challenge-dataset", "name": "Coveo Data Challenge Dataset", "description": "The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for \"In-session prediction for purchase intent and recommendations\". The challenge addresses the growing need for reliable predictions within the boundaries of a shopping session, as customer intentions can be different depending on the occasion. The need for efficient procedures for personalization is even clearer if we consider the e-commerce landscape more broadly: outside of giant digital retailers, the constraints of the problem are stricter, due to smaller user bases and the realization that most users are not frequently returning customers. We release a new session-based dataset including more than 30M fine-grained browsing events (product detail, add, purchase), enriched by linguistic behavior (queries made by shoppers, with items clicked and items not clicked after the query) and catalog meta-data (images, text, pricing information). On this dataset, we ask participants to showcase innovative solutions for two open problems: a recommendation task (where a model is shown some events at the start of a session, and it is asked to predict future product interactions); an intent prediction task, where a model is shown a session containing an add-to-cart event, and it is asked to predict whether the item will be bought before the end of the session."}, {"id": "ufpr-alpr", "name": "UFPR-ALPR", "description": "This dataset includes 4,500 fully annotated images (over 30,000 license plate characters) from 150 vehicles in real-world scenarios where both the vehicle and the camera (inside another vehicle) are moving."}, {"id": "clevrtex", "name": "ClevrTex", "description": "ClevrTex is a new benchmark designed as the next challenge to compare, evaluate and analyze algorithms for unsupervised multi-object segmentation. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques."}, {"id": "aishell-1", "name": "AISHELL-1", "description": "AISHELL-1 is a corpus for speech recognition research and building speech recognition systems for Mandarin. "}, {"id": "rof-real-world-occluded-faces", "name": "ROF (Real World Occluded Faces)", "description": "ROF is a dataset for occluded face recognition that contains faces with both upper face occlusion, due to sunglasses, and lower face occlusion, due to masks."}, {"id": "lastfm-asia", "name": "LastFM Asia", "description": "A social network of LastFM users which was collected from the public API in March 2020. Nodes are LastFM users from Asian countries and edges are mutual follower relationships between them. The vertex features are extracted based on the artists liked by the users. The task related to the graph is multinomial node classification - one has to predict the location of users. This target feature was derived from the country field for each user."}, {"id": "deepbeam", "name": "DeepBeam", "description": "It contains 19 HDF5 files that represent a data collection campaign run on the NI mmWave Transceiver System with four SiBeam 60 GHz radio heads and on two Pi-Radio digital 60 GHz radios."}, {"id": "sketchyscene", "name": "SketchyScene", "description": "SketchyScene is a large-scale dataset of scene sketches to advance research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. "}, {"id": "off-near-sequential", "name": "Off_Near_sequential", "description": "SMAC+ offensive near scenario with sequential episodic buffer"}, {"id": "12-scenes", "name": "12 Scenes", "description": "Dataset containing RGB-D data of 4 large scenes, comprising a total of 12 rooms, for the purpose of RGB and RGB-D camera relocalization. The RGB-D data was captured using a Structure.io depth sensor coupled with an iPad color camera. Each room was scanned multiple times, with the multiple sequences run through a global bundle adjustment in order to obtain globally aligned camera poses though all sequences of the same scene."}, {"id": "stanford-dogs", "name": "Stanford Dogs", "description": "The Stanford Dogs dataset contains 20,580 images of 120 classes of dogs from around the world, which are divided into 12,000 images for training and 8,580 images for testing."}, {"id": "quartz-quartz-dataset", "name": "QuaRTz (QuaRTz Dataset)", "description": "QuaRTz is a crowdsourced dataset of 3864 multiple-choice questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs)."}, {"id": "atomic", "name": "ATOMIC", "description": "ATOMIC is an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., \"if X pays Y a compliment, then Y will likely return the compliment\")."}, {"id": "x-srl", "name": "X-SRL", "description": "SRL is the task of extracting semantic predicate-argument structures from sentences. X-SRL is a multilingual parallel Semantic Role Labelling (SRL) corpus for English (EN), German (DE), French (FR) and Spanish (ES) that is based on English gold annotations and shares the same labelling scheme across languages."}, {"id": "imgur5k", "name": "Imgur5K", "description": "Imgur5k is a large-scale handwritten in-the-wild dataset, containing challenging real world handwritten samples from nearly 5K writers. It consists of ~135K handwritten English words from 5K different images. As opposed to existing dataests for OCR which have limited variability in their images, the images in Imgur5K contain a diverse set of styles."}, {"id": "biogrid-biological-general-repository-for-interaction-datasets", "name": "BioGRID (Biological General Repository for Interaction Datasets)", "description": "BioGRID is a biomedical interaction repository with data compiled through comprehensive curation efforts. The current index is version 4.2.192 and searches 75,868 publications for 1,997,840 protein and genetic interactions, 29,093 chemical interactions and 959,750 post translational modifications from major model organism species."}, {"id": "isic-2018-task-3", "name": "ISIC 2018 Task 3", "description": "The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 dataset is the challenge on lesion classification. It includes 2594 images. The task is to classify the dermoscopic images into one of the following categories: melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis / Bowen\u2019s disease, benign keratosis, dermatofibroma, and vascular lesion."}, {"id": "dfdc-deepfake-detection-challenge", "name": "DFDC (Deepfake Detection Challenge)", "description": "The DFDC (Deepfake Detection Challenge) is a dataset for deepface detection consisting of more than 100,000 videos."}, {"id": "lareqa", "name": "LAReQA", "description": "A challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. "}, {"id": "vot2014-visual-object-tracking-challenge-2014", "name": "VOT2014 (Visual Object Tracking Challenge 2014)", "description": "The dataset comprises 25 short sequences showing various objects in challenging backgrounds. Eight sequences are from the VOT2013 challenge (bolt, bicycle, david, diving, gymnastics, hand, sunshade, woman). The new sequences show complementary objects and backgrounds, for example a fish underwater or a surfer riding a big wave. The sequences were chosen from a large pool of sequences using a methodology based on clustering visual features of object and background so that those 25 sequences sample evenly well the existing pool."}, {"id": "hwu64", "name": "HWU64", "description": "This project contains natural language data for human-robot interaction in home domain which we collected and annotated for evaluating NLU Services/platforms."}, {"id": "sun360-scene-understanding-360deg-panorama", "name": "SUN360 (Scene UNderstanding 360\u00b0 panorama)", "description": "The goal of the SUN360 panorama database is to provide academic researchers in computer vision, computer graphics and computational photography, cognition and neuroscience, human perception, machine learning and data mining, with a comprehensive collection of annotated panoramas covering 360x180-degree full view for a large variety of environmental scenes, places and the objects within. To build the core of the dataset, the authors download a huge number of high-resolution panorama images from the Internet, and group them into different place categories. Then, they designed a WebGL annotation tool for annotating the polygons and cuboids for objects in the scene."}, {"id": "the-stack", "name": "The Stack", "description": "The Stack contains over 3TB of permissively-licensed source code files covering 30 programming languages crawled from GitHub. The dataset was created as part of the BigCode Project, an open scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs)."}, {"id": "jsut-corpus", "name": "JSUT Corpus", "description": "JSUT Corpus is a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist."}, {"id": "children-s-book-test", "name": "Children's Book Test", "description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled)."}, {"id": "20-hours-american-english-speech-synthesis-corpus-male", "name": "20 Hours - American English Speech Synthesis Corpus-Male", "description": "Description: Male audio data of American English. It is recorded by American English native speakers, with authentic accent. The phoneme coverage is balanced. Professional phonetician participates in the annotation. It precisely matches with the research and development needs of the speech synthesis."}, {"id": "cube", "name": "Cube++", "description": "Cube++ is a novel dataset for the color constancy problem that continues on the Cube+ dataset. It includes 4890 images of different scenes under various conditions. For calculating the ground truth illumination, a calibration object with known surface colors was placed in every scene."}, {"id": "newstsc", "name": "NewsTSC", "description": "NewsTSC is a dataset for target-dependent sentiment classification (TSC), to investigate TSC in news articles, a much less researched domain, despite the importance of news as an essential information source in individual and societal decision making."}, {"id": "yup-yup-dynamic-scenes-dataset", "name": "YUP++ (YUP++ Dynamic Scenes dataset)", "description": "A new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se. "}, {"id": "autochart", "name": "AutoChart", "description": "AutoChart is a dataset for chart-to-text generation, a task that consists on generating analytical descriptions of visual plots."}, {"id": "mlqe-pe-multilingual-quality-estimation-and-automatic-post-editing-dataset", "name": "MLQE-PE (Multilingual Quality Estimation and Automatic Post-editing Dataset)", "description": "The Multilingual Quality Estimation and Automatic Post-editing (MLQE-PE) Dataset is a dataset for Machine Translation (MT) Quality Estimation (QE) and Automatic Post-Editing (APE). The dataset contains seven language pairs, with human labels for 9,000 translations per language pair in the following formats: sentence-level direct assessments and post-editing effort, and word-level good/bad labels. It also contains the post-edited sentences, as well as titles of the articles where the sentences were extracted from, and the neural MT models used to translate the text."}, {"id": "visdial-visual-dialog", "name": "VisDial (Visual Dialog)", "description": "Visual Dialog (VisDial) dataset contains human annotated questions based on images of MS COCO dataset. This dataset was developed by pairing two subjects on Amazon Mechanical Turk to chat about an image. One person was assigned the job of a \u2018questioner\u2019 and the other person acted as an \u2018answerer\u2019. The questioner sees only the text description of an image (i.e., an image caption from MS COCO dataset) and the original image remains hidden to the questioner. Their task is to ask questions about this hidden image to \u201cimagine the scene better\u201d. The answerer sees the image, caption and answers the questions asked by the questioner. The two of them can continue the conversation by asking and answering questions for 10 rounds at max."}, {"id": "e-commerce", "name": "E-commerce", "description": "We release E-commerce Dialogue Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of E-commerical Conversation Corpus are shown in the following table. "}, {"id": "ucc-unhealthy-comments-corpus", "name": "UCC (Unhealthy Comments Corpus)", "description": "The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online."}, {"id": "kinship", "name": "Kinship", "description": "This relational database consists of 24 unique names in two families (they have equivalent structures)."}, {"id": "svg-icons8", "name": "SVG-Icons8", "description": "A new large-scale dataset along with an open-source library for SVG manipulation."}, {"id": "photosynth", "name": "PhotoSynth", "description": "The PhotoSynth (PS) dataset for patch matching consists of a total of 30 scenes with 25 scenes for training and 5 scenes for validation. The different image pairs are captured in different illumination conditions, at different scales and with different viewpoints."}, {"id": "symbolicdata-a-tree-based-symbolic-dataset-for-symbolic-regression", "name": "SymbolicData (A Tree-based Symbolic Dataset For Symbolic Regression)", "description": "This dataset is a collection of input-label pairs where each input is in the form of a numerical dataset, itself a set of input and output pairs {(x, y)}, and the corresponding label is a string encoding the symbolic expression governing the relationship between variables in the numerical dataset."}, {"id": "p3m-10k-privacy-preserving-portrait-matting-dataset", "name": "P3M-10k (Privacy-Preserving Portrait Matting Dataset)", "description": "P3M-10k contains 10421 high-resolution real-world face-blurred portrait images, along with their manually labeled alpha mattes. The Dataset is aimed to aid research efforts in the area of portrait image matting and related topics."}, {"id": "chebi-20", "name": "ChEBI-20", "description": "Dataset contains 33,010 molecule-description pairs split into 80\\%/10\\%/10\\% train/val/test splits. The goal of the task is to retrieve the relevant molecule for a natural language description. It is defined as follows:"}, {"id": "convref", "name": "ConvRef", "description": "ConvRef is a conversational QA benchmark with reformulations.  It consists of around 11k natural conversations with about 205k reformulations. ConvRef builds upon the conversational KG-QA benchmark ConvQuestions. Questions come from five different domains: books, movies, music, TV series and soccer and answers are Wikidata entities.  We used conversation sessions in ConvQuestions as input to our user study. Study participants interacted with a baseline QA system, that was trained using the available paraphrases in ConvQuestions as proxies for reformulations. Users were shown follow-up questions in a given conversation interactively, one after the other, along with the answer coming from the baseline QA system. For wrong answers, the user was prompted to reformulate the question up to four times if needed. In this way, users were able to pose reformulations based on previous wrong answers and the conversation history."}, {"id": "twitter-conversations-dataset", "name": "Twitter Conversations Dataset", "description": "This dataset is used for the task of conversational document prediction. The dataset includes conversations that occurred between users and customer care agents in 25 organizations on the Twitter platform. Each conversation ends with a customer care agent providing a URL to a document to resolve the issue the user is facing. The task is to predict the document given a dialog context. The train, dev and test datasets include 10000, 525 and 500 conversations respectively."}, {"id": "topology-optimization-dataset", "name": "Topology Optimization Dataset", "description": "TOP is a synthetic dataset for topology optimization generated using Topy. The generated dataset has 10,000 objects which consist on 100 iterations of the optimization process for the problem defined on a regular 40 x 40 grid."}, {"id": "patzig", "name": "Patzig", "description": "Patzig contains handwritten texts written in modern German. Train sample consists of 485 lines, validation - 38 lines and test -118 lines."}, {"id": "openeds", "name": "OpenEDS", "description": "OpenEDS (Open Eye Dataset) is a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eyefacing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. "}, {"id": "cirr-compose-image-retrieval-on-real-life-images", "name": "CIRR (Compose Image Retrieval on Real-life images)", "description": "Composed Image Retrieval (or, Image Retreival conditioned on Language Feedback) is a relatively new retrieval task, where an input query consists of an image and short textual description of how to modify the image. "}, {"id": "gd-vcr", "name": "GD-VCR", "description": "Geo-Diverse Visual Commonsense Reasoning (GD-VCR) is a new dataset to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense."}, {"id": "switchboard-1-corpus", "name": "Switchboard-1 Corpus", "description": "The Switchboard-1 Telephone Speech Corpus (LDC97S62) consists of approximately 260 hours of speech and was originally collected by Texas Instruments in 1990-1, under DARPA sponsorship. The first release of the corpus was published by NIST and distributed by the LDC in 1992-3."}, {"id": "rel3d", "name": "Rel3D", "description": "Understanding spatial relations (e.g., \u201claptop on table\u201d) in visual input is important for both humans and robots. Existing datasets are insufficient as they lack largescale, high-quality 3D ground truth information, which is critical for learning spatial relations. In this paper, we fill this gap by constructing Rel3D: the first large-scale, human-annotated dataset for grounding spatial relations in 3D. Rel3D enables quantifying the effectiveness of 3D information in predicting spatial relations on large-scale human data. Moreover, we propose minimally contrastive data collection\u2014a novel crowdsourcing method for reducing dataset bias. The 3D scenes in our dataset come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. We empirically validate that minimally contrastive examples can diagnose issues with current relation detection models as well as lead to sample-efficient training. Code and data are available at https://github.com/princeton-vl/Rel3D."}, {"id": "english-web-treebank", "name": "English Web Treebank", "description": "English Web Treebank is a dataset containing 254,830 word-level tokens and 16,624 sentence-level tokens of webtext in 1174 files annotated for sentence- and word-level tokenization, part-of-speech, and syntactic structure. The data is roughly evenly divided across five genres: weblogs, newsgroups, email, reviews, and question-answers. The files were manually annotated following the sentence-level tokenization guidelines for web text and the word-level tokenization guidelines developed for English treebanks in the DARPA GALE project. Only text from the subject line and message body of posts, articles, messages and question-answers were collected and annotated."}, {"id": "esad-saras-endoscopic-surgeon-action-detection", "name": "ESAD (SARAS Endoscopic Surgeon Action Detection)", "description": "ESAD is a large-scale dataset designed to tackle the problem of surgeon action detection in endoscopic minimally invasive surgery. ESAD aims at contributing to increase the effectiveness and reliability of surgical assistant robots by realistically testing their awareness of the actions performed by a surgeon. The dataset provides bounding box annotation for 21 action classes on real endoscopic video frames captured during prostatectomy, and was used as the basis of a recent MIDL 2020 challenge. "}, {"id": "dis-rex", "name": "DiS-ReX", "description": "DiS-ReX is a multilingual dataset for distantly supervised (DS) relation extraction (RE). The dataset has over 1.5 million instances, spanning 4 languages (English, Spanish, German and French). The dataset has 36 positive relation types + 1 no relation (NA) class."}, {"id": "mafw", "name": "MAFW", "description": "MAFW is a large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild. It contains 10,045 video-audio clips, annotated with a compound emotional category and a couple of sentences that describe the subjects' affective behaviors in the clip. For the compound emotion annotation, each clip is categorized into one or more of the 11 widely-used emotions, i.e., anger, disgust, fear, happiness, neutral, sadness, surprise, contempt, anxiety, helplessness, and disappointment."}, {"id": "project-codenet", "name": "Project CodeNet", "description": "Project CodeNet is a large-scale dataset with approximately 14 million code samples, each of which is an intended solution to one of 4000 coding problems. The code samples are written in over 50 programming languages (although the dominant languages are C++, C, Python, and Java) and they are annotated with a rich set of information, such as its code size, memory footprint, cpu run time, and status, which indicates acceptance or error types. The dataset is accompanied by a repository, where we provide a set of tools to aggregate codes samples based on user criteria and to transform code samples into token sequences, simplified parse trees and other code graphs. A detailed discussion of Project CodeNet is available in this paper."}, {"id": "mdid-multimodal-document-intent-dataset", "name": "MDID (Multimodal Document Intent Dataset)", "description": "The Multimodal Document Intent Dataset (MDID) is a dataset for computing author intent from multimodal data from Instagram. It contains 1,299 Instagram posts covering a variety of topics, annotated with labels from three taxonomies. The samples are labelled with 7 labels of intent: Provocative, Informative, Advocative, Entertainment, Expositive, Expressive, Promotive"}, {"id": "synthderm", "name": "SynthDerm", "description": "SynthDerm is a synthetically generated dataset inspired by the real-world characteristics of melanoma skin lesions in dermatology settings. These characteristics include whether the lesion is asymmetrical, its border is irregular or jagged, is unevenly colored, has a diameter more than 0.25 inches, or is evolving in size, shape, or color over time. These qualities are usually referred to as ABCDE of melanoma. We generate SynthDerm algorithmically by varying several factors: skin tone, lesion shape, lesion size, lesion location (vertical and horizontal), and whether there are surgical markings present. We randomly assign one of the following to the lesion shape: round, asymmetrical, with jagged borders, or multi-colored (two different shades of colors overlaid with salt-and-pepper noise). For skin tone values, we simulate Fitzpatrick ratings. Fitzpatrick scale is a commonly used approach to classify the skin by its reaction to sunlight exposure modulated by the density of melanin pigments in the skin. This rating has six values, where 1 represents skin that always burns (lowest melanin) and 6 represents skin that never burns in sunlight (highest melanin). For our synthetic generation, we consider six base skin tones that similarly resemble different amounts of melanin. We also add a small amount of random noise to the base color to add further variety. Overall, SynthDerm includes more than 2,600 images of size 64x64."}, {"id": "ukp-ukp-argument-annotated-essays", "name": "UKP (UKP Argument Annotated Essays)", "description": "The UKP Argument Annotated Essays corpus consists of argument annotated persuasive essays including annotations of argument components and argumentative relations."}, {"id": "graspnet-1billion", "name": "GraspNet-1Billion", "description": "GraspNet-1Billion provides large-scale training data and a standard evaluation platform for the task of general robotic grasping. The dataset contains 97,280 RGB-D image with over one billion grasp poses."}, {"id": "wikimulti-wikimulti-a-corpus-for-cross-lingual-summarization", "name": "WikiMulti (WikiMulti: a Corpus for Cross-Lingual Summarization)", "description": "wikimulti is a dataset for cross-lingual summarization based on Wikipedia articles in 15 languages."}, {"id": "article-bias-prediction", "name": "Article Bias Prediction", "description": "The articles crawled from www.allsides.com are available in the ./data folder, along with the different evaluation splits."}, {"id": "advance-audio-visual-aerial-scene-recognition-dataset", "name": "ADVANCE (AuDio Visual Aerial sceNe reCognition datasEt)", "description": "The AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE) is a brand-new multimodal learning dataset, which aims to explore the contribution of both audio and conventional visual messages to scene recognition. This dataset in summary contains 5075 pairs of geotagged aerial images and sounds, classified into 13 scene classes, i.e., airport, sports land, beach, bridge, farmland, forest, grassland, harbor, lake, orchard, residential area, shrub land, and train station."}, {"id": "donerf-evaluation-dataset", "name": "DONeRF: Evaluation Dataset", "description": "This is the dataset for the CGF 2021 paper \"DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks\"."}, {"id": "monoperfcap-dataset", "name": "MonoPerfCap Dataset", "description": "MonoPerfCap is a benchmark dataset for human 3D performance capture from monocular video input  consisting of around 40k frames, which covers a variety of different scenarios."}, {"id": "malex", "name": "MaleX", "description": "MaleX is a curated dataset of malware and benign Windows executable samples for malware researchers. The dataset contains 1,044,394 Windows executable binaries with 864,669 labelled as malware and 179,725 as benign. This dataset has reasonable number of samples and is sufficient to test data-driven machine learning classification methods and also to measure the performance of the designed models in terms of scalability and adaptability."}, {"id": "lodopab-ct", "name": "LoDoPaB-CT", "description": "LoDoPaB-CT is a dataset of computed tomography images and simulated low-dose measurements. It contains over 40,000 scan slices from around 800 patients selected from the LIDC/IDRI Database. "}, {"id": "jet-flavor-dataset", "name": "Jet Flavor dataset", "description": "Dataset for 'Jet Flavor Classification in High-Energy Physics with Deep Neural Networks'"}, {"id": "interhand2-6m", "name": "InterHand2.6M", "description": "The InterHand2.6M dataset is a large-scale real-captured dataset with accurate GT 3D interacting hand poses, used for 3D hand pose estimation The dataset contains 2.6M labeled single and interacting hand frames."}, {"id": "msu-super-resolution-for-video-compression", "name": "MSU Super-Resolution for Video Compression", "description": "This is a dataset for a super-resolution task. The dataset contains 480x270 videos that were decoded with 6 different bitrates (100 - 4000 kbps) using 5 different codecs (H.264, H.265, H.266, AV1, and AVS3 standards). The dataset contains indoor and outdoor videos as well as animation. All videos have low SI/TI values and simple textures. It was made to minimize compression artifacts that may occur to make restoration of details possible."}, {"id": "mot15-multiple-object-tracking-15", "name": "MOT15 (Multiple Object Tracking 15)", "description": "MOT2015 is a dataset for multiple object tracking. It contains 11 different indoor and outdoor scenes of public places with pedestrians as the objects of interest, where camera motion, camera angle and imaging condition vary greatly. The dataset provides detections generated by the ACF-based detector."}, {"id": "ssl4eo-s12", "name": "SSL4EO-S12", "description": "SSL4EO-S12 is a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 & -2 satellite missions."}, {"id": "rit-18", "name": "RIT-18", "description": "The RIT-18 dataset was built for the semantic segmentation of remote sensing imagery. It was collected with the Tetracam Micro-MCA6 multispectral imaging sensor flown on-board a DJI-1000 octocopter. "}, {"id": "spmrl-hebrew-segmentation-data", "name": "SPMRL Hebrew segmentation data", "description": "Training data for Hebrew morphological word segmentation"}, {"id": "ranking-social-media-news-feed", "name": "Ranking social media news feed", "description": "A dataset consisting of recipient 46 users and, 26180 tweets. The dataset includes the news feed of the users and 13 features that may influence the relevance of the tweets."}, {"id": "how2qa", "name": "How2QA", "description": "To collect How2QA for video QA task, the same set of selected video clips are presented to another group of AMT workers for multichoice QA annotation. Each worker is assigned with one video segment and asked to write one question with four answer candidates (one correctand three distractors). Similarly, narrations are hidden from the workers to ensure the collected QA pairs are not biased by subtitles. Similar to TVQA, the start and end points are provided for the relevant moment for each question. After filtering low-quality annotations, the final dataset contains 44,007 QA pairs for 22k 60-second clips selected from 9035 videos."}, {"id": "ptc-predictive-toxicology-challenge", "name": "PTC (Predictive Toxicology Challenge)", "description": "PTC is a collection of 344 chemical compounds represented as graphs which report the carcinogenicity for rats. There are 19 node labels for each node."}, {"id": "lecturebank", "name": "LectureBank", "description": "LectureBank Dataset is a manually collected dataset of lecture slides. It contains 1,352 online lecture files from 60 courses covering 5 different domains, including Natural Language Processing (nlp), Machine Learning (ml), Artificial Intelligence (ai), Deep Learning (dl) and Information Retrieval (ir). In addition, it also contains the corresponding annotations for each slide."}, {"id": "ted-lium-3", "name": "TED-LIUM 3", "description": "TED-LIUM 3 is an audio dataset collected from TED Talks. It contains:"}, {"id": "wmt-2020", "name": "WMT 2020", "description": "WMT 2020 is a collection of datasets used in shared tasks of the Fifth Conference on Machine Translation. The conference builds on a series of annual workshops and conferences on  Statistical Machine Translation."}, {"id": "toyota-smarthome-dataset-toyota-smarthome-trimmed", "name": "Toyota Smarthome dataset (Toyota Smarthome Trimmed)", "description": "Toyota Smarthome Trimmed has been designed for the activity classification task of 31 activities. The videos were clipped per activity, resulting in a total of 16,115 short RGB+D video samples.  activities were performed in a natural manner. As a result, the dataset poses a unique combination of challenges: high intra-class variation, high-class imbalance, and activities with similar motion and high duration variance. Activities were annotated with both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome Trimmed from other datasets for activity classification."}, {"id": "pytorrent", "name": "PyTorrent", "description": "PyTorrent  contains 218,814 Python package libraries from PyPI and Anaconda environment. This is because earlier studies have shown that much of the code is redundant and Python packages from these environments are better in quality and are well-documented. PyTorrent enables users (such as data scientists, students, etc.) to build off the shelf machine learning models directly without spending months of effort on large infrastructure."}, {"id": "fsns-test", "name": "FSNS - Test", "description": "Arabic handwriting dataset."}, {"id": "long-video-dataset", "name": "Long Video Dataset", "description": "We randomly selected three videos from the Internet, that are longer than 1.5K frames and have their main objects continuously appearing. Each video has 20 uniformly sampled frames manually annotated for evaluation."}, {"id": "hjdataset", "name": "HJDataset", "description": "HJDataset is a large dataset of Historical Japanese Documents with Complex Layouts. It contains over 250,000 layout element annotations of seven types. In addition to bounding boxes and masks of the content regions, it also includes the hierarchical structures and reading orders for layout elements. The dataset is constructed using a combination of human and machine efforts. "}, {"id": "biodivtab", "name": "BiodivTab", "description": "The BioDiv dataset includes manually labeled tables for CTA and CEA from the biodiversity domain."}, {"id": "ai2d-rst", "name": "AI2D-RST", "description": "AI2D-RST is a multimodal corpus of 1000 English-language diagrams that represent topics in primary school natural sciences, such as food webs, life cycles, moon phases and human physiology. The corpus is based on the Allen Institute for Artificial Intelligence Diagrams (AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which was originally developed to support research on automatic diagram understanding and visual question answering. "}, {"id": "aeslc", "name": "AESLC", "description": "To study the task of email subject line generation: automatically generating an email subject line from the email body. "}, {"id": "howtovqa69m", "name": "HowToVQA69M", "description": "A dataset of 69,270,581 video clip, question and answer triplets (v, q, a). HowToVQA69M is two orders of magnitude larger than any of the currently available VideoQA datasets."}, {"id": "aishell-3", "name": "AISHELL-3", "description": "AISHELL-3 is a large-scale and high-fidelity multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-to-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers and total 88035 utterances. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Accordingly, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. The  word & tone transcription accuracy rate is above 98%, through professional speech annotation and strict quality inspection for tone and prosody."}, {"id": "tsinghua-tencent-100k-traffic-sign-detection-and-classification-in-the-wild", "name": "Tsinghua-Tencent 100K (Traffic-Sign Detection and Classification in the Wild)", "description": "Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas, going beyond previous benchmarks. We call this benchmark Tsinghua-Tencent 100K. It provides 100000 images containing 30000 traffic-sign instances. These images cover large variations in illuminance and weather conditions. Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify traffic-signs. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available."}, {"id": "mit-indoor-scenes", "name": "MIT Indoor Scenes", "description": "Context This is the Original data provided by MIT ."}, {"id": "tempo-localizing-moments-in-video-with-temporal-language", "name": "TEMPO (Localizing Moments in Video with Temporal Language)", "description": "TEMPOral reasoning in video and language (TEMPO) is a dataset that consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language)."}, {"id": "consep-colorectal-nuclear-segmentation-and-phenotypes", "name": "CoNSeP (Colorectal Nuclear Segmentation and Phenotypes)", "description": "The colorectal nuclear segmentation and phenotypes (CoNSeP) dataset consists of 41 H&E stained image tiles, each of size 1,000\u00d71,000 pixels at 40\u00d7 objective magnification. The images were extracted from 16 colorectal adenocarcinoma (CRA) WSIs, each belonging to an individual patient, and scanned with an Omnyx VL120 scanner within the department of pathology at University Hospitals Coventry and Warwickshire, UK."}, {"id": "loli-phone", "name": "LoLi-Phone", "description": "LoLi-Phone is a large-scale low-light image and video dataset for Low-light image enhancement (LLIE). The images and videos are taken by different mobile phones' cameras under diverse illumination conditions."}, {"id": "crello-crello-dataset", "name": "Crello (Crello dataset)", "description": "Crello dataset consists of design templates obtained from online design service, crello.com. The dataset contains designs for various display formats, such as social media posts, banner ads, blog headers, or printed posters, all in a vector format. In dataset construction, design templates and associated resources (e.g., linked images) from crello.com were first downloaded. After the initial data acquisition, the data structure was inspected and identified useful vector graphic information in each template. Next, mal-formed templates or those having more than 50 elements were eliminated, resulting in 23,182 templates. The data was paritioned to 18,768 / 2,315 / 2,278 examples for train, validation, and test splits."}, {"id": "fiw-families-in-the-wild", "name": "FIW (Families In The Wild)", "description": "FIW is a large and comprehensive database available for kinship recognition. FIW is made up of 11,932 natural family photos of 1,000 families-- nearly 10x more than the next-to-largest, Family-101 database. Also, it contains 656,954 image pairs split between the 11 relationships, which is much larger than the 2nd to largest KinFaceW-II with 2,000 pairs for only 4 kinship types."}, {"id": "scicap", "name": "SCICAP", "description": "SCICAP is a large-scale image captioning dataset that contains real-world scientific figures and captions. SCICAP was constructed using more than two million images from over 290,000 papers collected and released by arXiv."}, {"id": "ebb", "name": "EBB!", "description": "This dataset contains around 5K pairs of aligned images captured using Canon 70D DSLR with low and high apertures, modeling normal photos and photos with bokeh (blur) effect. The height of each image in the dataset is 1024 pixels, the width varies over the images."}, {"id": "zhihurec", "name": "ZhihuRec", "description": "ZhihuRec dataset is collected from a knowledge-sharing platform (Zhihu), which is composed of around 100M interactions collected within 10 days, 798K users, 165K questions, 554K answers, 240K authors, 70K topics, and more than 501K user query logs. There are also descriptions of users, answers, questions, authors, and topics, which are anonymous. To the best of our knowledge, this is the largest real-world interaction dataset for personalized recommendation."}, {"id": "tvr-tv-show-retrieval", "name": "TVR (TV show Retrieval)", "description": "A new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. "}, {"id": "imc-phototourism-image-matching-challenge-phototourism", "name": "IMC PhotoTourism (Image Matching Challenge Phototourism)", "description": "Dataset provided by the Image Matching Workshop"}, {"id": "messidor-messidor-database", "name": "MESSIDOR (MESSIDOR DATABASE)", "description": "The Messidor database has been established to facilitate studies on computer-assisted diagnoses of diabetic retinopathy. The research community is welcome to test its algorithms on this database. In this section, you will find instructions on how to download the database."}, {"id": "v2xset", "name": "V2XSet", "description": "A large-scale V2X perception dataset using CARLA and OpenCDA"}, {"id": "dvs128-gesture", "name": "DVS128 Gesture", "description": "Comprises 11 hand gesture categories from 29 subjects under 3 illumination conditions."}, {"id": "collab", "name": "COLLAB", "description": "COLLAB is a scientific collaboration dataset. A graph corresponds to a researcher\u2019s ego network, i.e., the researcher and its collaborators are nodes and an edge indicates collaboration between two researchers. A researcher\u2019s ego network has three possible labels, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics, which are the fields that the researcher belongs to. The dataset has 5,000 graphs and each graph has label 0, 1, or 2."}, {"id": "semanticposs", "name": "SemanticPOSS", "description": "The SemanticPOSS dataset for 3D semantic segmentation contains 2988 various and complicated LiDAR scans with large quantity of dynamic instances. The data is collected in Peking University and uses the same data format as SemanticKITTI."}, {"id": "flickr30k-entities", "name": "Flickr30K Entities", "description": "The Flickr30K Entities dataset is an extension to the Flickr30K dataset. It augments the original 158k captions with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. This is used to define a new benchmark for localization of textual entity mentions in an image."}, {"id": "konzil-konzilsprotokolle-c", "name": "Konzil (Konzilsprotokolle_C)", "description": "Konzil dataset was created by specialists of the University of Greifswald. It contains manuscripts written in modern German. Train sample consists of 353 lines, validation - 29 lines and test - 87 lines."}, {"id": "2ballellsbergdata-two-ball-ellsberg-paradox-representative-us-data", "name": "2BallEllsbergData (Two Ball Ellsberg Paradox: Representative US Data)", "description": "Two Ball Ellsberg Paradox: Representative US Data. You can find all the Stata code for data analysis, including commented lines, explanations and step-by-step implementation of ORIV."}, {"id": "mkqa-multilingual-knowledge-questions-and-answers", "name": "MKQA (Multilingual Knowledge Questions and Answers)", "description": "Multilingual Knowledge Questions and Answers (MKQA) is an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). The goal of this dataset is to provide a challenging benchmark for question answering quality across a wide set of languages. Answers are based on a language-independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering."}, {"id": "aflw-annotated-facial-landmarks-in-the-wild", "name": "AFLW (Annotated Facial Landmarks in the Wild)", "description": "The Annotated Facial Landmarks in the Wild (AFLW) is a large-scale collection of annotated face images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total about 25K faces are annotated with up to 21 landmarks per image."}, {"id": "supercaustics", "name": "SuperCaustics", "description": "SuperCaustics is a simulation tool made in Unreal Engine for generating massive computer vision datasets that include transparent objects."}, {"id": "kitti-mots-kitti-multi-object-tracking-and-segmentation-mots-evaluation", "name": "KITTI MOTS (KITTI Multi-Object Tracking and Segmentation (MOTS) Evaluation)", "description": "The Multi-Object and Segmentation (MOTS) benchmark [2] consists of 21 training sequences and 29 test sequences. It is based on the KITTI Tracking Evaluation 2012 and extends the annotations to the Multi-Object and Segmentation (MOTS) task. To this end, we added dense pixel-wise segmentation labels for every object. We evaluate submitted results using the metrics HOTA, CLEAR MOT, and MT/PT/ML. We rank methods by HOTA [1]. Our development kit and GitHub evaluation code provide details about the data format as well as utility functions for reading and writing the label files. (adapted for the segmentation case). Evaluation is performed using the code from the TrackEval repository."}, {"id": "bioasq-biomedical-semantic-indexing-and-question-answering", "name": "BioASQ (Biomedical Semantic Indexing and Question Answering)", "description": "BioASQ is a question answering dataset. Instances in the BioASQ dataset are composed of a question (Q), human-annotated answers (A), and the relevant contexts (C) (also called snippets)."}, {"id": "snips-snips-natural-language-understanding-benchmark", "name": "SNIPS (SNIPS Natural Language Understanding benchmark)", "description": "The SNIPS Natural Language Understanding benchmark is a dataset of over 16,000 crowdsourced queries distributed among 7 user intents of various complexity:"}, {"id": "ucr-anomaly-archive", "name": "UCR Anomaly Archive", "description": "The UCR Anomaly Archive is a collection of 250 uni-variate time series collected in human medicine, biology, meteorology and industry. The collected time series contain a few natural anomalies though the majority of the anomalies are artificial . The dataset was first used in an anomaly detection contest preceding the ACM SIGKDD conference 2021. Each of the time series contains exactly one, occasionally subtle anomaly after a given time stamp. The data before that timestamp can be considered normal. The time series collected in the UCR Anomaly Archive can be categorized into 12 types originating from the four domains human medicine, meteorology, biology and industry. The distribution across the domains is highly imbalanced with around 64% of the times series being collected in human medicine applications, 22% in biology, 9% in industry and 5% being air temperature measurements. The time series within a single type (e.g. ECG) are not completely unique, but differ in terms of injected anomalies or a modification of the original time series through added Gaussian noise and wandering baselines. "}, {"id": "mhp-multiple-human-parsing", "name": "MHP (Multiple-Human Parsing)", "description": "The MHP dataset contains multiple persons captured in real-world scenes with pixel-level fine-grained semantic annotations in an instance-aware setting."}, {"id": "gta-im-dataset-gta-indoor-motion", "name": "GTA-IM Dataset (GTA Indoor Motion)", "description": "The GTA Indoor Motion dataset (GTA-IM) that emphasizes human-scene interactions in the indoor environments. It consists of HD RGB-D image sequences of 3D human motion from a realistic game engine. The dataset has clean 3D human pose and camera pose annotations, and large diversity in human appearances, indoor environments, camera views, and human activities."}, {"id": "fm2-foolmetwice", "name": "FM2 (FoolMeTwice)", "description": "FoolMeTwice (FM2 for short) is a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using \"shortcuts\" compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players \"pay\" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks."}, {"id": "rhythmic-gymnastic", "name": "Rhythmic Gymnastic", "description": "The Rhythmic Gymnastics dataset contains videos of four different types of gymnastics routines: ball, clubs, hoop and ribbon. Each type of routine has 250 associated videos, and the length of each video is approximately 1 min 35 s. We chose high-standard international competition videos, including videos from the 36th and 37th International Artistic Gymnastics Competitions, to construct the dataset. We have edited out the irrelevant parts of the original videos (such as replay shots and athlete warmups). We have annotated each video with three scores (a difficulty score, an execution score and a total score), which were given by the referee in accordance with the official scoring system."}, {"id": "camo-camouflaged-object", "name": "CAMO (Camouflaged Object)", "description": "Camouflaged Object (CAMO) dataset specifically designed for the task of camouflaged object segmentation. We focus on two categories, i.e., naturally camouflaged objects and artificially camouflaged objects, which usually correspond to animals and humans in the real world, respectively. Camouflaged object images consists of 1250 images (1000 images for the training set and 250 images for the testing set). Non-camouflaged object images are collected from the MS-COCO dataset (1000 images for the training set and 250 images for the testing set). CAMO has objectness mask ground-truth."}, {"id": "unsw-nb15-unsq-nb15", "name": "UNSW-NB15 (UNSQ-NB15)", "description": "UNSW-NB15 is a network intrusion dataset. It contains nine different attacks, includes DoS, worms, Backdoors, and Fuzzers. The dataset contains raw network packets. The number of records in the training set is 175,341 records and the testing set is 82,332 records from the different types, attack and normal."}, {"id": "savee-surrey-audio-visual-expressed-emotion", "name": "SAVEE (Surrey Audio-Visual Expressed Emotion)", "description": "The Surrey Audio-Visual Expressed Emotion (SAVEE) dataset was recorded as a pre-requisite for the development of an automatic emotion recognition system. The database consists of recordings from 4 male actors in 7 different emotions, 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically-balanced for each emotion. The data were recorded in a visual media lab with high quality audio-visual equipment, processed and labeled. To check the quality of performance, the recordings were evaluated by 10 subjects under audio, visual and audio-visual conditions. Classification systems were built using standard features and classifiers for each of the audio, visual and audio-visual modalities, and speaker-independent recognition rates of 61%, 65% and 84% achieved respectively."}, {"id": "clipart1k", "name": "Clipart1k", "description": "In Clipart1k, the target domain classes to be detected are the same as those in the source domain. All the images for a clipart domain were collected from one dataset (i.e., CMPlaces) and two image search engines (i.e., Openclipart2 and Pixabay3). Search queries used are 205 scene classes (e.g., pasture) used in CMPlaces to collect various objects and scenes with complex backgrounds."}, {"id": "amz-computers-amazon-electronics-computers", "name": "AMZ Computers (amazon_electronics_computers)", "description": "AMZ Computers is a co-purchase graph extracted from Amazon, where nodes represent products, edges represent the co-purchased relations of products, and features are bag-of-words vectors extracted from product reviews."}, {"id": "7-scenes", "name": "7-Scenes", "description": "The 7-Scenes dataset is a collection of tracked RGB-D camera frames. The dataset may be used for evaluation of methods for different applications such as dense tracking and mapping and relocalization techniques. All scenes were recorded from a handheld Kinect RGB-D camera at 640\u00d7480 resolution. The dataset creators use an implementation of the KinectFusion system to obtain the \u2018ground truth\u2019 camera tracks, and a dense 3D model. Several sequences were recorded per scene by different users, and split into distinct training and testing sequence sets."}, {"id": "soccernet-v2", "name": "SoccerNet-v2", "description": "A novel large-scale corpus of manual annotations for the SoccerNet video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production."}, {"id": "qc-science", "name": "QC-Science", "description": "QC-Science contains 47832 question-answer pairs belonging to the science domain tagged with labels of the form subject - chapter - topic. The dataset was collected with the help of a leading e-learning platform. The dataset consists of 40895 samples for training, 2153 samples for validation and 4784 samples for testing."}, {"id": "textocr", "name": "TextOCR", "description": "TextOCR is a dataset to benchmark text recognition on arbitrary shaped scene-text. TextOCR requires models to perform text-recognition on arbitrary shaped scene-text present on natural images. TextOCR provides ~1M high quality word annotations on TextVQA images allowing application of end-to-end reasoning on downstream tasks such as visual question answering or image captioning."}, {"id": "rose-retinal-octa-segmentation-dataset", "name": "ROSE (Retinal OCTA SEgmentation dataset)", "description": "Retinal OCTA SEgmentation dataset (ROSE) consists of 229 OCTA images with vessel annotations at either centerline-level or pixel level."}, {"id": "v3c-vimeo-creative-commons-collection", "name": "V3C (Vimeo Creative Commons Collection)", "description": "The Vimeo Creative Commons Collection, in short V3C, is a collection of 28\u2019450 videos (with overall length of about 3\u2019800\u00a0h) published under creative commons license on Vimeo. V3C comes with a shot segmentation for each video, together with the resulting keyframes in original as well as reduced resolution and additional metadata. It is intended to be used from 2019 at the International large-scale TREC Video Retrieval Evaluation campaign (TRECVid)."}, {"id": "hotpotqa", "name": "HotpotQA", "description": "HotpotQA is a question answering dataset collected on the English Wikipedia, containing about 113K crowd-sourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. "}, {"id": "chinese-text-in-the-wild", "name": "Chinese Text in the Wild", "description": "Chinese Text in the Wild is a dataset of Chinese text with about 1 million Chinese characters from 3850 unique ones annotated by experts in over 30000 street view images. This is a challenging dataset with good diversity containing planar text, raised text, text under poor illumination, distant text, partially occluded text, etc."}, {"id": "disfl-qa", "name": "Disfl-QA", "description": "Disfl-QA is a targeted dataset for contextual disfluencies in an information seeking setting, namely question answering over Wikipedia passages. Disfl-QA builds upon the SQuAD-v2 dataset, where each question in the dev set is annotated to add a contextual disfluency using the paragraph as a source of distractors."}, {"id": "food-101n", "name": "Food-101N", "description": "The Food-101N dataset is introduced in \"CleanNet: Transfer Learning for Scalable  Image Training with Label Noise (CVPR'18). It is an image dataset containing about 310,009 images of food recipes classified in 101 classes (categories). Food-101N and the Food-101 dataset share the same 101 classes, whereas Food-101N has much more images and is more noisy."}, {"id": "lc-quad-2-0-largescale-complex-question-answering-dataset", "name": "LC-QuAD 2.0 (Largescale Complex Question Answering Dataset)", "description": "LC-QuAD 2.0 is a Large Question Answering dataset with 30,000 pairs of question and its corresponding SPARQL query. The target knowledge base is Wikidata and DBpedia, specifically the 2018 version."}, {"id": "srd-shadow-removal-dataset", "name": "SRD (Shadow Removal Dataset)", "description": "SRD is a dataset for shadow removal that contains 3088 shadow and shadow-free image pairs."}, {"id": "paq-probably-asked-questions", "name": "PAQ (Probably Asked Questions)", "description": "Probably Asked Questions (PAQ) is a very large resource of 65M automatically-generated QA-pairs. PAQ is a semi-structured Knowledge Base (KB) of 65M natural language QA-pairs, which models can memorise and/or learn to retrieve from. PAQ differs from traditional KBs in that questions and answers are stored in natural language, and that questions are generated such that they are likely to appear in ODQA datasets. PAQ is automatically constructed using a question generation model and Wikipedia."}, {"id": "taco", "name": "TACO", "description": "TACO is a growing image dataset of waste in the wild. It contains images of litter taken under diverse environments: woods, roads and beaches. These images are manually labelled and segmented according to a hierarchical taxonomy to train and evaluate object detection algorithms. The annotations are provided in COCO format."}, {"id": "parcorfull-parallel-corpus-annotated-with-full-coreference", "name": "ParCorFull (Parallel Corpus Annotated with Full Coreference)", "description": "ParCorFull is a parallel corpus annotated with full coreference chains that has been created to address an important problem that machine translation and other multilingual natural language processing (NLP) technologies face -- translation of coreference across languages. This corpus contains parallel texts for the language pair English-German, two major European languages. Despite being typologically very close, these languages still have systemic differences in the realisation of coreference, and thus pose problems for multilingual coreference resolution and machine translation. This parallel corpus covers the genres of planned speech (public lectures) and newswire. It is richly annotated for coreference in both languages, including annotation of both nominal coreference and reference to antecedents expressed as clauses, sentences and verb phrases."}, {"id": "tabfact", "name": "TabFact", "description": "TabFact is a large-scale dataset which consists of 117,854 manually annotated statements with regard to 16,573 Wikipedia tables, their relations are classified as ENTAILED and REFUTED. TabFact is the first dataset to evaluate language inference on structured data, which involves mixed reasoning skills in both symbolic and linguistic aspects. "}, {"id": "panda", "name": "PANDA", "description": "PANDA is the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world scenes with both wide field-of-view (~1 square kilometer area) and high-resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100x scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions."}, {"id": "fbms-freiburg-berkeley-motion-segmentation", "name": "FBMS (Freiburg-Berkeley Motion Segmentation)", "description": "The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is an extension of the BMS dataset with 33 additional video sequences. A total of 720 frames is annotated. It has pixel-accurate segmentation annotations of moving objects. FBMS-59 comes with a split into a training set and a test set."}, {"id": "emovie", "name": "EMOVIE", "description": "EMOVIE is a Mandarin emotion speech dataset including 9,724 samples with audio files and its emotion human-labeled annotation."}, {"id": "alpix-vsr-dataset", "name": "ALPIX-VSR (Dataset)", "description": "we collected a new real-world dataset, called ALPIXVSR, using a ALPIX-Eiger event camera1 . The camera outputs well aligned RGB frames and events. The RGB frames enjoy a resolution of 3264 \u00d7 2448 and are generated by a carefully designed image signal processor(ISP) from RAW data with the Quad Bayer pattern , and the events have a resolution with 1632 \u00d7 1224."}, {"id": "realy-region-aware-benchmark-based-on-the-lyhm", "name": "REALY (Region-aware benchmark based on the LYHM)", "description": "The REALY benchmark aims to introduce a region-aware evaluation pipeline to measure the fine-grained normalized mean square error (NMSE) of 3D face reconstruction methods from under-controlled image sets."}, {"id": "rtasc-robin-technical-acquisition-speech-corpus", "name": "RTASC (ROBIN Technical Acquisition Speech Corpus)", "description": "The ROBIN Technical Acquisition Speech Corpus (ROBINTASC) was developed within the ROBIN project. Its main purpose was to improve the behaviour of a conversational agent, allowing human-machine interaction in the context of purchasing technical equipment. It contains over 6 hours of read speech in Romanian language. We provide text files, associated speech files (WAV, 44.1KHz, 16-bit, single channel), annotated text files in CoNLL-U format."}, {"id": "20-mad-20-mad-mozilla-apache-dataset", "name": "20-MAD (20-MAD: Mozilla Apache Dataset)", "description": "20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects. It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue comments, and its compressed size is over 6 GB. The data contains all the typical information about source code commits (e.g., lines added and removed, message and commit time) and issues (status, severity, votes, and summary). The issue comments have been pre-processed for natural language processing and sentiment analysis. This includes emoticons and valence and arousal scores."}, {"id": "kilt-kilt-benchmark", "name": "KILT (KILT Benchmark)", "description": "KILT (Knowledge Intensive Language Tasks) is a benchmark consisting of 11 datasets representing 5 types of tasks:"}, {"id": "nli-tr-natural-language-inference-in-turkish", "name": "NLI-TR (Natural Language Inference in Turkish)", "description": "Natural Language Inference in Turkish (NLI-TR) provides translations of two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. "}, {"id": "cure-tsd-cure-traffic-sign-detection", "name": "CURE-TSD (CURE Traffic Sign Detection)", "description": "Based on simulated challenging conditions that correspond to adversaries that can occur in real-world environments and systems. "}, {"id": "ucy", "name": "UCY", "description": "The UCY dataset consist of real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (\u0394t=0.4s). It is composed of three sequences (Zara01, Zara02, and UCY), taken in public spaces from top-view."}, {"id": "dart", "name": "DART", "description": "DART is a large dataset for open-domain structured data record to text generation. DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set."}, {"id": "flickrlogos-32", "name": "FlickrLogos-32", "description": "Object detection benchmark for logo detection."}, {"id": "clue-chinese-language-understanding-evaluation-benchmark", "name": "CLUE (Chinese Language Understanding Evaluation Benchmark)", "description": "CLUE is a Chinese Language Understanding Evaluation benchmark. It consists of different NLU datasets. It is a community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text."}, {"id": "atd-12k", "name": "ATD-12K", "description": "ATD-12K is a large-scale animation triplet dataset, which comprises 12,000 triplets(train10k,test2k) by manually inspect and the test2k with rich annotations, including levels of difficulty, the Regions of Interest (RoIs) on movements, and tags on motion categories"}, {"id": "qampari", "name": "QAMPARI", "description": "QAMPARI is an ODQA benchmark, where question answers are lists of entities, spread across many paragraphs. It was created by (a) generating questions with multiple answers from Wikipedia's knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer."}, {"id": "cail2019-scm", "name": "CAIL2019-SCM", "description": "Chinese AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets."}, {"id": "ubfc-rppg-univ-bourgogne-franche-comte-remote-photoplethysmography", "name": "UBFC-rPPG (Univ. Bourgogne Franche-Comt\u00e9 Remote PhotoPlethysmoGraphy)", "description": "We introduce here a new database called UBFC-rPPG (stands for Univ. Bourgogne Franche-Comt\u00e9 Remote PhotoPlethysmoGraphy) comprising two datasets that are focused specifically on rPPG analysis. The UBFC-RPPG database was created using a custom C++ application for video acquisition with a simple low cost webcam (Logitech C920 HD Pro) at 30fps with a resolution of 640x480 in uncompressed 8-bit RGB format. A CMS50E transmissive pulse oximeter was used to obtain the ground truth PPG data comprising the PPG waveform as well as the PPG heart rates. During the recording, the subject sits in front of the camera (about 1m away from the camera) with his/her face visible. All experiments are conducted indoors with a varying amount of sunlight and indoor illumination. The link to download the complete video dataset is available on request. A basic Matlab implementation can also be provided to read ground truth data acquired with a pulse oximeter."}, {"id": "mutualfriends", "name": "MutualFriends", "description": "In MutualFriends, two agents, A and B, each have a private knowledge base, which contains a list of friends with multiple attributes (e.g., name, school, major, etc.). The agents must chat with each other to find their unique mutual friend."}, {"id": "vnds-vnds-a-vietnamese-dataset-for-summarization", "name": "VNDS (VNDS: A Vietnamese Dataset for Summarization)", "description": "A single-document Vietnamese summarization dataset"}, {"id": "anatomy-of-video-editing-ave", "name": "Anatomy of Video Editing (AVE)", "description": "Machine learning is transforming the video editing industry. Recent advances in computer vision have leveled-up video editing tasks such as intelligent reframing, rotoscoping, color grading, or applying digital makeups. However, most of the solutions have focused on video manipulation and VFX. This work introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster research in AI-assisted video editing. Our benchmark suite focuses on video editing tasks, beyond visual effects, such as automatic footage organization and assisted video assembling. To enable research on these fronts, we annotate more than 1.5M tags, with relevant concepts to cinematography, from 196176 shots sampled from movie scenes. We establish competitive baseline methods and detailed analyses for each of the tasks. We hope our work sparks innovative research towards underexplored areas of AI-assisted video editing."}, {"id": "repack", "name": "RePack", "description": "RePack is a dataset to study the detection of repackaged Android apps."}, {"id": "kumar", "name": "Kumar", "description": "The Kumar dataset contains 30 1,000\u00d71,000 image tiles from seven organs (6 breast, 6 liver, 6 kidney, 6 prostate, 2 bladder, 2 colon and 2 stomach) of The Cancer Genome Atlas (TCGA) database acquired at 40\u00d7 magnification. Within each image, the boundary of each nucleus is fully annotated."}, {"id": "pats-pose-audio-transcript-style", "name": "PATS (Pose Audio Transcript Style)", "description": "PATS dataset consists of a diverse and large amount of aligned pose, audio and transcripts. With this dataset, we hope to provide a benchmark that would help develop technologies for virtual agents which generate natural and relevant gestures."}, {"id": "tough-tables", "name": "Tough Tables", "description": "The ToughTables (2T) dataset was created for the SemTab challenge and includes 180 tables in total. The tables in this dataset can be categorized in two groups: the control (CTRL) group tables and tough (TOUGH) group tables. "}, {"id": "maniskill", "name": "ManiSkill", "description": "ManiSkill is a large-scale learning-from-demonstrations benchmark for articulated object manipulation with visual input (point cloud and image). ManiSkill supports object-level variations by utilizing a rich and diverse set of articulated objects, and each task is carefully designed for learning manipulations on a single category of objects. ManiSkill is equipped with high-quality demonstrations to facilitate learning-from-demonstrations approaches and perform evaluations on common baseline algorithms. ManiSkill can encourage the robot learning community to explore more on learning generalizable object manipulation skills."}, {"id": "openforensics", "name": "OpenForensics", "description": "OpenForensics is a large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation. With its rich annotations, the OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection."}, {"id": "middlebury-2006", "name": "Middlebury 2006", "description": "The Middlebury 2006 is a stereo dataset of indoor scenes with multiple handcrafted layouts."}, {"id": "center-tbi-collaborative-european-neurotrauma-effectiveness-research-in-tbi", "name": "CENTER-TBI (Collaborative European NeuroTrauma Effectiveness Research in TBI)", "description": "The CENTER-TBI database contains prospectively collected data of more than 4,500 patients with TBI in Europe. The Registry and Acute Care data has been collected during a 3 years\u2019 period (2015-2017) in 65 centers in Europe. For all patients, outcome data has been collected up to 2 years after injury."}, {"id": "set12", "name": "Set12", "description": "Set12 is a collection of 12 grayscale images of different scenes that are widely used for evaluation of image denoising methods. The size of each image is 256\u00d7256."}, {"id": "marine-debris-turntable", "name": "Marine Debris Turntable", "description": "Marine Debris Turntable is a dataset for sonar perception."}, {"id": "ui-prmd-university-of-idaho-physical-rehabilitation-movement-dataset", "name": "UI-PRMD (University of Idaho \u2013 Physical Rehabilitation Movement Dataset)", "description": "UI-PRMD is a data set of movements related to common exercises performed by patients in physical therapy and rehabilitation programs. The data set consists of 10 rehabilitation exercises. A sample of 10 healthy individuals repeated each exercise 10 times in front of two sensory systems for motion capturing: a Vicon optical tracker, and a Kinect camera. The data is presented as positions and angles of the body joints in the skeletal models provided by the Vicon and Kinect mocap systems."}, {"id": "4d-temperature-monitoring", "name": "4D Temperature Monitoring", "description": "This Kaggle repository is still under construction (as of October 2022)."}, {"id": "reds-realistic-and-diverse-scenes-dataset-realistic-and-dynamic-scenes", "name": "REDS (REalistic and Diverse Scenes dataset\nrealistic and dynamic scenes)", "description": "The realistic and dynamic scenes (REDS) dataset was proposed in the NTIRE19 Challenge. The dataset is composed of 300 video sequences with resolution of 720\u00d71,280, and each video has 100 frames, where the training set, the validation set and the testing set have 240, 30 and 30 videos, respectively"}, {"id": "supplementary-data-revealing-drivers-and-risks-for-power-grid-frequency-stability-with-explainable-ai", "name": "Supplementary data: \"Revealing drivers and risks for power grid frequency stability with explainable AI\"", "description": "This repository contains processed data and result files for the paper \"Revealing drivers and risks for power grid frequency stability with explainable AI\"."}, {"id": "lks-liver-kidney-stomach", "name": "LKS (Liver Kidney Stomach)", "description": "LKS is a dataset of 684 Liver-Kidney-Stomach immunofluorescence whole slide images (WSIs) used in the investigation of autoimmune liver disease."}, {"id": "off-complicated-parallel-smac-off-complicated-parallel-20", "name": "Off_Complicated_parallel (SMAC+_Off_Complicated_parallel_20)", "description": "smac+ offensive complicated scenario with 20 parallel episodic buffer."}, {"id": "live-yt-hfr-live-youtube-high-frame-rate", "name": "LIVE-YT-HFR (LIVE YouTube High Frame Rate)", "description": "LIVE-YT-HFR comprises of 480 videos having 6 different frame rates, obtained from 16 diverse contents."}, {"id": "stair-captions", "name": "STAIR Captions", "description": "STAIR Captions is a large-scale dataset containing 820,310 Japanese captions. This dataset can be used for caption generation, multimodal retrieval, and image generation."}, {"id": "biored", "name": "BioRED", "description": "BioRED is a first-of-its-kind biomedical relation extraction dataset with multiple entity types (e.g. gene/protein, disease, chemical) and relation pairs (e.g. gene\u2013disease; chemical\u2013chemical) at the document level, on a set of600 PubMed abstracts. Furthermore, BioRED label each relation as describing either a novel finding or previously known background knowledge, enabling automated algorithms to differentiate between novel and background information."}, {"id": "wechat", "name": "WeChat", "description": "The WeChat dataset for fake news detection contains more than 20k news labelled as fake news or not."}, {"id": "trans10k", "name": "Trans10K", "description": "A large-scale dataset for transparent object segmentation, named Trans10K, consisting of 10,428 images of real scenarios with carefully manual annotations, which are 10 times larger than the existing datasets. "}, {"id": "trance-transformation-driven-visual-reasoning", "name": "TRANCE (Transformation Driven Visual Reasoning)", "description": "TRANCE extends CLEVR by asking a uniform question, i.e. what is the transformation between two given images, to test the ability of transformation reasoning. TRANCE includes three levels of settings, i.e. Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transformation with variant views). Detailed information can be found in https://hongxin2019.github.io/TVR."}, {"id": "ccpe-m-coached-conversational-preference-elicitation-dataset-for-movies", "name": "CCPE-M (Coached Conversational Preference Elicitation dataset for Movies)", "description": "A dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language."}, {"id": "booksum", "name": "BookSum", "description": "BookSum is a collection of datasets for long-form narrative summarization. This dataset covers source documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of this dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures."}, {"id": "lrs2-lip-reading-sentences-2", "name": "LRS2 (Lip Reading Sentences 2)", "description": "The Oxford-BBC Lip Reading Sentences 2 (LRS2) dataset is one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists of mainly news and talk shows from BBC programs. Each sentence is up to 100 characters in length. The training, validation and test sets are divided according to broadcast date. It is a challenging set since it contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances and the test set contains 1,242 utterances."}, {"id": "s2looking", "name": "S2Looking", "description": "S2Looking is a building change detection dataset that contains large-scale side-looking satellite images captured at varying off-nadir angles. The S2Looking dataset consists of 5,000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel) of rural areas throughout the world and more than 65,920 annotated change instances. We provide two label maps to separately indicate the newly built and demolished building regions for each sample in the dataset. We establish a benchmark task based on this dataset, i.e., identifying the pixel-level building changes in the bi-temporal images."}, {"id": "fathomnet", "name": "FathomNet", "description": "FathomNet is an open-source image database that can be used to train, test, and validate state-of-the-art artificial intelligence algorithms to help us understand our ocean and its inhabitants. Inspired by annotated image databases such as ImageNet and COCO, FathomNet aims to establish the same kind of reference data set for images of ocean life. The long-term goal of FathomNet is to aggregate >1k fully annotated and localized images per marine species of Animalia (>200k), with the ability to expand and include other underwater concepts (e.g., substrate type, equipment, debris, etc.) for training and validating machine learning models. We hope that contributions from the broader community will realize our goals for FathomNet."}, {"id": "cmu-dog-cmu-document-grounded-conversations-dataset", "name": "CMU DoG (CMU Document Grounded Conversations Dataset)", "description": "This is a document grounded dataset for text conversations. \"Document Grounded Conversations\" are conversations that are about the contents of a specified document. In this dataset the specified documents are Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation."}, {"id": "deepfashion2", "name": "DeepFashion2", "description": "DeepFashion2 is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs"}, {"id": "moments-in-time", "name": "Moments in Time", "description": "Moments in Time is a large-scale dataset for recognizing and understanding action in videos. The dataset includes a collection of one million labeled 3 second videos, involving people, animals, objects or natural phenomena, that capture the gist of a dynamic scene."}, {"id": "soccerdb", "name": "SoccerDB", "description": "Comprises of 171,191 video segments from 346 high-quality soccer games. The database contains 702,096 bounding boxes, 37,709 essential event labels with time boundary and 17,115 highlight annotations for object detection, action recognition, temporal action localization, and highlight detection tasks. "}, {"id": "2010-i2b2-va", "name": "2010 i2b2/VA", "description": "2010 i2b2/VA is a biomedical dataset for relation classification and entity typing."}, {"id": "phm2017", "name": "PHM2017", "description": "PHM2017 is a new dataset consisting of 7,192 English tweets across six diseases and conditions: Alzheimer\u2019s Disease, heart attack (any severity), Parkinson\u2019s disease, cancer (any type), Depression (any severity), and Stroke. The Twitter search API was used to retrieve the data using the colloquial disease names as search keywords, with the expectation of retrieving a high-recall, low precision dataset. After removing the re-tweets and replies, the tweets were manually annotated. The labels are:"}, {"id": "oc-drowsiness-detection", "name": "OC (Drowsiness-Detection)", "description": "These images were generated using UnityEyes simulator, after including essential eyeball physiology elements and modeling binocular vision dynamics. The images are annotated with head pose and gaze direction information, besides 2D and 3D landmarks of eye's most important features. Additionally, the images are distributed into two classes denoting the status of the eye (Open for open eyes, Closed for closed eyes). This dataset was used to train a DNN model for detecting drowsiness status of a driver. The dataset contains 1,704 training images, 4,232 testing images and additional 4,103 images for improvements."}, {"id": "durecdial", "name": "DuRecDial", "description": "A human-to-human Chinese dialog dataset (about 10k dialogs, 156k utterances), which contains multiple sequential dialogs for every pair of a recommendation seeker (user) and a recommender (bot). "}, {"id": "laborotvspeech", "name": "LaboroTVSpeech", "description": "LaboroTVSpeech is a large-scale Japanese speech corpus built from broadcast TV recordings and their subtitles. It contains over 2,000 hours of speech."}, {"id": "pdebench", "name": "PDEBench", "description": "PDEBench provides a diverse and comprehensive set of benchmarks for scientific machine learning, including challenging and realistic physical problems. The repository consists of the code used to generate the datasets, to upload and download the datasets from the data repository, as well as to train and evaluate different machine learning models as baseline. PDEBench features a much wider range of PDEs than existing benchmarks and included realistic and difficult problems (both forward and inverse), larger ready-to-use datasets comprising various initial and boundary conditions, and PDE parameters. Moreover, PDEBench was crated to make the source code extensible and we invite active participation to improve and extent the benchmark."}, {"id": "word-sense-disambiguation-a-unified-evaluation-framework-and-empirical-comparison", "name": "Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison", "description": "The Evaluation framework of Raganato et al. 2017 includes two training sets (SemCor-Miller et al., 1993- and OMSTI-Taghipour and Ng, 2015-) and five test sets from the Senseval/SemEval series (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli et al., 2013; Moro and Navigli, 2015), standardized to the same format and sense inventory (i.e. WordNet 3.0)."}, {"id": "fiqa-financial-opinion-mining-and-question-answering", "name": "FIQA (Financial Opinion Mining and Question Answering)", "description": "The growing maturity of Natural Language Processing (NLP) techniques and resources is drastically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The financial domain, with its dependency on the interpretation of multiple unstructured and structured data sources and with its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques. This challenge, FIQA, focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain. "}, {"id": "cure-tsr-cure-traffic-sign-recognition", "name": "CURE-TSR (CURE Traffic Sign Recognition)", "description": "Includes more than two million traffic sign images that are based on real-world and simulator data. "}, {"id": "2018-data-science-bowl-2018-data-science-bowl-find-the-nuclei-in-divergent-images-to-advance-medical-discovery", "name": "2018 Data Science Bowl (2018 Data Science Bowl Find the nuclei in divergent images to advance medical discovery)", "description": "This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations."}, {"id": "rwwd-real-world-worry-dataset", "name": "RWWD (Real World Worry Dataset)", "description": "Real World Worry Dataset (RWWD) captures the emotional responses of UK residents to COVID-19 at a point in time where the impact of the COVID19 situation affected the lives of all individuals in the UK. The data were collected on the 6th and 7th of April 2020, a time at which the UK was under lockdown (news, 2020), and death tolls were increasing. On April 6, 5,373 people in the UK had died of the virus, and 51,608 tested positive. On the day before data collection, the Queen addressed the nation via a television broadcast. Furthermore, it was also announced that Prime Minister Boris Johnson was admitted to intensive care in a hospital for COVID-19 symptoms."}, {"id": "def-armored-sequential", "name": "Def_Armored_sequential", "description": "SMAC+ defensive armored scenario with sequential episodic buffer"}, {"id": "a-3d", "name": "A*3D", "description": "The A*3D dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world. Characteristics: * 230K human-labeled 3D object annotations in 39,179 LiDAR point cloud frames and corresponding frontal-facing RGB images. * Captured at different times (day, night) and weathers (sun, cloud, rain)."}, {"id": "emodb-dataset-berlin-database-of-emotional-speech", "name": "EmoDB Dataset (Berlin Database of Emotional Speech)", "description": "The EMODB database is the freely available German emotional database. The database is created by the Institute of Communication Science, Technical University, Berlin, Germany. Ten professional speakers (five males and five females) participated in data recording. The database contains a total of 535 utterances. The EMODB database comprises of seven emotions: 1) anger; 2) boredom; 3) anxiety; 4) happiness; 5) sadness; 6) disgust; and 7) neutral. The data was recorded at a 48-kHz sampling rate and then down-sampled to 16-kHz."}, {"id": "bc5cdr-biocreative-v-cdr-corpus", "name": "BC5CDR (BioCreative V CDR corpus)", "description": "BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions."}, {"id": "taxibj", "name": "TaxiBJ", "description": "TaxiBJ consists of trajectory data from taxicab GPS data and meteorology data in Beijing from four time intervals: 1st Jul. 2013 - 30th Otc. 2013, 1st Mar. 2014 - 30th Jun. 2014, 1st Mar. 2015 - 30th Jun. 2015, 1st Nov. 2015 - 10th Apr. 2016. "}, {"id": "office-31-office-dataset", "name": "Office-31 (Office Dataset)", "description": "The Office dataset contains 31 object categories in three domains: Amazon, DSLR and Webcam. The 31 categories in the dataset consist of objects commonly encountered in office settings, such as keyboards, file cabinets, and laptops. The Amazon domain contains on average 90 images per class and 2817 images in total. As these images were captured from a website of online merchants, they are captured against clean background and at a unified scale. The DSLR domain contains 498 low-noise high resolution images (4288\u00d72848). There are 5 objects per category. Each object was captured from different viewpoints on average 3 times. For Webcam, the 795 images of low resolution (640\u00d7480) exhibit significant noise and color as well as white balance artifacts."}, {"id": "mc-afp", "name": "MC-AFP", "description": "A dataset of around 2 million examples for machine reading-comprehension."}, {"id": "clariq", "name": "ClariQ", "description": "ClariQ is an extension of the Qulac dataset with additional new topics, questions, and answers in the training set. The test set is completely unseen and newly collected. Like Qulac, ClariQ consists of single-turn conversations (initial_request, followed by clarifying question and answer). In addition, it comes with synthetic multi-turn conversations (up to three turns). ClariQ features approximately 18K single-turn conversations, as well as 1.8 million multi-turn conversations. "}, {"id": "partimagenet", "name": "PartImageNet", "description": "PartImageNet is a large, high-quality dataset with part segmentation annotations. It consists of 158 classes from ImageNet with approximately 24000 images. PartImageNet offers part-level annotations on a general set of classes with non-rigid, articulated objects, while having an order of magnitude larger size compared to existing datasets. It can be utilized in multiple vision tasks including but not limited to: Part Discovery, Semantic Segmentation, Few-shot Learning."}, {"id": "hui-speech-corpus-hof-university-iisys-speech-dataset", "name": "HUI speech corpus (Hof University iisys speech dataset)", "description": "The data set contains several speakers. The 5 largest are listed individually, the rest are summarized as other. All audio files have a sampling rate of 44.1kHz. For each speaker, there is a clean variant in addition to the full data set, where the quality is even higher. Furthermore, there are various statistics. The dataset can also be used for automatic speech recognition (ASR) if audio files are converted to 16 kHz."}, {"id": "uit-viquad", "name": "UIT-ViQuAD", "description": "A new dataset for the low-resource language as Vietnamese to evaluate MRC models. This dataset comprises over 23,000 human-generated question-answer pairs based on 5,109 passages of 174 Vietnamese articles from Wikipedia. "}, {"id": "photobook", "name": "PhotoBook", "description": "A large-scale collection of visually-grounded, task-oriented dialogues in English designed to investigate shared dialogue history accumulating during conversation."}, {"id": "quizbowl", "name": "Quizbowl", "description": "Consists of multiple sentences whose clues are arranged by difficulty (from obscure to obvious) and uniquely identify a well-known entity such as those found on Wikipedia."}, {"id": "nvgesture", "name": "NVGesture", "description": "The NVGesture dataset focuses on touchless driver controlling. It contains 1532 dynamic gestures fallen into 25 classes. It includes 1050 samples for training and 482 for testing. The videos are recorded with three modalities (RGB, depth, and infrared)."}, {"id": "imdb-wiki-sbs", "name": "IMDB-WIKI-SbS", "description": "IMDB-WIKI-SbS is a new large-scale dataset for evaluation pairwise comparisons, building on the success of a well-known benchmark for computer vision systems IMDB-WIKI. This dataset uses the age information offered by IMDB-WIKI as ground truth while providing a balanced distribution of ages and genders of people in photos."}, {"id": "coarsewsd-20", "name": "CoarseWSD-20", "description": "The CoarseWSD-20 dataset is a coarse-grained sense disambiguation dataset built from Wikipedia (nouns only) targeting 2 to 5 senses of 20 ambiguous words. It was specifically designed to provide an ideal setting for evaluating Word Sense Disambiguation (WSD) models (e.g. no senses in test sets missing from training), both quantitively and qualitatively."}, {"id": "spanish-timebank-1-0", "name": "Spanish TimeBank 1.0", "description": "Spanish TimeBank 1.0 was developed by researchers at Barcelona Media and consists of Spanish texts in the AnCora corpus annotated with temporal and event information according to the TimeML specification language."}, {"id": "scruples", "name": "Scruples", "description": "Dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members."}, {"id": "geolifeclef-2020", "name": "GeoLifeCLEF 2020", "description": "The GeoLifeCLEF 2020 dataset is a large-scale remote sensing dataset. More specifically, it consists of 1.9 million species observations from the community science platform iNaturalist, each of which is paired with high-resolution covariates (RGB-IR imagery, land cover, and altitude). The dataset is roughly evenly split between the US and France, and covers over 31k plant and animal species."}, {"id": "weibo", "name": "Weibo", "description": "This dataset is from DeepHawkes: Bridging the Gap between Prediction and Understanding of Information Cascades, CIKM 2017. It includes Weibo tweets and their retweets posted in a day."}, {"id": "cmu-movie-summary-corpus", "name": "CMU Movie Summary Corpus", "description": "Dataset [46 M] and readme: 42,306 movie plot summaries extracted from Wikipedia + aligned metadata extracted from Freebase, including: Movie box office revenue, genre, release date, runtime, and language Character names and aligned information about the actors who portray them, including gender and estimated age at the time of the movie's release Supplement: Stanford CoreNLP-processed summaries [628 M]. All of the plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref)."}, {"id": "itacola", "name": "ItaCoLA", "description": "ItaCoLA is a corpus for monolingual and cross-lingual acceptability judgments which contains almost 10,000 sentences with acceptability judgments."}, {"id": "humicroedit", "name": "Humicroedit", "description": "Humicroedit is a humorous headline dataset. The data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. The authors carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline."}, {"id": "hieve-human-in-events", "name": "HiEve (Human-in-Events)", "description": "A new large-scale dataset for understanding human motions, poses, and actions in a variety of realistic events, especially crowd & complex events. It contains a record number of poses (>1M), the largest number of action labels (>56k) for complex events, and one of the largest number of trajectories lasting for long terms (with average trajectory length >480). Besides, an online evaluation server is built for researchers to evaluate their approaches."}, {"id": "itb-informative-tracking-benchmark", "name": "ITB (Informative Tracking Benchmark)", "description": "Informative Tracking Benchmark (ITB) is a small and informative tracking benchmark with 7% out of 1.2 M frames of existing and newly collected datasets, which enables efficient evaluation while ensuring effectiveness. Specifically, the authors designed a quality assessment mechanism to select the most informative sequences from existing benchmarks taking into account 1) challenging level, 2) discriminative strength, 3) and density of appearance variations. Furthermore, they collect additional sequences to ensure the diversity and balance of tracking scenarios, leading to a total of 20 sequences for each scenario."}, {"id": "goodsounds", "name": "GoodSounds", "description": "GoodSounds dataset contains around 28 hours of recordings of single notes and scales played by 15 different professional musicians, all of them holding a music degree and having some expertise in teaching. 12 different instruments (flute, cello, clarinet, trumpet, violin, alto sax alto, tenor sax, baritone sax, soprano sax, oboe, piccolo and bass) were recorded using one or up to 4 different microphones. For all the instruments the whole set of playable semitones in the instrument is recorded several times with different tonal characteristics. Each note is recorded into a separate monophonic audio file of 48kHz and 32 bits. Rich annotations of the recordings are available, including details on recording environment and rating on tonal qualities of the sound (\u201cgood-sound\u201d, \u201cbad\u201d, \u201cscale-good\u201d, \u201cscale-bad\u201d)."}, {"id": "nethack-learning-environment", "name": "NetHack Learning Environment", "description": "The NetHack Learning Environment (NLE) is a Reinforcement Learning environment based on NetHack 3.6.6. It is designed to provide a standard reinforcement learning interface to the game, and comes with tasks that function as a first step to evaluate agents on this new environment. NetHack is one of the oldest and arguably most impactful videogames in history, as well as being one of the hardest roguelikes currently being played by humans. It is procedurally generated, rich in entities and dynamics, and overall an extremely challenging environment for current state-of-the-art RL agents, while being much cheaper to run compared to other challenging testbeds. Through NLE, the authors wish to establish NetHack as one of the next challenges for research in decision making and machine learning."}, {"id": "tallyqa", "name": "TallyQA", "description": "TallyQA is a large-scale dataset for open-ended counting."}, {"id": "merl-rav-merl-reannotation-of-aflw-with-visibility", "name": "MERL-RAV (MERL Reannotation of AFLW with Visibility)", "description": "The MERL-RAV (MERL Reannotation of AFLW with Visibility) Dataset contains over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmark is unoccluded, self-occluded (due to extreme head poses), or externally occluded. The images were annotated by professional labelers, supervised by researchers at Mitsubishi Electric Research Laboratories (MERL)."}, {"id": "bulgarian-reading-comprehension-dataset", "name": "Bulgarian Reading Comprehension Dataset", "description": "A dataset containing 2,221 questions from matriculation exams for twelfth grade in various subjects -history, biology, geography and philosophy-, and 412 additional questions from online quizzes in history. "}, {"id": "carrada", "name": "CARRADA", "description": "CARRADA is a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. "}, {"id": "aff-wild", "name": "Aff-Wild", "description": "Aff-Wild is a dataset for emotion recognition from facial images in a variety of head poses, illumination conditions and occlusions."}, {"id": "extmarker-3d-motion-of-chest-external-markers", "name": "ExtMarker (3D motion of chest external markers)", "description": "Three-dimensional position of external markers placed on the chest and abdomen of healthy individuals breathing during intervals from 73s to 222s. The markers move because of the respiratory motion, and their position is sampled at approximately 10Hz. Markers are metallic objects used during external beam radiotherapy to track and predict the motion of tumors due to breathing for accurate dose delivery."}, {"id": "medico-automatic-polyp-segmentation-challenge-dataset", "name": "Medico automatic polyp segmentation challenge (dataset)", "description": "The \u201cMedico automatic polyp segmentation challenge\u201d aims to develop computer-aided diagnosis systems for automatic polyp segmentation to detect all types of polyps (for example, irregular polyp, smaller or flat polyps) with high efficiency and accuracy. The main goal of the challenge is to benchmark semantic segmentation algorithms on a publicly available dataset, emphasizing robustness, speed, and generalization."}, {"id": "imagenet-r-imagenet-rendition", "name": "ImageNet-R (ImageNet-Rendition)", "description": "ImageNet-R(endition) contains art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, and video game renditions of ImageNet classes."}, {"id": "robustpointset", "name": "RobustPointSet", "description": "A dataset for robustness analysis of point cloud classification models (independent of data augmentation) to input transformations."}, {"id": "flat", "name": "FLAT", "description": "FLAT, a synthetic dataset of 2000 ToF measurements that capture all of these nonidealities, and can be used to simulate different hardware"}, {"id": "segthor-segmentation-of-thoracic-organs-at-risk", "name": "SegTHOR (Segmentation of THoracic Organs at Risk)", "description": "SegTHOR (Segmentation of THoracic Organs at Risk) is a dataset dedicated to the segmentation of organs at risk (OARs) in the thorax, i.e. the organs surrounding the tumour that must be preserved from irradiations during radiotherapy. In this dataset, the OARs are the heart, the trachea, the aorta and the esophagus, which have varying spatial and appearance characteristics. The dataset includes 60 3D CT scans, divided into a training set of 40 and a test set of 20 patients, where the OARs have been contoured manually by an experienced radiotherapist. "}, {"id": "fsdkaggle2018", "name": "FSDKaggle2018", "description": "FSDKaggle2018 is an audio dataset containing 11,073 audio files annotated with 41 labels of the AudioSet Ontology. FSDKaggle2018 has been used for the DCASE Challenge 2018 Task 2. All audio samples are gathered from Freesound and are provided as uncompressed PCM 16 bit, 44.1 kHz mono audio files. The 41 categories of the AudioSet Ontology are: \"Acoustic_guitar\", \"Applause\", \"Bark\", \"Bass_drum\", \"Burping_or_eructation\", \"Bus\", \"Cello\", \"Chime\", \"Clarinet\", \"Computer_keyboard\", \"Cough\", \"Cowbell\", \"Double_bass\", \"Drawer_open_or_close\", \"Electric_piano\", \"Fart\", \"Finger_snapping\", \"Fireworks\", \"Flute\", \"Glockenspiel\", \"Gong\", \"Gunshot_or_gunfire\", \"Harmonica\", \"Hi-hat\", \"Keys_jangling\", \"Knock\", \"Laughter\", \"Meow\", \"Microwave_oven\", \"Oboe\", \"Saxophone\", \"Scissors\", \"Shatter\", \"Snare_drum\", \"Squeak\", \"Tambourine\", \"Tearing\", \"Telephone\", \"Trumpet\", \"Violin_or_fiddle\", \"Writing\"."}, {"id": "occluded-reid", "name": "Occluded REID", "description": "Occluded REID is an occluded person dataset captured by mobile cameras, consisting of 2,000 images of 200 occluded persons (see Fig. (c)). Each identity has 5 full-body person images and 5 occluded person images with different types of occlusion."}, {"id": "3d-affordancenet", "name": "3D AffordanceNet", "description": "3D AffordanceNet is a dataset of 23k shapes for visual affordance. It consists of 56,307 well-defined affordance information annotations for 22,949 shapes covering 18 affordance classes and 23 semantic object categories."}, {"id": "anime-drawings-dataset", "name": "Anime Drawings Dataset", "description": "A dataset for 2D pose estimation of anime/manga images."}, {"id": "pascal-context", "name": "PASCAL Context", "description": "The PASCAL Context dataset is an extension of the PASCAL VOC 2010 detection challenge, and it contains pixel-wise labels for all training images. It contains more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids). Many of the object categories of this dataset are too sparse and; therefore, a subset of 59 frequent classes are usually selected for use."}, {"id": "gtea-georgia-tech-egocentric-activity", "name": "GTEA (Georgia Tech Egocentric Activity)", "description": "The Georgia Tech Egocentric Activities (GTEA) dataset contains seven types of daily activities such as making sandwich, tea, or coffee. Each activity is performed by four different people, thus totally 28 videos. For each video, there are about 20 fine-grained action instances such as take bread, pour ketchup, in approximately one minute."}, {"id": "imagenet-9", "name": "ImageNet-9", "description": "ImageNet-9 consists of images with different amounts of background and foreground signal, which you can use to measure the extent to which your models rely on image backgrounds. This dataset is helpful in testing the robustness of vision models with respect to their dependence on the backgrounds of images."}, {"id": "multilingual-dataset-for-training-and-evaluating-diacritics-restoration-systems", "name": "Multilingual Dataset for Training and Evaluating Diacritics Restoration Systems", "description": "The dataset contains training and evaluation data for 12 languages: - Vietnamese - Romanian - Latvian - Czech - Polish - Slovak - Irish - Hungarian - French - Turkish - Spanish - Croatian"}, {"id": "ecb-extension-to-the-eventcorefbank", "name": "ECB+ (extension to the EventCorefBank)", "description": "The ECB+ corpus is an extension to the EventCorefBank (ECB, Bejan and Harabagiu, 2010). A newly added corpus component consists of 502 documents that belong to the 43 topics of the ECB but that describe different seminal events than those already captured in the ECB. All corpus texts were found through Google Search and were annotated with mentions of events and their times, locations, human and non-human participants as well as with within- and cross-document event and entity coreference information. The 2012 version of annotation of the ECB corpus (Lee et al., 2012) was used as a starting point for re-annotation of the ECB according to the ECB+ annotation guideline."}, {"id": "x-csqa", "name": "X-CSQA", "description": "X-CSQA is a multilingual dataset for Commonsense reasoning research, based on CSQA."}, {"id": "speech-coco", "name": "SPEECH-COCO", "description": "SPEECH-COCO contains speech captions that are generated using text-to-speech (TTS) synthesis resulting in 616,767 spoken captions (more than 600h) paired with images. "}, {"id": "dns-challenge-deep-noise-suppression-challenge", "name": "DNS Challenge (Deep Noise Suppression Challenge)", "description": "The DNS Challenge at INTERSPEECH 2020 intended to promote collaborative research in single-channel Speech Enhancement aimed to maximize the perceptual quality and intelligibility of the enhanced speech. The challenge evaluated the speech quality using the online subjective evaluation framework ITU-T P.808. The challenge provides large datasets for training noise suppressors."}, {"id": "amazon-fine-foods", "name": "Amazon Fine Foods", "description": "Amazon Fine Foods is a dataset that consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plaintext review."}, {"id": "msp-podcast-a-large-naturalistic-speech-emotional-dataset", "name": "MSP-Podcast (A large naturalistic speech emotional dataset)", "description": "The MSP-Podcast corpus contains speech segments from podcast recordings which are perceptually annotated using crowdsourcing. The collection of this corpus is an ongoing process. Version 1.7 of the corpus has 62,140 speaking turns (100hrs)."}, {"id": "massivetext", "name": "MassiveText", "description": "MassiveText is a collection of large English-language text datasets from multiple sources: web pages, books, news articles, and code. The data pipeline includes text quality filtering, removal of repetitious text, deduplication of similar documents, and removal of documents with significant test-set overlap. MassiveText contains 2.35 billion documents or about 10.5 TB of text. "}, {"id": "bbq-bias-benchmark-for-qa", "name": "BBQ (Bias Benchmark for QA)", "description": "Bias Benchmark for QA (BBQ) is a dataset consisting of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine different social dimensions relevant for U.S. English-speaking contexts."}, {"id": "iiit-ar-13k", "name": "IIIT-AR-13K", "description": "IIIT-AR-13K is created by manually annotating the bounding boxes of graphical or page objects in publicly available annual reports. This dataset contains a total of 13k annotated page images with objects in five different popular categories - table, figure, natural image, logo, and signature. It is the largest manually annotated dataset for graphical object detection."}, {"id": "qmul-survface", "name": "QMUL-SurvFace", "description": "QMUL-SurvFace is a surveillance face recognition benchmark that contains 463,507 face images of 15,573 distinct identities captured in real-world uncooperative surveillance scenes over wide space and time."}, {"id": "tydi-qa-typologically-diverse-question-answering", "name": "TyDi QA (Typologically Diverse Question Answering)", "description": "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 200K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology \u2014 the set of linguistic features that each language expresses \u2014 such that the authors expect models performing well on this set to generalize across a large number of the languages in the world."}, {"id": "rock-corpus", "name": "Rock Corpus", "description": "This dataset contains 200 famous songs in different genres (mostly in rock) and the beats and downbeat annotations are provided by  T. de Clercq and D. Temperley [1]."}, {"id": "cloth-cloze-test-by-teachers", "name": "CLOTH (CLOze test by TeacHers)", "description": "The Cloze Test by Teachers (CLOTH) benchmark is a collection of nearly 100,000 4-way multiple-choice cloze-style questions from middle- and high school-level English language exams, where the answer fills a blank in a given text. Each question is labeled with a type of deep reasoning it involves, where the four possible types are grammar, short-term reasoning, matching/paraphrasing, and long-term reasoning, i.e., reasoning over multiple sentences"}, {"id": "people-snapshot-dataset", "name": "People Snapshot Dataset", "description": "Enables detailed human body model reconstruction in clothing from a single monocular RGB video without requiring a pre scanned template or manually clicked points."}, {"id": "lost-and-found", "name": "Lost and Found", "description": "Lost and Found is a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic."}, {"id": "beatles", "name": "Beatles", "description": "This dataset includes the beat and downbeat annotations for  Beatles albums. The annotations are provided by M. E. P. Davies et. al [1]."}, {"id": "davis-2017", "name": "DAVIS 2017", "description": "DAVIS17 is a dataset for video object segmentation.  It contains a total of 150 videos - 60 for training, 30 for validation, 60 for testing"}, {"id": "lasher", "name": "LasHeR", "description": "LasHeR consists of 1224 visible and thermal infrared video pairs with more than 730K frame pairs in total. Each frame pair is spatially aligned and manually annotated with a bounding box, making the dataset well and densely annotated. LasHeR is highly diverse capturing from a broad range of object categories, camera viewpoints, scene complexities and environmental factors across seasons, weathers, day and night."}, {"id": "ferg-facial-expression-research-group-database", "name": "FERG (Facial Expression Research Group Database)", "description": "FERG is a database of cartoon characters with annotated facial expressions containing 55,769 annotated face images of six characters. The images for each character are grouped into 7 types of cardinal expressions, viz. anger, disgust, fear, joy, neutral, sadness and surprise."}, {"id": "3d-datasets-of-broccoli-in-the-field", "name": "3D Datasets of Broccoli in the Field", "description": "This work was undertaken by members of the Lincoln Centre for Autonomous Systems, University of Lincoln, UK. The four data collection sessions were conducted at three different sites in Lincolnshire, UK and one in Murcia, Spain (see Fig. 1). The sessions were conducted at the beginning and towards the end of harvesting season in UK and at the end of the harvest in Spain. The variety of broccoli plants grown in UK is called Iron Man whilst the variety grown in Spain is called Titanium.The weather during UK data capture included a mixture of different conditions including sunny, overcast and raining with broccoli varying in maturity levels from small to larger to already harvested, while the conditions for data capture in Spain included strong sunlight and mature plants at the very end of the harvesting season. The tractor was driven through the broccoli field at a slow walking speed with two rows of broccoli plants being imaged by the RGB-D sensor."}, {"id": "narrativeqa", "name": "NarrativeQA", "description": "The NarrativeQA dataset includes a list of documents with Wikipedia summaries, links to full stories, and questions and answers."}, {"id": "arc-100", "name": "ARC-100", "description": "The ARC-100 dataset was collected as part of a prototype retail checkout system titled ARC (Automatic Retail Checkout). It consists of 31,000 $640\\times480$ RGB images of 100 commonly found retail items in Lahore, Pakistan. Each retail item has 310 images captured at various logical orientations (on a black, matte finish conveyor belt) by a Logitech C310 webcam, under a wooden hood frame illuminated by LED strips (luminance set to approximately $70lx$). In the proposed setup, images were pre-processed and standardized before feeding into a Convolutional Neural Network for identification."}, {"id": "mc4", "name": "mC4", "description": "mC4 is a multilingual variant of the C4 dataset called mC4. mC4 comprises natural text in 101 languages drawn from the public Common Crawl web scrape. "}, {"id": "moleculenet", "name": "MoleculeNet", "description": "MoleculeNet is a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance."}, {"id": "github-typo-corpus", "name": "GitHub Typo Corpus", "description": "Are you the kind of person who makes a lot of typos when writing code? Or are you the one who fixes them by making \"fix typo\" commits? Either way, thank you\u2014you contributed to the state-of-the-art in the NLP field."}, {"id": "arxiv-summarization-dataset", "name": "arXiv Summarization Dataset", "description": "This is a dataset for evaluating summarisation methods for research papers."}, {"id": "multi-xscience", "name": "Multi-XScience", "description": "Multi-XScience is a large-scale dataset for multi-document summarization of scientific articles. It has 30,369, 5,066 and 5,093 samples for the train, validation and test split respectively. The average document length is 778.08 words and the average summary length is 116.44 words."}, {"id": "finnish-paraphrase-corpus", "name": "Finnish Paraphrase Corpus", "description": "Finnish Paraphrase Corpus is a fully manually annotated paraphrase corpus for Finnish containing 53,572 paraphrase pairs harvested from alternative subtitles and news headings. Out of all paraphrase pairs in the corpus 98% are manually classified to be paraphrases at least in their given context, if not in all contexts."}, {"id": "acs-pums", "name": "ACS PUMS", "description": "ACS PUMS stands for American Community Survey (ACS) Public Use Microdata Sample (PUMS) and has been used to construct several tabular datasets for studying fairness in machine learning:"}, {"id": "hugadb", "name": "HuGaDB", "description": "HuGaDB is human gait data collection for analysis and activity recognition consisting of continues recordings of combined activities, such as walking, running, taking stairs up and down, sitting down, and so on; and the data recorded are segmented and annotated. Data were collected from a body sensor network consisting of six wearable inertial sensors (accelerometer and gyroscope) located on the right and left thighs, shins, and feet. Additionally, two electromyography sensors were used on the quadriceps (front thigh) to measure muscle activity. This database can be used not only for activity recognition but also for studying how activities are performed and how the parts of the legs move relative to each other. Therefore, the data can be used (a) to perform health-care-related studies, such as in walking rehabilitation or Parkinson's disease recognition, (b) in virtual reality and gaming for simulating humanoid motion, or (c) for humanoid robotics to model humanoid walking."}, {"id": "voice", "name": "VOICe", "description": "VOICe is a dataset for the development and evaluation of domain adaptation methods for sound event detection. VOICe consists of mixtures with three different sound events (\"baby crying\", \"glass breaking\", and \"gunshot\"), which are over-imposed over three different categories of acoustic scenes: vehicle, outdoors, and indoors. Moreover, the mixtures are also offered without any background noise."}, {"id": "airogs-rotterdam-eyepacs-airogs", "name": "AIROGS (Rotterdam EyePACS AIROGS)", "description": "The Rotterdam EyePACS AIROGS dataset (in full, so including train and test) contains 113,893 color fundus images from 60,357 subjects and approximately 500 different sites with a heterogeneous ethnicity."}, {"id": "criteo-display-advertising-challenge", "name": "Criteo (Display Advertising Challenge)", "description": "Criteo contains 7 days of click-through data, which is widely used for CTR prediction benchmarking. There are 26 anonymous categorical fields and 13 continuous fields in Criteo dataset."}, {"id": "webnlg", "name": "WebNLG", "description": "The WebNLG corpus comprises of sets of triplets describing facts (entities and relations between them) and the corresponding facts in form of natural language text. The corpus contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories."}, {"id": "citeseer", "name": "Citeseer", "description": "The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words."}, {"id": "panlex-bli-panlex-based-bilingual-lexicons-for-210-language-pairs", "name": "PanLex-BLI (PanLex-based bilingual lexicons for 210 language pairs)", "description": "PanLex-based bilingual lexicons for 210 language pairs"}, {"id": "fluent-speech-commands", "name": "Fluent Speech Commands", "description": "Fluent Speech Commands is an open source audio dataset for spoken language understanding (SLU) experiments. Each utterance is labeled with \"action\", \"object\", and \"location\" values; for example, \"turn the lights on in the kitchen\" has the label {\"action\": \"activate\", \"object\": \"lights\", \"location\": \"kitchen\"}. A model must predict each of these values, and a prediction for an utterance is deemed to be correct only if all values are correct. "}, {"id": "real275-nocs-real275", "name": "REAL275 (NOCS-REAL275)", "description": "REAL275 is a benchmark for category-level pose estimation. It contains 4300 training frames, 950 validation and 2750 for testing across 18 different real scenes."}, {"id": "dsse-200", "name": "DSSE-200", "description": "The DSSE-200 is a complex document layout dataset including various dataset styles. The dataset contains 200 images from pictures, PPT, brochure documents, old newspapers and scanned documents."}, {"id": "colored-mnist-with-spurious-correlation", "name": "Colored-MNIST(with spurious correlation)", "description": "This is a dataset with spurious correlations which can be used to evaluate machine learning methods for out-of-distribution generalization, causal inference, and related field."}, {"id": "rmfd-real-world-masked-face-dataset", "name": "RMFD (Real-World Masked Face Dataset)", "description": "Real-World Masked Face Dataset (RMFD) is a large dataset for masked face detection."}, {"id": "semeval2017", "name": "SemEval2017", "description": "DOI: 10.18653/v1/S17-2091"}, {"id": "ocid-object-clutter-indoor-dataset", "name": "OCID (Object Clutter Indoor Dataset)", "description": "Developing robot perception systems for handling objects in the real-world requires computer vision algorithms to be carefully scrutinized with respect to the expected operating domain. This demands large quantities of ground truth data to rigorously evaluate the performance of algorithms."}, {"id": "charades", "name": "Charades", "description": "The Charades dataset is composed of 9,848 videos of daily indoors activities with an average length of 30 seconds, involving interactions with 46 objects classes in 15 types of indoor scenes and containing a vocabulary of 30 verbs leading to 157 action classes. Each video in this dataset is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacting objects. 267 different users were presented with a sentence, which includes objects and actions from a fixed vocabulary, and they recorded a video acting out the sentence. In total, the dataset contains 66,500 temporal annotations for 157 action classes, 41,104 labels for 46 object classes, and 27,847 textual descriptions of the videos. In the standard split there are7,986 training video and 1,863 validation video."}, {"id": "hoc-hallmarks-of-cancer", "name": "HOC (Hallmarks of Cancer)", "description": "The Hallmarks of Cancer (*HOC) corpus consists of 1852 PubMed publication abstracts manually annotated by experts according to the Hallmarks of Cancer taxonomy. The taxonomy consists of 37 classes in a hierarchy. Zero or more class labels are assigned to each sentence in the corpus. "}, {"id": "fruits-360-a-dataset-of-images-containing-fruits-and-vegetables", "name": "Fruits 360 (A dataset of images containing fruits and vegetables)", "description": "The following fruits and are included:  Apples (different varieties: Crimson Snow, Golden, Golden-Red, Granny Smith, Pink Lady, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red, Lady Finger), Beetroot Red, Blueberry, Cactus fruit, Cantaloupe (2 varieties), Carambula, Cauliflower, Cherry (different varieties, Rainier), Cherry Wax (Yellow, Red, Black), Chestnut, Clementine, Cocos, Corn (with husk), Cucumber (ripened), Dates, Eggplant, Fig, Ginger Root, Granadilla, Grape (Blue, Pink, White (different varieties)), Grapefruit (Pink, White), Guava, Hazelnut, Huckleberry, Kiwi, Kaki, Kohlrabi, Kumsquats, Lemon (normal, Meyer), Lime, Lychee, Mandarine, Mango (Green, Red), Mangostan, Maracuja, Melon Piel de Sapo, Mulberry, Nectarine (Regular, Flat), Nut (Forest, Pecan), Onion (Red, White), Orange, Papaya, Passion fruit, Peach (different varieties), Pepino, Pear (different varieties, Abate, Forelle, Kaiser, Monster, Red, Stone, Williams), Pepper (Red, Green, Orange, Yellow), Physalis (normal, with Husk), Pineapple (normal, Mini), Pitahaya Red, Plum (different varieties), Pomegranate, Pomelo Sweetie, Potato (Red, Sweet, White), Quince, Rambutan, Raspberry, Redcurrant, Salak, Strawberry (normal, Wedge), Tamarillo, Tangelo, Tomato (different varieties, Maroon, Cherry Red, Yellow, not ripened, Heart), Walnut, Watermelon."}, {"id": "interiornet", "name": "InteriorNet", "description": "InteriorNet is a RGB-D for large scale interior scene understanding and mapping. The dataset contains 20M images created by pipeline:"}, {"id": "spokensts", "name": "SpokenSTS", "description": "Spoken versions of the Semantic Textual Similarity dataset for testing semantic sentence level embeddings. Contains thousands of sentence pairs annotated by humans for semantic similarity. The spoken sentences can be used in sentence embedding models to test whether your model learns to capture sentence semantics. All sentences available in 6 synthetic Wavenet voices and a subset (5%) in 4 real voices recorded in a sound attenuated booth. Code to train a visually grounded spoken sentence embedding model and evaluation code is available at https://github.com/DannyMerkx/speech2image/tree/Interspeech21"}, {"id": "mgif", "name": "MGif", "description": "MGif is a dataset of videos containing movements of different cartoon animals. Each video is a moving gif file. The dataset consists of 1000 videos. The dataset is particularly challenging because of the high appearance variation and motion diversity. "}, {"id": "wnut-2020-task-2-wnut-2020-task-2-identification-of-informative-covid-19-english-tweets", "name": "WNUT-2020 Task 2 (WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets)", "description": "Briefly describe the dataset. Provide:"}, {"id": "data-science-problems", "name": "Data Science Problems", "description": "Evaluate a natural language code generation model on real data science pedagogical notebooks! Data Science Problems (DSP) includes well-posed data science problems in Markdown along with unit tests to verify correctness and a Docker environment for reproducible execution. About 1/3 of notebooks in this benchmark also include data dependencies, so this benchmark not only can test a model's ability to chain together complex tasks, but also evaluate the solutions on real data! See our paper Training and Evaluating a Jupyter Notebook Data Science Assistant for more details about state of the art results and other properties of the dataset."}, {"id": "mvor-multi-view-operating-room", "name": "MVOR (Multi-View Operating Room)", "description": "Multi-View Operating Room (MVOR) is a dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter. "}, {"id": "somos-the-samsung-open-mos-dataset-for-the-evaluation-of-neural-text-to-speech-synthesis", "name": "SOMOS (The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis)", "description": "The SOMOS dataset is a large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 TTS systems including vanilla neural acoustic models as well as models which allow prosodic variations."}, {"id": "voicebank-demand", "name": "VoiceBank+DEMAND", "description": "VoiceBank+DEMAND  is a noisy speech database for training speech enhancement algorithms and TTS models.  The database was designed to train and test speech enhancement methods that operate at 48kHz. A more detailed description can be found in the paper associated with the database. Some of the noises were obtained from the Demand database, available here: http://parole.loria.fr/DEMAND/ . The speech database was obtained from the Voice Banking Corpus, available here: http://homepages.inf.ed.ac.uk/jyamagis/release/VCTK-Corpus.tar.gz ."}, {"id": "pemsd8", "name": "PeMSD8", "description": "This dataset contains the traffic data in San Bernardino from July to August in 2016, with 170 detectors on 8 roads with a time interval of 5 minutes. This dataset is popular as a benchmark traffic forecasting dataset."}, {"id": "smac-exp-starcraft-multi-agent-exploration-challenge", "name": "SMAC-Exp (StarCraft Multi-Agent Exploration Challenge)", "description": "The StarCraft Multi-Agent Challenges+ requires agents to learn completion of multi-stage tasks and usage of environmental factors without precise reward functions. The previous challenges (SMAC) recognized as a standard benchmark of Multi-Agent Reinforcement Learning are mainly concerned with ensuring that all agents cooperatively eliminate approaching adversaries only through fine manipulation with obvious reward functions. This challenge, on the other hand, is interested in the exploration capability of MARL algorithms to efficiently learn implicit multi-stage tasks and environmental factors as well as micro-control. This study covers both offensive and defensive scenarios. In the offensive scenarios, agents must learn to first find opponents and then eliminate them. The defensive scenarios require agents to use topographic features. For example, agents need to position themselves behind protective structures to make it harder for enemies to attack."}, {"id": "mfnet", "name": "MFNet", "description": "The first RGB-Thermal urban scene image dataset with pixel-level annotation. We published this new RGB-Thermal semantic segmentation dataset in support of further development of autonomous vehicles in the future. This dataset contains 1569 images (820 taken at daytime and 749 taken at nighttime). Eight classes of obstacles commonly encountered during driving (car, person, bike, curve, car stop, guardrail, color cone, and bump) are labeled in this dataset."}, {"id": "mixsnips", "name": "MixSNIPs", "description": "A multi intent dataset based on SNIPS dataset."}, {"id": "famulus", "name": "Famulus", "description": "This is a dataset for segmentation and classification of epistemic activities in diagnostic reasoning texts."}, {"id": "fraunhofer-ipa-bin-picking", "name": "Fraunhofer IPA Bin-Picking", "description": "The Fraunhofer IPA Bin-Picking dataset is a large-scale dataset comprising both simulated and real-world scenes for various objects (potentially having symmetries) and is fully annotated with 6D poses. A pyhsics simulation is used to create scenes of many parts in bulk by dropping objects in a random position and orientation above a bin. Additionally, this dataset extends the Sil\u00e9ane dataset by providing more samples. This allows to e.g. train deep neural networks and benchmark the performance on the public Sil\u00e9ane dataset"}, {"id": "ropes-reasoning-over-paragraph-effects-in-situations", "name": "ROPES (Reasoning Over Paragraph Effects in Situations)", "description": "ROPES is a QA dataset which tests a system's ability to apply knowledge from a passage of text to a new situation. A system is presented a background passage containing a causal or qualitative relation(s), a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the back-ground passage in the context of the situation."}, {"id": "ai2d-ai2-diagrams", "name": "AI2D (AI2 Diagrams)", "description": "AI2 Diagrams (AI2D) is a dataset of over 5000 grade school science diagrams with over 150000 rich annotations, their ground truth syntactic parses, and more than 15000 corresponding multiple choice questions."}, {"id": "lakh-midi-dataset", "name": "Lakh MIDI Dataset", "description": "The Lakh MIDI dataset is a collection of 176,581 unique MIDI files, 45,129 of which have been matched and aligned to entries in the Million Song Dataset. Its goal is to facilitate large-scale music information retrieval, both symbolic (using the MIDI files alone) and audio content-based (using information extracted from the MIDI files as annotations for the matched audio files). Around 10% of all MIDI files include timestamped lyrics events with lyrics are often transcribed at the word, syllable or character level."}, {"id": "ufpr-admr-v1", "name": "UFPR-ADMR-v1", "description": "This dataset contains 2,000 dial meter images obtained on-site by employees of the Energy Company of Paran\u00e1 (Copel), which serves more than 4 million consuming units in the Brazilian state of Paran\u00e1. The images were acquired with many different cameras and are available in the JPG format with 320\u00d7640 or 640\u00d7320 pixels (depending on the camera orientation). "}, {"id": "cvusa", "name": "CVUSA", "description": "The CVUSA dataset is a matching task between street- and aerial views, from different regions of the US. This task helps to determine localization without GPS coordinates for the street-view images. Google Street View panoramas are used as ground images, and matching aerial images at zoom level 19 are obtained from Microsoft Bing Maps. The dataset comprises 35,532 image pairs for training and 8,884 image pairs for testing, and recall is the primary metric for evaluation."}, {"id": "trecqa-text-retrieval-conference-question-answering", "name": "TrecQA (Text Retrieval Conference Question Answering)", "description": "Text Retrieval Conference Question Answering (TrecQA) is a dataset created from the TREC-8 (1999) to TREC-13 (2004) Question Answering tracks. There are two versions of TrecQA: raw and clean. Both versions have the same training set but their development and test sets differ. The commonly used clean version of the dataset excludes questions in development and test sets with no answers or only positive/negative answers. The clean version has 1,229/65/68 questions and 53,417/1,117/1,442 question-answer pairs for the train/dev/test split."}, {"id": "codebrim-concrete-defect-bridge-image-dataset", "name": "CODEBRIM (COncrete DEfect BRidge IMage Dataset)", "description": "Dataset for multi-target classification of five commonly appearing concrete defects."}, {"id": "fss-1000", "name": "FSS-1000", "description": "FSS-1000 is a 1000 class dataset for few-shot segmentation. The dataset contains significant number of objects that have never been seen or annotated in previous datasets, such as tiny daily objects, merchandise, cartoon characters, logos, etc."}, {"id": "3d-shapes-dataset", "name": "3D Shapes Dataset", "description": "3dshapes is a dataset of 3D shapes procedurally generated from 6 ground truth independent latent factors. These factors are floor colour, wall colour, object colour, scale, shape and orientation."}, {"id": "farstail", "name": "FarsTail", "description": "Natural Language Inference (NLI), also called Textual Entailment, is an important task in NLP with the goal of determining the inference relationship between a premise p and a hypothesis h. It is a three-class problem, where each pair (p, h) is assigned to one of these classes: \"ENTAILMENT\" if the hypothesis can be inferred from the premise, \"CONTRADICTION\" if the hypothesis contradicts the premise, and \"NEUTRAL\" if none of the above holds. There are large datasets such as SNLI, MNLI, and SciTail for NLI in English, but there are few datasets for poor-data languages like Persian. Persian (Farsi) language is a pluricentric language spoken by around 110 million people in countries like Iran, Afghanistan, and Tajikistan. FarsTail is the first relatively large-scale Persian dataset for NLI task. A total of 10,367 samples are generated from a collection of 3,539 multiple-choice questions. The train, validation, and test portions include 7,266, 1,537, and 1,564 instances, respectively."}, {"id": "arabic-dataset-for-commonsense-validation", "name": "Arabic Dataset for Commonsense Validation\u00ac\u2020", "description": "A benchmark Arabic dataset for commonsense understanding and validation as well as a baseline research and models trained using the same dataset. "}, {"id": "an-extensive-dataset-of-handwritten-central-kurdish-isolated-characters-rebin-m-ahmed", "name": "An extensive dataset of handwritten central Kurdish isolated characters (Rebin M. Ahmed)", "description": "Data collection: Finding a suitable source of data is considered a first step toward building a database. The first step in building a database is finding a suitable source. Here, the main goal is to collect images of Kurdish handwritten characters written by many writers. So, a form is designed to do so. The form is shown in Figure 1. It consists of 1 alphabet at a time letter that has been printed on the top right corner, and it has 125 empty blocks. The writers have been asked to write each letter three times in the three empty blocks. The total number of writers is 390. The forms have been distributed among two main categories: The academic staff of the Information Technology department at Tishk International University, the university students of the University of Kurdistan-Hawler, Salahaddin University, and Tishk International University As shown in Table 2. In total there were ten sets of forms, each set with 35 forms for 35 different letters, at first, we decided that nine sets, which will give us at least 1100 images for each letter were the best option for the time that we had. Then there were some problems with the collection process, in first prints of the forms there was confusion for instance in Set 2, there were 2 forms for the letter (\u0686) and none for (\u062c), and since we printed and distributed the form at the same time, we were not aware of this problem until the stage of pre-processing, This was creating an inconsistency in the number of samples that we had, for example by the 9th set we had 504 images of the letter (\u06a4), which was much less than other letters that they had at least 1000 images. So we decided to add the 10th set as a complementary to other sets, it only contained those letter, which was missing in the first 9 forms, which was (\u0632\u060c\u0698\u060c\u0634\u060c\u063a\u060c\u06a4\u060c\u0642\u060c\u06a9\u060c\u0644\u060c\u0646\u060c\u06cc), as explained in Table 3, the First column is the letter and columns 2-11 represent several images gathered in each set accordingly, while the first row the header row 2-36 are letters in each set, last row, and last columns are for the total of each letter and each set. Labeling and Organizing : Each image is labeled with three numbers and separated by an underscore, the first number is the id of the letter according to its positing in the alphabetical order which is shown in Table 4, the second number being the number of the set of form which there was 10 sets each giving to a specific group of writers, the third number is the order of that character in the form which was between 1 to 126, so each image had a label like following 02_01_94.jpg, 02 is the order of the letter which in this case is Alef (\u0661), then 01 being in the set number 1 which was given to 4th-grade students of Information Technology department in Tishk International University, and 94 is the order of that image in the form. Each letter was stored in a folder with its ID as the name of that folder, with each folder containing approximately 1134 images of that letter."}, {"id": "occluded-coco", "name": "Occluded COCO", "description": "Occluded COCO is automatically generated subset of COCO val dataset, collecting partially occluded objects for a large variety of categories in real images in a scalable manner, where target object is partially occluded but the segmentation mask is connected."}, {"id": "danetqa-yes-no-question-answering-dataset-for-the-russian", "name": "DaNetQA (Yes/no Question Answering Dataset for the Russian)", "description": "DaNetQA is a question answering dataset for yes/no questions. These questions are naturally occurring ---they are generated in unprompted and unconstrained settings."}, {"id": "kinetics-700", "name": "Kinetics-700", "description": "Kinetics-700 is a video dataset of 650,000 clips that covers 700 human action classes. The videos include human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging. Each action class has at least 700 video clips. Each clip is annotated with an action class and lasts approximately 10 seconds."}, {"id": "on-the-origins-of-memes-by-means-of-fringe-web-communities", "name": "On the Origins of Memes by Means of Fringe Web Communities", "description": "This dataset was collected with research funding from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie Grant Agreement No 691025."}, {"id": "dped-dslr-photo-enhancement-dataset", "name": "DPED (DSLR Photo Enhancement Dataset)", "description": "A large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera."}, {"id": "jhu-crowd", "name": "JHU-CROWD", "description": "(JHU-CROWD) a crowd counting dataset that contains 4,250 images with 1.11 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level."}, {"id": "jetnet", "name": "JetNet", "description": "JetNet is a particle cloud dataset, containing gluon, top quark, light quark jets saved in .csv format."}, {"id": "kaist-urban", "name": "KAIST Urban", "description": "This data set provides Light Detection and Ranging (LiDAR) data and stereo image with various position sensors targeting a highly complex urban environment. The presented data set captures features in urban environments (e.g. metropolis areas, complex buildings and residential areas). The data of 2D and 3D LiDAR are provided, which are typical types of LiDAR sensors. Raw sensor data for vehicle navigation is presented in a file format. For convenience, development tools are provided in the Robot Operating System (ROS) environment."}, {"id": "videoforensicshq", "name": "VideoForensicsHQ", "description": "VideoForensicsHQ is a benchmark dataset for face video forgery detection, providing high quality visual manipulations.  It is one of the first face video manipulation benchmark sets that also contains audio and thus complements existing datasets along a new challenging dimension. VideoForensicsHQ shows manipulations at much higher video quality and resolution, and shows manipulations that are provably much harder to detect by humans than videos in other datasets. "}, {"id": "moral-stories", "name": "Moral Stories", "description": "Moral Stories is a crowd-sourced dataset of structured narratives that describe normative and norm-divergent actions taken by individuals to accomplish certain intentions in concrete situations, and their respective consequences."}, {"id": "h-dibco-2012", "name": "H-DIBCO 2012", "description": "H-DIBCO 2012 is the International Document Image Binarization Competition which is dedicated to handwritten document images organized in conjunction with ICFHR 2012 conference. The objective of the contest is to identify current advances in handwritten document image binarization using meaningful evaluation performance measures."}, {"id": "bd-4sk-asr-basic-dataset-for-sorani-kurdish-automatic-speech-recognition", "name": "BD-4SK-ASR (Basic Dataset for Sorani Kurdish Automatic Speech Recognition)", "description": "The Basic Dataset for Sorani Kurdish Automatic Speech Recognition (BD-4SK-ASR) is a dataset for automatic speech recognition for Sorani Kurdish."}, {"id": "prontoqa-proof-and-ontology-generated-question-answering", "name": "PrOntoQA (Proof and Ontology-Generated Question-Answering)", "description": "PrOntoQA is a question-answering dataset which generates examples with chains-of-thought that describe the reasoning required to answer the questions correctly. The sentences in the examples are syntactically simple and amenable to semantic parsing. It can be used to formally analyze the predicted chain-of-thought from large language models such as GPT-3."}, {"id": "rc-49", "name": "RC-49", "description": "RC-49 is a benchmark dataset for generating images conditional on a continuous scalar variable. It is made by rendering 49 3-D chair models from ShapeNet individually. Each chair model is rendered at 899 yaw angles from $0.1^{\\circ}$ to $89.9^{\\circ}$ with a stepsize of $0.1^{\\circ}$. This dataset contains 44,051 RGB images of size $64\\times64$ with corresponding yaw angles as labels. "}, {"id": "co-skel-dataset", "name": "CO-SKEL dataset", "description": "A benchmark dataset for the co-skeletonization task."}, {"id": "gtzan", "name": "GTZAN", "description": "The gtzan8 audio dataset contains 1000 tracks of 30 second length. There are 10 genres, each containing 100 tracks which are all 22050Hz Mono 16-bit audio files in .wav format. The genres are:"}, {"id": "isic-2017-task-1", "name": "ISIC 2017 Task 1", "description": "The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 1 challenge dataset for lesion segmentation contains 2,000 images for training with ground truth segmentations (2000 binary mask images)."}, {"id": "compcars-comprehensive-cars", "name": "CompCars (Comprehensive Cars)", "description": "The Comprehensive Cars (CompCars) dataset contains data from two scenarios, including images from web-nature and surveillance-nature. The web-nature data contains 163 car makes with 1,716 car models. There are a total of 136,726 images capturing the entire cars and 27,618 images capturing the car parts. The full car images are labeled with bounding boxes and viewpoints. Each car model is labeled with five attributes, including maximum speed, displacement, number of doors, number of seats, and type of car. The surveillance-nature data contains 50,000 car images captured in the front view. "}, {"id": "brats-2013", "name": "BraTS 2013", "description": "BRATS 2013 is a brain tumor segmentation dataset consists of synthetic and real images, where each of them is further divided into high-grade gliomas (HG) and low-grade gliomas (LG). There are 25 patients with both synthetic HG and LG images and 20 patients with real HG and 10 patients with real LG images. For each patient, FLAIR, T1, T2, and post-Gadolinium T1 magnetic resonance (MR) image sequences are available."}, {"id": "codah-commonsense-dataset-adversarially-authored-by-humans", "name": "CODAH (COmmonsense Dataset Adversarially-authored by Humans)", "description": "The COmmonsense Dataset Adversarially-authored by Humans (CODAH) is an evaluation set for commonsense question-answering in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions. It contains 2801 questions in total, and uses 5-fold cross validation for evaluation."}, {"id": "new-college", "name": "New College", "description": "The New College Data is a freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford. The data includes odometry, laser scan, and visual information. The dataset URL is not working anymore."}, {"id": "agar-annotated-germs-for-automated-recognition", "name": "AGAR (Annotated Germs for Automated Recognition)", "description": "The Annotated Germs for Automated Recognition (AGAR) dataset is an image database of microbial colonies cultured on an agar plate. It contains 18000 photos of five different microorganisms, taken under diverse lighting conditions with two different cameras. All images are classified into countable, uncountable, and empty, with the former being labeled by microbiologists with colony location and 5 species identification (336 442 colonies)."}, {"id": "quick-draw-dataset", "name": "Quick, Draw! Dataset", "description": "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located."}, {"id": "xview-mejdi", "name": "xView (mejdi)", "description": "xView is one of the largest publicly available datasets of overhead imagery. It contains images from complex scenes around the world, annotated using bounding boxes. It contains over 1M object instances from 60 different classes."}, {"id": "tvqa-2", "name": "TVQA", "description": "The TVQA dataset is a large-scale vido dataset for video question answering. It is based on 6 popular TV shows (Friends, The Big Bang Theory, How I Met Your Mother, House M.D., Grey's Anatomy, Castle). It includes 152,545 QA pairs from 21,793 TV show clips. The QA pairs are split into the ratio of 8:1:1 for training, validation, and test sets. The TVQA dataset provides the sequence of video frames extracted at 3 FPS, the corresponding subtitles with the video clips, and the query consisting of a question and four answer candidates. Among the four answer candidates, there is only one correct answer."}, {"id": "gvfc-gun-violence-frame-corpus", "name": "GVFC (Gun Violence Frame Corpus)", "description": "This is a new dataset of news headlines and their frames related to the issue of gun violence in the United States. This Gun Violence Frame Corpus (GVFC) was curated and annotated by journalism and communication experts. The articles in this dataset are drawn from a sample of news articles from a list of 30 top U.S. news websites defined in terms of traffic to the websites; and collected from four time periods over the course of 2018 in order to capture a diversity of articles."}, {"id": "mathqa", "name": "MathQA", "description": "MathQA significantly enhances the AQuA dataset with fully-specified operational programs. "}, {"id": "jericho", "name": "Jericho", "description": "Jericho is a learning environment for man-made Interactive Fiction (IF) games."}, {"id": "jvs", "name": "JVS", "description": "JVS is a Japanese multi-speaker voice corpus which contains voice data of 100 speakers in three styles (normal, whisper, and falsetto). The corpus contains 30 hours of voice data including 22 hours of parallel normal voices."}, {"id": "rusentrel", "name": "RuSentRel", "description": "RuSentRel is a corpus of analytical articles translated into Russian texts in the domain of international politics obtained from foreign authoritative sources. The collected articles contain both the author's opinion on the subject matter of the article and a large number of references mentioned between the participants of the described situations. In total, 73 large analytical texts were labeled with about 2000 relations."}, {"id": "ccaligned", "name": "CCAligned", "description": "CCAligned consists of parallel or comparable web-document pairs in 137 languages aligned with English. These web-document pairs were constructed by performing language identification on raw web-documents, and ensuring corresponding language codes were corresponding in the URLs of web documents. This pattern matching approach yielded more than 100 million aligned documents paired with English. Recognizing that each English document was often aligned to multiple documents in different target language, it is possible to join on English documents to obtain aligned documents that directly pair two non-English documents (e.g., Arabic-French)."}, {"id": "rice-remote-sensing-image-cloud-removing", "name": "RICE (Remote sensing Image Cloud rEmoving)", "description": "RICE is a remote sensing image dataset for cloud removal. The proposed dataset consists of two parts: RICE1 contains 500 pairs of images, each pair has images with cloud and cloudless size of 512512; RICE2 contains 450 sets of images, each set contains three 512512 size images, respectively, the reference picture without clouds, the picture of the cloud and the mask of its cloud."}, {"id": "curiosity", "name": "Curiosity", "description": "The Curiosity dataset consists of 14K dialogs (with 181K utterances) with fine-grained knowledge groundings, dialog act annotations, and other auxiliary annotation. In this dataset users and virtual assistants converse about geographic topics like geopolitical entities and locations. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, grounding to Wikipedia, and user reactions to messages."}, {"id": "opra-online-product-reviews-for-affordances", "name": "OPRA (Online Product Reviews for Affordances)", "description": "The OPRA Dataset was introduced in Demo2Vec: Reasoning Object Affordances From Online Videos (CVPR'18) for reasoning object affordances from online demonstration videos. It contains 11,505 demonstration clips and 2,512 object images scraped from 6 popular YouTube product review channels along with the corresponding affordance annotations. More details can be found on our https://sites.google.com/view/demo2vec/."}, {"id": "3d-faces-collection-data", "name": "3D Faces Collection Data", "description": "The dataset includes 1,078 People 3D Faces Collection Data. The collection device is Realsense SR300. Each subject was collected once a week, 6 times in total, so the time span is 6 weeks. The number of videos collected for one subject is 16."}, {"id": "duolingo-staple-shared-task", "name": "Duolingo STAPLE Shared Task", "description": "This is the dataset for the 2020 Duolingo shared task on Simultaneous Translation And Paraphrase for Language Education (STAPLE). Sentence prompts, along with automatic translations, and high-coverage sets of translation paraphrases weighted by user response are provided in 5 language pairs. Starter code for this task can be found here: github.com/duolingo/duolingo-sharedtask-2020/. More details on the data set and task are available at: sharedtask.duolingo.com"}, {"id": "abo-amazon-berkeley-objects", "name": "ABO (Amazon Berkeley Objects)", "description": "ABO is a large-scale dataset designed for material prediction and multi-view retrieval experiments. The dataset contains Blender renderings of 30 viewpoints for each of the 7,953 3D objects, as well as camera intrinsics and extrinsic for each rendering."}, {"id": "dbrd-dutch-book-reviews-dataset", "name": "DBRD (Dutch Book Reviews Dataset)", "description": "The DBRD (pronounced dee-bird) dataset contains over 110k book reviews along with associated binary sentiment polarity labels. It is greatly influenced by the Large Movie Review Dataset and intended as a benchmark for sentiment classification in Dutch. "}, {"id": "aids-antiviral-screen", "name": "AIDS Antiviral Screen", "description": "The AIDS Antiviral Screen dataset is a dataset of screens checking tens of thousands of compounds for evidence of anti-HIV activity. The available screen results are chemical graph-structured data of these various compounds."}, {"id": "atm22", "name": "ATM\u201922", "description": "ATM'22 is a multi-site, multi-domain dataset for pulmonary airway segmentation. It contains large-scale CT scans with detailed pulmonary airways annotation, including 500 CT scans (300 for training, 50 for validation, and 150 for testing). The dataset was collected from different sites and it further included a portion of noisy COVID19 CTs with ground-glass opacity and consolidation."}, {"id": "grasp-affordance", "name": "Grasp Affordance", "description": "This is a dataset for visual grasp affordance prediction that promotes more robust and heterogeneous robotic grasping methods. The dataset contains different attributes from 30 different objects. Each object instance is related not only to the semantic descriptions, but also the physical features describing visual attributes, locations and different grasping regions related to a variety of actions."}, {"id": "asset", "name": "ASSET", "description": "ASSET is a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations."}, {"id": "abc-dataset", "name": "ABC Dataset", "description": "The ABC Dataset is a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms."}, {"id": "dotmark-discrete-optimal-transport-benchmark", "name": "DOTmark (Discrete Optimal Transport Benchmark)", "description": "DOTmark is a benchmark for discrete optimal transport, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy."}, {"id": "mvk-marine-video-kit", "name": "MVK (Marine Video Kit)", "description": "The dataset contains single-shot videos taken from moving cameras in underwater environments. The first shard of a new Marine Video Kit dataset is presented to serve for video retrieval and other computer vision challenges. In addition to basic meta-data statistics, we present several insights based on low-level features as well as semantic annotations of selected keyframes.  1379 videos with a length from 2 s to 4.95 min, with the mean and median duration of each video is 29.9 s, and 25.4 s, respectively. We capture data from 11 di\ufb00erent regions and countries during the time from 2011 to 2022."}, {"id": "bird-blocksworld-image-reasoning-dataset", "name": "BIRD (Blocksworld Image Reasoning Dataset)", "description": "Blocksworld Image Reasoning Dataset (BIRD) contains images of wooden blocks in different configurations, and the sequence of moves to rearrange one configuration to the other. "}, {"id": "mtnt", "name": "MTNT", "description": "The Machine Translation of Noisy Text (MTNT) dataset is a Machine Translation dataset that consists of noisy comments on Reddit and professionally sourced translation. The translation are between French, Japanese and French, with between 7k and 37k sentence per language pair."}, {"id": "derisi-funcat", "name": "Derisi Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "lag-large-scale-attention-based-glaucoma", "name": "LAG (Large-scale Attention based Glaucoma)", "description": "Includes 5,824 fundus images labeled with either positive glaucoma (2,392) or negative glaucoma (3,432)."}, {"id": "bach-doodle", "name": "Bach Doodle", "description": "The Bach Doodle Dataset is composed of 21.6 million harmonizations submitted from the Bach Doodle. The dataset contains both metadata about the composition (such as the country of origin and feedback), as well as a MIDI of the user-entered melody and a MIDI of the generated harmonization. The dataset contains about 6 years of user entered music."}, {"id": "mvtec-d2s-mvtec-densely-segmented-supermarket", "name": "MVTec D2S (MVTec Densely Segmented Supermarket)", "description": "MVTec D2S is a benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21,000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse."}, {"id": "riddle-sense", "name": "Riddle Sense", "description": "Question: I have five fingers but I am not alive. What am I? Answer: a glove."}, {"id": "ttstroke-21-me22-ttstroke-21-for-mediaeval-2022", "name": "TTStroke-21 ME22 (TTStroke-21 for MediaEval 2022)", "description": "TTStroke-21 for MediaEval 2022. The task is of interest to researchers in the areas of machine learning (classification), visual content analysis, computer vision and sport performance. We explicitly encourage researchers focusing specifically in domains of computer-aided analysis of sport performance."}, {"id": "dpb-5l-french", "name": "DPB-5L (French)", "description": "DPB-5L is a Multilingual KG dataset containing 5 KGs in English, French, Japanese, Greek, and Spanish.  The dataset is used for the Knowledge Graph Completion and Entity Alignment task. DPB-5L (French) is a subset of DPB-5L with French KG."}, {"id": "sms-wsj-spatialized-multi-speaker-wall-street-journal", "name": "SMS-WSJ (Spatialized Multi-Speaker Wall Street Journal)", "description": "Spatialized Multi-Speaker Wall Street Journal (SMS-WSJ) consists of artificially mixed speech taken from the WSJ database, but unlike earlier databases this one considers all WSJ0+1 utterances and takes care of strictly separating the speaker sets present in the training, validation and test sets. "}, {"id": "covid-q", "name": "COVID-Q", "description": "COVID-Q consists of COVID-19 questions which have been annotated into a broad category (e.g. Transmission, Prevention) and a more specific class such that questions in the same class are all asking the same thing."}, {"id": "visual-madlibs", "name": "Visual Madlibs", "description": "Visual Madlibs is a dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context."}, {"id": "def-outnumbered-sequential", "name": "Def_Outnumbered_sequential", "description": "SMAC+ defensive outnumbered scenario with sequential episodic buffer"}, {"id": "scan-simplified-versions-of-the-commai-navigation-tasks", "name": "SCAN (Simplified versions of the CommAI Navigation tasks)", "description": "SCAN is a dataset for grounded navigation which consists of a set of simple compositional navigation commands paired with the corresponding action sequences. "}, {"id": "lhc-olympics-2020-lhc-olympics-2020-anomaly-detection-challenge", "name": "LHC Olympics 2020 (LHC Olympics 2020 Anomaly Detection Challenge)", "description": "These are the official datasets for the LHC Olympics 2020 Anomaly Detection Challenge. Each \"black box\" contains 1M events meant to be representative of actual LHC data. These events may include signal(s) and the challenge consists of finding these signals using the method of your choice. We have uploaded a total of THREE black boxes to be used for the challenge."}, {"id": "synwoodscape-synthetic-surround-view-fisheye-camera-dataset-for-autonomous-driving", "name": "SynWoodScape (Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving)", "description": "SynWoodScape is a synthetic version of the surround-view dataset covering many of its weaknesses and extending it. WoodScape comprises four surround-view cameras and nine tasks, including segmentation, depth estimation, 3D bounding box detection, and a novel soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for the fisheye camera instead of using naive rectification."}, {"id": "tid2013", "name": "TID2013", "description": "TID2013 is a dataset for image quality assessment that contains 25 reference images and 3000 distorted images (25 reference images x 24 types of distortions x 5 levels of distortions). "}, {"id": "mle2e", "name": "MLe2e", "description": "MLe2 is a dataset for the evaluation of scene text end-to-end reading systems and all intermediate stages such as text detection, script identification and text recognition. The dataset contains a total of 711 scene images covering four different scripts (Latin, Chinese, Kannada, and Hangul)."}, {"id": "imagenet-lt-imagenet-long-tailed", "name": "ImageNet-LT (ImageNet Long-Tailed)", "description": "ImageNet Long-Tailed is a subset of /dataset/imagenet dataset consisting of 115.8K images from 1000 categories, with maximally 1280 images per class and minimally 5 images per class. The additional classes of images in ImageNet-2010 are used as the open set."}, {"id": "safe-control-gym", "name": "safe-control-gym", "description": "safe-control-gym is an open-source benchmark suite that extends OpenAI's Gym API with (i) the ability to specify (and query) symbolic models and constraints and (ii) introduce simulated disturbances in the control inputs, measurements, and inertial properties. We provide implementations for three dynamic systems -- the cart-pole, 1D, and 2D quadrotor -- and two control tasks -- stabilization and trajectory tracking."}, {"id": "cbsd68-color-bsd68", "name": "CBSD68 (Color BSD68)", "description": "Color BSD68 dataset for image denoising benchmarks is part of The Berkeley Segmentation Dataset and Benchmark. It is used for measuring image denoising algorithms performance. It contains 68 images."}, {"id": "medicat", "name": "MedICaT", "description": "MedICaT is a dataset of medical images, captions, subfigure-subcaption annotations, and inline textual references. Figures and captions are extracted from open access articles in PubMed Central and corresponding reference text is derived from S2ORC. The dataset consists of: 217,060 figures from 131,410 open access papers 7507 subcaption and subfigure annotations for 2069 compound figures Inline references for ~25K figures in the ROCO dataset"}, {"id": "pix3d", "name": "Pix3D", "description": "The Pix3D dataset is a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc."}, {"id": "v2c-video-to-commonsense", "name": "V2C (Video-to-Commonsense)", "description": "Contains ~9K videos of human agents performing various actions, annotated with 3 types of commonsense descriptions."}, {"id": "rlbench", "name": "RLBench", "description": "RLBench is an ambitious large-scale benchmark and learning environment designed to facilitate research in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning."}, {"id": "hsd-honda-scenes-dataset", "name": "HSD (Honda Scenes Dataset)", "description": "An annotated dataset is released to enable dynamic scene classification that includes 80 hours of diverse high quality driving video data clips collected in the San Francisco Bay area. The dataset includes temporal annotations for road places, road types, weather, and road surface conditions. "}, {"id": "nightcity", "name": "NightCity", "description": "The largest real-world night-time semantic segmentation dataset with pixel-level labels."}, {"id": "dyml-vehicle-dynamic-metric-learning-vehicle", "name": "DyML-Vehicle (Dynamic Metric Learning Vehicle)", "description": "DyML-Vehicle merges two vehicle re-ID datasets PKU VehicleID [1], VERI-Wild [1]. Since these two datasets have only annotations on the identity (fine) level, we manually annotate each image with \u201cmodel\u201d label (e.g., Toyota Camry, Honda Accord, Audi A4) and \u201cbody type\u201d label (e.g., car, suv, microbus, pickup). Moreover, we label all the taxi images as a novel testing class under coarse level."}, {"id": "plantvillage", "name": "PlantVillage", "description": "The PlantVillage dataset consists of 54303 healthy and unhealthy leaf images divided into 38 categories by species and disease."}, {"id": "apron-dataset", "name": "Apron Dataset", "description": "The Apron Dataset focuses on training and evaluating classification and detection models for airport-apron logistics. In addition to bounding boxes and object categories the dataset is enriched with meta parameters to quantify the models\u2019 robustness against environmental influences."}, {"id": "triviaqa", "name": "TriviaQA", "description": "TriviaQA is a realistic text-based question answering dataset which includes 950K question-answer pairs from 662K documents collected from Wikipedia and the web. This dataset is more challenging than standard QA benchmark datasets such as Stanford Question Answering Dataset (SQuAD), as the answers for a question may not be directly obtained by span prediction and the context is very long. TriviaQA dataset consists of both human-verified and machine-generated QA subsets."}, {"id": "hazards-robots-hazards-robots-a-dataset-for-visual-anomaly-detection-in-robotics", "name": "Hazards&Robots (Hazards&Robots: A Dataset for Visual Anomaly Detection in Robotics)", "description": "We consider the problem of detecting, in the visual sensing data stream of an autonomous mobile robot, semantic patterns that are unusual (i.e., anomalous) with respect to the robot\u2019s previous experience in similar environments. These anomalies might indicate unforeseen hazards and, in scenarios where failure is costly, can be used to trigger an avoidance behavior. We contribute three novel image-based datasets acquired in robot exploration scenarios, comprising a total of more than 200k labeled frames, spanning various types of anomalies."}, {"id": "mnist-m", "name": "MNIST-M", "description": "MNIST-M is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background. It contains 59,001 training and 90,001 test images."}, {"id": "activitynet-thumbnails", "name": "ActivityNet Thumbnails", "description": "Consists of 10,000+ video-sentence pairs with each accompanied by an annotated sentence specified video thumbnail. "}, {"id": "explor-all", "name": "Explor_all", "description": "Explor_all font image dataset https://drive.google.com/file/d/1P2DbNbVw4Q__WcV1YdzE7zsDKilmd3pO/view"}, {"id": "cub-200-2011-caltech-ucsd-birds-200-2011", "name": "CUB-200-2011 (Caltech-UCSD Birds-200-2011)", "description": "The Caltech-UCSD Birds-200-2011 (CUB-200-2011) dataset is the most widely-used dataset for fine-grained visual categorization task. It contains 11,788 images of 200 subcategories belonging to birds, 5,994 for training and 5,794 for testing. Each image has detailed annotations: 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. The textual information comes from Reed et al.. They expand the CUB-200-2011 dataset by collecting fine-grained natural language descriptions. Ten single-sentence descriptions are collected for each image. The natural language descriptions are collected through the Amazon Mechanical Turk (AMT) platform, and are required at least 10 words, without any information of subcategories and actions."}, {"id": "vipl-hr", "name": "VIPL-HR", "description": "VIPL-HR database is a database for remote heart rate (HR) estimation from face videos under less-constrained situations. It contains 2,378 visible light videos (VIS) and 752 near-infrared (NIR) videos of 107 subjects. Nine different conditions, including various head movements and illumination conditions are taken into consideration. All the videos are recorded using Logitech C310, RealSense F200 and the front camera of HUAWEI P9 smartphone, and the ground-truth HR is recorded using a CONTEC CMS60C BVP sensor (a FDA approved device). "}, {"id": "backstabbers-knife-collection", "name": "Backstabber\u2019s Knife Collection", "description": "Backstabber\u2019s Knife Collection is a dataset of 174 malicious software packages that were used in real-world attacks on open source software supply chains, and which were distributed via the popular package repositories npm, PyPI, and RubyGems. Those packages, dating from November 2015 to November 2019, were manually collected and analyzed."}, {"id": "medhop", "name": "MedHop", "description": "With the same format as WikiHop, the MedHop dataset is based on research paper abstracts from PubMed, and the queries are about interactions between pairs of drugs. The correct answer has to be inferred by combining information from a chain of reactions of drugs and proteins."}, {"id": "talk-the-walk", "name": "Talk the Walk", "description": "Talk The Walk is a large-scale dialogue dataset grounded in action and perception. The task involves two agents (a \u201cguide\u201d and a \u201ctourist\u201d) that communicate via natural language in order to achieve a common goal: having the tourist navigate to a given target location."}, {"id": "ijb-b-iarpa-janus-benchmark-b", "name": "IJB-B (IARPA Janus Benchmark-B)", "description": "The IJB-B dataset is a template-based face dataset that contains 1845 subjects with 11,754 images, 55,025 frames and 7,011 videos where a template consists of a varying number of still images and video frames from different sources. These images and videos are collected from the Internet and are totally unconstrained, with large variations in pose, illumination, image quality etc. In addition, the dataset comes with protocols for 1-to-1 template-based face verification, 1-to-N template-based open-set face identification, and 1-to-N open-set video face identification."}, {"id": "parasitic-egg-detection-and-classification-in-microscopic-images", "name": "Parasitic Egg Detection and Classification in Microscopic Images", "description": "Parasitic infections have been recognized as one of the most significant causes of illnesses by WHO. Most infected persons shed cysts or eggs in their living environment, and unwittingly cause transmission of parasites to other individuals. Diagnosis of intestinal parasites is usually based on direct examination in the laboratory, of which capacity is obviously limited. Targeting to automate routine fecal examination for parasitic diseases, this challenge aims to gather experts in the field to develop robust automated methods to detect and classify eggs of parasitic worms in a variety of microscopic images. Participants will work with a large-scale dataset, containing 11 types of parasitic eggs from fecal smear samples. They are the main interest because of causing major diseases and illness in developing countries. We open to any techniques used for parasitic egg recognition, ranging from conventional approaches based on statistical models to deep learning techniques. Finally, the organizers expect a new collaboration come out from the challenge."}, {"id": "cars196", "name": "CARS196", "description": "CARS196  is composed of 16,185 car images of 196 classes."}, {"id": "i-raven-impartial-raven", "name": "I-RAVEN (Impartial-RAVEN)", "description": "To fix the defacts of RAVEN dataset, we generate an alternative answer set for each RPM question in RAVEN, forming an improved dataset named Impartial-RAVEN (I-RAVEN for short)."}, {"id": "sketchycoco", "name": "SketchyCOCO", "description": "SketchyCOCO dataset consists of two parts:"}, {"id": "msmt17-c", "name": "MSMT17-C", "description": "MSMT17-C is an evaluation set that consists of algorithmically generated corruptions applied to the MSMT17 test-set.  These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."}, {"id": "harm-c", "name": "Harm-C", "description": "Harm-C is a dataset for detecting harmful memes related to Covid-19."}, {"id": "taskmaster-1", "name": "Taskmaster-1", "description": "Taskmaster-1 is a dialog dataset consisting of 13,215 task-based dialogs in English, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations."}, {"id": "mip-nerf-360-unbounded-anti-aliased-neural-radiance-fields", "name": "Mip-NeRF 360 (Unbounded Anti-Aliased Neural Radiance Fields)", "description": "Mip-NeRF 360 is an extension to the Mip-NeRF that uses a non-linear parameterization, online distillation, and a novel distortion-based regularize to overcome the challenge of unbounded scenes. The dataset consists of 9 scenes with 5 outdoors and 4 indoors, each containing a complex central object or area with a detailed background."}, {"id": "haspart-kb", "name": "hasPart KB", "description": "This dataset is a new knowledge-base (KB) of hasPart relationships, extracted from a large corpus of generic statements. Complementary to other resources available, it is the first which is all three of: accurate (90% precision), salient (covers relationships a person may mention), and has high coverage of common terms (approximated as within a 10 year old\u2019s vocabulary), as well as having several times more hasPart entries than in the popular ontologies ConceptNet and WordNet. In addition, it contains information about quantifiers, argument modifiers, and links the entities to appropriate concepts in Wikipedia and WordNet."}, {"id": "mslr-web30k", "name": "MSLR-WEB30K", "description": "The MSLR-WEB30K dataset consists of 30,000 search queries over the documents from search results. The data also contains the values of 136 features and a corresponding user-labeled relevance factor on a scale of one to five with respect to each query-document pair."}, {"id": "3d-zef-3d-zebrafish-tracking-benchmark", "name": "3D-ZeF (3D ZebraFish Tracking Benchmark)", "description": "3D-ZeF dataset consists of eight sequences with a duration between 15-120 seconds and 1-10 free moving zebrafish. The videos have been annotated with a total of 86,400 points and bounding boxes."}, {"id": "scanbank", "name": "ScanBank", "description": "ScanBank is a benchmark dataset for figure extraction from scanned electronic theses and dissertations containing 10 thousand scanned page images, manually labeled by humans as to the presence of the 3.3 thousand figures or tables found therein."}, {"id": "ddi-100-distorted-document-images", "name": "DDI-100 (Distorted Document Images)", "description": "The DDI-100 dataset is a synthetic dataset for text detection and recognition based on 7000 real unique document pages and consists of more than 100000 augmented images. The ground truth comprises text and stamp masks, text and characters bounding boxes with relevant annotations."}, {"id": "akces-gec", "name": "AKCES-GEC", "description": "AKCES-GEC is a new dataset on grammatical error correction for Czech."}, {"id": "occ-traj120", "name": "Occ-Traj120", "description": "Occ-Traj120 is a trajectory dataset that contains occupancy representations of different local-maps with associated trajectories. This dataset contains 400 locally-structured maps with occupancy representation and roughly around 120K trajectories in total."}, {"id": "ava-atomic-visual-actions", "name": "AVA (Atomic Visual Actions)", "description": "AVA is a project that provides audiovisual annotations of video for improving our understanding of human activity. Each of the video clips has been exhaustively annotated by human annotators, and together they represent a rich variety of scenes, recording conditions, and expressions of human activity. There are annotations for:"}, {"id": "nemo-corpus-nemo-hebrew-ner-and-morphology-corpus", "name": "NEMO-Corpus (NEMO Hebrew NER and Morphology Corpus)", "description": "Named Entity (NER) annotations of the Hebrew Treebank (Haaretz newspaper) corpus, including: morpheme and token level NER labels, nested mentions, and more. We publish the NEMO corpus in the TACL paper \"Neural Modeling for Named Entities and Morphology (NEMO^2)\" [1], where we use it in extensive experiments and analyses, showing the importance of morphological boundaries for neural modeling of NER in morphologically rich languages. Code for these models and experiments can be found in the NEMO code repo."}, {"id": "spice-small-molecule-protein-interaction-chemical-energies", "name": "SPICE (Small-Molecule/Protein Interaction Chemical Energies)", "description": "SPICE is a collection of quantum mechanical data for training potential functions. The emphasis is particularly on simulating drug-like small molecules interacting with proteins. It is designed to achieve the following goals:"}, {"id": "lasvr", "name": "LasVR", "description": "A large-scale video database for rain removal (LasVR), which consists of 316 rain videos."}, {"id": "heavy-snowfall-dense", "name": "Heavy Snowfall (DENSE)", "description": "We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are: - We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions. - The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects. - In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods. - Please check out our paper for more information."}, {"id": "magnatagatune", "name": "MagnaTagATune", "description": "MagnaTagATune dataset contains 25,863 music clips. Each clip is a 29-seconds-long excerpt belonging to one of the 5223 songs, 445 albums and 230 artists. The clips span a broad range of genres like Classical, New Age, Electronica, Rock, Pop, World, Jazz, Blues, Metal, Punk, and more. Each audio clip is supplied with a vector of binary annotations of 188 tags. These annotations are obtained by humans playing the two-player online TagATune game. In this game, the two players are either presented with the same or a different audio clip. Subsequently, they are asked to come up with tags for their specific audio clip. Afterward, players view each other\u2019s tags and are asked to decide whether they were presented the same audio clip. Tags are only assigned when more than two players agreed. The annotations include tags like \u2019singer\u2019, \u2019no singer\u2019, \u2019violin\u2019, \u2019drums\u2019, \u2019classical\u2019, \u2019jazz\u2019. The top 50 most popular tags are typically used for evaluation to ensure that there is enough training data for each tag. There are 16 parts, and researchers comonnly use parts 1-12 for training, part 13 for validation and parts 14-16 for testing."}, {"id": "ucr-time-series-classification-archive", "name": "UCR Time Series Classification Archive", "description": "The UCR Time Series Archive - introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one data set from the archive. The original incarnation of the archive had sixteen data sets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 to 85 data sets. This paper introduces and will focus on the new data expansion from 85 to 128 data sets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline (1-nearest neighbor classification), a large fraction may be misattributing the reasons for their improvement. Moreover, they may have been able to achieve the same improvement with a much simpler modification, requiring just a single line of code."}, {"id": "odms-object-depth-via-motion-and-segmentation", "name": "ODMS (Object Depth via Motion and Segmentation)", "description": "ODMS is a dataset for learning Object Depth via Motion and Segmentation. ODMS training data are configurable and extensible, with each training example consisting of a series of object segmentation masks, camera movement distances, and ground truth object depth. As a benchmark evaluation, the dataset provides four ODMS validation and test sets with 15,650 examples in multiple domains, including robotics and driving."}, {"id": "imagenet-vidvrd", "name": "ImageNet-VidVRD", "description": "ImageNet-VidVRD dataset contains 1,000 videos selected from ILVSRC2016-VID dataset based on whether the video contains clear visual relations. It is split into 800 training set and 200 test set, and covers common subject/objects of 35 categories and predicates of 132 categories. Ten people contributed to labeling the dataset, which includes object trajectory labeling and relation labeling. Since the ILVSRC2016-VID dataset has the object trajectory annotation for 30 categories already, we supplemented the annotations by labeling the remaining 5 categories. In order to save the labor of relation labeling, we labeled typical segments of the videos in the training set and the whole of the videos in the test set."}, {"id": "arvsu-addressee-recognition-in-visual-scenes-with-utterances", "name": "ARVSU (Addressee Recognition in Visual Scenes with Utterances)", "description": "ARVSU contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario."}, {"id": "dabs-domain-agnostic-benchmark-for-self-supervised-learning", "name": "DABS (Domain-Agnostic Benchmark for Self-supervised learning)", "description": "DABS is a domain-agnostic benchmark for self-supervised learning to encourage research and progress towards domain-agnostic methods."}, {"id": "objectsroom", "name": "ObjectsRoom", "description": "The ObjectsRoom dataset is based on the MuJoCo environment used by the Generative Query Network [4] and is a multi-object extension of the 3d-shapes dataset. The training set contains 1M scenes with up to three objects. We also provide ~1K test examples for the following variants:"}, {"id": "ijb-c-iarpa-janus-benchmark-c", "name": "IJB-C (IARPA Janus Benchmark-C)", "description": "The IJB-C dataset is a video-based face recognition dataset. It is an extension of the IJB-A dataset with about 138,000 face images, 11,000 face videos, and 10,000 non-face images."}, {"id": "astock", "name": "Astock", "description": "(1) provide financial news for each specific stock. (2) provide various stock technical factors and fundamental factors for each stock."}, {"id": "k-hairstyle", "name": "K-Hairstyle", "description": "K-hairstyle is a novel large-scale Korean hairstyle dataset with 256,679 high-resolution images. In addition, K-hairstyle contains various hair attributes annotated by Korean expert hair stylists and hair segmentation masks."}, {"id": "ubi-fights-abnormal-event-detection-dataset", "name": "UBI-Fights (Abnormal Event Detection Dataset)", "description": "UBI-Fights - Concerning a specific anomaly detection and still providing a wide diversity in fighting scenarios, the UBI-Fights dataset is a unique new large-scale dataset of 80 hours of video fully annotated at the frame level. Consisting of 1000 videos, where 216 videos contain a fight event, and 784 are normal daily life situations. All unnecessary video segments (e.g., video introductions, news, etc.) that could disturb the learning process were removed."}, {"id": "hellaswag", "name": "HellaSwag", "description": "HellaSwag is a challenge dataset for evaluating commonsense NLI that is specially hard for state-of-the-art models, though its questions are trivial for humans (>95% accuracy)."}, {"id": "dfuc2021-diabetic-foot-ulcers-2021", "name": "DFUC2021 (Diabetic Foot Ulcers 2021)", "description": "The Diabetic Foot Ulcers dataset (DFUC2021) is a dataset for analysis of pathology, focusing on infection and ischaemia. The final release of DFUC2021 consists of 15,683 DFU patches, with 5,955 training, 5,734 for testing and 3,994 unlabeled DFU patches. The ground truth labels are four classes, i.e. control, infection, ischaemia and both conditions."}, {"id": "valse-valse-a-task-independent-benchmark-for-vision-and-language-models-centered-on-linguistic-phenomena", "name": "VALSE (VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena)", "description": "We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We expect VALSE to serve as an important benchmark to measure future progress of pretrained V&L models from a linguistic perspective, complementing the canonical task-centred V&L evaluations."}, {"id": "logo-2k", "name": "Logo-2K+", "description": "Logo-2K+:A Large-Scale Logo Dataset for Scalable Logo Classi\ufb01cation The Logo-2K+ dataset contains a diverse range of logo classes from real-world logo images. It contains 167,140 images with 10 root categories and 2,341 leaf categories. The 10 different root categories are: Food, Clothes, Institution, Accessories, Transportation, Electronic, Necessities, Cosmetic, Leisure and Medical."}, {"id": "pinterest", "name": "Pinterest", "description": "The Pinterest dataset contains more than 1 million images associated to Pinterest users\u2019 who have \u201cpinned\u201d them."}, {"id": "xglue", "name": "XGLUE", "description": "XGLUE is an evaluation benchmark XGLUE,which is composed of 11 tasks that span 19 languages. For each task, the training data is only available in English. This means that to succeed at XGLUE, a model must have a strong zero-shot cross-lingual transfer capability to learn from the English data of a specific task and transfer what it learned to other languages. Comparing to its concurrent work XTREME, XGLUE has two characteristics: First, it includes cross-lingual NLU and cross-lingual NLG tasks at the same time; Second, besides including 5 existing cross-lingual tasks (i.e. NER, POS, MLQA, PAWS-X and XNLI), XGLUE selects 6 new tasks from Bing scenarios as well, including News Classification (NC), Query-Ad Matching (QADSM), Web Page Ranking (WPR), QA Matching (QAM), Question Generation (QG) and News Title Generation (NTG). Such diversities of languages, tasks and task origin provide a comprehensive benchmark for quantifying the quality of a pre-trained model on cross-lingual natural language understanding and generation."}, {"id": "sgd-schema-guided-dialogue", "name": "SGD (Schema-Guided Dialogue)", "description": "The Schema-Guided Dialogue (SGD) dataset consists of over 20k annotated multi-domain, task-oriented conversations between a human and a virtual assistant. These conversations involve interactions with services and APIs spanning 20 domains, ranging from banks and events to media, calendar, travel, and weather. For most of these domains, the dataset contains multiple different APIs, many of which have overlapping functionalities but different interfaces, which reflects common real-world scenarios. The wide range of available annotations can be used for intent prediction, slot filling, dialogue state tracking, policy imitation learning, language generation, user simulation learning, among other tasks in large-scale virtual assistants. Besides these, the dataset has unseen domains and services in the evaluation set to quantify the performance in zero-shot or few shot settings."}, {"id": "atrw-amur-tiger-re-identification-in-the-wild", "name": "ATRW (Amur Tiger Re-identification in the Wild)", "description": "The ATRW Dataset contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. "}, {"id": "expr-funcat", "name": "Expr Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "binkit", "name": "BinKit", "description": "BinKit is a binary code similarity analysis (BCSA) benchmark. BinKit provides scripts for building a cross-compiling environment, as well as the compiled dataset. The original dataset includes 1,352 distinct combinations of compiler options of 8 architectures, 5 optimization levels, and 13 compilers."}, {"id": "comqa", "name": "ComQA", "description": "ComQA is a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology."}, {"id": "gta5-grand-theft-auto-5", "name": "GTA5 (Grand Theft Auto 5)", "description": "The GTA5 dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game Grand Theft Auto 5 and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset."}, {"id": "lidar-mos-lidar-based-moving-object-segmentation", "name": "LiDAR-MOS (LiDAR-based Moving Object Segmentation)", "description": "In moving object segmentation of point cloud sequences, one has to provide motion labels for each point of the test sequences 11-21. Therefore, the input to all evaluated methods is a list of coordinates of the three-dimensional points along with their remission, i.e., the strength of the reflected laser beam which depends on the properties of the surface that was hit. Each method should then output a label for each point of a scan, i.e., one full turn of the rotating LiDAR sensor. Here, we only distinguish between static and moving object classes."}, {"id": "d3-dblp-discovery-dataset", "name": "D3 (DBLP Discovery Dataset)", "description": "DBLP is the largest open-access repository of scientific articles on computer science and provides metadata associated with publications, authors, and venues. We retrieved more than 6 million publications from DBLP and extracted pertinent metadata (e.g., abstracts, author affiliations, citations) from the publication texts to create the D3 Discovery Dataset (D3). D3 can be used to identify trends in research activity, productivity, focus, bias, accessibility, and impact of computer science research."}, {"id": "chestx-ray14", "name": "ChestX-ray14", "description": "ChestX-ray14 is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia."}, {"id": "road-anomaly", "name": "Road Anomaly", "description": "This dataset contains images of unusual dangers which can be encountered by a vehicle on the road \u2013 animals, rocks, traffic cones and other obstacles. Its purpose is testing autonomous driving perception algorithms in rare but safety-critical circumstances."}, {"id": "cn-celeb", "name": "CN-CELEB", "description": "CN-Celeb is a large-scale speaker recognition dataset collected `in the wild'. This dataset contains more than 130,000 utterances from 1,000 Chinese celebrities, and covers 11 different genres in real world."}, {"id": "qtuna", "name": "QTuna", "description": "The QTUNA dataset is the result of a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions in order to inform possible Natural Language Generation algorithms that mimic humans' use of quantified expressions."}, {"id": "visual-question-answering-vqa", "name": "Visual Question Answering (VQA)", "description": "Visual Question Answering (VQA) is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. The first version of the dataset was released in October 2015. VQA v2.0 was released in April 2017."}, {"id": "headcam", "name": "Headcam", "description": "This dataset contains panoramic video captured from a helmet-mounted camera while riding a bike through suburban Northern Virginia. "}, {"id": "evogym-evolution-gym", "name": "EvoGym (Evolution Gym)", "description": "EvoGym is a large-scale benchmark for co-optimizing the design and control of soft robots."}, {"id": "vln-ce-vision-and-language-navigation-in-continuous-environments", "name": "VLN-CE (Vision-and-Language Navigation in Continuous Environments)", "description": "Vision and Language Navigation in Continuous Environments (VLN-CE) is an instruction-guided navigation task with crowdsourced instructions, realistic environments, and unconstrained agent navigation. The dataset consists of 4475 trajectories converted from Room-to-Room train and validation splits. For each trajectory, multiple natural language instructions from Room-to-Room and a pre-computed shortest path are provided following the waypoints via low-level actions."}, {"id": "merl-shopping", "name": "MERL Shopping", "description": "MERL Shopping is a dataset for training and testing action detection algorithms. The MERL Shopping Dataset consists of 106 videos, each of which is a sequence about 2 minutes long. The videos are from a fixed overhead camera looking down at people shopping in a grocery store setting. Each video contains several instances of the following 5 actions: \"Reach To Shelf\" (reach hand into shelf), \"Retract From Shelf \" (retract hand from shelf), \"Hand In Shelf\" (extended period with hand in the shelf), \"Inspect Product\" (inspect product while holding it in hand), and \"Inspect Shelf\" (look at shelf while not touching or reaching for the shelf)."}, {"id": "af-classification-from-a-short-single-lead-ecg-recording-the-physionet-computing-in-cardiology-challenge-2017", "name": "AF Classification from a Short Single Lead ECG Recording - The PhysioNet Computing in Cardiology Challenge 2017", "description": "The 2017 PhysioNet/CinC Challenge aims to encourage the development of algorithms to classify, from a single short ECG lead recording (between 30 s and 60 s in length), whether the recording shows normal sinus rhythm, atrial fibrillation (AF), an alternative rhythm, or is too noisy to be classified."}, {"id": "lytro-illum", "name": "Lytro Illum", "description": "Lytro Illum is a new light field dataset using a Lytro Illum camera. 640 light fields are collected with significant variations in terms of size, textureness, background clutter and illumination, etc. Micro-lens image arrays and central viewing images are generated, and corresponding ground-truth maps are produced."}, {"id": "aih-amodal-interhand", "name": "AIH (Amodal InterHand)", "description": "AIH is created for hand deocclusion and removal."}, {"id": "xp3", "name": "xP3", "description": "xP3 is a multilingual dataset for multitask prompted finetuning. It is a composite of supervised datasets in 46 languages with English and machine-translated prompts."}, {"id": "timo-timo-time-of-flight-indoor-monitoring", "name": "TIMo (TIMo (Time-of-Flight Indoor Monitoring))", "description": "TIMo (Time-of-Flight Indoor Monitoring) is a dataset of infrared and depth videos intended for the use in Anomaly Detection and Person Detection/People Counting. It features more than 1,500 sequences for anomaly detection, which sum up to more than 500,000 individual frames. For person detection the dataset contains more than than 240 sequences. The data was captured using a Microsoft Azure Kinect RGB-D camera. In addition, we provide annotations of anomalous frame ranges for use with anomaly detection and bounding boxes and segmentation masks for use with person detection. The data was captured in parts from a tilted view and a top-down perspective."}, {"id": "multithumos", "name": "MultiTHUMOS", "description": "The MultiTHUMOS dataset contains dense, multilabel, frame-level action annotations for 30 hours across 400 videos in the THUMOS'14 action detection dataset. It consists of 38,690 annotations of 65 action classes, with an average of 1.5 labels per frame and 10.5 action classes per video."}, {"id": "a-large-dataset-of-object-scans", "name": "A Large Dataset of Object Scans", "description": "A Large Dataset of Object Scans is a dataset of more than ten thousand 3D scans of real objects. To create the dataset, the authors recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators scanned objects of their choosing, outside the laboratory and without direct supervision by computer vision professionals. The result is a large and diverse collection of object scans: from shoes, mugs, and toys to grand pianos, construction vehicles, and large outdoor sculptures. The authors worked with an attorney to ensure that data acquisition did not violate privacy constraints. The acquired data was placed in the public domain and is available freely."}, {"id": "avd-active-vision-dataset", "name": "AVD (Active Vision Dataset)", "description": "AVD focuses on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes."}, {"id": "hla-chat", "name": "HLA-Chat", "description": "Models character profiles and gives dialogue agents the ability to learn characters' language styles through their HLAs."}, {"id": "shmoop-corpus", "name": "Shmoop Corpus", "description": "Shmoop Corpus is a dataset of 231 stories that are paired with detailed multi-paragraph summaries for each individual chapter (7,234 chapters), where the summary is chronologically aligned with respect to the story chapter. From the corpus, a set of common NLP tasks are constructed, including Cloze-form question answering and a simplified form of abstractive summarization, as benchmarks for reading comprehension on stories."}, {"id": "coaid", "name": "CoAID", "description": "CoAID include diverse COVID-19 healthcare misinformation, including fake news on websites and social platforms, along with users' social engagement about such news. CoAID includes 4,251 news, 296,000 related user engagements, 926 social platform posts about COVID-19, and ground truth labels."}, {"id": "vocalfolds", "name": "VocalFolds", "description": "The Vocal Folds dataset is a dataset for automatic segmentation of laryngeal endoscopic images. The dataset consists of 8 sequences from 2 patients containing 536 hand segmented in vivo colour images of the larynx during two different resection interventions with a resolution of 512x512 pixels."}, {"id": "indicnlp-corpus", "name": "IndicNLP Corpus", "description": "The IndicNLP corpus is a large-scale, general-domain corpus containing 2.7 billion words for 10 Indian languages from two language families."}, {"id": "dicm", "name": "DICM", "description": "DICM is a dataset for low-light enhancement which consists of 69 images collected with commercial digital cameras."}, {"id": "soccernet", "name": "SoccerNet", "description": "A benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). "}, {"id": "sketchgraphs", "name": "SketchGraphs", "description": "SketchGraphs is a dataset of 15 million sketches extracted from real-world CAD models intended to facilitate research in both ML-aided design and geometric program induction. Each sketch is represented as a geometric constraint graph where edges denote designer-imposed geometric relationships between primitives, the nodes of the graph."}, {"id": "minneapple", "name": "MinneApple", "description": "MinneApple is a benchmark dataset for apple detection and segmentation. The fruits are labelled using polygonal masks for each object instance to aid in precise object detection, localization, and segmentation. Additionally, the dataset also contains data for patch-based counting of clustered fruits. The dataset contains over 41, 000 annotated object instances in 1000 images."}, {"id": "a-simulated-4-dof-ship-motion-dataset-for-system-identification-under-environmental-disturbances", "name": "A Simulated 4-DOF Ship Motion Dataset for System Identification under Environmental Disturbances", "description": "This dataset contains data of 125 1-hour simulations of ship motion during various sea states performing random maneuvers in 4 degrees of freedom (surge-sway-yaw-roll). The original ship is a patrol ship developed by Perez et al. [1]. We have extended it with a set of two symmetrically placed rudder propellers. Additionally, we simulate wind forces according to Isherwood's wind model [2]. Wind-induced waves are generated with the JONSWAP spectrum [3] and the corresponding wave forces are then computed using wave force response amplitude operators (ROA)."}, {"id": "def-armored-parallel-smac-def-armored-parallel-20", "name": "Def_Armored_parallel (SMAC+_Def_Armored_parallel_20)", "description": "smac+ defense armored scenario with parallel episodic buffer"}, {"id": "caltech-101", "name": "Caltech-101", "description": "The Caltech101 dataset contains images from 101 object categories (e.g., \u201chelicopter\u201d, \u201celephant\u201d and \u201cchair\u201d etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300\u00d7200 pixels."}, {"id": "m2e2", "name": "M2E2", "description": "Aims to extract events and their arguments from multimedia documents. Develops the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments."}, {"id": "tosca-tosca-high-resolution", "name": "TOSCA (TOSCA high-resolution)", "description": "Hi-resolution three-dimensional nonrigid shapes in a variety of poses for non-rigid shape similarity and correspondence experiments. The database contains a total of 80 objects, including 11 cats, 9 dogs, 3 wolves, 8 horses, 6 centaurs, 4 gorillas, 12 female figures, and two different male figures, containing 7 and 20 poses. Typical vertex count is about 50,000. Objects within the same class have the same triangulation and an equal number of vertices numbered in a compatible way. This can be used as a per-vertex ground truth correspondence in correspondence experiments. Two representations are available: MATLAB file (.mat) and ASCII text files containing the 1-based list of triangular faces (.tri), and a list of vertex XYZ coordinates (.vert). A .png thumbnail is available for each object."}, {"id": "bug", "name": "BUG", "description": "BUG is a large-scale gender bias dataset of 108K diverse real-world English sentences, sampled semiautomatically from large corpora using lexical syntactic pattern matching"}, {"id": "tgif-tumblr-gif", "name": "TGIF (Tumblr GIF)", "description": "The Tumblr GIF (TGIF) dataset contains 100K animated GIFs and 120K sentences describing visual content of the animated GIFs. The animated GIFs have been collected from Tumblr, from randomly selected posts published between May and June of 2015. The dataset provides the URLs of animated GIFs. The sentences are collected via crowdsourcing, with a carefully designed annotation interface that ensures high quality dataset. There is one sentence per animated GIF for the training and validation splits, and three sentences per GIF for the test split. The dataset can be used to evaluate animated GIF/video description techniques."}, {"id": "rainbow", "name": "Rainbow", "description": "Rainbow is multi-task benchmark for common-sense reasoning that uses different existing QA datasets: aNLI, Cosmos QA, HellaSWAG. Physical IQa, Social IQa, WinoGrande."}, {"id": "gasch2-funcat", "name": "Gasch2 Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "asc-til-19-tasks-task-incremental-aspect-sentiment-classification", "name": "ASC (TIL, 19 tasks) (Task Incremental Aspect Sentiment Classification)", "description": "A set of 19 ASC datasets (reviews of 19 products) producing a sequence of 19 tasks. Each dataset represents a task. The datasets are from 4 sources: (1) HL5Domains (Hu and Liu, 2004) with reviews of 5 products; (2) Liu3Domains (Liu et al., 2015) with reviews of 3 products; (3) Ding9Domains (Ding et al., 2008) with reviews of 9 products; and (4) SemEval14 with reviews of 2 products - SemEval 2014 Task 4 for laptop and restaurant. For (1), (2) and (3), we split about 10% of the original data as the validate data, another about 10% of the original data as the testing data. For (4), We use 150 examples from the training set for validation. To be consistent with existing research(Tang et al., 2016), examples belonging to the conflicting polarity (both positive and negative sentiments are expressed about an aspect term) are not used. Statistics and details of the 19 datasets are given on Page https://github.com/ZixuanKe/PyContinual."}, {"id": "a-comparison-of-different-maturity-models", "name": "A comparison of different maturity models", "description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled)."}, {"id": "synthetic-rain-datasets", "name": "Synthetic Rain Datasets", "description": "The Synthetic Rain Datasets consists of 13,712 clean-rain image pairs gathered from multiple datasets (Rain14000, Rain1800, Rain800, Rain12). With a single trained model, evaluation could be performed on various test sets, including Rain100H, Rain100L, Test100, Test2800, and Test1200."}, {"id": "ov", "name": "OV", "description": "OV dataset is the camera calibration dataset. There are 16 lenses ranging from 90\u00b0 to 180\u00b0 FOV:"}, {"id": "frmt-few-shot-region-aware-machine-translation", "name": "FRMT (Few-shot Region-aware Machine Translation)", "description": "FRMT is a dataset and evaluation benchmark for Few-shot Region-aware Machine Translation, a type of style-targeted translation. The dataset consists of human translations of a few thousand English Wikipedia sentences into regional variants of Portuguese and Mandarin. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms."}, {"id": "iqiyi-vid", "name": "iQIYI-VID", "description": "iQIYI-VID dataset, which comprises video clips from iQIYI variety shows, films, and television dramas. The whole dataset contains 500,000 videos clips of 5,000 celebrities. The length of each video is 1~30 seconds. "}, {"id": "qrecc", "name": "QReCC", "description": "QReCC contains 14K conversations with 81K question-answer pairs. QReCC is built on questions from TREC CAsT, QuAC and Google Natural Questions. While TREC CAsT and QuAC datasets contain multi-turn conversations, Natural Questions is not a conversational dataset. Questions in NQ dataset were used as prompts to create conversations explicitly balancing types of context-dependent questions, such as anaphora (co-references) and ellipsis."}, {"id": "dagw-danish-gigaword", "name": "DAGW (Danish Gigaword)", "description": "It\u2019s hard to develop good tools for processing Danish with computers when no large and wide-coverage dataset of Danish text is readily available. To address this, the Danish Gigaword Project (DAGW) maintains a corpus for Danish with over a billion words. The general goals are to create a dataset that is:"}, {"id": "urbancars", "name": "UrbanCars", "description": "UrbanCars facilitates multi-shortcut learning under the controlled setting with two shortcuts\u2014background and co-occurring object. The task is classifying the car body type into two categories: urban car and country car. The dataset contains three splits: training, validation, and testing. In the training set, two shortcuts spuriously correlate with the car body type. Both validation and testing sets are balanced, i.e., no spurious correlations. The validation set is used for model selection, and the testing set evaluates the mitigation of two shortcuts."}, {"id": "litis-rouen", "name": "LITIS Rouen", "description": "The LITIS-Rouen dataset  is a dataset for audio scenes. It consists of 3026 examples of 19 scene categories. Each class is specific to a location such as a train station or an open market. The audio recordings have a duration of 30 seconds and a sampling rate of 22050 Hz. The dataset has a total duration of 1500 minutes."}, {"id": "interview", "name": "Interview", "description": "A large-scale (105K conversations) media dialog dataset collected from news interview transcripts."}, {"id": "vot2017-visual-object-tracking-challenge", "name": "VOT2017 (Visual Object Tracking Challenge)", "description": "VOT2017 is a Visual Object Tracking dataset for different tasks that contains 60 short sequences annotated with 6 different attributes."}, {"id": "rae-rainforest-automation-energy", "name": "RAE (Rainforest Automation Energy)", "description": "The Rainforest Automation Energy (RAE) dataset was create to help smart grid researchers test their algorithms which make use of smart meter data. This initial release of RAE contains 1Hz data (mains and sub-meters) from two residential houses. In addition to power data, environmental and sensor data from the house's thermostat is included. Sub-meter data from one of the houses includes heat pump and rental suite captures which is of interest to power utilities."}, {"id": "visdrone", "name": "VisDrone", "description": "VisDrone is a large-scale benchmark with carefully annotated ground-truth for various important computer vision tasks, to make vision meet drones. The VisDrone2019 dataset is collected by the AISKYEYE team at Lab of Machine Learning and Data Mining, Tianjin University, China. The benchmark dataset consists of 288 video clips formed by 261,908 frames and 10,209 static images, captured by various drone-mounted cameras, covering a wide range of aspects including location (taken from 14 different cities separated by thousands of kilometers in China), environment (urban and country), objects (pedestrian, vehicles, bicycles, etc.), and density (sparse and crowded scenes). Note that, the dataset was collected using various drone platforms (i.e., drones with different models), in different scenarios, and under various weather and lighting conditions. These frames are manually annotated with more than 2.6 million bounding boxes of targets of frequent interests, such as pedestrians, cars, bicycles, and tricycles. Some important attributes including scene visibility, object class and occlusion, are also provided for better data utilization."}, {"id": "arc-da-arc-direct-answer-questions", "name": "ARC-DA (ARC Direct Answer Questions)", "description": "ARC Direct Answer Questions (ARC-DA) dataset consists of 2,985 grade-school level, direct-answer (\"open response\", \"free form\") science questions derived from the ARC multiple-choice question set released as part of the AI2 Reasoning Challenge in 2018."}, {"id": "simjeb-simulated-jet-engine-bracket", "name": "SimJEB (Simulated Jet Engine Bracket)", "description": "Simulated Jet Engine Bracket Dataset (SimJEB)  is a public collection of crowdsourced mechanical brackets and high-fidelity structural simulations designed specifically for surrogate modeling. SimJEB models are more complex, diverse, and realistic than the synthetically generated datasets commonly used in parametric surrogate model evaluation. In contrast to existing engineering shape collections, SimJEB's models are all designed for the same engineering function and thus have consistent structural loads and support conditions. The models in SimJEB were collected from the original submissions to the GrabCAD Jet Engine Bracket Challenge: an open engineering design competition with over 700 hand-designed CAD entries from 320 designers representing 56 countries. Each model has been cleaned, categorized, meshed, and simulated with finite element analysis according to the original competition specifications. The result is a collection of diverse, high-quality and application-focused designs for advancing geometric deep learning and engineering surrogate models."}, {"id": "refseer", "name": "RefSeer", "description": "A data set containing citations, citation contexts, and papers."}, {"id": "bb-mas-behavioural-biometrics-multi-device-and-multi-activity-data-from-same-users", "name": "BB-MAS (Behavioural Biometrics Multi-device and multi-Activity data from Same users)", "description": "BB-MAS is a behavioural biometrics dataset. It consists of data collected from 117 subjects for typing (both fixed and free text), gait (walking, upstairs and downstairs) and touch on Desktop, Tablet and Phone. The dataset consists a total of about: 3.5 million keystroke events; 57.1 million data-points for accelerometer and gyroscope each; 1.7 million data-points for swipes; and enables future research to explore previously unexplored directions in inter-device and inter-modality biometrics. "}, {"id": "forecastqa", "name": "ForecastQA", "description": "ForecastQA is a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. The forecasting problem for this dataset is formulated as a restricted-domain, multiple-choice, question-answering (QA) task that simulates the forecasting scenario."}, {"id": "wsc-winograd-schema-challenge", "name": "WSC (Winograd Schema Challenge)", "description": "The Winograd Schema Challenge was introduced both as an alternative to the Turing Test and as a test of a system\u2019s ability to do commonsense reasoning. A Winograd schema is a pair of sentences differing in one or two words with a highly ambiguous pronoun, resolved differently in the two sentences, that appears to require commonsense knowledge to be resolved correctly. The examples were designed to be easily solvable by humans but difficult for machines, in principle requiring a deep understanding of the content of the text and the situation it describes."}, {"id": "hopeedi-hopeedi-a-multilingual-hope-speech-detection-dataset-for-equality-diversity-and-inclusion", "name": "HopeEDI (HopeEDI: A Multilingual Hope Speech Detection Dataset for Equality, Diversity, and Inclusion)", "description": "Over the past few years, systems have been developed to control online content and eliminate abusive, offensive or hate speech content. However, people in power sometimes misuse this form of censorship to obstruct the democratic right of freedom of speech. Therefore, it is imperative that research should take a positive reinforcement approach towards online content that is encouraging, positive and supportive contents. Until now, most studies have focused on solving this problem of negativity in the English language, though the problem is much more than just harmful content. Furthermore, it is multilingual as well. Thus, we have constructed a Hope Speech dataset for Equality, Diversity and Inclusion (HopeEDI) containing user-generated comments from the social media platform YouTube with 28,451, 20,198 and 10,705 comments in English, Tamil and Malayalam, respectively, manually labelled as containing hope speech or not. To our knowledge, this is the first research of its kind to annotate hope speech for equality, diversity and inclusion in a multilingual setting. We determined that the inter-annotator agreement of our dataset using Krippendorff\u2019s alpha. Further, we created several baselines to benchmark the resulting dataset and the results have been expressed using precision, recall and F1-score. The dataset is publicly available for the research community. We hope that this resource will spur further research on encouraging inclusive and responsive speech that reinforces positiveness."}, {"id": "cure-or-challenging-unreal-and-real-environments-for-object-recognition", "name": "CURE-OR (Challenging Unreal and Real Environments for Object Recognition)", "description": "CURE-OR is a large-scale, controlled, and multi-platform object recognition dataset denoted as Challenging Unreal and Real Environments for Object Recognition. In this dataset, there are 1,000,000 images of 100 objects with varying size, color, and texture that are positioned in five different orientations and captured using five devices including a webcam, a DSLR, and three smartphone cameras in real-world (real) and studio (unreal) environments. The controlled challenging conditions include underexposure, overexposure, blur, contrast, dirty lens, image noise, resizing, and loss of color information."}, {"id": "ott-qa", "name": "OTT-QA", "description": "The Open Table-and-Text Question Answering (OTT-QA) dataset contains open questions which require retrieving tables and text from the web to answer. This dataset is re-annotated from the previous HybridQA dataset. The dataset is collected by UCSB NLP group and issued under MIT license."}, {"id": "q-traffic", "name": "Q-Traffic", "description": "Q-Traffic is a large-scale traffic prediction dataset, which consists of three sub-datasets: query sub-dataset, traffic speed sub-dataset and road network sub-dataset."}, {"id": "1003-people-driver-behavior-collection-data", "name": "1,003 People-Driver Behavior Collection Data", "description": "Description: 1,003 People-Driver Behavior Collection Data. The data includes multiple ages and multiple time periods. The driver behaviors includes Dangerous behavior, fatigue behavior and visual movement behavior. In terms of device, binocular cameras of RGB and infrared channels were applied. This data can be used for tasks such as driver behavior analysis."}, {"id": "birdclef-2019", "name": "BirdCLEF 2019", "description": "BirdClef 2019 is a bird soundscape dataset. It contains around 350 hours of manually annotated soundscapes using 30 field recorders between January and June of 2017 in Ithaca, NY, USA. There are around 50,000 recordings in the dataset in total, with 659 classes. The dataset also contains species tags."}, {"id": "rostd-real-out-of-domain-sentences-from-task-oriented-dialog", "name": "ROSTD (Real Out-of-Domain Sentences From Task-oriented Dialog)", "description": "A dataset of 4K out-of-domain (OOD) examples for the publicly available dataset from (Schuster et al. 2019). In contrast to existing settings which synthesize OOD examples by holding out a subset of classes, the examples were authored by annotators with apriori instructions to be out-of-domain with respect to the sentences in an existing dataset. "}, {"id": "reverb-challenge-reverberant-voice-enhancement-and-recognition-benchmark", "name": "ReVerb Challenge (REverberant Voice Enhancement and Recognition Benchmark)", "description": "The REVERB (REverberant Voice Enhancement and Recognition Benchmark) challenge is a benchmark for evaluation of automatic speech recognition techniques. The challenge assumes the scenario of capturing utterances spoken by a single stationary distant-talking speaker with 1-channe, 2-channel or 8-channel microphone-arrays in reverberant meeting rooms. It features both real recordings and simulated data."}, {"id": "cc100", "name": "CC100", "description": "This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages. This was constructed using the urls and paragraph indices provided by the CC-Net repository by processing January-December 2018 Commoncrawl snapshots. Each file comprises of documents separated by double-newlines and paragraphs within the same document separated by a newline. The data is generated using the open source CC-Net repository."}, {"id": "penn-treebank", "name": "Penn Treebank", "description": "The English Penn Treebank (PTB) corpus, and in particular the section of the corpus corresponding to the articles of Wall Street Journal (WSJ), is one of the most known and used corpus for the evaluation of models for sequence labelling. The task consists of annotating each word with its Part-of-Speech tag. In the most common split of this corpus,  sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens). The corpus is also commonly used for character-level and word-level Language Modelling."}, {"id": "vot2016", "name": "VOT2016", "description": "VOT2016 is a video dataset for visual object tracking. It contains 60 video clips and 21,646 corresponding ground truth maps with pixel-wise annotation of salient objects."}, {"id": "voicebank-demand-noisy-speech-database-for-training-speech-enhancement-algorithms-and-tts-models", "name": "VoiceBank + DEMAND (Noisy speech database for training speech enhancement algorithms and TTS models)", "description": "VoiceBank+DEMAND is a noisy speech database for training speech enhancement algorithms and TTS models. The database was designed to train and test speech enhancement methods that operate at 48kHz. A more detailed description can be found in the paper associated with the database. Some of the noises were obtained from the Demand database, available here: http://parole.loria.fr/DEMAND/ . The speech database was obtained from the Voice Banking Corpus, available here: http://homepages.inf.ed.ac.uk/jyamagis/release/VCTK-Corpus.tar.gz ."}, {"id": "alfred-action-learning-from-realistic-environments-and-directives", "name": "ALFRED (Action Learning From Realistic Environments and Directives)", "description": "ALFRED (Action Learning From Realistic Environments and Directives), is a new benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks."}, {"id": "dnd-darmstadt-noise-dataset", "name": "DND (Darmstadt Noise Dataset)", "description": "Benchmarking Denoising Algorithms with Real Photographs"}, {"id": "wikisum", "name": "WikiSum", "description": "WikiSum is a dataset based on English Wikipedia and suitable for a task of multi-document abstractive summarization. In each instance, the input is comprised of a Wikipedia topic (title of article) and a collection of non-Wikipedia reference documents, and the target is the Wikipedia article text. The dataset is restricted to the articles with at least one crawlable citation. The official split divides the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively."}, {"id": "youcook2", "name": "YouCook2", "description": "YouCook2 is the largest task-oriented, instructional video dataset in the vision community. It contains 2000 long untrimmed videos from 89 cooking recipes; on average, each distinct recipe has 22 videos. The procedure steps for each video are annotated with temporal boundaries and described by imperative English sentences (see the example below). The videos were downloaded from YouTube and are all in the third-person viewpoint. All the videos are unconstrained and can be performed by individual persons at their houses with unfixed cameras. YouCook2 contains rich recipe types and various cooking styles from all over the world."}, {"id": "s3dis-stanford-3d-indoor-scene-dataset-s3dis", "name": "S3DIS (Stanford 3D Indoor Scene Dataset (S3DIS))", "description": "The Stanford 3D Indoor Scene Dataset (S3DIS) dataset contains 6 large-scale indoor areas with 271 rooms. Each point in the scene point cloud is annotated with one of the 13 semantic categories."}, {"id": "virtual-gallery", "name": "Virtual Gallery", "description": "The Virtual Gallery dataset is a synthetic dataset that targets multiple challenges such as varying lighting conditions and different occlusion levels for various tasks such as depth estimation, instance segmentation and visual localization."}, {"id": "cimat-cyclist", "name": "CIMAT-Cyclist", "description": "This provides a benchmark for cyclist's orientation detection, \"CIMAT-Cyclist\" with bounding box based labels according to eight different classes depending on the orientation. Which contains 11, 103 images, of which 6,605 images were collected in approximately 450 videos and images taken from sports events and the streets of the state of Zacatecas, Mexico, while 4,498 additional images were obtained from the web in pages such as pixabay, pexels, freephotos, among others. \"CIMAT-Cyclist\" provide 20,229 instances over 11,103 cyclist's images, where 80% of the images were split for the training set and 20% for the test set."}, {"id": "librivoxdeen", "name": "LibriVoxDeEn", "description": "LibriVoxDeEn is a corpus of sentence-aligned triples of German audio, German text, and English translation, based on German audiobooks. The speech translation data consist of 110 hours of audio material aligned to over 50k parallel sentences. An even larger dataset comprising 547 hours of German speech aligned to German text is available for speech recognition. The audio data is read speech and thus low in disfluencies."}, {"id": "arcov-19", "name": "ArCOV-19", "description": "ArCOV-19 is an Arabic COVID-19 Twitter dataset that covers the period from 27th of January till 30th of April 2020. ArCOV-19 is the first publicly-available Arabic Twitter dataset covering COVID-19 pandemic that includes over 1M tweets alongside the propagation networks of the most-popular subset of them (i.e., most-retweeted and -liked)."}, {"id": "imaterialist", "name": "iMaterialist", "description": "Constructed from over one million fashion images with a label space that includes 8 groups of 228 fine-grained attributes in total. Each image is annotated by experts with multiple, high-quality fashion attributes."}, {"id": "3dcsr-dataset-3d-cross-source-point-cloud-registration-dataset", "name": "3DCSR dataset (3D cross-source point cloud registration dataset)", "description": "Cross-source point cloud dataset for registration task. It includes point clouds from structure from motion (SFM), Kinect, Lidar."}, {"id": "void-visual-odometry-with-inertial-and-depth", "name": "VOID (Visual Odometry with Inertial and Depth)", "description": "The dataset was collected using the Intel RealSense D435i camera, which was configured to produce synchronized accelerometer and gyroscope measurements at 400 Hz, along with synchronized VGA-size (640 x 480) RGB and depth streams at 30 Hz. The depth frames are acquired using active stereo and is aligned to the RGB frame using the sensor factory calibration. All the measurements are timestamped."}, {"id": "billion-word-benchmark", "name": "Billion Word Benchmark", "description": "The One Billion Word dataset is a dataset for language modeling. The training/held-out data was produced from the WMT 2011 News Crawl data using a combination of Bash shell and Perl scripts."}, {"id": "com2sense-complementary-commonsense", "name": "Com2Sense (Complementary Commonsense)", "description": "Complementary Commonsense (Com2Sense) is a dataset for benchmarking commonsense reasoning ability of NLP models. This dataset contains 4k statement true/false sentence pairs. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, the dataset is designed along the dimensions of knowledge domains, reasoning scenarios and numeracy."}, {"id": "jec-qa", "name": "JEC-QA", "description": "JEC-QA is a LQA (Legal Question Answering) dataset collected from the National Judicial Examination of China. It contains 26,365 multiple-choice and multiple-answer questions in total. The task of the dataset is to predict the answer using the questions and relevant articles. To do well on JEC-QA, both retrieving and answering are important."}, {"id": "medquad-medical-question-answering-dataset", "name": "MedQuAD (Medical Question Answering Dataset)", "description": "MedQuAD includes 47,457 medical question-answer pairs created from 12 NIH websites (e.g. cancer.gov, niddk.nih.gov, GARD, MedlinePlus Health Topics). The collection covers 37 question types (e.g. Treatment, Diagnosis, Side Effects) associated with diseases, drugs and other medical entities such as tests.  "}, {"id": "usr-topicalchat", "name": "USR-TopicalChat", "description": "This dataset was collected with the goal of assessing dialog evaluation metrics. In the paper, USR: An Unsupervised and Reference Free Evaluation Metric for Dialog (Mehri and Eskenazi, 2020), the authors collect this data to measure the quality of several existing word-overlap and embedding-based metrics, as well as their newly proposed USR metric."}, {"id": "whu-building-dataset", "name": "WHU Building Dataset", "description": "We manually edited an aerial and a satellite imagery dataset of building samples and named it a WHU building dataset. The aerial dataset consists of more than 220, 000 independent buildings extracted from aerial images with 0.075 m spatial resolution and 450 km2 covering in Christchurch, New Zealand. The satellite imagery dataset consists of two subsets. One of them is collected from cities over the world and from various remote sensing resources including QuickBird, Worldview series, IKONOS, ZY-3, etc. The other satellite building sub-dataset consists of 6 neighboring satellite images covering 550 km2 on East Asia with 2.7 m ground resolution."}, {"id": "memetracker", "name": "MemeTracker", "description": "The Memetracker corpus contains articles from mainstream media and blogs from August 1 to October 31, 2008 with about 1 million documents per day. It has 10,967 hyperlink cascades among 600 media sites."}, {"id": "otb-2015", "name": "OTB-2015", "description": "OTB-2015, also referred as Visual Tracker Benchmark, is a visual tracking dataset. It contains 100 commonly used video sequences for evaluating visual tracking."}, {"id": "hrwsi-high-resolution-web-stereo-image", "name": "HRWSI (High-Resolution Web Stereo Image)", "description": "The HRWSI dataset consists of about 21K diverse high-resolution RGB-D image pairs derived from the Web stereo images. Also, it provides sky segmentation masks, instance segmentation masks as well as invalid pixel masks."}, {"id": "simitate", "name": "Simitate", "description": "Simitate is a hybrid benchmarking suite targeting the evaluation of approaches for imitation learning. It consists on a dataset containing 1938 sequences where humans perform daily activities in a realistic environment. The dataset is strongly coupled with an integration into a simulator. RGB and depth streams with a resolution of 960\u00d7540 at 30Hz and accurate ground truth poses for the demonstrator's hand, as well as the object in 6 DOF at 120Hz are provided. Along with the dataset the 3D model of the used environment and labelled object images are also provided."}, {"id": "ms-asl", "name": "MS-ASL", "description": "MS-ASL is a real-life large-scale sign language data set comprising over 25,000 annotated videos."}, {"id": "signaltrain-la2a-dataset", "name": "SignalTrain LA2A Dataset", "description": "LA-2A Compressor data to accompany the paper \"SignalTrain: Profiling Audio Compressors with Deep Neural Networks,\" https://arxiv.org/abs/1905.11928"}, {"id": "isic-2018-task-1", "name": "ISIC 2018 Task 1", "description": "The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. This Task 1 dataset is the challenge on lesion segmentation. It includes 2594 images."}, {"id": "industrial-benchmark", "name": "Industrial Benchmark", "description": "A benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. "}, {"id": "redial", "name": "ReDial", "description": "ReDial (Recommendation Dialogues) is an annotated dataset of dialogues, where users recommend movies to each other. The dataset consists of over 10,000 conversations centered around the theme of providing movie recommendations. "}, {"id": "arkitscenes", "name": "ARKitScenes", "description": "ARKitScenes is an RGB-D dataset captured with the widely available Apple LiDAR scanner. Along with the per-frame raw data (Wide Camera RGB, Ultra Wide camera RGB, LiDar scanner depth, IMU) the authors also provide the estimated ARKit camera pose and ARKit scene reconstruction for each iPad Pro sequence. In addition to the raw and processed data from the mobile device, ARKit."}, {"id": "inspec", "name": "Inspec", "description": "Paper: Improved automatic keyword extraction given more linguistic knowledge Doi: 10.3115/1119355.1119383"}, {"id": "fraxtil", "name": "Fraxtil", "description": "Fraxtil is an audio dataset where given a raw audio track, the goal is to produce a choreography step chart, similar to those used in the Dance Dance Revolution video game. It contains 90 songs choreographed by a single author, with 450 charts for the 90 songs."}, {"id": "waymo-open-dataset", "name": "Waymo Open Dataset", "description": "The Waymo Open Dataset is comprised of high resolution sensor data collected by autonomous vehicles operated by the Waymo Driver in a wide variety of conditions. "}, {"id": "animalweb", "name": "AnimalWeb", "description": "A large-scale, hierarchical annotated dataset of animal faces, featuring 21.9K faces from 334 diverse species and 21 animal orders across biological taxonomy. These faces are captured `in-the-wild' conditions and are consistently annotated with 9 landmarks on key facial features. The proposed dataset is structured and scalable by design; its development underwent four systematic stages involving rigorous, manual annotation effort of over 6K man-hours."}, {"id": "classic5", "name": "Classic5", "description": "Five classic grayscale images commonly used for image quality assessment tasks."}, {"id": "convquestions", "name": "ConvQuestions", "description": "ConvQuestions is the first realistic benchmark for conversational question answering over knowledge graphs. It contains 11,200 conversations which can be evaluated over Wikidata. They are compiled from the inputs of 70 Master crowdworkers on Amazon Mechanical Turk, with conversations from five domains: Books, Movies, Soccer, Music, and TV Series. The questions feature a variety of complex question phenomena like comparisons, aggregations, compositionality, and temporal reasoning. Answers are grounded in Wikidata entities to enable fair comparison across diverse methods. The data gathering setup was kept as natural as possible, with the annotators selecting entities of their choice from each of the five domains, and formulating the entire conversation in one session. All questions in a conversation are from the same Turker, who also provided gold answers to the questions. For suitability to knowledge graphs, questions were constrained to be objective or factoid in nature, but no other restrictive guidelines were set. A notable property of ConvQuestions is that several questions are not answerable by Wikidata alone (as of September 2019), but the required facts can, for example, be found in the open Web or in Wikipedia. For details, please refer to our CIKM 2019 full paper."}, {"id": "100-people-handwriting-ocr-data-of-japanese-and-korean", "name": "100 People - Handwriting OCR Data of Japanese and Korean", "description": "Description: 100 People - Handwriting OCR Data of Japanese and Korean,. This dadaset was collected from 100 subjects including 50 Japanese, 49 Koreans and 1 Afghan. For different subjects, the corpus are different. The data diversity includes multiple cellphone models and different corpus. This dataset can be used for tasks, such as handwriting OCR data of Japanese and Korean."}, {"id": "27-class-asl-sign-language-27-class-american-sign-language-based-dataset", "name": "27 Class ASL Sign Language (27 Class American Sign Language-Based Dataset)", "description": "This 27 Class American Sign Language-based dataset consists of photographs collected from 173 individuals asked to display gestures with their hands. Using a camera, these were taken to a 3024 by 3024 pixels frame size within RGB color space. 130 photos were taken from each person, 5 per class (minor changes on sample sizes in classes can be observed) - 26 classes containing phrases, letters, and numbers with a 27th class null category made up of 314 images for control purposes. The main motivation was contributing to technology development use cases that can reduce the communication challenges faced speech-impaired people with new data to meet the diversity and sample size necessary for intelligent computer vision studies and sign language applications."}, {"id": "rent3d", "name": "Rent3D", "description": "A dataset which contains over 200 apartments."}, {"id": "celeba-dialog", "name": "CelebA-Dialog", "description": "The CelebA-Dialog dataset has the following properties: 1) Facial images are annotated with rich fine-grained labels, which classify one attribute into multiple degrees according to its semantic meaning; 2) Accompanied with each image, there are captions describing the attributes and a user request sample."}, {"id": "duts", "name": "DUTS", "description": "DUTS is a saliency detection dataset containing 10,553 training images and 5,019 test images. All training images are collected from the ImageNet DET training/val sets, while test images are collected from the ImageNet DET test set and the SUN data set. Both the training and test set contain very challenging scenarios for saliency detection. Accurate pixel-level ground truths are manually annotated by 50 subjects."}, {"id": "story-commonsense", "name": "Story Commonsense", "description": "Story Commonsense is a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research."}, {"id": "french-wikipedia", "name": "French Wikipedia", "description": "French Wikipedia is a dataset used for pretraining the CamemBERT French language model. It uses the official 2019 French Wikipedia dumps"}, {"id": "chest-imagenome", "name": "Chest ImaGenome", "description": "Chest ImaGenome is a dataset with a scene graph data structure to describe 242,072 images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, the following are provided: i) 1256 combinations of relation annotations between 29 CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over 670,000 localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from 500 unique patients."}, {"id": "motchallenge", "name": "MOTChallenge", "description": "The MOTChallenge datasets are designed for the task of multiple object tracking. There are several variants of the dataset released each year, such as MOT15, MOT17, MOT20."}, {"id": "veri-776", "name": "VeRi-776", "description": "VeRi-776 is a vehicle re-identification dataset which contains 49,357 images of 776 vehicles from 20 cameras. The dataset is collected in the real traffic scenario, which is close to the setting of CityFlow. The dataset contains bounding boxes, types, colors and brands."}, {"id": "floorplancad", "name": "FloorPlanCAD", "description": "FloorPlanCAD is a large-scale real-world CAD drawing dataset containing over 15,000 floor plans, ranging from residential to commercial buildings."}, {"id": "dsifn-cd", "name": "DSIFN-CD", "description": "The dataset is manually collected from Google Earth. It consists of six large bi-temporal high resolution images covering six cities (i.e., Beijing, Chengdu, Shenzhen, Chongqing, Wuhan, Xian) in China. The five large image-pairs (i.e., Beijing, Chengdu, Shenzhen, Chongqing, Wuhan) are clipped into 394 subimage pairs with sizes of 512\u00d7512. After data augmentation, a collection of 3940 bi-temporal image pairs is acquired. Xian image pair is clipped into 48 image pairs for model testing. There are 3600 image pairs in the training dataset, 340 image paris in the validation dataset, and 48 image pairs in the test dataset."}, {"id": "eurosat", "name": "EuroSAT", "description": "Eurosat is a dataset and deep learning benchmark for land use and land cover classification. The dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images."}, {"id": "sicapv2", "name": "SICAPv2", "description": "SICAPv2 is a database containing prostate histology whole slide images with both annotations of global Gleason scores and path-level Gleason grades. "}, {"id": "sentiment140", "name": "Sentiment140", "description": "Sentiment140 is a dataset that allows you to discover the sentiment of a brand, product, or topic on Twitter."}, {"id": "jaffe-japanese-female-facial-expression", "name": "JAFFE (Japanese Female Facial Expression)", "description": "The JAFFE dataset consists of 213 images of different facial expressions from 10 different Japanese female subjects. Each subject was asked to do 7 facial expressions (6 basic facial expressions and neutral) and the images were annotated with average semantic ratings on each facial expression by 60 annotators."}, {"id": "synpick", "name": "SynPick", "description": "SynPick is a synthetic dataset for dynamic scene understanding in bin-picking scenarios. In contrast to existing datasets, this dataset is both situated in a realistic industrial application domain -- inspired by the well-known Amazon Robotics Challenge (ARC) -- and features dynamic scenes with authentic picking actions as chosen by our picking heuristic developed for the ARC 2017. The dataset is compatible with the popular BOP dataset format."}, {"id": "metu-trademark", "name": "METU Trademark", "description": "The METU Trademark Dataset is a large dataset (the largest publicly available logo dataset as of 2014, and the largest one not requiring any preprocessing as of 2017), which is composed of more than 900K real logos belonging to real companies worldwide. The dataset also includes query sets of varying difficulties, allowing Trademark Retrieval researchers to benchmark their methods against other methods to progress the field."}, {"id": "cihp-crowd-instance-level-human-parsing", "name": "CIHP (Crowd Instance-level Human Parsing)", "description": "The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations on 20 categories and instance-level identification. The dataset can be used for the human part segmentation task."}, {"id": "dicova", "name": "DiCOVA", "description": "The DiCOVA Challenge dataset is derived from the Coswara dataset, a crowd-sourced dataset of sound recordings from COVID-19 positive and non-COVID-19 individuals. The Coswara data is collected using a web-application2, launched in April-2020, accessible through the internet by anyone around the globe. The volunteering subjects are advised to record their respiratory sounds in a quiet environment. "}, {"id": "radiate-radar-dataset-in-adverse-weather", "name": "RADIATE (RAdar Dataset In Adverse weaThEr)", "description": "RADIATE (RAdar Dataset In Adverse weaThEr) is new automotive dataset created by Heriot-Watt University which includes Radar, Lidar, Stereo Camera and GPS/IMU. The data is collected in different weather scenarios (sunny, overcast, night, fog, rain and snow) to help the research community to develop new methods of vehicle perception. The radar images are annotated in 7 different scenarios: Sunny (Parked), Sunny/Overcast (Urban), Overcast (Motorway), Night (Motorway), Rain (Suburban), Fog (Suburban) and Snow (Suburban). The dataset contains 8 different types of objects (car, van, truck, bus, motorbike, bicycle, pedestrian and group of pedestrians)."}, {"id": "oracle-mnist-oracle-mnist-a-realistic-image-dataset-for-benchmarking-machine-learning-algorithms", "name": "Oracle-MNIST (Oracle-MNIST: a Realistic Image Dataset for Benchmarking Machine Learning Algorithms)", "description": "We introduce the Oracle-MNIST dataset, comprising of 2828 grayscale images of 30,222 ancient characters from 10 categories, for benchmarking pattern classification, with particular challenges on image noise and distortion. The training set totally consists of 27,222 images, and the test set contains 300 images per class. Oracle-MNIST shares the same data format with the original MNIST dataset, allowing for direct compatibility with all existing classifiers and systems, but it constitutes a more challenging classification task than MNIST. The images of ancient characters suffer from 1) extremely serious and unique noises caused by three-thousand years of burial and aging and 2) dramatically variant writing styles by ancient Chinese, which all make them realistic for machine learning research. The dataset is freely available at https://github.com/wm-bupt/oracle-mnist."}, {"id": "imet-collection", "name": "iMet Collection", "description": "A dataset for fine-grained art attribute recognition introduced in the 6th FGVC Workshop at CVPR 2019. It is a high-quality artwork image dataset with professional photographs of artworks from The Metropolitan Museum of Art and attribute labels curated or verified by experts."}, {"id": "eurocrops", "name": "EuroCrops", "description": "EuroCrops is a dataset for automatic vegetation classification from multi-spectral and multi-temporal satellite data, annotated with official LIPS reporting data from countries of the European Union, curated by the Technical University of Munich and GAF AG. The project is managed by the DLR Space Administration and funded by BMWI (Federal Ministry for Economic Affairs and Energy). This dataset is publicly available for research causes with the idea in mind to assist in the subsidy control of agricultural self-declarations."}, {"id": "off-distant-sequential", "name": "Off_Distant_sequential", "description": "SMAC+ offensive distant scenario with sequential episodic buffer"}, {"id": "cadc-canadian-adverse-driving-conditions", "name": "CADC (Canadian Adverse Driving Conditions)", "description": "Collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ."}, {"id": "obp-open-bandit-dataset", "name": "OBP (Open Bandit Dataset)", "description": "Open Bandit Dataset is a public real-world logged bandit feedback data. The dataset is provided by ZOZO, Inc., the largest Japanese fashion e-commerce company with over 5 billion USD market capitalization (as of May 2020). The company uses multi-armed bandit algorithms to recommend fashion items to users in a large-scale fashion e-commerce platform called ZOZOTOWN."}, {"id": "reddit-5k-reddit-multi-5k", "name": "REDDIT-5K (REDDIT-MULTI-5K)", "description": "Reddit-5K is a relational dataset extracted from Reddit."}, {"id": "autoregressive-paraphrase-dataset-arpd", "name": "Autoregressive Paraphrase Dataset (ARPD)", "description": "For more details see https://huggingface.co/datasets/jpwahle/autoregressive-paraphrase-dataset"}, {"id": "qed", "name": "QED", "description": "QED is a linguistically principled framework for explanations in question answering. Given a question and a passage, QED represents an explanation of the answer as a combination of discrete, human-interpretable steps: sentence selection := identification of a sentence implying an answer to the question referential equality := identification of noun phrases in the question and the answer sentence that refer to the same thing predicate entailment := confirmation that the predicate in the sentence entails the predicate in the question once referential equalities are abstracted away. The QED dataset is an expert-annotated dataset of QED explanations build upon a subset of the Google Natural Questions dataset."}, {"id": "dialfact", "name": "DialFact", "description": "DialFact is a testing benchmark dataset of 22,245 annotated conversational claims, paired with pieces of evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether a response carries verifiable factual information; 2) Evidence retrieval task retrieves the most relevant Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue response to be supported, refuted, or not enough information."}, {"id": "the-bioscope-corpus", "name": "The BioScope Corpus", "description": "It is a  freely available resource for research on handling negation and uncertainty in biomedical texts . The corpus consists of three parts, namely medical free texts,biological full papers and biological scientific abstracts. The dataset contains annotations at the token level for negative and speculative keywords and at the sentence level for their linguistic scope. The annotation process was carried out by two independent linguist annotators and a chief annotator \u2013 also responsible for setting up the annotation guidelines \u2013 who resolved cases where the annotators disagreed."}, {"id": "wikicoref", "name": "WikiCoref", "description": "WikiCoref is an English corpus annotated for anaphoric relations, where all documents are from the English version of Wikipedia. "}, {"id": "adobeindoornav", "name": "AdobeIndoorNav", "description": "AdobeIndoorNav is a dataset collected in real-world to facilitate the research in DRL based visual navigation. The dataset includes 3D reconstruction for real-world scenes as well as densely captured real 2D images from the scenes. It provides high-quality visual inputs with real-world scene complexity to the robot at dense grid locations."}, {"id": "e-vil", "name": "e-ViL", "description": "e-ViL is a benchmark for explainable vision-language tasks. e-ViL spans across three datasets of human-written NLEs (natural language explanations), and provides a unified evaluation framework that is designed to be re-usable for future works."}, {"id": "imageclef-da", "name": "ImageCLEF-DA", "description": "The ImageCLEF-DA dataset is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which contains three domains: Caltech-256 (C), ImageNet ILSVRC 2012 (I) and Pascal VOC 2012 (P). For each domain, there are 12 categories and 50 images in each category."}, {"id": "dkhate", "name": "DKhate", "description": "A corpus of Offensive Language and Hate Speech Detection for Danish"}, {"id": "loveda-remote-sensing-land-cover-dataset-for-domain-adaptive-semantic-segmentation", "name": "LoveDA (Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation)", "description": "Two contests are held on the Codalab: LoveDA Semantic Segmentation Challenge, LoveDA Unsupervised Domain Adaptation Challenge"}, {"id": "weibo21", "name": "Weibo21", "description": "Weibo21 is a benchmark of fake news dataset for multi-domain fake news detection (MFND) with domain label annotated, which consists of 4,488 fake news and 4,640 real news from 9 different domains."}, {"id": "trecvid", "name": "TRECVID", "description": "TRECVID is a yearly set of competitions centered on video retrieval and indexing, hosting a variety of video data sets."}, {"id": "pku-reid", "name": "PKU-Reid", "description": "This dataset contains 114 individuals including 1824 images captured from two disjoint camera views. For each person, eight images are captured from eight different orientations under one camera view and are normalized to 128x48 pixels. This dataset is also split into two parts randomly. One contains 57 individuals for training, and the other contains 57 individuals for testing."}, {"id": "ovrseen", "name": "OVRseen", "description": "https://athinagroup.eng.uci.edu/projects/ovrseen/"}, {"id": "ace-2004-ace-2004-multilingual-training-corpus", "name": "ACE 2004 (ACE 2004 Multilingual Training Corpus)", "description": "ACE 2004 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2004 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities and relations and was created by Linguistic Data Consortium with support from the ACE Program, with additional assistance from the DARPA TIDES (Translingual Information Detection, Extraction and Summarization) Program. The objective of the ACE program is to develop automatic content extraction technology to support automatic processing of human language in text form. In September 2004, sites were evaluated on system performance in six areas: Entity Detection and Recognition (EDR), Entity Mention Detection (EMD), EDR Co-reference, Relation Detection and Recognition (RDR), Relation Mention Detection (RMD), and RDR given reference entities. All tasks were evaluated in three languages: English, Chinese and Arabic."}, {"id": "2d-naca-rans", "name": "2D_NACA_RANS", "description": "Dataset of low fidelity resolutions of the RANS equations over airfoils."}, {"id": "cpp-chinese-polyphones-with-pinyin", "name": "CPP (Chinese Polyphones with Pinyin)", "description": "A benchmark dataset that consists of 99,000+ sentences for Chinese polyphone disambiguation. "}, {"id": "artifact-artificial-and-factual-image-dataset-for-synthetic-image-detection", "name": "ArtiFact (Artificial and Factual Image Dataset for Synthetic Image Detection)", "description": "The ArtiFact dataset is a large-scale image dataset that aims to include a diverse collection of real and synthetic images from multiple categories, including Human/Human Faces, Animal/Animal Faces, Places, Vehicles, Art, and many other real-life objects. The dataset comprises 8 sources that were carefully chosen to ensure diversity and includes images synthesized from 25 distinct methods, including 13 GANs, 7 Diffusion, and 5 other miscellaneous generators. The dataset contains 2,496,738 images, comprising 964,989 real images and 1,531,749 fake images."}, {"id": "alfworld", "name": "ALFWorld", "description": "ALFWorld contains interactive TextWorld environments (C\u00f4t\u00e9 et. al) that parallel embodied worlds in the ALFRED dataset (Shridhar et. al). The aligned environments allow agents to reason and learn high-level policies in an abstract space before solving embodied tasks through low-level actuation.   "}, {"id": "mtass", "name": "MTASS", "description": "MTASS is an open-source dataset in which mixtures contain three types of audio signals."}, {"id": "fisheye", "name": "Fisheye", "description": "Fisheye dataset comprises of synthetically generated fisheye sequences and fisheye video sequences captured with an actual fisheye camera designed for fisheye motion estimation."}, {"id": "spacenet-2-spacenet-2-building-detection-v2", "name": "SpaceNet 2 (SpaceNet 2: Building Detection v2)", "description": "SpaceNet 2: Building Detection v2 - is a dataset for building footprint detection in geographically diverse settings from very high resolution satellite images. It contains over 302,701 building footprints, 3/8-band Worldview-3 satellite imagery at 0.3m pixel res., across 5 cities (Rio de Janeiro, Las Vegas, Paris, Shanghai, Khartoum), and covers areas that are both urban and suburban in nature. The dataset was split using 60%/20%/20% for train/test/validation."}, {"id": "ai-tod-tiny-object-detection-in-aerial-images", "name": "AI-TOD (Tiny Object Detection in Aerial Images)", "description": "AI-TOD comes with 700,621 object instances for eight categories across 28,036 aerial images. Compared to existing object detection datasets in aerial images, the mean size of objects in AI-TOD is about 12.8 pixels, which is much smaller than others."}, {"id": "visuelle2-0", "name": "VISUELLE2.0", "description": "Visuelle 2.0 is a dataset containing real data for 5355 clothing products of the retail fast-fashion Italian company, Nuna Lie. Specifically, Visuelle 2.0 provides data from 6 fashion seasons (partitioned in Autumn-Winter and Spring-Summer) from 2017-2019, right before the Covid-19 pandemic. Each product is accompanied by an HD image, textual tags and more. The time series data are disaggregated at the shop level, and include the sales, inventory stock, max-normalized prices (for the sake of confidentiality} and discounts. Exogenous time series data is also provided, in the form of Google Trends based on the textual tags and multivariate weather conditions of the stores\u2019 locations. Finally, we also provide purchase data for 667K customers whose identity has been anonymized, to capture personal preferences. With these data, Visuelle 2.0 allows to cope with several problems which characterize the activity of a fast fashion company: new product demand forecasting, short-observation new product sales forecasting, and product recommendation."}, {"id": "maven", "name": "MAVEN", "description": "Contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. "}, {"id": "manymodalqa", "name": "ManyModalQA", "description": "Collects the data by scraping Wikipedia and then utilize crowdsourcing to collect question-answer pairs. "}, {"id": "ciao", "name": "Ciao", "description": "The Ciao dataset contains rating information of users given to items, and also contain item category information. The data comes from the Epinions dataset."}, {"id": "sevir-storm-event-imagry", "name": "SEVIR (Storm EVent ImagRy)", "description": "SEVIR is an annotated, curated and spatio-temporally aligned dataset containing over 10,000 weather events that each consist of 384 km x 384 km image sequences spanning 4 hours of time. Images in SEVIR were sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD vertically integrated liquid mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. Many events in SEVIR were selected and matched to the NOAA Storm Events database so that additional descriptive information such as storm impacts and storm descriptions can be linked to the rich imagery provided by the sensors."}, {"id": "wilds", "name": "Wilds", "description": "Builds on top of recent data collection efforts by domain experts in these applications and provides a unified collection of datasets with evaluation metrics and train/test splits that are representative of real-world distribution shifts."}, {"id": "thfood-50-thai-food-50-image-classification", "name": "THFOOD-50 (Thai Food 50 Image Classification)", "description": "Fine-Grained Thai Food Image Classification Datasets"}, {"id": "come15k", "name": "COME15K", "description": "COME15K is an RGB-D saliency detection dataset which contains 15,625 image pairs with high quality polygon-/scribble-/object-/instance-/rank-level annotations."}, {"id": "ricordi", "name": "Ricordi", "description": "Ricordi contains handwritten texts written in Italian. Train sample consists of 295 lines, validation - 19 lines and test - 69 lines."}, {"id": "kinetics-kinetics-human-action-video-dataset", "name": "Kinetics (Kinetics Human Action Video Dataset)", "description": "The Kinetics dataset is a large-scale, high-quality dataset for human action recognition in videos. The dataset consists of around 500,000 video clips covering 600 human action classes with at least 600 video clips for each action class. Each video clip lasts around 10 seconds and is labeled with a single action class. The videos are collected from YouTube."}, {"id": "acinoset", "name": "AcinoSet", "description": "AcinoSet is a dataset of free-running cheetahs in the wild that contains 119,490 frames of multi-view synchronized high-speed video footage, camera calibration files and 7,588 human-annotated frames. The authors utilized markerless animal pose estimation with DeepLabCut to provide 2D keypoints (in the 119K frames). It also includes 3D trajectories, human-checked 3D ground truth, and an interactive tool to inspect the data."}, {"id": "paranmt-50m", "name": "PARANMT-50M", "description": "PARANMT-50M is a dataset for training paraphrastic sentence embeddings. It consists of more than 50 million English-English sentential paraphrase pairs. "}, {"id": "ucsd-ped2-ucsd-anomaly-detection-dataset", "name": "UCSD Ped2 (UCSD Anomaly Detection Dataset)", "description": "The UCSD Anomaly Detection Dataset was acquired with a stationary camera mounted at an elevation, overlooking pedestrian walkways. The crowd density in the walkways was variable, ranging from sparse to very crowded. In the normal setting, the video contains only pedestrians. Abnormal events are due to either: the circulation of non pedestrian entities in the walkways anomalous pedestrian motion patterns Commonly occurring anomalies include bikers, skaters, small carts, and people walking across a walkway or in the grass that surrounds it. A few instances of people in wheelchair were also recorded. All abnormalities are naturally occurring, i.e. they were not staged for the purposes of assembling the dataset. The data was split into 2 subsets, each corresponding to a different scene. The video footage recorded from each scene was split into various clips of around 200 frames."}, {"id": "objectnet", "name": "ObjectNet", "description": "ObjectNet is a test set of images collected directly using crowd-sourcing. ObjectNet is unique as the objects are captured at unusual poses in cluttered, natural scenes, which can severely degrade recognition performance. There are 50,000 images in the test set which controls for rotation, background and viewpoint. There are 313 object classes with 113 overlapping ImageNet."}, {"id": "phoner-covid19", "name": "PhoNER COVID19", "description": "PhoNER_COVID19 is a dataset for recognising COVID-19 related named entities in Vietnamese, consisting of 35K entities over 10K sentences. The authors defined 10 entity types with the aim of extracting key information related to COVID-19 patients, which are especially useful in downstream applications. In general, these entity types can be used in the context of not only the COVID-19 pandemic but also in other future epidemics."}, {"id": "olid-offensive-language-identification-dataset", "name": "OLID (Offensive Language Identification Dataset)", "description": "The OLID is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and publicly available. There are 14,100 tweets in total, in which 13,240 are in the training set, and 860 are in the test set. For each tweet, there are three levels of labels: (A) Offensive/Not-Offensive, (B) Targeted-Insult/Untargeted, (C) Individual/Group/Other. The relationship between them is hierarchical. If a tweet is offensive, it can have a target or no target. If it is offensive to a specific target, the target can be an individual, a group, or some other objects. This dataset is used in the OffensEval-2019 competition in SemEval-2019."}, {"id": "fsdnoisy18k", "name": "FSDnoisy18k", "description": "The FSDnoisy18k dataset is an open dataset containing 42.5 hours of audio across 20 sound event classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. The audio content is taken from Freesound, and the dataset was curated using the Freesound Annotator. The noisy set of FSDnoisy18k consists of 15,813 audio clips (38.8h), and the test set consists of 947 audio clips (1.4h) with correct labels. The dataset features two main types of label noise: in-vocabulary (IV) and out-of-vocabulary (OOV). IV applies when, given an observed label that is incorrect or incomplete, the true or missing label is part of the target class set. Analogously, OOV means that the true or missing label is not covered by those 20 classes."}, {"id": "wikihow", "name": "WikiHow", "description": "WikiHow is a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and represent high diversity styles."}, {"id": "an-amharic-news-text-classification-dataset", "name": "An Amharic News Text classification Dataset", "description": "In NLP, text classification is one of the primary problems we try to solve and its uses in language analyses are indisputable. The lack of labeled training data made it harder to do these tasks in low resource languages like Amharic. The task of collecting, labeling, annotating, and making valuable this kind of data will encourage junior researchers, schools, and machine learning practitioners to implement existing classification models in their language. In this short paper, we aim to introduce the Amharic text classification dataset that consists of more than 50k news articles that were categorized into 6 classes. This dataset is made available with easy baseline performances to encourage studies and better performance experiments."}, {"id": "piqa-physical-interaction-question-answering", "name": "PIQA (Physical Interaction: Question Answering)", "description": "PIQA is a dataset for commonsense reasoning, and was created to investigate the physical knowledge of existing models in NLP. "}, {"id": "rstpreid-real-scenario-text-based-person-re-identification", "name": "RSTPReid (Real Scenario Text-based Person Re-identification)", "description": "RSTPReid contains 20505 images of 4,101 persons from 15 cameras. Each person has 5 corresponding images taken by different cameras with complex both indoor and outdoor scene transformations and backgrounds in various periods of time, which makes RSTPReid much more challenging and more adaptable to real scenarios. Each image is annotated with 2 textual descriptions. For data division, 3701 (index < 18505), 200 (18505 <= index < 19505) and 200 (index >= 19505) identities are utilized for training, validation and testing, respectively (Marked by item 'split' in the JSON file). Each sentence is no shorter than 23 words."}, {"id": "evalution", "name": "EVALution", "description": "EVALution dataset is evenly distributed among the three classes (hypernyms, co-hyponyms and random) and involves three types of parts of speech (noun, verb, adjective). The full dataset contains a total of 4,263 distinct terms consisting of 2,380 nouns, 958 verbs and 972 adjectives."}, {"id": "avasym", "name": "AvaSym", "description": "Global Symmetry Ground-truth for AVA dataset."}, {"id": "expose-expressive-pose-and-shape-regression", "name": "ExPose (EXpressive POse and Shape rEgression)", "description": "Curates a dataset of SMPL-X fits on in-the-wild images."}, {"id": "armenian-paraphrase-detection-corpus", "name": "Armenian Paraphrase Detection Corpus", "description": "This dataset contains 2,360 paraphrases in Armenian that can be used for paraphrase detection. The dataset is constructed by back-translating sentences from Armenian to English twice, and manually filtering the result."}, {"id": "sonyc-ust-v2", "name": "SONYC-UST-V2", "description": "A dataset for urban sound tagging with spatiotemporal information. This dataset is aimed for the development and evaluation of machine listening systems for real-world urban noise monitoring. While datasets of urban recordings are available, this dataset provides the opportunity to investigate how spatiotemporal metadata can aid in the prediction of urban sound tags. SONYC-UST-V2 consists of 18510 audio recordings from the \"Sounds of New York City\" (SONYC) acoustic sensor network, including the timestamp of audio acquisition and location of the sensor. "}, {"id": "douban-conversation-corpus", "name": "Douban Conversation Corpus", "description": "We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. "}, {"id": "space", "name": "SPACE", "description": "SPACE is a simulator for physical Interactions and causal learning in 3D environments. The SPACE simulator is used to generate the SPACE dataset, a synthetic video dataset in a 3D environment, to systematically evaluate physics-based models on a range of physical causal reasoning tasks. Inspired by daily object interactions, the SPACE dataset comprises videos depicting three types of physical events: containment, stability and contact."}, {"id": "mednli-medical-natural-language-inference", "name": "MedNLI (Medical Natural Language Inference)", "description": "The MedNLI dataset consists of the sentence pairs developed by Physicians from the Past Medical History section of MIMIC-III clinical notes annotated for Definitely True, Maybe True and Definitely False. The dataset contains 11,232 training, 1,395 development and 1,422 test instances. This provides a natural language inference task (NLI) grounded in the medical history of patients."}, {"id": "quantumnoise", "name": "quantumNoise", "description": "The dataset consists in many runs of the same quantum circuit on different IBM quantum machines. We used 9 different machines and for each one of them, we run 2000 executions of the circuit. The circuit has 9 differents measurement steps along it. To obtain the 9 outcome distributions, for each execution, parts of the circuit are appended 9 times (in the same call to the IBM API, thus, in the shortest possible time) measuring a new step each time. The calls to the IBM API followed two different strategies. One was adopted to maximize the number of calls to the interface, parallelizing the code with as many possible runs and even running 8000 shots per run but considering for 8 times 1000 out of the memory to get the probabilities. The other strategy was slower, without parallelization and with a minimum waiting time between subsequent executions. The latter was adopted to get more uniformly distributed executions in time."}, {"id": "tim-tremor-technology-in-motion-tremor", "name": "TIM-Tremor (Technology in Motion Tremor)", "description": "Contains static tasks as well as a multitude of more dynamic tasks, involving larger motion of the hands. The dataset has 55 tremor patient recordings together with: associated ground truth accelerometer data from the most affected hand, RGB video data, and aligned depth data."}, {"id": "sa-1b", "name": "SA-1B", "description": "SA-1B consists of 11M diverse, high resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks."}, {"id": "cluecorpus2020", "name": "CLUECorpus2020", "description": "CLUECorpus2020 is a large-scale corpus that can be used directly for self-supervised learning such as pre-training of a language model, or language generation. It has 100G raw corpus with 35 billion Chinese characters, which is retrieved from Common Crawl. "}, {"id": "pgm-procedurally-generated-matrices-pgm", "name": "PGM (Procedurally Generated Matrices (PGM))", "description": "PGM dataset serves as a tool for studying both abstract reasoning and generalisation in models. Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience. The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications."}, {"id": "cvact", "name": "CVACT", "description": "The CVACT dataset is a matching task between street- and aerial views, from Canberra (Australia). This task helps to determine localization without GPS coordinates for the street-view images. Google Street View panoramas are used as ground images, and matching aerial images also from the Google Maps API. The dataset comprises 35,532 image pairs for training and 8,884 image pairs for evaluation, and recall is the primary metric for evaluation. To further test the generalization in comparison to the CVUSA dataset, CVACT features 92,802 test images."}, {"id": "conceptnet", "name": "ConceptNet", "description": "ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. "}, {"id": "a-dataset-of-journalists-interactions-with-their-readership", "name": "A Dataset of Journalists' Interactions with Their Readership", "description": "We present a dataset of dialogs in which journalists of The Guardian replied to reader comments and identify the reasons why. Based on this data, we formulate the novel task of recommending reader comments to journalists that are worth reading or replying to, i.e., ranking comments in such a way that the top comments are most likely to require the journalists' reaction."}, {"id": "kitti-step", "name": "KITTI-STEP", "description": "The Segmenting and Tracking Every Pixel (STEP) benchmark consists of 21 training sequences and 29 test sequences. It is based on the KITTI Tracking Evaluation and the Multi-Object Tracking and Segmentation (MOTS) benchmark. This benchmark extends the annotations to the Segmenting and Tracking Every Pixel (STEP) task. [Copy-pasted from http://www.cvlibs.net/datasets/kitti/eval_step.php]"}, {"id": "carfusion", "name": "CarFusion", "description": "We provide manual annotations of 14 semantic keypoints for 100,000 car instances (sedan, suv, bus, and truck) from 53,000 images captured from 18 moving cameras at Multiple intersections in Pittsburgh, PA. Please fill the google form to get a email with the download links:"}, {"id": "numtadb-assembled-bengali-handwritten-digits", "name": "NumtaDB (Assembled Bengali Handwritten Digits)", "description": "To benchmark Bengali digit recognition algorithms, a large publicly available dataset is required which is free from biases originating from geographical location, gender, and age. With this aim in mind, NumtaDB, a dataset consisting of more than 85,000 images of hand-written Bengali digits, has been assembled."}, {"id": "agora", "name": "AGORA", "description": "AGORA is a synthetic human dataset with high realism and accurate ground truth. It consists of around 14K training and 3K test images by rendering between 5 and 15 people per image using either image-based lighting or rendered 3D environments, taking care to make the images physically plausible and photoreal. In total, AGORA contains 173K individual person crops. AGORA provides (1) SMPL/SMPL-X parameters and (2) segmentation masks for each subject in images."}, {"id": "ao-clevr", "name": "AO-CLEVr", "description": "AO-CLEVr is a new synthetic-images dataset containing images of \"easy\" Attribute-Object categories, based on the CLEVr. AO-CLEVr has attribute-object pairs created from 8 attributes: { red, purple, yellow, blue, green, cyan, gray, brown } and 3 object shapes {sphere, cube, cylinder}, yielding 24 attribute-object pairs. Each pair consists of 7500 images. Each image has a single object that consists of the attribute-object pair. The object is randomly assigned one of two sizes (small/large), one of two materials (rubber/metallic), a random position, and random lightning according to CLEVr defaults."}, {"id": "av-digits-database", "name": "AV Digits Database", "description": "AV Digits Database is an audiovisual database which contains normal, whispered and silent speech. 53 participants were recorded from 3 different views (frontal, 45 and profile) pronouncing digits and phrases in three speech modes."}, {"id": "dmqa-deepmind-q-a", "name": "DMQA (DeepMind Q&A)", "description": "The DeepMind Q&A Dataset consists of two datasets for Question Answering, CNN and DailyMail. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context."}, {"id": "dstc7-task-1-dialog-system-technology-challenges-task-1", "name": "DSTC7 Task 1 (Dialog System Technology Challenges Task 1)", "description": "The DSTC7 Task 1 dataset is a dataset and task for goal-oriented dialogue. The data originates from human-human conversations, which is built from online resources, specifically the Ubuntu Internet Relay Chat (IRC) channel and an Advising dataset from the University of Michigan."}, {"id": "logic-bombs", "name": "Logic Bombs", "description": "This is a set of small programs with logic bombs. The logic bomb can be triggered when certain conditions are met. Any dynamic testing tools (especially symbolic execution) can employ the dataset to benchmark their capabilities."}, {"id": "beoid-bristol-egocentric-object-interactions-dataset", "name": "BEOID (Bristol Egocentric Object Interactions Dataset)", "description": "The BEOID dataset includes object interactions ranging from preparing a coffee to operating a weight lifting machine and opening a door. The dataset is recorded at six locations: kitchen, workspace, laser printer, corridor with a locked door, cardiac gym, and weight-lifting machine. For the first four locations, sequences from five different operators were recorded (two sequences per operator), and from three operators for the last two locations (three sequences per operator). The wearable gaze tracker hardware (ASL Mobile Eye XG) was used to record the dataset. Synchronized wide-lens video data with calibrated 2D gaze fixations are available. Moreover, we release 3D information using a pre-built cloud point map and PTAM tracking. Three-dimensional information of the image and the gaze fixations are included."}, {"id": "bci-competition-iv-ecog-to-finger-movements", "name": "BCI Competition IV: ECoG to Finger Movements", "description": "The goal of this dataset is to predict the flexion of individual fingers from signals recorded from the surface of the brain (electrocorticography (ECoG)). This data set contains brain signals from three subjects, as well as the time courses of the flexion of each of five fingers. The task in this competition is to use the provided flexion information in order to predict finger flexion for a provided test set. The performance of the classifier will be evaluated by calculating the average correlation coefficient r between actual and predicted finger flexion."}, {"id": "metr-la", "name": "METR-LA", "description": "METR-LA is a dataset for traffic prediction."}, {"id": "gpa-geometric-pose-affordance", "name": "GPA (Geometric Pose Affordance)", "description": "multi-view imagery of people interacting with a variety of rich 3D environments"}, {"id": "ag-news-ags-news-corpus", "name": "AG News (AG\u2019s News Corpus)", "description": "AG News (AG\u2019s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (\u201cWorld\u201d, \u201cSports\u201d, \u201cBusiness\u201d, \u201cSci/Tech\u201d) of AG\u2019s Corpus. The AG News contains 30,000 training and 1,900 test samples per class."}, {"id": "aste-data-v2", "name": "ASTE-Data-V2", "description": "A benchmark dataset for the Aspect Sentiment Triplet Extraction, an updated version of ASTE-Data-V1."}, {"id": "celeba-spoof", "name": "CelebA-Spoof", "description": "CelebA-Spoof is a large-scale face anti-spoofing dataset with the following properties: "}, {"id": "jamendo-lyrics", "name": "Jamendo Lyrics", "description": "Dataset for lyrics alignment and transcription evaluation. It contains 20 music pieces under CC license from the Jamendo website along with their lyrics, with:"}, {"id": "eyth-egoyoutubehands", "name": "EYTH (EgoYouTubeHands)", "description": "Includes egocentric videos containing hands in the wild."}, {"id": "shapeglot-shapeglot-learning-language-for-shape-differentiation", "name": "ShapeGlot (ShapeGlot: Learning Language for Shape Differentiation)", "description": "ShapeGlot: Learning Language for Shape Differentiation"}, {"id": "kqa-pro", "name": "KQA Pro", "description": "A large-scale dataset for Complex KBQA."}, {"id": "miccai-2015-head-and-neck-challenge", "name": "MICCAI 2015 Head and Neck Challenge", "description": "This database is provided and maintained by Dr. Gregory C Sharp (Harvard Medical School \u2013 MGH, Boston) and his group."}, {"id": "active-tls-stack-fingerprinting-measurement-data", "name": "Active TLS Stack Fingerprinting Measurement Data", "description": "Measurement data related to the publication \u201eActive TLS Stack Fingerprinting: Characterizing TLS Server Deployments at Scale\u201c. It contains weekly TLS and HTTP scan data and the TLS fingerprints for each target."}, {"id": "told-br-toxic-language-detection-for-brazilian-portuguese", "name": "ToLD-Br (Toxic Language Detection for Brazilian Portuguese)", "description": "The Toxic Language Detection for Brazilian Portuguese (ToLD-Br) is a dataset with tweets in Brazilian Portuguese annotated according to different toxic aspects."}, {"id": "a-collection-of-lfr-benchmark-graphs", "name": "A collection of LFR benchmark graphs", "description": "This dataset is a collection of undirected and unweighted LFR benchmark graphs as proposed by Lancichinetti et al. [1]. We generated the graphs using the code provided by Santo Fortunato on his personal website [2], embedded in our evaluation framework [3], with two different parameter sets. Let N denote the number of vertices in the network, then"}, {"id": "covidx-covidx-crx-2", "name": "COVIDx (COVIDx CRX-2)", "description": "An open access benchmark dataset comprising of 13,975 CXR images across 13,870 patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge."}, {"id": "jnlpba", "name": "JNLPBA", "description": "JNLPBA is a biomedical dataset that comes from the GENIA version 3.02 corpus (Kim et al., 2003). It was created with a controlled search on MEDLINE. From this search 2,000 abstracts were selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification. 36 terminal classes were used to annotate the GENIA corpus."}, {"id": "contactdb", "name": "ContactDB", "description": "ContactDB is a dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping, enabled by use of a thermal camera. ContactDB includes 3,750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images."}, {"id": "covid-ct", "name": "COVID-CT", "description": "Contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist who has been diagnosing and treating COVID-19 patients since the outbreak of this pandemic. "}, {"id": "ffhq-flickr-faces-hq", "name": "FFHQ (Flickr-Faces-HQ)", "description": "Flickr-Faces-HQ (FFHQ) consists of 70,000 high-quality PNG images at 1024\u00d71024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped using dlib. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Amazon Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos."}, {"id": "timers-and-such", "name": "Timers and Such", "description": "Timers and Such is an open source dataset of spoken English commands for common voice control use cases involving numbers. The dataset has four intents, corresponding to four common offline voice assistant uses: SetTimer, SetAlarm, SimpleMath, and UnitConversion. The semantic label for each utterance is a dictionary with the intent and a number of slots. "}, {"id": "linnaeus", "name": "LINNAEUS", "description": "LINNAEUS is a general-purpose dictionary matching software, capable of processing multiple types of document formats in the biomedical domain (MEDLINE, PMC, BMC, OTMI, text, etc.). It can produce multiple types of output (XML, HTML, tab-separated-value file, or save to a database). It also contains methods for acting as a server (including load balancing across several servers), allowing clients to request matching over a network. A package with files for recognizing and identifying species names is available for LINNAEUS, showing 94% recall and 97% precision compared to LINNAEUS-species-corpus."}, {"id": "egoshots", "name": "EgoShots", "description": "Egoshots is a 2-month Ego-vision Dataset with Autographer Wearable Camera annotated \"for free\" with transfer learning. Three state of the art pre-trained image captioning models are used. The dataset represents the life of 2 interns while working at Philips Research (Netherlands) (May-July 2015) generously donating their data."}, {"id": "vocaset", "name": "VOCASET", "description": "VOCASET is a 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio. The dataset has 12 subjects and 480 sequences of about 3-4 seconds each with sentences chosen from an array of standard protocols that maximize phonetic diversity."}, {"id": "gmeg-yahoo", "name": "GMEG-yahoo", "description": "Grammatical error correction dataset for text from Yahoo! Answers"}, {"id": "faceforensics", "name": "FaceForensics", "description": "FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video forgeries. All videos are downloaded from Youtube and are cut down to short continuous clips that contain mostly frontal faces. This dataset has two versions:"}, {"id": "ffhq-aging", "name": "FFHQ-Aging", "description": "FFHQ-Aging is a Dataset of human faces designed for benchmarking age transformation algorithms as well as many other possible vision tasks. This dataset is an extention of the NVIDIA FFHQ dataset, on top of the 70,000 original FFHQ images, it also contains the following information for each image: * Gender information (male/female with confidence score) * Age group information (10 classes with confidence score) * Head pose (pitch, roll & yaw) * Glasses type (none, normal or dark) * Eye occlusion score (0-100, different score for each eye) * Full semantic map (19 classes, based on CelebAMask-HQ labels)"}, {"id": "ulm-tsst-ulm-trier-social-stress-dataset", "name": "Ulm-TSST (Ulm-Trier Social Stress Dataset)", "description": "Ulm-TSST is a dataset continuous emotion (valence and arousal) prediction and `physiological-emotion' prediction. It consists of a multimodal richly annotated dataset of self-reported, and external dimensional ratings of emotion and mental well-being. After a brief period of preparation the subjects are asked to give an oral presentation, within a job-interview setting.  Ulm-TSST includes biological recordings, such as Electrocardiogram (ECG),  Electrodermal Activity (EDA), Respiration, and Heart Rate (BPM) as well as continuous arousal and valence annotations. With 105 participants (69.5% female) aged between 18 and 39 years, a total of 10 hours were accumulated."}, {"id": "iglue-image-grounded-language-understanding-evaluation", "name": "IGLUE (Image-Grounded Language Understanding Evaluation)", "description": "The Image-Grounded Language Understanding Evaluation (IGLUE) benchmark brings together\u2014by both aggregating pre-existing datasets and creating new ones\u2014visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. The benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups."}, {"id": "mlqe-multilingual-quality-estimation", "name": "MLQE (MultiLingual Quality Estimation)", "description": "The MLQE dataset is a dataset for sentence-level Machine Translation Quality Estimation. It consists of 6 language pairs representing NMT training in high, medium, and low-resource scenarios. The corpus is extracted from Wikipedia, and 10K segments per language pair are annotated."}, {"id": "total-text", "name": "Total-Text", "description": "Total-Text is a text detection dataset that consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively."}, {"id": "flores-200", "name": "FLORES-200", "description": "FLORES-200 doubles the existing language coverage of FLORES-101. Given the nature of the new languages, which have less standardization and require more specialized professional translations, the verification process became more complex. This required modifications to the translation workflow. FLORES-200 has several languages which were not translated from English. Specifically, several languages were translated from Spanish, French, Russian and Modern Standard Arabic."}, {"id": "visual-domain-decathlon", "name": "Visual Domain Decathlon", "description": "The goal of this challenge is to solve simultaneously ten image classification problems representative of very different visual domains. The data for each domain is obtained from the following image classification benchmarks:"}, {"id": "cholect40-cholecystectomy-action-triplet", "name": "CholecT40 (Cholecystectomy Action Triplet)", "description": "CholecT40 is the first endoscopic dataset introduced to enable research on fine-grained action recognition in laparoscopic surgery. "}, {"id": "easycall", "name": "EasyCall", "description": "EasyCall is a new dysarthric speech command dataset in Italian. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure. The corpus aims at providing a resource for the development of ASR-based assistive technologies for patients with dysarthria. In particular, it may be exploited to develop a voice-controlled contact application for commercial smartphones, aiming at improving dysarthric patients' ability to communicate with their family and caregivers. Before recording the dataset, participants were administered a survey to evaluate which commands are more likely to be employed by dysarthric individuals in a voice-controlled contact application. In addition, the dataset includes a list of non-commands (i.e., words near/inside commands or phonetically close to commands) that can be leveraged to build a more robust command recognition system."}, {"id": "zerowaste", "name": "ZeroWaste", "description": "ZeroWaste is a dataset for automatic waste detection and segmentation. This dataset contains over 1,800 fully segmented video frames collected from a real waste sorting plant along with waste material labels for training and evaluation of the segmentation methods, as well as over 6,000 unlabeled frames that can be further used for semi-supervised and self-supervised learning techniques. ZeroWaste also provides frames of the conveyor belt before and after the sorting process, comprising a novel setup that can be used for weakly-supervised segmentation."}, {"id": "mucgec-multi-reference-multi-source-evaluation-dataset-for-chinese-grammatical-error-correction", "name": "MuCGEC (Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction)", "description": "MuCGEC is a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three different Chinese-as-a-Second-Language (CSL) learner sources. Each sentence has been corrected by three annotators, and their corrections are meticulously reviewed by an expert, resulting in 2.3 references per sentence."}, {"id": "cord-19", "name": "CORD-19", "description": "CORD-19 is a free resource of tens of thousands of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses for use by the global research community."}, {"id": "theodore-learning-from-theodore", "name": "THEODORE (Learning from THEODORE)", "description": "Recent work about synthetic indoor datasets from perspective views has shown significant improvements of object detection results with Convolutional Neural Networks(CNNs). In this paper, we introduce THEODORE: a novel, large-scale indoor dataset containing 100,000 high- resolution diversified fisheye images with 14 classes. To this end, we create 3D virtual environments of living rooms, different human characters and interior textures. Beside capturing fisheye images from virtual environments we create annotations for semantic segmentation, instance masks and bounding boxes for object detection tasks. We compare our synthetic dataset to state of the art real-world datasets for omnidirectional images. Based on MS COCO weights, we show that our dataset is well suited for fine-tuning CNNs for object detection. Through a high generalization of our models by means of image synthesis and domain randomization we reach an AP up to 0.84 for class person on High-Definition Analytics dataset."}, {"id": "avsbench-audio-visual-segmentation", "name": "AVSBench (Audio \u2212Visual Segmentation)", "description": "AVSBench is a pixel-level audio-visual segmentation benchmark that provides ground truth labels for sounding objects. The dataset is divided into three subsets: AVSBench-object (Single-source subset, Multi-sources subset) and AVSBench-semantic (Semantic-labels subset). Accordingly, three settings are studied: "}, {"id": "referitgame", "name": "ReferItGame", "description": "The ReferIt dataset contains 130,525 expressions for referring to 96,654 objects in 19,894 images of natural scenes."}, {"id": "elevater-evaluation-of-language-augmented-visual-task-level-transfer", "name": "ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer)", "description": "The ELEVATER benchmark is a collection of resources for training, evaluating, and analyzing language-image models on image classification and object detection. ELEVATER consists of:"}, {"id": "aids", "name": "AIDS", "description": "AIDS is a graph dataset. It consists of 2000 graphs representing molecular compounds which are constructed from the AIDS Antiviral Screen Database of Active Compounds. It contains 4395 chemical compounds, of which 423 belong to class CA, 1081 to CM, and the remaining compounds to CI."}, {"id": "w3c-experts", "name": "W3C Experts", "description": "This is a subset of the TREC 2005 enterprise track data, and consists of 48 topics and 200 candidates per topic, with each candidate labeled as an expert or non-expert for the topic. The task is to rank the candidates based on their expertise on a topic, using a corpus of mailing lists from the World Wide Web Consortium (W3C). This is an application where the  unconstrained algorithm does better for the minority protected group."}, {"id": "fddb-360", "name": "FDDB-360", "description": "A 360-degree fisheye-like version of the popular FDDB face detection dataset."}, {"id": "chart2text-chart-summarization-dataset", "name": "Chart2Text (Chart Summarization Dataset)", "description": "Chart2Text is a dataset that was crawled from 23,382 freely accessible pages from statista.com in early March of 2020, yielding a total of 8,305 charts, and associated summaries. For each chart, the chart image, the underlying data table, the title, the axis labels, and a human-written summary describing the statistic was downloaded."}, {"id": "qnli-question-answering-nli", "name": "QNLI (Question-answering NLI)", "description": "The QNLI (Question-answering NLI) dataset is a Natural Language Inference dataset automatically derived from the Stanford Question Answering Dataset v1.1 (SQuAD). SQuAD v1.1 consists of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The dataset was converted into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. The QNLI dataset is part of GLEU benchmark."}, {"id": "medic", "name": "MEDIC", "description": "MEDIC is a large social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. It consists data from several data sources such as CrisisMMD, data from AIDR and Damage Multimodal Dataset (DMD)."}, {"id": "raredis-corpus", "name": "RareDis corpus", "description": "The RareDis corpus contains more than 5,000 rare diseases and almost 6,000 clinical manifestations are annotated. Moreover, the Inter Annotator Agreement evaluation shows a relatively high agreement (F1-measure equal to 83.5% under exact match criteria for the entities and equal to 81.3% for the relations). Based on these results, this corpus is of high quality, supposing a significant step for the field since there is a scarcity of available corpus annotated with rare diseases."}, {"id": "google-landmarks", "name": "Google Landmarks", "description": "The Google Landmarks dataset contains 1,060,709 images from 12,894 landmarks, and 111,036 additional query images. The images in the dataset are captured at various locations in the world, and each image is associated with a GPS coordinate. This dataset is used to train and evaluate large-scale image retrieval models."}, {"id": "hurdl-human-robot-dialogue-learning-corpus", "name": "HuRDL (Human-Robot Dialogue Learning Corpus)", "description": "The Human-Robot Dialogue Learning (HuRDL) Corpus is a dataset about asking questions in situated task-based interactions. It is a dialogue corpus collected in an online interactive virtual environment in which human participants play the role of a robot performing a collaborative tool-organization task."}, {"id": "hm3dsem", "name": "HM3DSem", "description": "The Habitat-Matterport 3D Semantics Dataset (HM3DSem) is the largest-ever dataset of 3D real-world and indoor spaces with densely annotated semantics that is available to the academic community. HM3DSem v0.2 consists of 142,646 object instance annotations across 216 3D-spaces from HM3D and 3,100 rooms within those spaces. The HM3D scenes are annotated with the 142,646 raw object names, which are mapped to 40 Matterport categories. On average, each scene in HM3DSem v0.2 consists of 661 objects from 106 categories. This dataset is the result of 14,200+ hours of human effort for annotation and verification by 20+ annotators."}, {"id": "once-3dlanes-monocular-3d-lane-detection-dataset", "name": "ONCE-3DLanes (Monocular 3D Lane Detection Dataset)", "description": "ONCE-3DLanes is a real-world autonomous driving dataset with lane layout annotation in 3D space. A dataset annotation pipeline is designed to automatically generate high-quality 3D lane locations from 2D lane annotations by exploiting the explicit relationship between point clouds and image pixels in 211,000 road scenes."}, {"id": "hanabi-learning-environment", "name": "Hanabi Learning Environment", "description": "A new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information."}, {"id": "trashcan", "name": "TrashCan", "description": "The TrashCan dataset is an instance-segmentation dataset of underwater trash. It is comprised of annotated images (7,212 images) which contain observations of trash, ROVs, and a wide variety of undersea flora and fauna. The annotations in this dataset take the format of instance segmentation annotations: bitmaps containing a mask marking which pixels in the image contain each object. The imagery in TrashCan is sourced from the J-EDI (JAMSTEC E-Library of Deep-sea Images) dataset, curated by the Japan Agency of Marine Earth Science and Technology (JAMSTEC). "}, {"id": "nela-gt-2019", "name": "NELA-GT-2019", "description": "NELA-GT-2019 is an updated version of the NELA-GT-2018 dataset. NELA-GT-2019 contains 1.12M news articles from 260 sources collected between January 1st 2019 and December 31st 2019. Just as with NELA-GT-2018, these sources come from a wide range of mainstream news sources and alternative news sources. Included with the dataset are source-level ground truth labels from 7 different assessment sites covering multiple dimensions of veracity."}, {"id": "gof-gyroscope-optical-flow", "name": "GOF (Gyroscope Optical Flow)", "description": "Optical Flow in challenging scenes with gyroscope readings!"}, {"id": "alexa-point-of-view", "name": "Alexa Point of View", "description": "The Alexa Point of View dataset is point of view conversion dataset, a parallel corpus of messages spoken to a virtual assistant and the converted messages for delivery. The dataset contains parallel corpus of input (input column) message and POV converted messages (output column). An example of a pair is tell @CN@ that i'll be late [\\t] hi @CN@, @SCN@ would like you to know that they'll be late. The input and pov-converted output pair is tab separated. @CN@ tag is a placeholder for the contact name (receiver) and @SCN@ tag is a placeholder for source contact name (sender). The total dataset has 46563 pairs. This data is then test/train/dev split into 6985 pairs/32594 pairs/6985 pairs."}, {"id": "ascend", "name": "ASCEND", "description": "ASCEND (A Spontaneous Chinese-English Dataset) introduces a high-quality resource of spontaneous multi-turn conversational dialogue Chinese code-switching corpus collected in Hong Kong. ASCEND includes 23 bilinguals that are fluent in both Chinese and English and consists of 10.62 hours clean speech corpus."}, {"id": "pascal-s", "name": "PASCAL-S", "description": "PASCAL-S is a dataset for salient object detection consisting of a set of 850 images from PASCAL VOC 2010 validation set with multiple salient objects on the scenes."}, {"id": "gopro", "name": "GoPro", "description": "The GoPro dataset for deblurring consists of 3,214 blurred images with the size of 1,280\u00d7720 that are divided into 2,103 training images and 1,111 test images. The dataset consists of pairs of a realistic blurry image and the corresponding ground truth shapr image that are obtained by a high-speed camera."}, {"id": "cawac", "name": "caWaC", "description": "The corpus represents the largest existing corpus of Catalan containing 687 million words, which is a significant increase given that until now the biggest corpus of Catalan, CuCWeb, counts 166 million words. "}, {"id": "wrench", "name": "Wrench", "description": "Wrench is a benchmark platform for thorough and standardized evaluation of Weak Supervision (WS). It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods."}, {"id": "xwino", "name": "XWINO", "description": "XWINO is a multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities. "}, {"id": "avspeech", "name": "AVSpeech", "description": "AVSpeech is a large-scale audio-visual dataset comprising speech clips with no interfering background signals. The segments are of varying length, between 3 and 10 seconds long, and in each clip the only visible face in the video and audible sound in the soundtrack belong to a single speaking person. In total, the dataset contains roughly 4700 hours of video segments with approximately 150,000 distinct speakers, spanning a wide variety of people, languages and face poses."}, {"id": "kins", "name": "KINS", "description": "Augments the KITTI with more instance pixel-level annotation for 8 categories."}, {"id": "jparacrawl", "name": "JParaCrawl", "description": "JParaCrawl is a parallel corpus for English-Japanese, for which the amount of publicly available parallel corpora is still limited. The parallel corpus was constructed by broadly crawling the web and automatically aligning parallel sentences. The corpus amassed over 8.7 million sentence pairs."}, {"id": "hpo-b", "name": "HPO-B", "description": "HPO-B is a benchmark for assessing the performance of HPO (Hyperparameter optimization) algorithms."}, {"id": "shad3s-shad3s-dataset", "name": "SHAD3S (SHAD3S Dataset)", "description": "We introduce the SHAD3S dataset, that for a given contour representation of a mesh, under a given illumination condition, provides the illumination masks on the object, a shadow mask on the ground, its diffuse and sketch renders."}, {"id": "glips-german-lips", "name": "GLips (German Lips)", "description": "The German Lipreading dataset consists of 250,000 publicly available videos of the faces of speakers of the Hessian Parliament, which was processed for word-level lip reading using an automatic pipeline. The format is similar to that of the English language Lip Reading in the Wild (LRW) dataset, with each H264-compressed MPEG-4 video encoding one word of interest in a context of 1.16 seconds duration, which yields compatibility for studying transfer learning between both datasets. Choosing video material based on naturally spoken language in a natural environment ensures more robust results for real-world applications than artificially generated datasets with as little noise as possible. The 500 different spoken words ranging between 4-18 characters in length each have 500 instances and separate MPEG-4 audio- and text metadata-files, originating from 1018 parliamentary sessions. Additionally, the complete TextGrid files containing the segmentation information of those sessions are also included. The size of the uncompressed dataset is 15GB."}, {"id": "hc-stvg1-human-centric-spatio-temporal-video-grounding", "name": "HC-STVG1 (Human-centric Spatio-Temporal Video Grounding)", "description": "The newly proposed HC-STVG task aims to localize the target person spatio-temporally in an untrimmed video. For this task, we collect a new benchmark dataset, which has spatio temporal annotations related to the target persons in complex multi-person scenes, together with full interaction and rich action information."}, {"id": "trackingnet", "name": "TrackingNet", "description": "TrackingNet is a large-scale tracking dataset consisting of videos in the wild. It has a total of 30,643 videos split into 30,132 training videos and 511 testing videos, with an average of 470,9 frames."}, {"id": "dawt-densely-annotated-wikipedia-texts", "name": "DAWT (Densely Annotated Wikipedia Texts)", "description": "The DAWT dataset consists of Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. "}, {"id": "ace-2005-ace-2005-multilingual-training-corpus", "name": "ACE 2005 (ACE 2005 Multilingual Training Corpus)", "description": "ACE 2005 Multilingual Training Corpus contains the complete set of English, Arabic and Chinese training data for the 2005 Automatic Content Extraction (ACE) technology evaluation. The corpus consists of data of various types annotated for entities, relations and events by the Linguistic Data Consortium (LDC) with support from the ACE Program and additional assistance from LDC."}, {"id": "amalgum-a-machine-annotated-lookalike-of-gum", "name": "AMALGUM (A Machine Annotated Lookalike of GUM)", "description": "AMALGUM is a machine annotated multilayer corpus following the same design and annotation layers as GUM, but substantially larger (around 4M tokens). The goal of this corpus is to close the gap between high quality, richly annotated, but small datasets, and the larger but shallowly annotated corpora that are often scraped from the Web."}, {"id": "streets", "name": "STREETS", "description": "A novel traffic flow dataset from publicly available web cameras in the suburbs of Chicago, IL."}, {"id": "bar-biased-action-recognition", "name": "BAR (Biased Action Recognition)", "description": "Biased Action Recognition (BAR) dataset is a real-world image dataset categorized as six action classes which are biased to distinct places. The authors settle these six action classes by inspecting imSitu, which provides still action images from Google Image Search with action and place labels. In detail, the authors choose action classes where images for each of these candidate actions share common place characteristics. At the same time, the place characteristics of action class candidates should be distinct in order to classify the action only from place attributes. The select pairs are six typical action-place pairs: (Climbing, RockWall), (Diving, Underwater), (Fishing, WaterSurface), (Racing, APavedTrack), (Throwing, PlayingField),and (Vaulting, Sky)."}, {"id": "advnet", "name": "AdvNet", "description": "AdvNet is a dataset of traffic signs images. Specifically, it includes adversarial traffic sign images (i.e., pictures of traffic signs with stickers on their surface) that can fool state-of-the-art neural network-based perception systems and clean traffic sign images without any stickers on them."}, {"id": "mold-marathi-offensive-language-dataset", "name": "MOLD (Marathi Offensive Language Dataset)", "description": "MOLD is a Marathi dataset for offensive language identification"}, {"id": "mobibits-multimodal-mobile-biometric-database", "name": "MobiBits (Multimodal Mobile Biometric Database)", "description": "A novel database comprising representations of five different biometric characteristics, collected in a mobile, unconstrained or semi-constrained setting with three different mobile devices, including characteristics previously unavailable in existing datasets, namely hand images, thermal hand images, and thermal face images, all acquired with a mobile, off-the-shelf device."}, {"id": "global-voices", "name": "Global Voices", "description": "Global Voices is a multilingual dataset for evaluating cross-lingual summarization methods. It is extracted from social-network descriptions of Global Voices news articles to cheaply collect evaluation data for into-English and from-English summarization in 15 languages. "}, {"id": "depth-in-the-wild", "name": "Depth in the Wild", "description": "Depth in the Wild is a dataset for single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. It consists of images in the wild annotated with relative depth between pairs of random points."}, {"id": "mvtec-loco-ad-mvtec-logical-constraints-anomaly-detection", "name": "MVTec LOCO AD (MVTec Logical Constraints Anomaly Detection)", "description": "MVTec Logical Constraints Anomaly Detection (MVTec LOCO AD) dataset is intended for the evaluation of unsupervised anomaly localization algorithms. The dataset includes both structural and logical anomalies. It contains 3644 images from five different categories inspired by real-world industrial inspection scenarios. Structural anomalies appear as scratches, dents, or contaminations in the manufactured products. Logical anomalies violate underlying constraints, e.g., a permissible object being present in an invalid location or a required object not being present at all. The dataset also includes pixel-precise ground truth data for each anomalous region."}, {"id": "exact-street2shop", "name": "Exact Street2Shop", "description": "A dataset containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos."}, {"id": "pcqm4mv2-lsc", "name": "PCQM4Mv2-LSC", "description": "PCQM4Mv2 is a quantum chemistry dataset originally curated under the PubChemQC project. Based on the PubChemQC, we define a meaningful ML task of predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular graphs. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties of molecules since it is related to reactivity, photoexcitation, and charge transport. Moreover, predicting the quantum chemical property only from 2D molecular graphs without their 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium structures requires DFT-based geometry optimization, which is expensive on its own."}, {"id": "red-miniimagenet-40-label-noise", "name": "Red MiniImageNet 40% label noise", "description": "Part of the Controlled Noisy Web Labels Dataset."}, {"id": "election2020", "name": "Election2020", "description": "Election2020 is a Twitter dataset on the 2020 US presidential elections. To facilitate the understanding of political discourse and try to empower the Computational Social Science research community, the authors decided to publicly release this massive-scale, longitudinal dataset of U.S. politics- and election-related tweets. This multilingual dataset encompasses hundreds of millions of tweets and tracks all salient U.S. politics trends, actors, and events between 2019 and 2020. It predates and spans the whole period of Republican and Democratic primaries, with real-time tracking of all presidential contenders of both sides of the isle. After that, it focuses on presidential and vice-presidential candidates. The dataset release is curated, documented and will be constantly updated on a weekly-basis, until the November 3, 2020 election and beyond."}, {"id": "eventkg-click", "name": "EventKG+Click", "description": "Builds upon the event-centric EventKG knowledge graph and language-specific information on user interactions with events, entities, and their relations derived from the Wikipedia clickstream."}, {"id": "gasch1-funcat", "name": "Gasch1 Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "office-caltech-10", "name": "Office-Caltech-10", "description": "Office-Caltech-10 a standard benchmark for domain adaptation, which consists of Office 10 and Caltech 10 datasets. It contains the 10 overlapping categories between the Office dataset and Caltech256 dataset. SURF BoW historgram features, vector quantized to 800 dimensions are also available for this dataset."}, {"id": "rsitmd", "name": "RSITMD", "description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled)."}, {"id": "doc2dial", "name": "doc2dial", "description": "A new dataset of goal-oriented dialogues that are grounded in the associated documents."}, {"id": "sv-ident-survey-variable-identification", "name": "SV-Ident (Survey Variable Identification)", "description": "SV-Ident comprises 4,248 sentences from social science publications in English and German. The data is the official data for the Shared Task: \u201cSurvey Variable Identification in Social Science Publications\u201d (SV-Ident) 2022. Sentences are labeled with variables that are mentioned either explicitly or implicitly. "}, {"id": "disrpt2021-disrpt2021-shared-task-on-discourse-unit-segmentation-connective-detection-and-discourse-relation-classification", "name": "DISRPT2021 (DISRPT2021 shared task on Discourse Unit Segmentation, Connective Detection and Discourse Relation Classification)", "description": "The DISRPT 2021 shared task, co-located with CODI 2021 at EMNLP, introduces the second iteration of a cross-formalism shared task on discourse unit segmentation and connective detection, as well as the first iteration of a cross-formalism discourse relation classification task."}, {"id": "diskne-disease-knowledge-evaluation", "name": "DisKnE (Disease Knowledge Evaluation)", "description": "DisKnE is a benchmark for Disease Knowledge Evaluation built from MedNLI and MEDIQA-NLI. This benchmark is constructed to specifically test the medical reasoning capabilities of ML models, such as mapping symptoms to diseases."}, {"id": "multilingual-librispeech-mls", "name": "Multilingual LibriSpeech (MLS)", "description": "Multilingual LibriSpeech is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. It includes about 44.5K hours of English and a total of about 6K hours for other languages. "}, {"id": "tut-sound-events-2017", "name": "TUT Sound Events 2017", "description": "The TUT Sound Events 2017 dataset contains 24 audio recordings in a street environment and contains 6 different classes. These classes are: brakes squeaking, car, children, large vehicle, people speaking, and people walking."}, {"id": "hdr-burst-photography-dataset", "name": "HDR+ Burst Photography Dataset", "description": "The dataset consists of 3640 bursts (made up of 28461 images in total), organized into subfolders, plus the results of an image processing pipeline. Each burst consists of the raw burst input (in DNG format) and certain metadata not present in the images, as sidecar files."}, {"id": "french-timebank", "name": "French Timebank", "description": "French TimeBank, a corpus for French annotated in ISO-TimeML."}, {"id": "cmu-mosei", "name": "CMU-MOSEI", "description": "CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence level sentiment analysis and emotion recognition in online videos. CMU-MOSEI contains more than 65 hours of annotated video from more than 1000 speakers and 250 topics."}, {"id": "openai-gym", "name": "OpenAI Gym", "description": "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It includes environment such as Algorithmic, Atari, Box2D, Classic Control, MuJoCo, Robotics, and Toy Text."}, {"id": "isic-2018-task-2", "name": "ISIC 2018 Task 2", "description": "The ISIC 2018 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 2 dataset is the challenge on lesion attribute detection. It includes 2594 images. The task is to detect the following dermoscopic attributes: pigment network, negative network, streaks, mila-like cysts and globules (including dots)."}, {"id": "acronym-identification", "name": "Acronym Identification", "description": "Is an acronym disambiguation (AD) dataset for scientific domain with 62,441 samples which is significantly larger than the previous scientific AD dataset."}, {"id": "ikea-asm", "name": "IKEA ASM", "description": "A three million frame, multi-view, furniture assembly video dataset that includes depth, atomic actions, object segmentation, and human pose. "}, {"id": "aadb2021ontology-ontology-representation-for-a-data-set-of-cation-coordinated-conformers-of-20-proteinogenic-amino-acids", "name": "AADB2021Ontology (Ontology representation for a data set of cation-coordinated conformers of 20 proteinogenic amino acids)", "description": "This onotology is populated with the data from AADB2021 (https://dx.doi.org/10.17172/NOMAD/2021.02.10-1). Details can be found in the related article on arXiv.org: https://arxiv.org/abs/2107.08855"}, {"id": "imagenet-32", "name": "ImageNet-32", "description": "Imagenet32 is a huge dataset made up of small images called the down-sampled version of Imagenet. Imagenet32 is composed of 1,281,167 training data and 50,000 test data with 1,000 labels."}, {"id": "crvd-captured-raw-video-denoising", "name": "CRVD (Captured Raw Video Denoising)", "description": "The CRVD dataset consists of 55 groups of noisy-clean videos with ISO values ranging from 1600 to 25600."}, {"id": "esc-50", "name": "ESC-50", "description": "The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. It comprises 2000 5s-clips of 50 different classes across natural, human and domestic sounds, again, drawn from Freesound.org."}, {"id": "clevr-ref", "name": "CLEVR-Ref+", "description": "CLEVR-Ref+ is a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators. "}, {"id": "million-aid", "name": "Million-AID", "description": "Million-AID is a large-scale benchmark dataset containing a million instances for RS scene classification. There are 51 semantic scene categories in Million-AID. And the scene categories are customized to match the land-use classification standards, which greatly enhance the practicability of the constructed Million-AID. Different form the existing scene classification datasets of which categories are organized with parallel or uncertain relationships, scene categories in Million-AID are organized with systematic relationship architecture, giving it superiority in management and scalability. Specifically, the scene categories in Million-AID are organized by the hierarchical category network of a three-level tree: 51 leaf nodes fall into 28 parent nodes at the second level which are grouped into 8 nodes at the first level, representing the 8 underlying scene categories of agriculture land, commercial land, industrial land, public service land, residential land, transportation land, unutilized land, and water area. The scene category network provides the dataset with excellent organization of relationship among different scene categories and also the property of scalability. The number of images in each scene category ranges from 2,000 to 45,000, endowing the dataset with the property of long tail distribution. Besides, Million-AID has superiorities over the existing scene classification datasets owing to its high spatial resolution, large scale, and global distribution."}, {"id": "panoptic-nuscenes", "name": "Panoptic nuScenes", "description": "Panoptic nuScenes is a benchmark dataset that extends the popular nuScenes dataset with point-wise groundtruth annotations for semantic segmentation, panoptic segmentation, and panoptic tracking tasks."}, {"id": "dic-c2dh-hela", "name": "DIC-C2DH-HeLa", "description": "HeLa cells on a flat glass Dr. G. van Cappellen. Erasmus Medical Center, Rotterdam, The Netherlands"}, {"id": "poser", "name": "Poser", "description": "The Poser dataset is a dataset for pose estimation which consists of 1927 training and 418 test images. These images are synthetically generated and tuned to unimodal predictions. The images were generated using the Poser software package."}, {"id": "rte-recognizing-textual-entailment", "name": "RTE (Recognizing Textual Entailment)", "description": "The Recognizing Textual Entailment (RTE) datasets come from a series of textual entailment challenges. Data from RTE1, RTE2, RTE3 and RTE5 is combined. Examples are constructed based on news and Wikipedia text."}, {"id": "textcomplexityde", "name": "TextComplexityDE", "description": "TextComplexityDE is a dataset consisting of 1000 sentences in German language taken from 23 Wikipedia articles in 3 different article-genres to be used for developing text-complexity predictor models and automatic text simplification in German language. The dataset includes subjective assessment of different text-complexity aspects provided by German learners in level A and B. In addition, it contains manual simplification of 250 of those sentences provided by native speakers and subjective assessment of the simplified sentences by participants from the target group. The subjective ratings were collected using both laboratory studies and crowdsourcing approach."}, {"id": "brace-the-breakdancing-competition-dataset-for-dance-motion-synthesis", "name": "BRACE (The Breakdancing Competition Dataset for Dance Motion Synthesis)", "description": "BRACE is a dataset for audio-conditioned dance motion synthesis challenging common assumptions for this task:"}, {"id": "semi-inat-semi-supervised-inaturalist", "name": "Semi-iNat (Semi-Supervised iNaturalist)", "description": "Semi-iNat is a challenging dataset for semi-supervised classification with a long-tailed distribution of classes, fine-grained categories, and domain shifts between labeled and unlabeled data. The data is obtained from iNaturalist, a community driven project aimed at collecting observations of biodiversity. "}, {"id": "trecvid-avs19-v3c1", "name": "TRECVID-AVS19 (V3C1)", "description": "The dataset has been designed to represent true web videos in the wild, with good visual quality and diverse content characteristics,  The test video collection for TRECVID-AVS2019-TRECVID-AVS2021, which contains 1,082,649 web video clips, with even more diverse content, no predominant characteristics and low self-similarity."}, {"id": "climate-fever", "name": "CLIMATE-FEVER", "description": "A new publicly available dataset for verification of climate change-related claims. "}, {"id": "urban-sed", "name": "URBAN-SED", "description": "URBAN-SED is a dataset of 10,000 soundscapes with sound event annotations generated using the scraper library. The dataset includes 10,000 soundscapes, totals almost 30 hours and includes close to 50,000 annotated sound events. Every soundscape is 10 seconds long and has a background of Brownian noise resembling the typical \u201chum\u201d often heard in urban environments. Every soundscape contains between 1-9 sound evnts from the following classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren and street_music. The source material for the sound events are the clips from the UrbanSound8K dataset. URBAN-SED comes pre-sorted into three sets: train, validate and test. There are 6000 soundscapes in the training set, generated using clips from folds 1-6 in UrbanSound8K, 2000 soundscapes in the validation set, generated using clips from fold 7-8 in UrbanSound8K, and 2000 soundscapes in the test set, generated using clips from folds 9-10 in UrbanSound8K."}, {"id": "omniglot", "name": "Omniglot", "description": "The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds."}, {"id": "buff-bodies-under-flowing-fashion", "name": "BUFF (Bodies Under Flowing Fashion)", "description": "BUFF consists of 5 subjects, 3 male and 2 female wearing 2 clothing styles: a) t-shirt and long pants and b) a soccer outfit. They perform 3 different motions i) hips ii) tilt_twist_left iii) shoulders_mill."}, {"id": "wmt-2015", "name": "WMT 2015", "description": "WMT 2015 is a collection of datasets used in shared tasks of the Tenth Workshop on Statistical Machine Translation. The workshop featured five tasks:"}, {"id": "m4", "name": "M4", "description": "The M4 dataset is a collection of 100,000 time series used for the fourth edition of the Makridakis forecasting Competition. The M4 dataset consists of time series of yearly, quarterly, monthly and other (weekly, daily and hourly) data, which are divided into training and test sets. The minimum numbers of observations in the training test are 13 for yearly, 16 for quarterly, 42 for monthly, 80 for weekly, 93 for daily and 700 for hourly series. The participants were asked to produce the following numbers of forecasts beyond the available data that they had been given: six for yearly, eight for quarterly, 18 for monthly series, 13 for weekly series and 14 and 48 forecasts respectively for the daily and hourly ones."}, {"id": "rushifteval", "name": "RuShiftEval", "description": "RuShiftEval is a manually annotated lexical semantic change dataset for Russian. Its novelty is ensured by a single set of target words annotated for their diachronic semantic shifts across three time periods, while the previous work either used only two time periods, or different sets of target words."}, {"id": "indiccorp", "name": "IndicCorp", "description": "IndicCorp is a large monolingual corpora with around 9 billion tokens covering 12 of the major Indian languages. It has been developed by discovering and scraping thousands of web sources - primarily news, magazines and books, over a duration of several months."}, {"id": "worldkg", "name": "WorldKG", "description": "The WorldKG knowledge graph is a comprehensive large-scale geospatial knowledge graph based on OpenStreetMap that provides a semantic representation of geographic entities from over 188 countries. WorldKG contains a higher number of representations of geographic entities compared to other knowledge graphs and can be used as an underlying data source for various applications such as geospatial question answering, geospatial data retrieval, and other cross-domain semantic data-driven applications."}, {"id": "leaftop", "name": "LEAFTOP", "description": "Nouns extracted automatically from Bible translations across 1580 languages."}, {"id": "advglue-adversarial-glue", "name": "AdvGLUE (Adversarial GLUE)", "description": "Adversarial GLUE (AdvGLUE) is a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations."}, {"id": "tutorialbank", "name": "TutorialBank", "description": "TutorialBank is a publicly available dataset which aims to facilitate NLP education and research. The dataset consists of links to over 6,300 high-quality resources on NLP and related fields. The corpus\u2019s magnitude, manual collection and focus on annotation for education in addition to research differentiates it from other corpora."}, {"id": "aist", "name": "AIST++", "description": "AIST++ is a 3D dance dataset which contains 3D motion reconstructed from real dancers paired with music. The AIST++ Dance Motion Dataset is constructed from the AIST Dance Video DB. With multi-view videos, an elaborate pipeline is designed to estimate the camera parameters, 3D human keypoints and 3D human dance motion sequences:"}, {"id": "hotels-50k", "name": "Hotels-50K", "description": "The Hotels-50K dataset consists of over 1 million images from 50,000 different hotels around the world. These images come from both travel websites, as well as the TraffickCam mobile application, which allows every day travelers to submit images of their hotel room in order to help combat trafficking. The TraffickCam images are more visually similar to images from trafficking investigations than the images from travel websites."}, {"id": "wit-wikipedia-based-image-text", "name": "WIT (Wikipedia-based Image Text)", "description": "Wikipedia-based Image Text (WIT) Dataset is a large multimodal multilingual dataset. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal machine learning models."}, {"id": "krauts-korpus-of-newspaper-articles-with-underlinded-temporal-expressions", "name": "KRAUTS (Korpus of newspapeR Articles with Underlinded Temporal expressionS)", "description": "KRAUTS (Korpus of newspapeR Articles with Underlinded Temporal expressionS) is a German temporally annotated news corpus accompanied with TimeML annotation guidelines for German. It was developed at Fondazione Bruno Kessler, Trento, Italy and at the Max Planck Institute for Informatics, Saarbr\u00fccken, Germany. Our goal is to boost temporal tagging research for German."}, {"id": "reddit-12k", "name": "REDDIT-12K", "description": "Reddit12k contains 11929 graphs each corresponding to an online discussion thread where nodes represent users, and an edge represents the fact that one of the two users responded to the comment of the other user. There is 1 of 11 graph labels associated with each of these 11929 discussion graphs, representing the category of the community."}, {"id": "vidsitu", "name": "VidSitu", "description": "VidSitu is a dataset for the task of semantic role labeling in videos (VidSRL). It is a large-scale video understanding data source with 29K 10-second movie clips richly annotated with a verb and semantic-roles every 2 seconds. Entities are co-referenced across events within a movie clip and events are connected to each other via event-event relations. Clips in VidSitu are drawn from a large collection of movies (\u223c3K) and have been chosen to be both complex (\u223c4.2 unique verbs within a video) as well as diverse (\u223c200 verbs have more than 100 annotations each)."}, {"id": "atom3d", "name": "ATOM3D", "description": "ATOM3D is a unified collection of datasets concerning the three-dimensional structure of biomolecules, including proteins, small molecules, and nucleic acids. These datasets are specifically designed to provide a benchmark for machine learning methods which operate on 3D molecular structure, and represent a variety of important structural, functional, and engineering tasks. All datasets are provided in a standardized format along with a Python package containing processing code, utilities, models, and dataloaders for common machine learning frameworks such as PyTorch. ATOM3D is designed to be a living database, where datasets are updated and tasks are added as the field progresses."}, {"id": "duolingo-spaced-repetition-data", "name": "Duolingo Spaced Repetition Data", "description": "This is a gzipped CSV file containing the 13 million Duolingo student learning traces used in experiments by Settles & Meeder (2016). For more details and replication source code, visit: https://github.com/duolingo/halflife-regression (2016-06-07)"}, {"id": "scitldr", "name": "SciTLDR", "description": "A new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. "}, {"id": "ccpm-chinese-classical-poetry-matching", "name": "CCPM (Chinese Classical Poetry Matching)", "description": "Introduction"}, {"id": "ndd20-northumberland-dolphin-dataset-2020", "name": "NDD20 (Northumberland Dolphin Dataset 2020)", "description": "Northumberland Dolphin Dataset 2020 (NDD20) is a challenging image dataset annotated for both coarse and fine-grained instance segmentation and categorisation. This dataset, the first release of the NDD, was created in response to the rapid expansion of computer vision into conservation research and the production of field-deployable systems suited to extreme environmental conditions -- an area with few open source datasets. NDD20 contains a large collection of above and below water images of two different dolphin species for traditional coarse and fine-grained segmentation."}, {"id": "qamr-question-answer-meaning-representation-dataset", "name": "QAMR (Question-Answer Meaning Representation Dataset)", "description": "Question-Answer Meaning Representation (QAMR) represents a predicate-argument structure of a sentence with a set of question-answer pairs, so that annotations can be easily provided by non-experts. QAMR is a dataset of over 5,000 sentences and 100,000 questions created by crowdsourcing workers."}, {"id": "tgif-qa", "name": "TGIF-QA", "description": "The TGIF-QA dataset contains 165K QA pairs for the animated GIFs from the TGIF dataset [Li et al. CVPR 2016]. The question & answer pairs are collected via crowdsourcing with a carefully designed user interface to ensure quality. The dataset can be used to evaluate video-based Visual Question Answering techniques."}, {"id": "minerl-basalt", "name": "MineRL BASALT", "description": "MineRL BASALT is an RL competition on solving human-judged tasks. The tasks in this competition do not have a pre-defined reward function: the goal is to produce trajectories that are judged by real humans to be effective at solving a given task."}, {"id": "kannada-mnist", "name": "Kannada-MNIST", "description": "The Kannada-MNIST dataset is a drop-in substitute for the standard MNIST dataset for the Kannada language."}, {"id": "listops", "name": "ListOps", "description": "The ListOps examples are comprised of summary operations on lists of single digit integers, written in prefix notation. The full sequence has a corresponding solution which is also a single-digit integer, thus making it a ten-way balanced classification problem. For example, [MAX 2 9 [MIN 4 7 ] 0 ] has the solution 9. Each operation has a corresponding closing square bracket that defines the list of numbers for the operation. In this example, MIN operates on {4, 7}, while MAX operates on {2, 9, 4, 0}. "}, {"id": "zs-f-vqa", "name": "ZS-F-VQA", "description": "The ZS-F-VQA dataset  is a new split of the F-VQA dataset for zero-shot problem. Firstly we obtain the original train/test split of F-VQA dataset and combine them together to filter out the triples whose answers appear in top-500 according to its occurrence frequency. Next, we randomly divide this set of answers into new training split (a.k.a. seen) $\\mathcal{A}_s$ and testing split (a.k.a. unseen) $\\mathcal{A}_u$ at the ratio of 1:1.  With reference to F-VQA standard dataset, the division process is repeated 5 times.  For each $(i,q,a)$ triplet in original F-VQA dataset, it is divided into training set if $a \\in \\mathcal{A}_s$. Else it is divided into testing set. The overlap of answer instance between training and testing set in F-VQA are $2565$ compared to $0$ in ZS-F-VQA."}, {"id": "mogaze", "name": "MoGaze", "description": "MoGaze is a dataset of full-body motion for everyday manipulation tasks, which includes 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze. The motion data was captured using a traditional motion capture system based on reflective markers. The eye-gaze was captured using a wearable pupil-tracking device."}, {"id": "sberquad-sberbank-question-answering-dataset", "name": "SberQuAD (Sberbank Question Answering Dataset)", "description": "A large scale analogue of Stanford SQuAD in the Russian language - is a valuable resource that has not been properly presented to the scientific community. "}, {"id": "scde", "name": "SCDE", "description": "SCDE is a human-created sentence cloze dataset, collected from public school English examinations in China. The task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers."}, {"id": "stepgame", "name": "StepGame", "description": "A Benchmark for Robust Multi-Hop Spatial Reasoning in Texts"}, {"id": "lambada", "name": "LAMBADA", "description": "The LAMBADA (LAnguage Modeling Broadened to Account for Discourse Aspects) benchmark is an open-ended cloze task which consists of about 10,000 passages from BooksCorpus where a missing target word is predicted in the last sentence of each passage. The missing word is constrained to always be the last word of the last sentence and there are no candidate words to choose from. Examples were filtered by humans to ensure they were possible to guess given the context, i.e., the sentences in the passage leading up to the last sentence. Examples were further filtered to ensure that missing words could not be guessed without the context, ensuring that models attempting the dataset would need to reason over the entire paragraph to answer questions."}, {"id": "affectnet", "name": "AffectNet", "description": "AffectNet is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal."}, {"id": "bsds500-berkeley-segmentation-dataset-500", "name": "BSDS500 (Berkeley Segmentation Dataset 500)", "description": "Berkeley Segmentation Data Set 500 (BSDS500) is a standard benchmark for contour detection. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users. The dataset is divided into three parts: 200 for training, 100 for validation and the rest 200 for test."}, {"id": "products-10k", "name": "Products-10K", "description": "Contains 10,000 fine-grained SKU-level products frequently bought by online customers in JD.com."}, {"id": "wikitablet", "name": "WikiTableT", "description": "WikiTableT contains Wikipedia article sections and their corresponding tabular data and various metadata. WikiTableT contains millions of instances while covering a broad range of topics and a variety of kinds of generation tasks."}, {"id": "intel-image-classification", "name": "Intel Image Classification", "description": "Context This is image data of Natural Scenes around the world."}, {"id": "ndpsid-wacv-2019-notre-dame-photometric-stereo-iris-dataset", "name": "NDPSID - WACV 2019 (Notre Dame Photometric Stereo Iris Dataset)", "description": "This database offers iris images (with and without contact lenses) of the same eyes captured shortly one after another with illumination coming from two different locations. 5,796 iris images in total were acquired by the LG IrisAccess 4000 sensor from 119 subjects. This set is divided into four subsets used in the experiments: (a) 1,800 images of irises wearing regular (with dot-like pattern) textured contact lenses, as shown in Fig. 6a in the wAcv 2019 paper; (b) 864 images of irises wearing irregular (without dot-like pattern) textured contact lenses, as shown in Fig. 6b in the WACV 2019 paper; (c) 1,728 images of irises wearing clear contact lenses (without any visible pattern), and (d) 1,404 images of authentic irises without any contact."}, {"id": "mpqa-opinion-corpus-multi-perspective-question-answering", "name": "MPQA Opinion Corpus (Multi-Perspective Question Answering)", "description": "The MPQA Opinion Corpus contains 535 news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.)."}, {"id": "pointpattern", "name": "PointPattern", "description": "PointPattern is a graph classification dataset constructed by simple point patterns from statistical mechanics. The authors simulated three point patterns in 2D: hard disks in equilibrium (HD), Poisson point process, and random sequential adsorption (RSA) of disks. The HD and Poisson distributions can be seen as simple models that describe the microstructures of liquids and gases while the RSA is a nonequilibrium stochastic process that introduces new particles one by one subject to nonoverlapping conditions. "}, {"id": "elas", "name": "ELAS", "description": "ELAS is a dataset for lane detection. It contains more than 20 different scenes (in more than 15,000 frames) and considers a variety of scenarios (urban road, highways, traffic, shadows, etc.). The dataset was manually annotated for several events that are of interest for the research community (i.e., lane estimation, change, and centering; road markings; intersections; LMTs; crosswalks and adjacent lanes)."}, {"id": "warblr", "name": "Warblr", "description": "Warblr is a dataset for the acoustic detection of birds. The dataset comes from a UK bird-sound crowdsourcing research spinout called Warblr. From this initiative the authors collected over 10,000 ten-second smartphone audio recordings from around the UK. The audio totals around 28 hours duration."}, {"id": "email-eu", "name": "Email-EU", "description": "EmailEU is a directed temporal network constructed from email exchanges in a large European research institution for a 803-day period. It contains 986 email addresses as nodes and 332,334 emails as edges with timestamps. There are 42 ground truth departments in the dataset."}, {"id": "iirc-incomplete-information-reading-comprehension", "name": "IIRC (Incomplete Information Reading Comprehension)", "description": "Contains more than 13K questions over paragraphs from English Wikipedia that provide only partial information to answer them, with the missing information occurring in one or more linked documents. The questions were written by crowd workers who did not have access to any of the linked documents, leading to questions that have little lexical overlap with the contexts where the answers appear. "}, {"id": "chalet-cornell-house-agent-learning-environment", "name": "CHALET (Cornell House Agent Learning Environment)", "description": "CHALET is a 3D house simulator with support for navigation and manipulation. Unlike existing systems, CHALET supports both a wide range of object manipulation, as well as supporting complex environemnt layouts consisting of multiple rooms. The range of object manipulations includes the ability to pick up and place objects, toggle the state of objects like taps or televesions, open or close containers, and insert or remove objects from these containers. In addition, the simulator comes with 58 rooms that can be combined to create houses, including 10 default house layouts. CHALET is therefore suitable for setting up challenging environments for various AI tasks that require complex language understanding and planning, such as navigation, manipulation, instruction following, and interactive question answering."}, {"id": "webtext", "name": "WebText", "description": "WebText is an internal OpenAI corpus created by scraping web pages with emphasis on document quality. The authors scraped all outbound links from Reddit which received at least 3 karma. The authors used the approach as a heuristic indicator for whether other users found the link interesting, educational, or just funny."}, {"id": "sof-specs-on-faces", "name": "SoF (Specs on Faces)", "description": "The Specs on Faces (SoF) dataset, a collection of 42,592 (2,662\u00d716) images for 112 persons (66 males and 46 females) who wear glasses under different illumination conditions. The dataset is FREE for reasonable academic fair use. The dataset presents a new challenge regarding face detection and recognition. It is focused on two challenges: harsh illumination environments and face occlusions, which highly affect face detection, recognition, and classification. The glasses are the common natural occlusion in all images of the dataset. However, there are two more synthetic occlusions (nose and mouth) added to each image. Moreover, three image filters, that may evade face detectors and facial recognition systems, were applied to each image. All generated images are categorized into three levels of difficulty (easy, medium, and hard). That enlarges the number of images to be 42,592 images (26,112 male images and 16,480 female images). There is metadata for each image that contains many information such as: the subject ID, facial landmarks, face and glasses rectangles, gender and age labels, year that the photo was taken, facial emotion, glasses type, and more."}, {"id": "alibaba-cluster-trace", "name": "Alibaba Cluster Trace", "description": "Alibaba Cluster Trace captures detailed statistics for the co-located workloads of long-running and batch jobs over a course of 24 hours. The trace consists of three parts: (1) statistics of the studied homogeneous cluster of 1,313 machines, including each machine\u2019s hardware configuration, and the runtime {CPU, Memory, Disk} resource usage for a duration of 12 hours (the 2nd half of the 24-hour period); (2) long-running job workloads, including a trace of all container deployment requests and actions, and a resource usage trace for 12 hours; (3) co-located batch job workloads, including a trace of all batch job requests and actions, and a trace of per-instance resource usage over 24 hours."}, {"id": "hacs-human-action-clips-and-segments", "name": "HACS (Human Action Clips and Segments)", "description": "HACS is a dataset for human action recognition. It uses a taxonomy of 200 action classes, which is identical to that of the ActivityNet-v1.3 dataset. It has 504K videos retrieved from YouTube. Each one is strictly shorter than 4 minutes, and the average length is 2.6 minutes. A total of 1.5M clips of 2-second duration are sparsely sampled by methods based on both uniform randomness and consensus/disagreement of image classifiers. 0.6M and 0.9M clips are annotated as positive and negative samples, respectively."}, {"id": "breakfast-the-breakfast-actions-dataset", "name": "Breakfast (The Breakfast Actions Dataset)", "description": "The Breakfast Actions Dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens. The dataset is one of the largest fully annotated datasets available. The actions are recorded \u201cin the wild\u201d as opposed to a single controlled lab environment. It consists of over 77 hours of video recordings."}, {"id": "apy-attribute-pascal-and-yahoo", "name": "aPY (Attribute Pascal and Yahoo)", "description": "aPY is a coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories (aeroplane, \u2026, zebra)."}, {"id": "virtual-kitti-2", "name": "Virtual KITTI 2", "description": "Virtual KITTI 2 is an updated version of the well-known Virtual KITTI dataset which consists of 5 sequence clones from the KITTI tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15\u25e6). For each sequence we provide multiple sets of images containing RGB, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset\u2019s capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds."}, {"id": "dcase-2016", "name": "DCASE 2016", "description": "DCASE 2016 is a dataset for sound event detection. It consists of 20 short mono sound files for each of 11 sound classes (from office environments, like clearthroat, drawer, or keyboard), each file containing one sound event instance. Sound files are annotated with event on- and offset times, however silences between actual physical sounds (like with a phone ringing) are not marked and hence \u201cincluded\u201d in the event."}, {"id": "audiocaps", "name": "AudioCaps", "description": "AudioCaps is a dataset of sounds with event descriptions that was introduced for the task of audio captioning, with sounds sourced from the AudioSet dataset. Annotators were provided the audio tracks together with category hints (and with additional video hints if needed)."}, {"id": "ddrel", "name": "DDRel", "description": "DDRel is a dataset for interpersonal relation classification in dyadic dialogues. It consists of 6,300 dyadic dialogue sessions between 694 pairs of speakers with 53,126 utterances in total. It is constructed by crawling movie scripts from IMSDb and annotating the relation labels for each session according to 13 pre-defines relationships."}, {"id": "bb-norm-habitat-bacteria-biotope-entity-normalization-bacterial-habitat", "name": "BB-norm-habitat (Bacteria Biotope -  entity normalization - bacterial habitat)", "description": "In the BB-norm modality of this task, participant systems had to normalize textual entity mentions according to the OntoBiotope ontology for habitats. See BB-dataset for more information."}, {"id": "gap-gap-benchmark-suite", "name": "GAP (GAP Benchmark Suite)", "description": "GAP is a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer differences between graph processing evaluations will make it easier to compare different research efforts and quantify improvements. The benchmark not only specifies graph kernels, input graphs, and evaluation methodologies, but it also provides optimized baseline implementations. These baseline implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement. The input graphs are sized appropriately for shared memory platforms, but any implementation on any platform that conforms to the benchmark's specifications could be compared. This benchmark suite can be used in a variety of settings. Graph framework developers can demonstrate the generality of their programming model by implementing all of the benchmark's kernels and delivering competitive performance on all of the benchmark's graphs. Algorithm designers can use the input graphs and the baseline implementations to demonstrate their contribution. Platform designers and performance analysts can use the suite as a workload representative of graph processing."}, {"id": "cross-dataset-testbed", "name": "Cross-Dataset Testbed", "description": "The Cross-dataset Testbed is a Decaf7 based cross-dataset image classification dataset, which contains 40 categories of images from 3 domains: 3,847 images in Caltech256, 4,000 images in ImageNet, and 2,626 images for SUN. In total there are 10,473 images of 40 categories from these three domains."}, {"id": "fddb-face-detection-dataset-and-benchmark", "name": "FDDB (Face Detection Dataset and Benchmark)", "description": "The Face Detection Dataset and Benchmark (FDDB) dataset is a collection of labeled faces from Faces in the Wild dataset. It contains a total of 5171 face annotations, where images are also of various resolution, e.g. 363x450 and 229x410. The dataset incorporates a range of challenges, including difficult pose angles, out-of-focus faces and low resolution. Both greyscale and color images are included."}, {"id": "udc-ubuntu-dialogue-corpus", "name": "UDC (Ubuntu Dialogue Corpus)", "description": "Ubuntu Dialogue Corpus (UDC) is a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. "}, {"id": "ntire-2021-hdr", "name": "NTIRE 2021 HDR", "description": "The NTIRE 2021 HDR was built for the first challenge on high-dynamic range (HDR) imaging that was part of the New Trends in Image Restoration and Enhancement (NTIRE) workshop, held in conjunction with CVPR 2021. The challenge aims at estimating a HDR image from one or multiple respective low-dynamic range (LDR) observations, which might suffer from under- or over-exposed regions and different sources of noise. The challenge is composed by two tracks: In Track 1 only a single LDR image is provided as input, whereas in Track 2 three differently-exposed LDR images with inter-frame motion are available. In both tracks, the ultimate goal is to achieve the best objective HDR reconstruction in terms of PSNR with respect to a ground-truth image, evaluated both directly and with a canonical tone mapping operation."}, {"id": "tweetqa", "name": "TweetQA", "description": "With social media becoming increasingly popular on which lots of news and real-time events are reported, developing automated question answering systems is critical to the effectiveness of many applications that rely on real-time knowledge. While previous question answering (QA) datasets have concentrated on formal text like news and Wikipedia, the first large-scale dataset for QA over social media data is presented. To make sure the tweets are meaningful and contain interesting information, tweets used by journalists to write news articles are gathered. Then human annotators are asked to write questions and answers upon these tweets. Unlike other QA datasets like SQuAD in which the answers are extractive, the answer are allowed to be abstractive. The task requires model to read a short tweet and a question and outputs a text phrase (does not need to be in the tweet) as the answer."}, {"id": "darpa-optc-darpa-operationally-transparent-cyber-optc-dataset", "name": "Darpa OpTC (Darpa Operationally Transparent Cyber (OpTC) Dataset)", "description": "Operationally Transparent Cyber (OpTC) was a technology transition pilot study funded under Boston Fusion Corp.'s Cyber APT Scenarios for Enterprise Systems (CASES) project. Its primary objective was to determine if DARPA Transparent Computing (TC) program technologies could scale without loss of detection performance to address cyber defense capability gaps identified in USTRANSCOM's Joint Deployment Distribution Enterprise (JDDE) solicitation for the government fiscal years 2019-2023. Boston Fusion along with two performers from the TC program (Five Directions providing endpoint telemetry (TA1) and BAE providing analysis over the data (TA2)) worked to scale their systems from two machines to one thousand machines. The OpTC team conducted scaling and detection tests in the fall of 2019. A third performer (Provatek), not originally associated with the TC program, acted as a red team and test coordinator. This data set represents a subset of that activity."}, {"id": "sprites-2d-video-game-character-sprites", "name": "Sprites (2D Video Game Character Sprites)", "description": "The Sprites dataset contains 60 pixel color images of animated characters (sprites). There are 672 sprites, 500 for training, 100 for testing and 72 for validation. Each sprite has 20 animations and 178 images, so the full dataset has 120K images in total. There are many changes in the appearance of the sprites, they differ in their body shape, gender, hair, armor, arm type, greaves, and weapon."}, {"id": "pronostia-bearing-dataset", "name": "PRONOSTIA Bearing Dataset", "description": "The PRONOSTIA (also called FEMTO) bearing dataset consists of 17 accelerated run-to-failures on a small bearing test rig. Both acceleration and temperature data was collected for each experiment."}, {"id": "restaurant-acos", "name": "Restaurant-ACOS", "description": "The Restaurant-ACOS dataset is constructed based on the SemEval 2016 Restaurant dataset (Pontiki et al., 2016) and its expansion datasets (Fan et al., 2019; Xu et al., 2020). The SemEval 2016 Restaurant dataset (Pontiki et al., 2016) was annotated with explicit and implicit aspects, categories, and sentiment. (Fan et al., 2019; Xu et al., 2020) further added the opinion annotations. We integrate their annotations to construct aspect-category-opinion-sentiment quadruples and further annotate the implicit opinions. The Restaurant-ACOS dataset contains 2286 sentences with 3658 quadruples. It is worth noting that the Restaurant-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc."}, {"id": "20-newsgroups", "name": "20 Newsgroups", "description": "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups."}, {"id": "geometry3k", "name": "Geometry3K", "description": "A new large-scale geometry problem-solving dataset - 3,002 multi-choice geometry problems  - dense annotations in formal language for the diagrams and text - 27,213 annotated diagram logic forms (literals) - 6,293 annotated text logic forms (literals)"}, {"id": "ggponc-german-guideline-program-in-oncology-nlp-corpus", "name": "GGPONC (German Guideline Program in Oncology NLP Corpus)", "description": "German Guideline Program in Oncology NLP Corpus (GGPONC) is a German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions."}, {"id": "quasimodo", "name": "Quasimodo", "description": "Quasimodo is commonsense knowledge base that focuses on salient properties of objects. We provide several subsets:"}, {"id": "facebook-page-page", "name": "Facebook Page-Page", "description": "This webgraph is a page-page graph of verified Facebook sites. Nodes represent official Facebook pages while the links are mutual likes between sites. Node features are extracted from the site descriptions that the page owners created to summarize the purpose of the site. This graph was collected through the Facebook Graph API in November 2017 and restricted to pages from 4 categories which are defined by Facebook. These categories are: politicians, governmental organizations, television shows and companies. The task related to this dataset is multi-class node classification for the 4 site categories."}, {"id": "scenicornot", "name": "ScenicOrNot", "description": "ScenicOrNot (SoN) is a dataset of 185,548 images with associated natural beauty rating histograms. Each image in the dataset was rated at least five times. The images also have metadata like title and location."}, {"id": "robotic-pushing", "name": "Robotic Pushing", "description": "The Robotic Pushing Dataset  is a dataset for video prediction for real-world interactive agents which consists of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action."}, {"id": "tutorialvqa", "name": "TutorialVQA", "description": "TutorialVQA is a new type of dataset used to find answer spans in tutorial videos. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software."}, {"id": "2d-hela", "name": "2D Hela", "description": "2D HeLa is a dataset of fluorescence microscopy images of HeLa cells stained with various organelle-specific fluorescent dyes. The images include 10 organelles, which are DNA (Nuclei), ER (Endoplasmic reticulum), Giantin, (cis/medial Golgi), GPP130 (cis Golgi), Lamp2 (Lysosomes), Mitochondria, Nucleolin (Nucleoli), Actin, TfR (Endosomes), Tubulin. The purpose of the dataset is to train a computer program to automatically identify sub-cellular organelles."}, {"id": "kvasir-seg", "name": "Kvasir-SEG", "description": "Kvasir-SEG is an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. "}, {"id": "a-datacube-for-the-analysis-of-wildfires-in-greece", "name": "A Datacube for the analysis of wildfires in Greece", "description": "This dataset is meant to be used to develop models for next-day fire hazard forecasting in Greece. It contains data from 2009 to 2020 at a 1km x 1km x 1 daily grid."}, {"id": "p-dukemtmc-reid", "name": "P-DukeMTMC-reID", "description": "P-DukeMTMC-reID is a modified version based on DukeMTMC-reID dataset. There are 12,927 images (665 identifies) in training set, 2,163 images (634 identities) for querying and 9,053 images in the gallery set."}, {"id": "chart-to-text", "name": "Chart-to-text", "description": "Chart-to-text is a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types."}, {"id": "aql-22-archive-query-log", "name": "AQL-22 (Archive Query Log)", "description": "The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 166 million search result pages, and 1.7 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. The AQL is the first publicly available query log that combines size, scope, and diversity, enabling research on new retrieval models and search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry."}, {"id": "alexa-domains", "name": "Alexa Domains", "description": "This dataset is composed of the URLs of the top 1 million websites. The domains are ranked using the Alexa traffic ranking which is determined using a combination of the browsing behavior of users on the website, the number of unique visitors, and the number of pageviews. In more detail, unique visitors are the number of unique users who visit a website on a given day, and pageviews are the total number of user URL requests for the website. However, multiple requests for the same website on the same day are counted as a single pageview. The website with the highest combination of unique visitors and pageviews is ranked the highest"}, {"id": "dakshina", "name": "Dakshina", "description": "The Dakshina dataset is a collection of text in both Latin and native scripts for 12 South Asian languages. For each language, the dataset includes a large collection of native script Wikipedia text, a romanization lexicon which consists of words in the native script with attested romanizations, and some full sentence parallel data in both a native script of the language and the basic Latin alphabet."}, {"id": "spades-semantic-parsing-of-declarative-sentences", "name": "Spades (Semantic PArsing of DEclarative Sentences)", "description": "Datasets Spades contains 93,319 questions derived from clueweb09 sentences. Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs."}, {"id": "fobie-focused-open-biological-information-extraction", "name": "FOBIE (Focused Open Biological Information Extraction)", "description": "The Focused Open Biology Information Extraction (FOBIE) dataset aims to support IE from Computer-Aided Biomimetics. The dataset contains ~1,500 sentences from scientific biological texts. These sentences are annotated with TRADE-OFFS and syntactically similar relations between unbounded arguments, as well as argument-modifiers."}, {"id": "muvihand", "name": "MuViHand", "description": "MuViHand is a dataset for 3D Hand Pose Estimation that consists of multi-view videos of the hand along with ground-truth 3D pose labels. The dataset includes more than 402,000 synthetic hand images available in 4,560 videos. The videos have been simultaneously captured from six different angles with complex backgrounds and random levels of dynamic lighting. The data has been captured from 10 distinct animated subjects using 12 cameras in a semi-circle topology."}, {"id": "cicids2017-intrusion-detection-evaluation-dataset-cic-ids2017", "name": "CICIDS2017 (Intrusion Detection Evaluation Dataset (CIC-IDS2017))", "description": "Intrusion Detection Evaluation Dataset (CIC-IDS2017) Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of reliable test and validation datasets, anomaly-based intrusion detection approaches are suffering from consistent and accurate performance evolutions."}, {"id": "mm-covid-multilingual-and-multidimensional-covid-19-fake-news-data-repository", "name": "MM-COVID (Multilingual and Multidimensional COVID-19 Fake News Data Repository)", "description": "MM-COVID is a dataset for fake news detection related to COVID-19. This dataset provides the multilingual fake news and the relevant social context. It contains 3,981 pieces of fake news content and 7,192 trustworthy information from English, Spanish, Portuguese, Hindi, French and Italian, 6 different languages."}, {"id": "goo-gaze-on-objects", "name": "GOO (Gaze on Objects)", "description": "GOO (Gaze-on-Objects) is a dataset for gaze object prediction, where the goal is to predict a bounding box for a person's gazed-at object. GOO is composed of a large set of synthetic images (GOO Synth) supplemented by a smaller subset of real images (GOO-Real) of people looking at objects in a retail environment."}, {"id": "aaac-artificial-argument-analysis-corpus", "name": "AAAC (Artificial Argument Analysis Corpus)", "description": "DeepA2 is a modular framework for deep argument analysis. DeepA2 datasets contain comprehensive logical reconstructions of informally presented arguments in short argumentative texts. This item references two two synthetic DeepA2 datasets for artificial argument analysis: AAAC01 and AAAC02."}, {"id": "lit-pcba-aldh1-aldh1-target-of-lit-pcba-dataset", "name": "LIT-PCBA(ALDH1) (ALDH1 target of LIT-PCBA Dataset)", "description": "Comparative evaluation of virtual screening methods requires a rigorous benchmarking procedure on diverse, realistic, and unbiased data sets. Recent investigations from numerous research groups unambiguously demonstrate that artificially constructed ligand sets classically used by the community (e.g., DUD, DUD-E, MUV) are unfortunately biased by both obvious and hidden chemical biases, therefore overestimating the true accuracy of virtual screening methods. We herewith present a novel data set (LIT-PCBA) specifically designed for virtual screening and machine learning. LIT-PCBA relies on 149 dose\u2013response PubChem bioassays that were additionally processed to remove false positives and assay artifacts and keep active and inactive compounds within similar molecular property ranges. To ascertain that the data set is suited to both ligand-based and structure-based virtual screening, target sets were restricted to single protein targets for which at least one X-ray structure is available in complex with ligands of the same phenotype (e.g., inhibitor, inverse agonist) as that of the PubChem active compounds. Preliminary virtual screening on the 21 remaining target sets with state-of-the-art orthogonal methods (2D fingerprint similarity, 3D shape similarity, molecular docking) enabled us to select 15 target sets for which at least one of the three screening methods is able to enrich the top 1%-ranked compounds in true actives by at least a factor of 2. The corresponding ligand sets (training, validation) were finally unbiased by the recently described asymmetric validation embedding (AVE) procedure to afford the LIT-PCBA data set, consisting of 15 targets and 7844 confirmed active and 407,381 confirmed inactive compounds. The data set mimics experimental screening decks in terms of hit rate (ratio of active to inactive compounds) and potency distribution. It is available online at http://drugdesign.unistra.fr/LIT-PCBA for download and for benchmarking novel virtual screening methods, notably those relying on machine learning."}, {"id": "160-subset-160x160-subset", "name": "160_subset (160x160 subset)", "description": "the 160x160 subset of the GasHisSDB dataset."}, {"id": "astrovision", "name": "AstroVision", "description": "AstroVision is a large-scale dataset comprised of 115,970 densely annotated, real images of 16 different small bodies from both legacy and ongoing deep space missions to facilitate the study of deep learning for autonomous navigation in the vicinity of a small body."}, {"id": "ildc-indian-legal-documents-corpus", "name": "ILDC (Indian Legal Documents Corpus)", "description": "The ILDC dataset (Indian Legal Documents Corpus) is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard explanations by legal experts. The dataset is used for Court Judgment Prediction and Explanation (CJPE). The task requires an automated system to predict an explainable outcome of a case."}, {"id": "causal3dident", "name": "Causal3DIdent", "description": "Update on 3DIdent, where we introduce six additional object classes (Hare, Dragon, Cow, Armadillo, Horse, and Head), and impose a causal graph over the latent variables. For further details, see Appendix B in the associated paper (https://arxiv.org/abs/2106.04619)."}, {"id": "gittables-semtab", "name": "GitTables-SemTab", "description": "The GitTables-SemTab dataset is a subset of the GitTables dataset and was created to be used during the SemTab challenge. The dataset consists of 1101 tables and is used to benchmark the Column Type Annotation (CTA) task. "}, {"id": "mrpc-microsoft-research-paraphrase-corpus", "name": "MRPC (Microsoft Research Paraphrase Corpus)", "description": "Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases)."}, {"id": "comic2k", "name": "Comic2k", "description": "Comic2k is a dataset used for cross-domain object detection which contains 2k comic images with image and instance-level annotations."}, {"id": "cora", "name": "Cora", "description": "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."}, {"id": "rwcp-ssd-onomatopoeia", "name": "RWCP-SSD-Onomatopoeia", "description": "RWCP-SSD-Onomatopoeia is a dataset consisting of 155,568 onomatopoeic words paired with audio samples for environmental sound synthesis."}, {"id": "countix", "name": "Countix", "description": "Countix is a real world dataset of repetition videos collected in the wild (i.e.YouTube) covering a wide range of semantic settings with significant challenges such as camera and object motion, diverse set of periods and counts, and changes in the speed of repeated actions. Countix include repeated videos of workout activities (squats, pull ups, battle rope training, exercising arm), dance moves (pirouetting, pumping fist), playing instruments (playing ukulele), using tools repeatedly (hammer hitting objects, chainsaw cutting wood, slicing onion), artistic performances (hula hooping, juggling soccer ball), sports (playing ping pong and tennis) and many others. Figure 6 illustrates some examples from the dataset as well as the distribution of repetition counts and period lengths."}, {"id": "ochuman", "name": "OCHuman", "description": "This dataset focuses on heavily occluded human with comprehensive annotations including bounding-box, humans pose and instance mask. This dataset contains 13,360 elaborately annotated human instances within 5081 images. With average 0.573 MaxIoU of each person, OCHuman is the most complex and challenging dataset related to human."}, {"id": "barknet-1-0", "name": "BarkNet 1.0", "description": "23,000 cropped images of tree bark, for 23 species of trees around Quebec City, Canada. The images were captured at a distance between 20-60 cm away from the trunk. Labels include: individual tree ID, its species, and its DBH (diameter at breast height). Pictures were taken with four different devices: Nexus 5, Samsung Galaxy S5, Samsung Galaxy S7, and a Panasonic Lumix DMC-TS5 camera. The dataset is sufficiently large to train a Deep network such as ResNet for species recognition."}, {"id": "rodosol-alpr", "name": "RodoSol-ALPR", "description": "This dataset, called RodoSol-ALPR dataset, contains 20,000 images captured by static cameras located at pay tolls owned by the Rodovia do Sol (RodoSol) concessionaire, which operates 67.5 kilometers of a highway (ES-060) in the Brazilian state of Esp\u00edrito Santo."}, {"id": "urban-environments", "name": "Urban Environments", "description": "The Urban Environments dataset is a dataset of 20 land use classes across 300 European cities paired with satellite imagery data."}, {"id": "sizer", "name": "SIZER", "description": "Dataset of clothing size variation which includes  different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. "}, {"id": "age-and-gender-age-and-gender-dataset", "name": "Age and Gender (Age and Gender Dataset)", "description": "EEG signals from 60 users have been recorded whose age range lies between 6 and 55 years. Among all, there were 25 females and 35 male users. In general, all the participants were either school children or belonged to the socioeconomic cross section of the population with no medical history. The EEG recordings were acquired from all 14 electrodes operating at a sampling rate of 128 Hz. During recording, the participants were asked to comfortably sit on the chair with clear thoughts and a relaxed state."}, {"id": "bbc-news-summary", "name": "BBC News Summary", "description": "This dataset was created using a dataset used for data categorization that onsists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005 used in the paper of D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006; whose all rights, including copyright, in the content of the original articles are owned by the BBC. More at http://mlg.ucd.ie/datasets/bbc.html"}, {"id": "modelnet", "name": "ModelNet", "description": "The ModelNet40 dataset contains synthetic object point clouds. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular because of its various categories, clean shapes, well-constructed dataset, etc. The original ModelNet40 consists of 12,311 CAD-generated meshes in 40 categories (such as airplane, car, plant, lamp), of which 9,843 are used for training while the rest 2,468 are reserved for testing. The corresponding point cloud data points are uniformly sampled from the mesh surfaces, and then further preprocessed by moving to the origin and scaling into a unit sphere."}, {"id": "robustbench", "name": "RobustBench", "description": "RobustBench is a benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models."}, {"id": "kenyanfood13", "name": "KenyanFood13", "description": "The Kenyan Food Type Dataset (KenyanFood13) is an image classification dataset for Kenyan food. The images are categorized into 13 different labels."}, {"id": "jit-dataset-jejueo-interview-transcripts", "name": "JIT Dataset (Jejueo Interview Transcripts)", "description": "The Jejueo Interview Transcripts (JIT) dataset is a parallel corpus containing 170k+ Jejueo-Korean sentences."}, {"id": "spot-sentiment-polarity-annotations-dataset", "name": "SPOT (Sentiment Polarity Annotations Dataset)", "description": "The SPOT dataset contains 197 reviews originating from the Yelp'13 and IMDB collections ([1][2]), annotated with segment-level polarity labels (positive/neutral/negative). Annotations have been gathered on 2 levels of granulatiry:"}, {"id": "vgaokao", "name": "VGaokao", "description": "VGaokao is a verification style reading comprehension dataset designed for native speakers' evaluation."}, {"id": "aethel-automatically-extracted-theorems-from-lassy", "name": "aethel (Automatically Extracted Theorems from Lassy)", "description": "A dataset of approximately 75,000  phrases and sentences, syntactically analyzed as typelogical derivations (i.e. proofs of modal intuitionistic linear logic, or programs of the corresponding \u03bb calculus). Analyses were obtained by transforming the dependency graphs of the Lassy-Small corpus."}, {"id": "liddi-linked-drug-drug-interactions", "name": "LIDDI (LInked Drug-Drug Interactions)", "description": "LInked Drug-Drug Interactions (LIDDI) is a public nanopublication-based RDF dataset with trusty URIs that encompasses some of the most cited prediction methods and sources to provide researchers a resource for leveraging the work of others into their prediction methods. As one of the main issues to overcome the usage of external resources is their mappings between drug names and identifiers used, the dataset also provides the set of mappings the authors curated to be able to compare the multiple sources aggregated in the dataset."}, {"id": "storycloze", "name": "StoryCloze", "description": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding."}, {"id": "biascorp", "name": "BiasCorp", "description": "BiasCorp is a dataset for racism detection containing 139,090 comments and news segment from three specific sources - Fox News, BreitbartNews and YouTube."}, {"id": "fleurs-fleurs-few-shot-learning-evaluation-of-universal-representations-of-speech", "name": "Fleurs (FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech)", "description": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding."}, {"id": "cc-19", "name": "CC-19", "description": "CC-19 is a small new dataset related to the latest family of coronavirus i.e. COVID-19. The proposed dataset \u201cCC-19\u201d contains 34,006 CT scan slices (images) belonging to 98 subjects out of which 28,395 CT scan slices belong to positive COVID patients."}, {"id": "panlex", "name": "Panlex", "description": "PanLex translates words in thousands of languages. Its database is panlingual (emphasizes coverage of every language) and lexical (focuses on words, not sentences)."}, {"id": "crowdpose", "name": "CrowdPose", "description": "The CrowdPose dataset contains about 20,000 images and a total of 80,000 human poses with 14 labeled keypoints. The test set includes 8,000 images. The crowded images containing homes are extracted from MSCOCO, MPII and AI Challenger."}, {"id": "musicnet", "name": "MusicNet", "description": "MusicNet is a collection of 330 freely-licensed classical music recordings, together with over 1 million annotated labels indicating the precise time of each note in every recording, the instrument that plays each note, and the note's position in the metrical structure of the composition. The labels are acquired from musical scores aligned to recordings by dynamic time warping. The labels are verified by trained musicians; we estimate a labeling error rate of 4%. We offer the MusicNet labels to the machine learning and music communities as a resource for training models and a common benchmark for comparing results."}, {"id": "mura", "name": "MURA", "description": "A large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. "}, {"id": "klexikon-klexikon-a-german-dataset-for-joint-summarization-and-simplification", "name": "Klexikon (Klexikon: A German Dataset for Joint Summarization and Simplification)", "description": "The dataset introduces document alignments between German Wikipedia and the children's lexicon Klexikon. The source texts in Wikipedia are both written in a more complex language than Klexikon, and also significantly longer, which makes this a suitable application for both summarization and simplification. In fact, previous research has so far only focused on either of the two, but not comprehensively been studied as a joint task."}, {"id": "dips-plus-the-enhanced-database-of-interacting-protein-structures-for-interface-prediction", "name": "DIPS-Plus (The Enhanced Database of Interacting Protein Structures for Interface Prediction)", "description": "How and where proteins interface with one another can ultimately impact the proteins' functions along with a range of other biological processes. As such, precise computational methods for protein interface prediction (PIP) come highly sought after as they could yield significant advances in drug discovery and design as well as protein function analysis. However, the traditional benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a paltry 230 complexes for training, validating, and testing different machine learning algorithms. In this work, we expand on a dataset recently introduced for this task, the Database of Interacting Protein Structures (DIPS), to present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for geometric deep learning of protein interfaces. The previous version of DIPS contains only the Cartesian coordinates and types of the atoms comprising a given protein complex, whereas DIPS-Plus now includes a plethora of new residue-level features including protrusion indices, half-sphere amino acid compositions, and new profile hidden Markov model (HMM)-based sequence features for each amino acid, giving researchers a large, well-curated feature bank for training protein interface prediction methods."}, {"id": "bimanual-actions-dataset", "name": "Bimanual Actions Dataset", "description": "The Bimanual Actions Dataset is a collection of 540 RGB-D videos, showing subjects perform bimanual actions in a kitchen or workshop context. The main purpose for its compilation is to research bimanual human behaviour in order to eventually improve the capabilities of humanoid robots."}, {"id": "midv-500", "name": "MIDV-500", "description": "500 video clips for 50 different identity document types with ground truth."}, {"id": "videomem", "name": "VideoMem", "description": "Composed of 10,000 videos annotated with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes after memorization and again 24-72 hours later. "}, {"id": "deepsport-dataset", "name": "DeepSport Dataset", "description": "This basketball dataset was acquired under the Walloon region project DeepSport, using the Keemotion system installed in multiple arenas. We would like to thanks both Keemotion for letting us use their system for raw image acquisition during live productions, and the LNB for the rights on their images."}, {"id": "sarcasm-corpus-v2", "name": "Sarcasm Corpus V2", "description": "The Sarcasm Corpus contains sarcastic and non-sarcastic utterances of three different types, which are balanced with half of the samples being sarcastic and half non-sarcastic. The three types are:"}, {"id": "alimeeting-multi-channel-multi-party-meeting-transcription-challenge", "name": "AliMeeting (Multi-Channel Multi-Party Meeting Transcription Challenge)", "description": "AliMeeting corpus consists of 120 hours of recorded Mandarin meeting data, including far-field data collected by 8-channel microphone array as well as near-field data collected by headset microphone. Each meeting session is composed of 2-4 speakers with different speaker overlap ratio, recorded in rooms with different size."}, {"id": "cyclone-data-global-cyclone-data-from-1841-to-2021", "name": "Cyclone Data (global cyclone data from 1841 to 2021)", "description": "Archive of Global Tropical Cyclone Tracks Tracks from 1980 to May 2019."}, {"id": "misaw-micro-surgical-anastomose-workflow-recognition-on-training-sessions", "name": "MISAW (MIcro-Surgical Anastomose Workflow recognition on training sessions)", "description": "The MISAW data set is composed of 27 sequences of micro-surgical anastomosis on artificial blood vessels performed by 3 surgeons and 3 engineering students. The dataset contained video, kinematic, and procedural descriptions synchronized at 30Hz. The procedural descriptions contained phases, steps, and activities performed by the participants."}, {"id": "div2krk-div2k-random-kernel", "name": "DIV2KRK (DIV2K Random Kernel)", "description": "Using the validation set (100 images) from the widely used DIV2K dataset, we blurred and subsampled each image with a different, randomly generated kernel. Kernels were 11x11 anisotropic gaussians with random lengths \u03bb1, \u03bb2\u223cU(0.6, 5) independently distributed for each axis, rotated by a random angle \u03b8\u223cU[\u2212\u03c0, \u03c0]."}, {"id": "arct-argument-reasoning-comprehension-task", "name": "ARCT (Argument Reasoning Comprehension Task)", "description": "Freely licensed dataset with warrants for 2k authentic arguments from news comments. On this basis, we present a new challenging task, the argument reasoning comprehension task. Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options. Both warrants are plausible and lexically close, but lead to contradicting claims."}, {"id": "disrpt2019-disrpt2019-shared-task-on-discourse-unit-segmentation-and-connective-detection", "name": "DISRPT2019 (DISRPT2019 shared task on Discourse Unit Segmentation and Connective Detection)", "description": "The DISRPT 2019 workshop introduces the first iteration of a cross-formalism shared task on discourse unit segmentation. Since all major discourse parsing frameworks imply a segmentation of texts into segments, learning segmentations for and from diverse resources is a promising area for converging methods and insights. We provide training, development and test datasets from all available languages and treebanks in the RST, SDRT and PDTB formalisms, using a uniform format. Because different corpora, languages and frameworks use different guidelines for segmentation, the shared task is meant to promote design of flexible methods for dealing with various guidelines, and help to push forward the discussion of standards for discourse units. For datasets which have treebanks, we will evaluate in two different scenarios: with and without gold syntax, or otherwise using provided automatic parses for comparison."}, {"id": "terra-textual-entailment-recognition-for-russian", "name": "TERRa (Textual Entailment Recognition for Russian)", "description": "Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text."}, {"id": "swissdial", "name": "SwissDial", "description": "SwissDial is an annotated parallel corpus of spoken Swiss German across 8 major dialects, plus a Standard German reference. It contains parallel spoken data for 8 different regions: Aargau (AG), Bern (BE), Basel (BS), Graubunden (GR), Luzern (LU), St. Gallen (SG), Wallis (VS) and Zurich (ZH)."}, {"id": "ev-imo", "name": "EV-IMO", "description": "Includes accurate pixel-wise motion masks, egomotion and ground truth depth. "}, {"id": "chid-chinese-idiom-dataset", "name": "ChID (Chinese IDiom dataset)", "description": "ChID is a large-scale Chinese IDiom dataset for cloze test. ChID contains 581K passages and 729K blanks, and covers multiple domains. In ChID, the idioms in a passage were replaced with blank symbols. For each blank, a list of candidate idioms including the golden idiom are provided as choice. "}, {"id": "cosql-conversational-text-to-sql-challenge", "name": "CoSQL (Conversational Text-to-SQL Challenge)", "description": "CoSQL is a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. "}, {"id": "hint3", "name": "HINT3", "description": "HINT3 is a dataset for intent detection. It consists of 3 different datasets each containing a diverse set of intents in a single domain - mattress products retail, fitness supplements retail and online gaming named SOFMattress, Curekart and Powerplay11."}, {"id": "scifact", "name": "SciFact", "description": "SciFact is a dataset of 1.4K expert-written claims, paired with evidence-containing abstracts annotated with veracity labels and rationales."}, {"id": "food2k", "name": "Food2K", "description": "Food2K is a large food recognition dataset with 2,000 categories and over 1 million images. Compared with existing food recognition datasets, Food2K bypasses them in both categories and images by one order of magnitude, and thus establishes a new challenging benchmark to develop advanced models for food visual representation learning.  Food2K can be further explored to benefit more food-relevant tasks including emerging and more complex ones (e.g., nutritional understanding of food), and the trained models on Food2K can be expected as backbones to improve the performance of more food-relevant tasks."}, {"id": "tinyvirat", "name": "TinyVIRAT", "description": "TinyVIRAT contains natural low-resolution activities. The actions in TinyVIRAT videos have multiple labels and they are extracted from surveillance videos which makes them realistic and more challenging."}, {"id": "luna", "name": "LUNA", "description": "The LUNA challenges provide datasets for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified."}, {"id": "femnist-federated-extended-mnist", "name": "FEMNIST (Federated Extended MNIST)", "description": "See paper:"}, {"id": "secbench", "name": "SECBENCH", "description": "Dataset of 676 security vulnerabilities patches. In 2017, we mined the commits messages of 238 projects using regular expressions for each vulnerability (cf. Patterns). In 2020, we classified vulnerabilities using the CWE taxonomy. Some vulnerabilities contain the score and severity information (CVEs)."}, {"id": "gap-coreference-dataset", "name": "GAP Coreference Dataset", "description": "GAP is a gender-balanced dataset containing 8,908 coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia and released by Google AI Language for the evaluation of coreference resolution in practical applications."}, {"id": "xling-xling-bli-dataset", "name": "XLING (XLING BLI Dataset)", "description": "The XLING BLI Dataset contains bilingual dictionaries for 28 language pairs. For each of the language pairs, there are 5 dictionary files: 4 training dictionaries of varying sizes (500, 1K, 3K, and 5K translation pairs) and one testing dictionary containing 2K test word pairs. All results reported in the above paper have been obtained on test dictionaries of respective language pairs."}, {"id": "birdclef-2018", "name": "BirdCLEF 2018", "description": "BirdClef 2018 is a bird soundscape dataset based on the contributions of the Xeno-canto network. The training set contains 36,496 recordings covering 1500 species of central and south America (the largest bioacoustic dataset in the literature). There are about 68 hours of recordings in total, with 1,500 classes and species tags."}, {"id": "recam-semeval-2021-task-4-reading-comprehension-of-abstract-meaning", "name": "ReCAM (SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning)", "description": "Tasks Our shared task has three subtasks. Subtask 1 and 2 focus on evaluating machine learning models' performance with regard to two definitions of abstractness (Spreen and Schulz, 1966; Changizi, 2008), which we call imperceptibility and nonspecificity, respectively. Subtask 3 aims to provide some insights to their relationships."}, {"id": "blvd", "name": "BLVD", "description": "BLVD is a large scale 5D semantics dataset collected by the Visual Cognitive Computing and Intelligent Vehicles Lab. This dataset contains 654 high-resolution video clips owing 120k frames extracted from Changshu, Jiangsu Province, China, where the Intelligent Vehicle Proving Center of China (IVPCC) is located. The frame rate is 10fps/sec for RGB data and 3D point cloud. The dataset contains fully annotated frames which yield 249,129 3D annotations, 4,902 independent individuals for tracking with the length of overall 214,922 points, 6,004 valid fragments for 5D interactive event recognition, and 4,900 individuals for 5D intention prediction. These tasks are contained in four kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime)."}, {"id": "bb-norm-phenotype-bacteria-biotope-entity-normalization-phenotype", "name": "BB-norm-phenotype (Bacteria Biotope - entity normalization - phenotype)", "description": "In the BB-norm modality of this task, participant systems had to normalize textual entity mentions according to the OntoBiotope ontology for phenotypes. See BB-dataset for more information."}, {"id": "sentimix", "name": "SentiMix", "description": "Sentiment analysis of codemixed tweets."}, {"id": "live-livestream", "name": "LIVE Livestream", "description": "LIVE Livestream is a database for Video Quality Assessment (VQA), specifically designed for live streaming VQA research. The dataset is called the Laboratory for Image and Video Engineering (LIVE) Live stream Database. The LIVE Livestream Database includes 315 videos of 45 contents impaired by 6 types of distortions."}, {"id": "roadanomaly21", "name": "RoadAnomaly21", "description": "RoadAnomaly21 is a dataset for anomaly segmentation, the task of identify the image regions containing objects that have never been seen during training. It consists of an evaluation dataset of 100 images with pixel-level annotations. Each image contains at least one anomalous object, e.g. animals or unknown vehicles. The anomalies can appear anywhere in the image and widely differ in size, covering from 0.5% to 40% of the image"}, {"id": "rlu-rl-unplugged", "name": "RLU (RL Unplugged)", "description": "RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, we provide the datasets with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning."}, {"id": "relx", "name": "RELX", "description": "RELX is a benchmark dataset for cross-lingual relation classification in English, French, German, Spanish and Turkish."}, {"id": "cmd-condensed-movies-dataset", "name": "CMD (Condensed Movies Dataset)", "description": "Consists of the key scenes from over 3K movies: each key scene is accompanied by a high level semantic description of the scene, character face-tracks, and metadata about the movie. The dataset is scalable, obtained automatically from YouTube, and is freely available for anybody to download and use. "}, {"id": "kp20k", "name": "KP20k", "description": "KP20k is a large-scale scholarly articles dataset with 528K articles for training, 20K articles for validation and 20K articles for testing."}, {"id": "openfwi", "name": "OpenFWI", "description": "OpenFWI is a collection of large-scale open-source benchmark datasets for seismic full waveform inversion (FWI). OpenFWI is catered for the geoscience and machine learning community to facilitate diversified, rigorous  and reproducible research on machine learning-based FWI."}, {"id": "bsard-belgian-statutory-article-retrieval-dataset", "name": "BSARD (Belgian Statutory Article Retrieval Dataset)", "description": "The Belgian Statutory Article Retrieval Dataset (BSARD) is a French native corpus for studying statutory article retrieval. BSARD consists of more than 22,600 statutory articles from Belgian law and about 1,100 legal questions posed by Belgian citizens and labeled by experienced jurists with relevant articles from the corpus."}, {"id": "ig-3-5b-17k", "name": "IG-3.5B-17k", "description": "IG-3.5B-17k is an internal Facebook AI Research dataset for training image classification models. It consists of hashtags for up to 3.5 billion public Instagram images."}, {"id": "tcr-temporal-and-causal-reasoning-dataset", "name": "TCR (Temporal and Causal Reasoning dataset)", "description": "A dataset of Joint Reasoning for Temporal and Causal Relations"}, {"id": "gum-georgetown-university-multilayer-corpus", "name": "GUM (Georgetown University Multilayer corpus)", "description": "GUM is an open source multilayer English corpus of richly annotated texts from twelve text types. Annotations include:"}, {"id": "clirmatrix", "name": "CLIRMatrix", "description": "CLIRMatrix is a large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval. It includes:"}, {"id": "laion-400m", "name": "LAION-400M", "description": "LAION-400M is a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search."}, {"id": "imdb-face", "name": "IMDb-Face", "description": "IMDb-Face is  large-scale noise-controlled dataset for face recognition research. The dataset contains about 1.7 million faces, 59k identities, which is manually cleaned from 2.0 million raw images. All images are obtained from the IMDb website. "}, {"id": "abstractive-text-summarization-from-fanpage", "name": "Abstractive Text Summarization from Fanpage", "description": "Fanpage dataset, containing news articles taken from Fanpage."}, {"id": "f-celeba-10-tasks-federated-celeba-10-tasks", "name": "F-CelebA (10 tasks) (Federated-CelebA (10 tasks))", "description": "F-CelebA - This dataset is adapted from federated learning. Federated learning is an emerging machine learning paradigm with an emphasis on data privacy. The idea is to train through model aggregation rather than conventional data aggregation and keep local data staying on the local device. This dataset naturally consists of similar tasks and each of the 10 tasks contains images of a celebrity labeled by whether he/she is smiling or not. More detailed please check page https://github.com/ZixuanKe/CAT"}, {"id": "uit-victsd-uit-vietnamese-constructive-and-toxic-speech-detection", "name": "UIT-ViCTSD (UIT Vietnamese Constructive and Toxic Speech Detection)", "description": "UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection) is a dataset for constructive and toxic speech detection in Vietnamese. It consists of 10,000 human-annotated comments."}, {"id": "art-dataset-abductive-reasoning-in-narrative-text", "name": "ART Dataset (Abductive Reasoning in narrative Text)", "description": "ART consists of over 20k commonsense narrative contexts and 200k explanations."}, {"id": "breakhis-breast-cancer-histopathological-database", "name": "BreakHis (Breast Cancer Histopathological Database)", "description": "The Breast Cancer Histopathological Image Classification (BreakHis) is  composed of 9,109 microscopic images of breast tumor tissue collected from 82 patients using different magnifying factors (40X, 100X, 200X, and 400X).  It contains 2,480  benign and 5,429 malignant samples (700X460 pixels, 3-channel RGB, 8-bit depth in each channel, PNG format). This database has been built in collaboration with the P&D Laboratory - Pathological Anatomy and Cytopathology, Parana, Brazil."}, {"id": "clams-cross-linguistic-analysis-of-models-on-syntax", "name": "CLAMS (Cross-linguistic Analysis of Models on Syntax)", "description": "Targeted syntactic evaluation datasets in 5 languages: English, French, German, Russian, and Hebrew. Data are translated from the targeted syntactic evaluation data of Marvin & Linzen (2018): https://aclanthology.org/D18-1151/ . All stimuli focus on subject-verb agreement."}, {"id": "densepose-densepose-coco", "name": "DensePose (DensePose-COCO)", "description": "DensePose-COCO is a large-scale ground-truth dataset with image-to-surface correspondences manually annotated on 50K COCO images and train DensePose-RCNN, to densely regress part-specific UV coordinates within every human region at multiple frames per second."}, {"id": "shiny-dataset", "name": "Shiny dataset", "description": "The shiny folder contains 8 scenes with challenging view-dependent effects used in our paper. We also provide additional scenes in the shiny_extended folder.  The test images for each scene used in our paper consist of one of every eight images in alphabetical order."}, {"id": "acappella", "name": "Acappella", "description": "Acappella comprises around 46 hours of a cappella solo singing videos sourced from YouTbe, sampled across different singers and languages. Four languages are considered: English, Spanish, Hindi and others.  "}, {"id": "hoi4d", "name": "HOI4D", "description": "A large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egOCentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms."}, {"id": "ucf101-ucf101-human-actions-dataset", "name": "UCF101 (UCF101 Human Actions dataset)", "description": "UCF101 dataset is an extension of UCF50 and consists of 13,320 video clips, which are classified into 101 categories. These 101 categories can be classified into 5 types (Body motion, Human-human interactions, Human-object interactions, Playing musical instruments and Sports). The total length of these video clips is over 27 hours. All the videos are collected from YouTube and have a fixed frame rate of 25 FPS with the resolution of 320 \u00d7 240."}, {"id": "easycall-corpus", "name": "EasyCall corpus", "description": "EasyCall corpus is a dysarthric speech command dataset in Italian. The dataset consists of 21386 audio recordings from 24 healthy and 31 dysarthric speakers, whose individual degree of speech impairment was assessed by neurologists through the Therapy Outcome Measure."}, {"id": "didemo-distinct-describable-moments", "name": "DiDeMo (Distinct Describable Moments)", "description": "The Distinct Describable Moments (DiDeMo) dataset is one of the largest and most diverse datasets for the temporal localization of events in videos given natural language descriptions. The videos are collected from Flickr and each video is trimmed to a maximum of 30 seconds. The videos in the dataset are divided into 5-second segments to reduce the complexity of annotation. The dataset is split into training, validation and test sets containing 8,395, 1,065 and 1,004 videos respectively. The dataset contains a total of 26,892 moments and one moment could be associated with descriptions from multiple annotators. The descriptions in DiDeMo dataset are detailed and contain camera movement, temporal transition indicators, and activities. Moreover, the descriptions in DiDeMo are verified so that each description refers to a single moment."}, {"id": "ph2", "name": "PH2", "description": "The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. The PH\u00b2 dataset has been developed for research and benchmarking purposes, in order to facilitate comparative studies on both segmentation and classification algorithms of dermoscopic images. PH\u00b2 is a dermoscopic image database acquired at the Dermatology Service of Hospital Pedro Hispano, Matosinhos, Portugal."}, {"id": "bbbc039", "name": "BBBC039", "description": "This image set is part of a high-throughput chemical screen on U2OS cells, with examples of 200 bioactive compounds. The effect of the treatments was originally imaged using the Cell Painting assay (fluorescence microscopy). This data set only includes the DNA channel of a single field of view per compound. These images present a variety of nuclear phenotypes, representative of high-throughput chemical perturbations. The main use of this data set is the study of segmentation algorithms that can separate individual nucleus instances in an accurate way, regardless of their shape and cell density. The collection has around 23,000 single nuclei manually annotated to establish a ground truth collection for segmentation evaluation."}, {"id": "kvasir-instrument", "name": "Kvasir-Instrument", "description": "Consists of  annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists."}, {"id": "met", "name": "Met", "description": "The Met dataset is a large-scale dataset for Instance-Level Recognition (ILR) in the artwork domain. It relies on the open access collection from the Metropolitan Museum of Art (The Met) in New York to form the training set, which consists of about 400k images from more than 224k classes, with artworks of world-level geographic coverage and chronological periods dating back to the Paleolithic period. Each museum exhibit corresponds to a unique artwork, and defines its own class. The training set exhibits a long-tail distribution with more than half of the classes represented by a single image, making it a special case of few-shot learning."}, {"id": "davis-s", "name": "DAVIS-S", "description": "To enrich the diversity, we also collect 92 images which are suitable for saliency detection from DAVIS [27], a densely annotated high-resolution video segmentation dataset. Im- ages in this dataset are precisely annotated and have very high resolutions (i.e.,1920!1080). We ignore the categories of the objects and generate saliency ground truth masks for this dataset. For convenience, the collected dataset is denot- ed as DAVIS-S."}, {"id": "arxiv-astro-ph-arxiv-astro-physics", "name": "arXiv Astro-Ph (arXiv Astro Physics)", "description": "Arxiv ASTRO-PH (Astro Physics) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to Astro Physics category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes."}, {"id": "summit", "name": "SUMMIT", "description": "SUMMIT is a high-fidelity simulator that facilitates the development and testing of crowd-driving algorithms. By leveraging the open-source OpenStreetMap map database and a heterogeneous multi-agent motion prediction model developed in our earlier work, SUMMIT simulates dense, unregulated urban traffic for heterogeneous agents at any worldwide locations that OpenStreetMap supports. SUMMIT is built as an extension of CARLA and inherits from it the physical and visual realism for autonomous driving simulation. SUMMIT supports a wide range of applications, including perception, vehicle control, planning, and end-to-end learning."}, {"id": "genius", "name": "genius", "description": "node classification on genius"}, {"id": "vt5000", "name": "VT5000", "description": "Includes 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms."}, {"id": "book-cover-dataset", "name": "Book Cover Dataset", "description": "A new challenging dataset that can be used for many pattern recognition tasks."}, {"id": "cute80-curve-text", "name": "CUTE80 (Curve Text)", "description": "CUTE80 is necessary in order to show the capability of the current text detection method in handling curved texts."}, {"id": "doqa", "name": "DoQA", "description": "A dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing."}, {"id": "icdar-2003", "name": "ICDAR 2003", "description": "The ICDAR2003 dataset is a dataset for scene text recognition. It contains 507 natural scene images (including 258 training images and 249 test images) in total. The images are annotated at character level. Characters and words can be cropped from the images."}, {"id": "apnd-arm-point-nav-dataset", "name": "APND (Arm Point Nav Dataset)", "description": "APND (Arm Point Nav Dataset) is a dataset for the generalizable object manipulation task called ARMPOINTNAV, which consists on moving an object in the scene from a source location to a target location."}, {"id": "calvin-composing-actions-from-language-and-vision", "name": "CALVIN (Composing Actions from Language and Vision)", "description": "CALVIN (Composing Actions from Language and Vision), is an open-source simulated benchmark to learn long-horizon language-conditioned robot manipulation tasks."}, {"id": "artemis", "name": "ArtEmis", "description": "ArtEmis is a large-scale dataset aimed at providing a detailed understanding of the interplay between visual content, its emotional effect, and explanations for the latter in language. In contrast to most existing annotation datasets in computer vision, this dataset focuses on the affective experience triggered by visual artworks an the annotators were asked to indicate the dominant emotion they feel for a given image and, crucially, to also provide a grounded verbal explanation for their emotion choice. This leads to a rich set of signals for both the objective content and the affective impact of an image, creating associations with abstract concepts (e.g., \u201cfreedom\u201d or \u201clove\u201d), or references that go beyond what is directly visible, including visual similes and metaphors, or subjective references to personal experiences. "}, {"id": "amfds-arabic-multi-fonts-dataset", "name": "AMFDS (Arabic  Multi-Fonts  Dataset)", "description": "A multi-word multi-font Arabic word-image dataset. "}, {"id": "onestopenglish", "name": "OneStopEnglish", "description": "Useful for through two applications - automatic readability assessment and automatic text simplification. The corpus consists of 189 texts, each in three versions (567 in total)."}, {"id": "pst900", "name": "PST900", "description": "PST900 is a dataset of 894 synchronized and calibrated RGB and Thermal image pairs with per pixel human annotations across four distinct classes from the DARPA Subterranean Challenge."}, {"id": "episurg-episurg-a-dataset-of-postoperative-mri-for-quantitative-analysis-of-resection-neurosurgery-for-refractory-epilepsy", "name": "EPISURG (EPISURG: a dataset of postoperative MRI for quantitative analysis of resection neurosurgery for refractory epilepsy)", "description": "EPISURG is a clinical dataset of $T_1$-weighted magnetic resonance images (MRI) from 430 epileptic patients who underwent resective brain surgery at the National Hospital of Neurology and Neurosurgery (Queen Square, London, United Kingdom) between 1990 and 2018."}, {"id": "97-synthetic-datasets", "name": "97 synthetic datasets", "description": "97 synthetic datasets consists of 97 datasets (as illustrated in the figure) and can be used to test graph-based clustering algorithms. "}, {"id": "the-pile", "name": "The Pile", "description": "The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together."}, {"id": "movienet", "name": "MovieNet", "description": "MovieNet is a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92 K tags of cinematic style."}, {"id": "koniq-10k-konstanz-image-quality-10k-database", "name": "KonIQ-10k (Konstanz Image Quality 10k Database)", "description": "KonIQ-10k is a large-scale IQA dataset consisting of 10,073 quality scored images. This is the first in-the-wild database aiming for ecological validity, with regard to the authenticity of distortions, the diversity of content, and quality-related indicators. Through the use of crowdsourcing, we obtained 1.2 million reliable quality ratings from 1,459 crowd workers, paving the way for more general IQA models."}, {"id": "agenda-abstract-generation-dataset", "name": "AGENDA (Abstract GENeration DAtaset)", "description": "Abstract GENeration DAtaset (AGENDA) is a dataset of knowledge graphs paired with scientific abstracts. The dataset consists of 40k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences."}, {"id": "x-wikire", "name": "X-WikiRE", "description": "X-WikiRE is a new, large-scale multilingual relation extraction dataset in which relation extraction is framed as a problem of reading comprehension to allow for generalization to unseen relations. "}, {"id": "banglalekhaimagecaptions", "name": "BanglaLekhaImageCaptions", "description": "This dataset consists of images and annotations in Bengali. The images are human annotated in Bengali by two adult native Bengali speakers. All popular image captioning datasets have a predominant western cultural bias with the annotations done in English. Using such datasets to train an image captioning system assumes that a good English to target language translation system exists and that the original dataset had elements of the target culture. Both these assumptions are false, leading to the need of a culturally relevant dataset in Bengali, to generate appropriate image captions of images relevant to the Bangladeshi and wider subcontinental context. The dataset presented consists of 9,154 images."}, {"id": "mef-multi-exposure-image-fusion", "name": "MEF (Multi-exposure image fusion)", "description": "Multi-exposure image fusion (MEF) is considered an effective quality enhancement technique widely adopted in consumer electronics, but little work has been dedicated to the perceptual quality assessment of multi-exposure fused images. In this paper, we first build an MEF database and carry out a subjective user study to evaluate the quality of images generated by different MEF algorithms. There are several useful findings. First, considerable agreement has been observed among human subjects on the quality of MEF images. Second, no single state-of-the-art MEF algorithm produces the best quality for all test images. Third, the existing objective quality models for general image fusion are very limited in predicting perceived quality of MEF images. Motivated by the lack of appropriate objective models, we propose a novel objective image quality assessment (IQA) algorithm for MEF images based on the principle of the structural similarity approach and a novel measure of patch structural consistency. Our experimental results on the subjective database show that the proposed model well correlates with subjective judgments and significantly outperforms the existing IQA models for general image fusion. Finally, we demonstrate the potential application of the proposed model by automatically tuning the parameters of MEF algorithms"}, {"id": "soda10m", "name": "SODA10M", "description": "SODA10M is a large-scale object detection benchmark for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data. SODA10M contains 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected every ten seconds per frame within 32 different cities under different weather conditions, periods and location scenes."}, {"id": "librispeech", "name": "LibriSpeech", "description": "The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the \u2019clean\u2019 and \u2019other\u2019 categories, respectively, depending upon how well or challenging Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words."}, {"id": "xnli-cross-lingual-natural-language-inference", "name": "XNLI (Cross-lingual Natural Language Inference)", "description": "The Cross-lingual Natural Language Inference (XNLI) corpus is the extension of the Multi-Genre NLI (MultiNLI) corpus to 15 languages. The dataset was created by manually translating the validation and test sets of MultiNLI into each of those 15 languages. The English training set was machine translated for all languages. The dataset is composed of 122k train, 2490 validation and 5010 test examples."}, {"id": "danewsroom-danewsroom-a-large-scale-danish-summarisation-dataset", "name": "DaNewsroom (DaNewsroom: A Large-scale Danish Summarisation Dataset)", "description": "The first large-scale non-English language dataset specifically curated for automatic summarisation. The document-summary pairs are news articles and manually written summaries in the Danish language."}, {"id": "regdb-dongguk-body-based-person-recognition-database-dbperson-recog-db1", "name": "RegDB (Dongguk Body-based Person Recognition Database (DBPerson-Recog-DB1))", "description": "RegDB is used for Visible-Infrared Re-ID which handles the cross-modality matching between the daytime visible and night-time infrared images. The dataset contains images of 412 people. It includes 10 color and 10 thermal images for each person."}, {"id": "2018-n2c2-track-2-adverse-drug-events-and-medication-extraction-2018-n2c2-shared-task-on-adverse-drug-events-and-medication-extraction-in-electronic-health-records", "name": "2018 n2c2 (Track 2) - Adverse Drug Events and Medication Extraction (2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records)", "description": "Abstract Objective This article summarizes the preparation, organization, evaluation, and results of Track 2 of the 2018 National NLP Clinical Challenges shared task. Track 2 focused on extraction of adverse drug events (ADEs) from clinical records and evaluated 3 tasks: concept extraction, relation classification, and end-to-end systems. We perform an analysis of the results to identify the state of the art in these tasks, learn from it, and build on it."}, {"id": "panoptic-cmu-panoptic-studio", "name": "Panoptic (CMU Panoptic Studio)", "description": "CMU Panoptic is a large scale dataset providing 3D pose annotations (1.5 millions) for multiple people engaging social activities. It contains 65 videos (5.5 hours) with multi-view annotations, but only 17 of them are in multi-person scenario and have the camera parameters."}, {"id": "wikitext-tl-39", "name": "WikiText-TL-39", "description": "WikiText-TL-39 is a benchmark language modeling dataset in Filipino that has 39 million tokens in the training set."}, {"id": "codesc", "name": "CoDesc", "description": "CoDesc is a large dataset of 4.2m Java source code and parallel data of their description from code search, and code summarization studies."}, {"id": "imagenet-p", "name": "ImageNet-P", "description": "ImageNet-P consists of noise, blur, weather, and digital distortions. The dataset has validation perturbations; has difficulty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 \u00d7 64, standard, and Inception-sized editions; and has been designed for benchmarking not training networks. ImageNet-P departs from ImageNet-C by having perturbation sequences generated from each ImageNet validation image. Each sequence contains more than 30 frames, so to counteract an increase in dataset size and evaluation time only 10 common perturbations are used."}, {"id": "chip-sts-semantic-textual-similarity-dataset", "name": "CHIP-STS (Semantic Textual Similarity Dataset)", "description": "CHIP Semantic Textual Similarity, a dataset for sentence similarity in the non-i.i.d. (non-independent and identically distributed) setting, is used for the CHIP-STS task. Specifically, the task aims to transfer learning between disease types on Chinese disease questions and answer data. Given question pairs related to 5 different diseases (The disease types in the training and testing set are different), the task intends to determine whether the semantics of the two sentences are similar."}, {"id": "leaf-benchmark", "name": "LEAF Benchmark", "description": "A suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments."}, {"id": "something-something-v2-20bn-something-something-dataset-v2", "name": "Something-Something V2 (20BN-Something-Something\u00a0Dataset V2)", "description": "The 20BN-SOMETHING-SOMETHING V2 dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 220,847 videos, with 168,913 in the training set, 24,777 in the validation set and 27,157 in the test set. There are 174 labels."}, {"id": "humbi", "name": "HUMBI", "description": "A new large multiview dataset for human body expressions with natural clothing. The goal of HUMBI is to facilitate modeling view-specific appearance and geometry of gaze, face, hand, body, and garment from assorted people. 107 synchronized HD cameras are used to capture 772 distinctive subjects across gender, ethnicity, age, and physical condition. "}, {"id": "multisubs-multisubs-a-large-scale-multimodal-and-multilingual-dataset", "name": "MultiSubs (MultiSubs: A Large-scale Multimodal and Multilingual Dataset)", "description": "MultiSubs is a dataset of multilingual subtitles gathered from the OPUS OpenSubtitles dataset, which in turn was sourced from opensubtitles.org. We have supplemented some text fragments (visually salient nouns in this release) within the subtitles with web images, where the word sense of the fragment has been disambiguated using a cross-lingual approach. We have introduced a fill-in-the-blank task and a lexical translation task to demonstrate the utility of the dataset. Please refer to our paper for a more detailed description of the dataset and tasks. Multisubs will benefit research on visual grounding of words especially in the context of free-form sentence."}, {"id": "covaxlies-v2", "name": "CoVaxLies v2", "description": "CoVaxLies v2 includes 47 Misinformation Targets (MisTs) found on Twitter about the COVID-19 vaccines. Language experts annotated tweets as Relevant or Not Relevant, and then further annotated Relevant tweets with Stance towards each MisT. This collection is a first step in providing large-scale resources for misinformation detection and misinformation stance identification."}, {"id": "lad-large-scale-attribute-dataset", "name": "LAD (Large-scale Attribute Dataset)", "description": "LAD (Large-scale Attribute Dataset) has 78,017 images of 5 super-classes and 230 classes. The image number of LAD is larger than the sum of the four most popular attribute datasets (AwA, CUB, aP/aY and SUN). 359 attributes of visual, semantic and subjective properties are defined and annotated in instance-level."}, {"id": "house3d-environment", "name": "House3D Environment", "description": "A rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.)"}, {"id": "safety-gym", "name": "safety-gym", "description": "openai.com/blog/safety-gym/"}, {"id": "eigenworms", "name": "EigenWorms", "description": "Caenorhabditis elegans is a roundworm commonly used as a model organism in the study of genetics. The movement of these worms is known to be a useful indicator for understanding behavioural genetics. Brown {\\em et al.}[1] describe a system for recording the motion of worms on an agar plate and measuring a range of human-defined features[2]. It has been shown that the space of shapes Caenorhabditis elegans adopts on an agar plate can be represented by combinations of six base shapes, or eigenworms. Once the worm outline is extracted, each frame of worm motion can be captured by six scalars representing the amplitudes along each dimension when the shape is projected onto the six eigenworms. Using data collected for the work described in[1], we address the problem of classifying individual worms as wild-type or mutant based on the time series. The data were extracted from the C. elegans behavioural database [3]. We have 259 cases, which we split 131 train and 128 test. We have truncated each series to the shortest usable. Each series has 17984 observations. Each worm is classified as either wild-type (the N2 reference strain) or one of four mutant types: goa-1; unc-1; unc-38 and unc-63. [1] A. Brown, E. Yemini, L. Grundy, T. Jucikas, and W. Schafer, A dictionary of behavioral motifs reveals clusters of genes affecting caenorhabditis elegans locomotion, Proceedings of the National Academy of Sciences of the United States of America (PNAS), vol. 10, no. 2, pp. 791 796, 2013. [2] E. Yemini, T. Jucikas, L. Grundy, A. Brown, and W. Schafer,  A database of caenorhabditis elegans behavioral phenotypes, Nature Methods, vol. 10, pp. 877 879, 2013. [3] C. elegans behavioural database"}, {"id": "tripod-turning-point-dataset", "name": "TRIPOD (TuRnIng POint Dataset)", "description": "TRIPOD contains screenplays and plot synopses with turning point (TP) annotations for 99 movies. Each movie contains:"}, {"id": "otb-2013", "name": "OTB-2013", "description": "OTB2013 is the previous version of the current OTB2015 Visual Tracker Benchmark. It contains only 50 tracking sequences, as opposed to the 100 sequences in the current version of the benchmark."}, {"id": "sweetrs", "name": "SweetRS", "description": "Uses a  platform with 77 candies and sweets to rank. Over 2000 users submitted over 44000 grades resulting in a matrix with 28% coverage."}, {"id": "stanford-ecm", "name": "Stanford-ECM", "description": "Stanford-ECM is an egocentric multimodal dataset which comprises about 27 hours of egocentric video augmented with heart rate and acceleration data. The lengths of the individual videos cover a diverse range from 3 minutes to about 51 minutes in length. A mobile phone was used to collect egocentric video at 720x1280 resolution and 30 fps, as well as triaxial acceleration at 30Hz. The mobile phone was equipped with a wide-angle lens, so that the horizontal field of view was enlarged from 45 degrees to about 64 degrees. A wrist-worn heart rate sensor was used to capture the heart rate every 5 seconds. The phone and heart rate monitor was time-synchronized through Bluetooth, and all data was stored in the phone\u2019s storage. Piecewise cubic polynomial interpolation was used to fill in any gaps in heart rate data. Finally, data was aligned to the millisecond level at 30 Hz."}, {"id": "plasticinelab", "name": "PlasticineLab", "description": "PasticineLab is a differentiable physics benchmark, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into the desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents."}, {"id": "sidd-image-segmented-intrusion-detection-dataset", "name": "SIDD-Image (Segmented Intrusion Detection Dataset)", "description": "This is the first image-based network intrusion detection dataset. This large-scale dataset included network traffic protocol communication-based images from 15 different observation locations of different countries in Asia. This dataset is used to identify two different types of anomalies from benign network traffic. Each image with a size of 48 \u00d7 48 contains multi-protocol communications within 128 seconds. The SIDD dataset can be to applied to a broad range of tasks such as machine learning-based network intrusion detection, non-iid federated learning, and so forth."}, {"id": "naf-national-archives-forms-dataset", "name": "NAF (National Archives Forms Dataset)", "description": "This dataset was created with images provided by the United States National Archive and FamilySearch."}, {"id": "deepmtj-ieeetbme", "name": "deepMTJ_IEEEtbme", "description": "This dataset comprises 1344 expert annotated images of muscle-tendon junctions recorded with 3 ultrasound imaging systems (Aixplorer V6, Esaote MyLab60, Telemed ArtUs), on 2 muscles (Lateral Gastrocnemius, Medial Gastrocnemius), and 2 movements (isometric maximum voluntary contractions, passive torque movements)."}, {"id": "crowdflow-tub-crowdflow", "name": "CrowdFlow (TUB CrowdFlow)", "description": "The TUB CrowdFlow is a synthetic dataset that contains 10 sequences showing 5 scenes. Each scene is rendered twice: with a static point of view and a dynamic camera to simulate drone/UAV based surveillance. The scenes are render using Unreal Engine at HD resolution (1280x720) at 25 fps, which is typical for current commercial CCTV surveillance systems. The total number of frames is 3200."}, {"id": "3d-platelet-em-platelet-electron-microscopy", "name": "3D Platelet EM (Platelet Electron Microscopy)", "description": "The platelet-em dataset contains two 3D scanning electron microscope (EM) images of human platelets, as well as instance and semantic segmentations of those two image volumes. This data has been reviewed by NIBIB, contains no PII or PHI, and is cleared for public release. All files use a multipage uint16 TIF format. A 3D image with size [Z, X, Y] is saved as Z pages of size [X, Y]. Image voxels are approximately 40x10x10 nm"}, {"id": "libricss", "name": "LibriCSS", "description": "Continuous speech separation (CSS) is an approach to handling overlapped speech in conversational audio signals. A real recorded dataset, called LibriCSS, is derived from LibriSpeech by concatenating the corpus utterances to simulate a conversation and capturing the audio replays with far-field microphones."}, {"id": "tai-chi-hd", "name": "Tai-Chi-HD", "description": "Thai-Chi-HD is a high resolution dataset which can be used as reference benchmark for evaluating frameworks for image animation and video generation. It consists of cropped videos of full human bodies performing Tai Chi actions."}, {"id": "scitail", "name": "SciTail", "description": "The SciTail dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis. We use information retrieval to obtain relevant text from a large text corpus of web sentences, and use these sentences as a premise P. We crowdsource the annotation of such premise-hypothesis pair as supports (entails) or not (neutral), in order to create the SciTail dataset. The dataset contains 27,026 examples with 10,101 examples with entails label and 16,925 examples with neutral label."}, {"id": "3dfaw", "name": "3DFAW", "description": "3DFAW contains 23k images with 66 3D face keypoint annotations."}, {"id": "satstereo", "name": "SatStereo", "description": "Provides a set of stereo-rectified images and the associated groundtruthed disparities for 10 AOIs (Area of Interest) drawn from two sources: 8 AOIs from IARPA's MVS Challenge dataset and 2 AOIs from the CORE3D-Public dataset."}, {"id": "csqa", "name": "CSQA", "description": "Contains around 200K dialogs with a total of 1.6M turns. Further, unlike existing large scale QA datasets which contain simple questions that can be answered from a single tuple, the questions in the dialogs require a larger subgraph of the KG. "}, {"id": "snips-smartspeaker", "name": "Snips-SmartSpeaker", "description": "The SmartSpeaker benchmark tests the performance of reacting to music player commands in English as well as in French. It has the difficulty of containing many artist or music tracks with uncommon names in the commands, like \u201cplay music by [a boogie wit da hoodie]\u201d or \u201cI\u2019d like to listen to [Kinokoteikoku]\u201d."}, {"id": "swinseg-singapore-whole-sky-nighttime-image-segmentation-database", "name": "SWINSEG (Singapore Whole sky Nighttime Image SEGmentation Database)", "description": "The SWINSEG dataset contains 115 nighttime images of sky/cloud patches along with their corresponding binary ground truth maps. The ground truth annotation was done in consultation with experts from Singapore Meteorological Services. All images were captured in Singapore using WAHRSIS, a calibrated ground-based whole sky imager, over a period of 12 months from January to December 2016. All image patches are 500x500 pixels in size, and were selected considering several factors such as time of the image capture, cloud coverage, and seasonal variations."}, {"id": "livecell-label-free-in-vitro-image-examples-of-cells", "name": "LIVECell (Label-free In Vitro image Examples of Cells)", "description": "The LIVECell (Label-free In Vitro image Examples of Cells) dataset is a large-scale microscopic image dataset for instance-segmentation of individual cells in 2D cell cultures."}, {"id": "pandora", "name": "PANDORA", "description": "PANDORA is the first large-scale dataset of Reddit comments labeled with three personality models (including the well-established Big 5 model) and demographics (age, gender, and location) for more than 10k users."}, {"id": "giantmidi-piano", "name": "GiantMIDI-Piano", "description": "GiantMIDI-Piano contains 10,854 unique piano solo pieces composed by 2,786 composers. GiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata information of each music piece."}, {"id": "casehold-case-holdings-on-legal-decisions", "name": "CaseHOLD (Case Holdings On Legal Decisions)", "description": "CaseHOLD (Case Holdings On Legal Decisions) is a law dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). The citing context from the judicial decision serves as the prompt for the question. The answer choices are holding statements derived from citations following text in a legal decision. There are five answer choices for each citing text. The correct answer is the holding statement that corresponds to the citing text. The four incorrect answers are other holding statements."}, {"id": "hc18", "name": "HC18", "description": "Automated measurement of fetal head circumference using 2D ultrasound images"}, {"id": "glge-general-language-generation-evaluation", "name": "GLGE (General Language Generation Evaluation)", "description": "GLGE is a general language generation evaluation benchmark which is composed of 8 language generation tasks, including Abstractive Text Summarization (CNN/DailyMail, Gigaword, XSUM, MSNews), Answer-aware Question Generation (SQuAD 1.1, MSQG), Conversational Question Answering (CoQA), and Personalizing Dialogue (Personachat)."}, {"id": "acid-aerial-coastline-imagery-dataset", "name": "ACID (Aerial Coastline Imagery Dataset)", "description": "ACID consists of thousands of aerial drone videos of different coastline and nature scenes on YouTube. Structure-from-motion is used to get camera poses."}, {"id": "uci-machine-learning-repository", "name": "UCI Machine Learning Repository", "description": "UCI Machine Learning Repository is a collection of over 550 datasets."}, {"id": "jta-joint-track-auto", "name": "JTA (Joint Track Auto)", "description": "JTA is a dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. "}, {"id": "sewa-db", "name": "SEWA DB", "description": "A database of more than 2000 minutes of audio-visual data of 398 people coming from six cultures, 50% female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies."}, {"id": "emotionlines", "name": "EmotionLines", "description": "EmotionLines contains a total of 29245 labeled utterances from 2000 dialogues. Each utterance in dialogues is labeled with one of seven emotions, six Ekman\u2019s basic emotions plus the neutral emotion. Each labeling was accomplished by 5 workers, and for each utterance in a label, the emotion category with the highest votes was set as the label of the utterance. Those utterances voted as more than two different emotions were put into the non-neutral category. Therefore the dataset has a total of 8 types of emotion labels, anger, disgust, fear, happiness, sadness, surprise, neutral, and non-neutral."}, {"id": "image-editing-request-dataset", "name": "Image Editing Request Dataset", "description": "A new language-guided image editing dataset that contains a large number of real image pairs with corresponding editing instructions. "}, {"id": "mindboggle", "name": "Mindboggle", "description": "Mindboggle is a large publicly available dataset of manually labeled brain MRI. It consists of 101 subjects collected from different sites, with cortical meshes varying from 102K to 185K vertices. Each brain surface contains 25 or 31 manually labeled parcels."}, {"id": "operanet", "name": "OPERAnet", "description": "OPERAnet is a multimodal activity recognition dataset acquired from radio frequency and vision-based sensors. Approximately 8 hours of annotated measurements are provided, which are collected across two different rooms from 6 participants performing 6 activities, namely, sitting down on a chair, standing from sit, lying down on the ground, standing from the floor, walking and body rotating. The dataset has been acquired from four synchronized modalities for the purpose of passive Human Activity Recognition (HAR) as well as localization and crowd counting."}, {"id": "anlamver", "name": "AnlamVer", "description": "In this paper, we present AnlamVer, which is a semantic model evaluation dataset for Turkish designed to evaluate word similarity and word relatedness tasks while discriminating those two relations from each other. Our dataset consists of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct scores for similarity and relatedness. Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., synonymy, antonymy). Our aim is to provide insights to semantic model researchers by evaluating models in multiple attributes. We balance dataset word-pairs by their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary and rare words problems, which are caused by the rich derivational and inflectional morphology of the Turkish language. (from the original abstract of the dataset paper)"}, {"id": "chren-cherokee-english-parallel-dataset", "name": "ChrEn (Cherokee-English Parallel Dataset)", "description": "Cherokee-English Parallel Dataset is a low-resource dataset of 14,151 pairs of sentences with around 313K English tokens and 206K Cherokee tokens. The parallel corpus is accompanied by a monolingual Cherokee dataset of 5,120 sentences. Both datasets are mostly derived from Cherokee monolingual books."}, {"id": "bc2gm", "name": "BC2GM", "description": "Created by Smith et al. at 2008, the BioCreative II Gene Mention Recognition (BC2GM) Dataset contains data where participants are asked to identify a gene mention in a sentence by giving its start and end characters. The training set consists of a set of sentences, and for each sentence a set of gene mentions (GENE annotations). [registration required for access], in English language. Containing 20 in n/a file format."}, {"id": "replay-mobile", "name": "Replay-Mobile", "description": "The Replay-Mobile Database for face spoofing consists of 1190 video clips of photo and video attack attempts to 40 clients, under different lighting conditions. These videos were recorded with current devices from the market -- an iPad Mini2 (running iOS) and a LG-G4 smartphone (running Android). This Database was produced at the Idiap Research Institute (Switzerland) within the framework of collaboration with Galician Research and Development Center in Advanced Telecommunications - Gradiant (Spain)."}, {"id": "fakbat", "name": "FAKBAT", "description": "The Freebase Annotations of TREC KBA 2014 Stream Corpus with Timestamps (FAKBAT) is an extension of the FAKBA1 dataset that contains entity age and entity timestamp. It comprises roughly 1.2 billion timestamped documents from global public news wires, blogs, forums, and shortened links shared on social media. It spans 572 days (October 7, 2011\u2013May 1, 2013)."}, {"id": "n-omniglot", "name": "N-Omniglot", "description": "N-Omniglot is a neuromorphic dataset for few-shot learning. It contains 1,623 categories of handwritten characters, with only 20 samples per class."}, {"id": "ixi-ixi-brain-development-dataset", "name": "IXI (IXI Brain Development Dataset)", "description": "IXI Dataset is a collection of 600 MR brain images from normal, healthy subjects. The MR image acquisition protocol for each subject includes:"}, {"id": "healthline", "name": "Healthline", "description": "Healthline is a nutrition related dataset for multi-document summarization, using scientific studies."}, {"id": "euroc-mav", "name": "EuRoC MAV", "description": "EuRoC MAV is a visual-inertial datasets collected on-board a Micro Aerial Vehicle (MAV). The dataset contains stereo images, synchronized IMU measurements, and accurate motion and structure ground-truth. The datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data"}, {"id": "pems-bay", "name": "PEMS-BAY", "description": "PEMS-BAY is a dataset for traffic prediction."}, {"id": "phyaat-physiology-of-auditory-attention", "name": "PhyAAt (Physiology of Auditory Attention)", "description": "The dataset contains a collection of physiological signals (EEG, GSR, PPG) obtained from an experiment of the auditory attention on natural speech. Ethical Approval was acquired for the experiment. Details of the experiment can be found here https://phyaat.github.io/experiment "}, {"id": "the-china-physiological-signal-challenge-2018", "name": "The China Physiological Signal Challenge 2018", "description": "The China Physiological Signal Challenge 2018 aims to encourage the development of algorithms to identify the rhythm/morphology abnormalities from 12-lead ECGs. The data used in CPSC 2018 include one normal ECG type and eight abnormal types."}, {"id": "aqualoc", "name": "Aqualoc", "description": "A new underwater dataset that has been recorded in an harbor and provides several sequences with synchronized measurements from a monocular camera, a MEMS-IMU and a pressure sensor."}, {"id": "webkb", "name": "WebKB", "description": "WebKB is a dataset that includes web pages from computer science departments of various universities. 4,518 web pages are categorized into 6 imbalanced categories (Student, Faculty, Staff, Department, Course, Project). Additionally there is Other miscellanea category that is not comparable to the rest."}, {"id": "egohands", "name": "EgoHands", "description": "The EgoHands dataset contains 48 Google Glass videos of complex, first-person interactions between two people. The main intention of this dataset is to enable better, data-driven approaches to understanding hands in first-person computer vision. The dataset offers"}, {"id": "beat-body-expression-audio-text", "name": "BEAT (Body-Expression-Audio-Text)", "description": "BEAT has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations. Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with \\textit{facial expressions}, \\textit{emotions}, and \\textit{semantics}, in addition to the known correlation with \\textit{audio}, \\textit{text}, and \\textit{speaker identity}. Based on this observation, we propose a baseline model, \\textbf{Ca}scaded \\textbf{M}otion \\textbf{N}etwork \\textbf{(CaMN)}, which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the semantic relevancy, we introduce a metric, Semantic Relevance Gesture Recall (\\textbf{SRGR}).  Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance.  To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields, including controllable gesture synthesis, cross-modality analysis, and emotional gesture recognition. The data, code and model are available on \\url{https://pantomatrix.github.io/BEAT/}."}, {"id": "visual-perception-viper", "name": "VIsual PERception (VIPER)", "description": "VIPER is a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. "}, {"id": "george-washington", "name": "George Washington", "description": "The George Washington dataset contains 20 pages of letters written by George Washington and his associates in 1755 and thereby categorized into historical collection. The images are annotated at word level and contain approximately 5,000 words."}, {"id": "mugen", "name": "MUGEN", "description": "MUGEN is a large-scale video-audio-text dataset MUGEN, collected using the open-sourced platform game CoinRun. MUGEN can help progress research in many tasks in multimodal understanding and generation."}, {"id": "rareact", "name": "RareAct", "description": "RareAct is a video dataset of unusual actions, including actions like \u201cblend phone\u201d, \u201ccut keyboard\u201d and \u201cmicrowave shoes\u201d. It aims at evaluating the zero-shot and few-shot compositionality of action recognition models for unlikely compositions of common action verbs and object nouns. It contains 122 different actions which were obtained by combining verbs and nouns rarely co-occurring together in the large-scale textual corpus from HowTo100M, but that frequently appear separately."}, {"id": "cicerov2-contextualized-commonsense-inference-in-dialogues-v2", "name": "CICEROv2 (Contextualized Commonsense Inference in Dialogues (V2))", "description": "The CICEROv2 dataset can be found in the data directory. Each line of the files is a json object indicating a single instance. The json objects have the following key-value pairs:"}, {"id": "make3d", "name": "Make3D", "description": "The Make3D dataset is a monocular Depth Estimation dataset that contains 400 single training RGB and depth map pairs, and 134 test samples. The RGB images have high resolution, while the depth maps are provided at low resolution."}, {"id": "rcb-russian-commitment-bank", "name": "RCB (Russian Commitment Bank)", "description": "The Russian Commitment Bank is a corpus of naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment cancelling operator (question, modal, negation, antecedent of conditional)."}, {"id": "mimii", "name": "MIMII", "description": "Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection (MIMII) is a sound dataset of industrial machine sounds."}, {"id": "phd2-personalized-highlight-detection-dataset", "name": "PHD\u00b2 (Personalized Highlight Detection Dataset)", "description": "The dataset contains information on what video segments a specific user considers a highlight. Having this kind of data allows for strong personalization models, as specific examples of what a user is interested in help models obtain a fine-grained understanding of that specific user."}, {"id": "ptb-tir", "name": "PTB-TIR", "description": "PTB-TIR is a Thermal InfraRed (TIR) pedestrian tracking benchmark, which provides 60  TIR sequences with mannuly annoations.  The benchmark is used to fair evaluate TIR trackers."}, {"id": "howmany-qa", "name": "HowMany-QA", "description": "HowMany-Qa is a object counting dataset. It is taken from the counting-specific union of VQA 2.0 (Goyal et al., 2017) and Visual Genome QA (Krishna et al., 2016)."}, {"id": "apps-automated-programming-progress-standard", "name": "APPS (Automated Programming Progress Standard)", "description": "The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and evaluating the correctness of solutions. The problems range in difficulty from introductory to collegiate competition level and measure coding ability as well as problem-solving. "}, {"id": "social-relation-dataset", "name": "Social Relation Dataset", "description": "Social Relation Dataset is a dataset for social relation trait prediction from face images. Traits are based on the interpersonal circle proposed by Kiesler, where human relations are divided into 16 segments. Each segment has its opposite side in the circle, such as 'friendly and hostile'. The dataset contains 8,306 images chosen from the internet and movies. Each image is labelled with faces\u2019 bounding boxes and their pairwise relations. "}, {"id": "timedial", "name": "TimeDial", "description": "TimeDial presents a crowdsourced English challenge set, for temporal commonsense reasoning, formulated as a multiple choice cloze task with around 1.5k carefully curated dialogs. The dataset is derived from the DailyDialog, which is a multi-turn dialog corpus."}, {"id": "deeploc", "name": "DeepLoc", "description": "DeepLoc is a large-scale urban outdoor localization dataset. The dataset is currently comprised of one scene spanning an area of 110 x 130 m, that a robot traverses multiple times with different driving patterns. The dataset creators use a LiDAR-based SLAM system with sub-centimeter and sub-degree accuracy to compute the pose labels that provided as groundtruth. Poses in the dataset are approximately spaced by 0.5 m which is twice as dense as other relocalization datasets."}, {"id": "synthinel-1", "name": "Synthinel-1", "description": "Synthinel-1 is a collection of synthetic overhead imagery with full pixel-wise building segmentation labels."}, {"id": "ms-marco-microsoft-machine-reading-comprehension-dataset", "name": "MS MARCO (Microsoft Machine Reading Comprehension Dataset)", "description": "The MS MARCO (Microsoft MAchine Reading Comprehension) is a collection of datasets focused on deep learning in search. The first dataset was a question answering dataset featuring 100,000 real Bing questions and a human generated answer. Over time the collection was extended with a 1,000,000 question dataset, a natural language generation dataset, a passage ranking dataset, keyphrase extraction dataset, crawling dataset, and a conversational search."}, {"id": "soc-salient-objects-in-clutter", "name": "SOC (Salient Objects in Clutter)", "description": "SOC (Salient Objects in Clutter) is a dataset for Salient Object Detection (SOD). It includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes."}, {"id": "bigdetection", "name": "BigDetection", "description": "BigDetection is a new large-scale benchmark to build more general and powerful object detection systems. It leverages the training data from existing datasets (LVIS, OpenImages and Object365) with carefully designed principles, and curate a larger dataset for improved detector pre-training. BigDetection dataset has 600 object categories and contains 3.4M training images with 36M object bounding boxes."}, {"id": "coco-captions", "name": "COCO Captions", "description": "COCO Captions contains over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions are be provided for each image."}, {"id": "openloris-object", "name": "OpenLORIS-object", "description": "(L)ifel(O)ng (R)obotic V(IS)ion (OpenLORIS) - Object Recognition Dataset (OpenLORIS-Object) is designed for accelerating the lifelong/continual/incremental learning research and application\uff0ccurrently focusing on improving the continuous learning capability of the common objects in the home scenario."}, {"id": "diode-dense-indoor-and-outdoor-depth", "name": "DIODE (Dense Indoor and Outdoor Depth)", "description": "Diode Dense Indoor/Outdoor DEpth (DIODE) is the first standard dataset for monocular depth estimation comprising diverse indoor and outdoor scenes acquired with the same hardware setup. The training set consists of 8574 indoor and 16884 outdoor samples from 20 scans each. The validation set contains 325 indoor and 446 outdoor samples with each set from 10 different scans. The ground truth density for the indoor training and validation splits are approximately 99.54% and 99%, respectively. The density of the outdoor sets are naturally lower with 67.19% for training and 78.33% for validation subsets. The indoor and outdoor ranges for the dataset are 50m and 300m, respectively."}, {"id": "dtd-describable-textures-dataset", "name": "DTD (Describable Textures Dataset)", "description": "The Describable Textures Dataset (DTD) contains 5640 texture images in the wild. They are annotated with human-centric attributes inspired by the perceptual properties of textures."}, {"id": "bugrepo-bug-reports", "name": "BugRepo (Bug Reports)", "description": "BugRepo maintains a collection of bug reports that are publicly available for research purposes. Bug reports are a main data source for facilitating NLP-based research in software engineering. We categorize the datasets into the following research directions."}, {"id": "wcep-wikipedia-current-events-portal", "name": "WCEP (Wikipedia Current Events Portal)", "description": "The WCEP dataset for multi-document summarization (MDS) consists of short, human-written summaries about news events, obtained from the Wikipedia Current Events Portal (WCEP), each paired with a cluster of news articles associated with an event. These articles consist of sources cited by editors on WCEP, and are extended with articles automatically obtained from the Common Crawl News dataset. "}, {"id": "robotpush", "name": "RobotPush", "description": "RobotPush is a dataset for object singulation \u2013 the task of separating cluttered objects through physical interaction. The dataset contains 3456 training images with labels and 1024 validation images with labels. It consists of simulated and real-world data collected from a PR2 robot that equipped with a Kinect 2 camera. The dataset also contains ground truth instance segmentation masks for 110 images in the test set."}, {"id": "oc20-open-catalyst-2020", "name": "OC20 (Open Catalyst 2020)", "description": "Open Catalyst 2020 is a dataset for catalysis in chemical engineering. Focusing on molecules that are important in renewable energy applications, the OC20 data set comprises over 1.3 million relaxations of molecular adsorptions onto surfaces, the largest data set of electrocatalyst structures to date."}, {"id": "multi-pie", "name": "Multi-PIE", "description": "The Multi-PIE (Multi Pose, Illumination, Expressions) dataset consists of face images of 337 subjects taken under different pose, illumination and expressions. The pose range contains 15 discrete views, capturing a face profile-to-profile. Illumination changes were modeled using 19 flashlights located in different places of the room."}, {"id": "dialoglue", "name": "DialoGLUE", "description": "DialoGLUE is a natural language understanding benchmark for task-oriented dialogue designed to encourage dialogue research in representation-based transfer, domain adaptation, and sample-efficient task learning. It consisting of 7 task-oriented dialogue datasets covering 4 distinct natural language understanding tasks."}, {"id": "fewglue", "name": "FewGlue", "description": "FewGLUE consists of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task."}, {"id": "acav100m-automatically-curated-audio-visual", "name": "ACAV100M (Automatically Curated Audio-Visual)", "description": "ACAV100M processes 140 million full-length videos (total duration 1,030 years) which are used to produce a dataset of 100 million 10-second clips (31 years) with high audio-visual correspondence. This is two orders of magnitude larger than the current largest video dataset used in the audio-visual learning literature, i.e., AudioSet (8 months), and twice as large as the largest video dataset in the literature, i.e., HowTo100M (15 years)."}, {"id": "vast-varied-stance-topics", "name": "VAST (VAried Stance Topics)", "description": "VAST consists of a large range of topics covering broad themes, such as politics (e.g., \u2018a Palestinian state\u2019), education (e.g., \u2018charter schools\u2019), and public health (e.g., \u2018childhood vaccination\u2019). In addition, the data includes a wide range of similar expressions (e.g., \u2018guns on campus\u2019 versus \u2018firearms on campus\u2019). This variation captures how humans might realistically describe the same topic and contrasts with the lack of variation in existing datasets."}, {"id": "bracs-breast-carcinoma-subtyping", "name": "BRACS (BReAst Carcinoma Subtyping)", "description": "BReAst Carcinoma Subtyping (BRACS) dataset, a large cohort of annotated Hematoxylin & Eosin (H&E)-stained images to facilitate the characterization of breast lesions. BRACS contains 547 Whole-Slide Images (WSIs), and 4539 Regions of Interest (ROIs) extracted from the WSIs. Each WSI, and respective ROIs, are annotated by the consensus of three board-certified pathologists into different lesion categories. Specifically, BRACS includes three lesion types, i.e., benign, malignant and atypical, which are further subtyped into seven categories."}, {"id": "newer-college", "name": "Newer College", "description": "The Newer College Dataset is a large dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km through New College, Oxford. The dataset includes data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR, which also provides inertial measurements. Additionally, the authors used a tripod-mounted survey grade LiDAR scanner to capture a detailed millimeter-accurate 3D map of the test location (containing \u223c290 million points). "}, {"id": "icons-50", "name": "Icons-50", "description": "Icons-50 is a dataset for studying surface variation robustness."}, {"id": "nico-non-i-i-d-image-dataset-with-contexts", "name": "NICO (Non-I.I.D. Image dataset with Contexts)", "description": "I.I.D. hypothesis between training and testing data is the basis of numerous image classification methods. Such property can hardly be guaranteed in practice where the Non-IIDness is common, causing in- stable performances of these models. In literature, however, the Non-I.I.D. image classification problem is largely understudied. A key reason is lacking of a well-designed dataset to support related research. In this paper, we construct and release a Non-I.I.D. image dataset called NICO, which uses contexts to create Non-IIDness consciously. Compared to other datasets, extended analyses prove NICO can support various Non-I.I.D. situations with sufficient flexibility. Meanwhile, we propose a baseline model with Con- vNet structure for General Non-I.I.D. image classification, where distribution of testing data is unknown but different from training data. The experimental results demonstrate that NICO can well support the training of ConvNet model from scratch, and a batch balancing module can help ConvNets to perform better in Non-I.I.D. settings."}, {"id": "wikilarge", "name": "WikiLarge", "description": "WikiLarge comprise 359 test sentences, 2000 development sentences and 300k training sentences. Each source sentences in test set has 8 simplified references"}, {"id": "edna-covid", "name": "EDNA-Covid", "description": "EDNA-Covid is a multilingual, large-scale dataset of coronavirus-related tweets collected since January 25, 2020. EDNA-Covid includes, at time of this publication, over 600M tweets from around the world in over 10 languages."}, {"id": "dream", "name": "DREAM", "description": "DREAM is a multiple-choice Dialogue-based REAding comprehension exaMination dataset. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding."}, {"id": "wd50k-wikidata-50k", "name": "WD50K (Wikidata 50K)", "description": "@inproceedings{StarE,   title={Message Passing for Hyper-Relational Knowledge Graphs},   author={Galkin, Mikhail and Trivedi, Priyansh and Maheshwari, Gaurav and Usbeck, Ricardo and Lehmann, Jens},   booktitle={EMNLP},   year={2020} }"}, {"id": "arca23k", "name": "ARCA23K", "description": "ARCA23K is a dataset of labelled sound events created to investigate real-world label noise. It contains 23,727 audio clips originating from Freesound, and each clip belongs to one of 70 classes taken from the AudioSet ontology. The dataset was created using an entirely automated process with no manual verification of the data. For this reason, many clips are expected to be labelled incorrectly."}, {"id": "gibson-environment", "name": "Gibson Environment", "description": "Gibson is an opensource perceptual and physics simulator to explore active and real-world perception. The Gibson Environment is used for Real-World Perception Learning."}, {"id": "arabic-text-diacritization", "name": "Arabic Text Diacritization", "description": "Extracted from the Tashkeela Corpus, the dataset consists of 55K lines containing about 2.3M words."}, {"id": "aolp-application-oriented-license-plate", "name": "AOLP (Application-oriented License Plate)", "description": "The application-oriented license plate (AOLP) benchmark database has 2049 images of Taiwan license plates. This database is categorized into three subsets: access control (AC) with 681 samples, traffic law enforcement (LE) with 757 samples, and road patrol (RP) with 611 samples. AC refers to the cases that a vehicle passes a fixed passage with a lower speed or full stop. This is the easiest situation. The images are captured under different illuminations and different weather conditions. LE refers to the cases that a vehicle violates traffic laws and is captured by roadside camera. The background are really cluttered, with road sign and multiple plates in one image. RP refers to the cases that the camera is held on a patrolling vehicle, and the images are taken with arbitrary viewpoints and distances."}, {"id": "talksumm", "name": "TalkSumm", "description": "The TalkSumm dataset contains 1705 automatically-generated summaries of scientific papers from ACL, NAACL, EMNLP, SIGDIAL (2015-2018), and ICML (2017-2018)."}, {"id": "apt-malware", "name": "APT-Malware", "description": "The APT Malware dataset is used to train classifiers to predict if a given malware belongs to the \u201cAdvanced Persistent Threat\u201d (APT) type or not. It contains 3131 samples spread over 24 different unique malware classes."}, {"id": "multi-domain-sentiment-dataset-v2-0", "name": "Multi-Domain Sentiment Dataset v2.0", "description": "The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from many product types (domains). Some domains (books and dvds) have hundreds of thousands of reviews. Others (musical instruments) have only a few hundred. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed."}, {"id": "stanford-online-products", "name": "Stanford Online Products", "description": "Stanford Online Products (SOP) dataset has 22,634 classes with 120,053 product images. The first 11,318 classes (59,551 images) are split for training and the other 11,316 (60,502 images) classes are used for testing"}, {"id": "codecontests", "name": "CodeContests", "description": "CodeContests is a competitive programming dataset for machine-learning. This dataset was used when training AlphaCode."}, {"id": "agriculture-vision", "name": "Agriculture-Vision", "description": "A large-scale aerial farmland image dataset for semantic segmentation of agricultural patterns. Collects 94,986 high-quality aerial images from 3,432 farmlands across the US, where each image consists of RGB and Near-infrared (NIR) channels with resolution as high as 10 cm per pixel. "}, {"id": "sentimental-liar", "name": "Sentimental LIAR", "description": "The Sentimental LIAR dataset is a modified and further extended version of the LIAR extension introduced by Kirilin et al. In this dataset, the multi-class labeling of LIAR is converted to a binary annotation by changing half-true, false, barely-true and pants-fire labels to False, and the remaining labels to True. Furthermore, the speaker names are converted to numerical IDs in order to avoid bias with regards to the textual representation of names. The binary-label dataset is then extended by adding sentiments derived using the Google NLP API. Sentiment analysis determines the overall attitude of the text (i.e., whether it is positive or negative), and is quantified by a numerical score. If the sentiment score is positive, then the sample is tagged as Positive for the sentiment attribute, otherwise Negative is assigned. A further extension is introduced by adding emotion scores extracted using the IBM NLP API for each claim, which determine the detected level of 6 emotional states namely anger, sadness, disgust, fear and joy. The score for each emotion is between the range of 0 and 1."}, {"id": "mannequinchallenge", "name": "MannequinChallenge", "description": "The MannequinChallenge Dataset (MQC) provides in-the-wild videos of people in static poses while a hand-held camera pans around the scene. The dataset consists of three splits for training, validation and testing."}, {"id": "bci-competition-datasets", "name": "BCI Competition Datasets", "description": "The goal of the \"BCI Competition\" is to validate signal processing and classification methods for Brain-Computer Interfaces (BCIs)."}, {"id": "lidc-idri", "name": "LIDC-IDRI", "description": "The LIDC-IDRI dataset contains lesion annotations from four experienced thoracic radiologists. LIDC-IDRI contains 1,018 low-dose lung CTs from 1010 lung patients."}, {"id": "upfd-pol-user-preference-aware-fake-news-detection", "name": "UPFD-POL (User Preference-aware Fake News Detection)", "description": "The PolitiFact variant of the UPFD dataset for benchmarking."}, {"id": "moses-molecular-sets-moses", "name": "MOSES (Molecular sets (MOSES))", "description": "The set is based on the ZINC Clean Leads collection. It contains 4,591,276 molecules in total, filtered by molecular weight in the range from 250 to 350 Daltons, a number of rotatable bonds not greater than 7, and XlogP less than or equal to 3.5. We removed molecules containing charged atoms or atoms besides C, N, S, O, F, Cl, Br, H or cycles longer than 8 atoms. The molecules were filtered via medicinal chemistry filters (MCFs) and PAINS filters."}, {"id": "visual-wake-words", "name": "Visual Wake Words", "description": "Visual Wake Words represents a common microcontroller vision use-case of identifying whether a person is present in the image or not, and provides a realistic benchmark for tiny vision models."}, {"id": "gaze360-physically-unconstrained-gaze-estimation-in-the-wild", "name": "Gaze360 (Physically Unconstrained Gaze Estimation in the Wild)", "description": "Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting."}, {"id": "rustance", "name": "RuStance", "description": "Includes Russian tweets and news comments from multiple sources, covering multiple stories, as well as text classification approaches to stance detection as benchmarks over this data in this language."}, {"id": "aider", "name": "AIDER", "description": "Dataset aimed to do automated aerial scene classification of disaster events from on-board a UAV."}, {"id": "cholec80-surgical-workflow-dataset", "name": "Cholec80 (Surgical Workflow Dataset)", "description": "Cholec80 is an endoscopic video dataset containing 80 videos of cholecystectomy surgeries performed by 13 surgeons. The videos are captured at 25 fps and downsampled to 1 fps for processing. The whole dataset is labeled with the phase and tool presence annotations. The phases have been defined by a senior surgeon in Strasbourg hospital, France. Since the tools are sometimes hardly visible in the images and thus difficult to be recognized visually, a tool is defined as present in an image if at least half of the tool tip is visible."}, {"id": "vigor", "name": "VIGOR", "description": "Similar to CVUSA and CVACT, the VIGOR dataset contains satellites and street imagery to match them to each other to find the location of the street imagery. For this purpose, data from 4 major American cities were used, namely San Francisco, New York, Seattle and Chicago. Unlike the previous datasets, there are two settings: The SAME-Area setting where images of all cities are available in training and validation split. Secondly, there is the CROSS area setting where training is done on two cities (New York, Seattle) and evaluation is done on Chicago and San Francisco. In addition, the dataset contains semi-positive images which are very close to an actual ground truth image and thus serve as a distraction for the matching task. In total, the dataset consists of 90,618 satellite images and 105,214 street images."}, {"id": "feedbackqa", "name": "FeedbackQA", "description": "\ud83d\udcc4 Read \ud83d\udcbe Code \ud83d\udd17 Webpage \ud83d\udcbb Demo \ud83e\udd17 Huggingface Dataset \ud83d\udcac Discussions"}, {"id": "yaclc-yet-another-chinese-learner-corpus", "name": "YACLC (Yet Another Chinese Learner Corpus)", "description": "YACLC is a large scale, multidimensional annotated Chinese learner corpus. To construct the corpus, the aurhots first obtain a large number of topic-rich texts generated by Chinese as Foreign Language (CFL) learners. The authors collected and annotated 32,124 sentences written by CFL learners from the lang-8 platform. Each sentence is annotated by 10 annotators. After post processing, a total of 469,000 revised sentences are obtained."}, {"id": "wikicaps", "name": "WikiCaps", "description": "WikiCaps is a large-scale multilingual but non-parallel data set for multimodal machine translation and retrieval. The image-caption data was extracted from Wikimedia Commons and is thus a representative of the collection of largely available non-descriptive image-caption pairs in the web. The current version of the dataset contains 3,816,940 images with 3,825,132 English captions and additional 1,000 image-caption pairs in German, French, and Russian together with their English counterparts."}, {"id": "m-vad-names-m-vad-names-dataset", "name": "M-VAD Names (M-VAD Names Dataset)", "description": "The dataset contains the annotations of characters' visual appearances, in the form of tracks of face bounding boxes, and the associations with characters' textual mentions, when available. The detection and annotation of the visual appearances of characters in each video clip of each movie was achieved through a semi-automatic approach. The released dataset contains more than 24k annotated video clips, including 63k visual tracks and 34k textual mentions, all associated with their character identities."}, {"id": "thumos14-thumos-2014", "name": "THUMOS14 (THUMOS 2014)", "description": "The THUMOS14 dataset is a large-scale video dataset that includes 1,010 videos for validation and 1,574 videos for testing from 20 classes. Among all the videos, there are 220 and 212 videos with temporal annotations in validation and testing set, respectively."}, {"id": "winogender-schemas", "name": "Winogender Schemas", "description": "Winogender Schemas is a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender."}, {"id": "pieapp-dataset", "name": "PieAPP dataset", "description": "The PieAPP dataset is a large-scale dataset used for training and testing perceptually-consistent image-error prediction algorithms. The dataset can be downloaded from:  server containing a zip file with all data (2.2GB) or Google Drive (ideal for quick browsing). "}, {"id": "nasa-c-mapss-turbofan-engine-degradation-simulation-data-set", "name": "NASA C-MAPSS (Turbofan Engine Degradation Simulation Data Set)", "description": "Engine degradation simulation was carried out using C-MAPSS. Four different were sets simulated under different combinations of operational conditions and fault modes. Records several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames."}, {"id": "vehicleid-pku-vehicleid", "name": "VehicleID (PKU VehicleID)", "description": "The \u201cVehicleID\u201d dataset contains CARS captured during the daytime by multiple real-world surveillance cameras distributed in a small city in China. There are 26,267 vehicles (221,763 images in total) in the entire dataset. Each image is attached with an id label corresponding to its identity in real world. In addition, the dataset contains manually labelled 10319 vehicles (90196 images in total) of their vehicle model information(i.e.\u201cMINI-cooper\u201d, \u201cAudi A6L\u201d and \u201cBWM 1 Series\u201d)."}, {"id": "tiny-imagenet", "name": "Tiny ImageNet", "description": "Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64\u00d764 colored images. Each class has 500 training images, 50 validation images and 50 test images."}, {"id": "cicids2018-cse-cic-ids2018-on-aws", "name": "CICIDS2018 (CSE-CIC-IDS2018 on AWS)", "description": "CICIDS2018 includes seven different attack scenarios: Brute-force, Heartbleed, Botnet, DoS, DDoS, Web attacks, and infiltration of the network from inside. The attacking infrastructure includes 50 machines and the victim organization has 5 departments and includes 420 machines and 30 servers. The dataset includes the captures network traffic and system logs of each machine, along with 80 features extracted from the captured traffic using CICFlowMeter-V3."}, {"id": "tg-redial", "name": "TG-ReDial", "description": "TG-ReDial is a a topic-guided conversational recommendation dataset for research on conversational/interactive recommender systems."}, {"id": "bindingdb-the-binding-database", "name": "BindingDB (The Binding Database)", "description": "BindingDB is a public, web-accessible database of measured binding affinities, focusing chiefly on the interactions of protein considered to be drug-targets with small, drug-like molecules. As of May 27, 2022, BindingDB contains 41,296 Entries, each with a DOI, containing 2,519,702 binding data for 8,810 protein targets and 1,080,101 small molecules. There are 5,988 protein-ligand crystal structures with BindingDB affinity measurements for proteins with 100% sequence identity, and 11,442 crystal structures allowing proteins to 85% sequence identity.You can also use BindingDB data through the Registry of Open Data on AWS: https://registry.opendata.aws/binding-db. This dataset using the split by TransformerCPI(doi.org/10.1093/bioinformatics/btaa524)"}, {"id": "tvseries", "name": "TVSeries", "description": "A realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 hours of footage annotated with 30 action classes, totaling 6,231 action instances."}, {"id": "decanlp-natural-language-decathlon-benchmark", "name": "decaNLP (Natural Language Decathlon Benchmark)", "description": "Natural Language Decathlon Benchmark (decaNLP) is a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. The tasks as cast as question answering over a context."}, {"id": "mpii-cooking-2-dataset", "name": "MPII Cooking 2 Dataset", "description": "A dataset which provides detailed annotations for activity recognition."}, {"id": "css10", "name": "CSS10", "description": "A collection of single speaker speech datasets for ten languages. It is composed of short audio clips from LibriVox audiobooks and their aligned texts."}, {"id": "bcn-20000", "name": "BCN_20000", "description": "BCN_20000 is a dataset composed of 19,424 dermoscopic images of skin lesions captured from 2010 to 2016 in the facilities of the Hospital Cl\u00ednic in Barcelona. The dataset can be used for lesion recognition tasks such as lesion segmentation, lesion detection and lesion classification."}, {"id": "nela-gt-2018", "name": "NELA-GT-2018", "description": "NELA-GT-2018 is a dataset for the study of misinformation that consists of 713k articles collected between 02/2018-11/2018. These articles are collected directly from 194 news and media outlets including mainstream, hyper-partisan, and conspiracy sources. It includes ground truth ratings of the sources from 8 different assessment sites covering multiple dimensions of veracity, including reliability, bias, transparency, adherence to journalistic standards, and consumer trust."}, {"id": "2012-i2b2-temporal-relations-2012-i2b2-clinical-temporal-relations", "name": "2012 i2b2 Temporal Relations (2012 i2b2 Clinical Temporal Relations)", "description": "The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions."}, {"id": "satimage", "name": "Satimage", "description": "The resources for this dataset can be found at https://www.openml.org/d/182"}, {"id": "cbt-childrens-book-test", "name": "CBT (Children\u2019s Book Test)", "description": "Children\u2019s Book Test (CBT) is designed to measure directly how well language models can exploit wider linguistic context. The CBT is built from books that are freely available thanks to Project Gutenberg. "}, {"id": "vcr-visual-commonsense-reasoning", "name": "VCR (Visual Commonsense Reasoning)", "description": "Visual Commonsense Reasoning (VCR) is a large-scale dataset for cognition-level visual understanding. Given a challenging question about an image, machines need to present two sub-tasks: answer correctly and provide a rationale justifying its answer. The VCR dataset contains over 212K (training), 26K (validation) and 25K (testing) questions, answers and rationales derived from 110K movie scenes."}, {"id": "ted-talks", "name": "TED-talks", "description": "In order to create the TED-talks dataset,  3,035 YouTube videos were downloaded using the \"TED talks\" query. From these initial candidates, videos in which the upper part of the person is visible for at least 64 frames, and the height of the person bounding box was at least 384 pixels were selected. Static videos were manually filtered out and videos in which a person is doing something other than presenting."}, {"id": "ilids-vid", "name": "iLIDS-VID", "description": "The iLIDS-VID dataset is a person re-identification dataset which involves 300 different pedestrians observed across two disjoint camera views in public open space. It comprises 600 image sequences of 300 distinct individuals, with one pair of image sequences from two camera views for each person. Each image sequence has variable length ranging from 23 to 192 image frames, with an average number of 73. The iLIDS-VID dataset is very challenging due to clothing similarities among people, lighting and viewpoint variations across camera views, cluttered background and random occlusions."}, {"id": "csrc-children-speech-recognition-challenge", "name": "CSRC (Children Speech Recognition Challenge)", "description": "CSRC is a collection of data for Children Speech Recognition. The data for this challenge is divided into 3 datasets, referred to as A (Adult speech training set), C1 (Children speech training set) and C2 (Children conversation training set). All dataset combined amount to 400 hours of Mandarin speech data."}, {"id": "weibo-ner", "name": "Weibo NER", "description": "The Weibo NER dataset is a Chinese Named Entity Recognition dataset drawn from the social media website Sina Weibo."}, {"id": "vizwiz-vqa-grounding", "name": "VizWiz-VQA-Grounding", "description": "The VizWiz-VQA-Grounding dataset is a dataset that visually grounds answers to visual questions asked by people with visual impairments."}, {"id": "wikicrem", "name": "WikiCREM", "description": "An unsupervised dataset for co-reference resolution. Presented in the publication: Kocijan et. al, WikiCREM: A Large Unsupervised Corpus for Coreference Resolution, presented at EMNLP 2019."}, {"id": "nvbench", "name": "nvBench", "description": "nvBench is a large-scale NL2VIS (natural languagge to visualisations) benchmark, containing 25,750 (NL, VIS) pairs from 750 tables over 105 domains, synthesized from (NL, SQL) benchmarks to support cross-domain NLPVIS (Natural Language Query to Visualization) task."}, {"id": "urbanloco", "name": "UrbanLoco", "description": "UrbanLoco is a mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite. The dataset includes 13 trajectories collected in San Francisco and Hong Kong, covering a total length of over 40 kilometers."}, {"id": "hic-hands-in-action", "name": "HIC (Hands in Action)", "description": "The Hands in action dataset (HIC) dataset has RGB-D sequences of hands interacting with objects."}, {"id": "eurlex57k", "name": "EURLEX57K", "description": "EURLEX57K is a new publicly available legal LMTC dataset, dubbed EURLEX57K, containing 57k English EU legislative documents from the EUR-LEX portal, tagged with \u223c4.3k labels (concepts) from the European Vocabulary (EUROVOC)."}, {"id": "vlcs", "name": "VLCS", "description": "VLCS is a dataset to test for domain generalization."}, {"id": "mq2008", "name": "MQ2008", "description": "The MQ2008 dataset is a dataset for Learning to Rank. It contains 800 queries with labelled documents."}, {"id": "20newsgroups", "name": "20NewsGroups", "description": "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups."}, {"id": "pfn-pic-pfn-picking-instructions-for-commodities-dataset", "name": "PFN-PIC (PFN Picking Instructions for Commodities Dataset)", "description": "This dataset is a collection of spoken language instructions for a robotic system to pick and place common objects. Text instructions and corresponding object images are provided. The dataset consists of situations where the robot is instructed by the operator to pick up a specific object and move it to another location: for example, Move the blue and white tissue box to the top right bin. This dataset consists of RGBD images, bounding box annotations, destination box annotations, and text instructions."}, {"id": "boostclir", "name": "BoostCLIR", "description": "BoostCLIR is a bilingual (Japanese-English) corpus of patent abstracts, extracted from the MAREC patent data, and the data from the NTCIR PatentMT workshop collections, accompanied with relevance judgements for the task of patent prior-art search."}, {"id": "fastmri", "name": "fastMRI", "description": "The fastMRI dataset includes two types of MRI scans: knee MRIs and the brain (neuro) MRIs, and containing training, validation, and masked test sets. The deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. Also, each DICOM image is manually inspected for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content. Knee MRI: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression. Brain MRI: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent."}, {"id": "jaquad", "name": "JaQuAD", "description": "JaQuAD (Japanese Question Answering Dataset) is a question answering dataset in Japanese that consists of 39,696 extractive question-answer pairs on Japanese Wikipedia articles."}, {"id": "dcase-2017", "name": "DCASE 2017", "description": "The DCASE 2017 rare sound events dataset contains isolated sound events for three classes: 148 crying babies (mean duration 2.25s), 139 glasses breaking (mean duration 1.16s), and 187 gun shots (mean duration 1.32s). As with the DCASE 2016 data, silences are not excluded from active event markings in the annotations. While this data set contains many samples per class, there are only three classes"}, {"id": "alto-aerial-view-large-scale-terrain-oriented", "name": "ALTO (Aerial-view Large-scale Terrain-Oriented)", "description": "ALTO is a vision-focused dataset for the development and benchmarking of Visual Place Recognition and Localization methods for Unmanned Aerial Vehicles. The dataset is composed of two long (approximately 150km and 260km) trajectories flown by a helicopter over Ohio and Pennsylvania, and it includes high precision GPS-INS ground truth location data, high precision accelerometer readings, laser altimeter readings, and RGB downward facing camera imagery.The dataset also comes with reference imagery over the flight paths, which makes this dataset suitable for VPR benchmarking and other tasks common in Localization, such as image registration and visual odometry."}, {"id": "vrai-vehicle-re-identification-for-aerial-image", "name": "VRAI (Vehicle Re-identification for Aerial Image)", "description": "VRAI is a large-scale vehicle ReID dataset for UAV-based intelligent applications. The dataset consists of 137, 613 images of 13, 022 vehicle instances. The images of each vehicle instance are captured by cameras of two DJI consumer UAVs at different locations, with a variety of view angles and flight-altitudes (15m to 80m)."}, {"id": "dpc-captions", "name": "DPC-Captions", "description": "This is an open-source image captions dataset for the aesthetic evaluation of images. The dataset is called DPC-Captions, which contains comments of up to five aesthetic attributes of one image through knowledge transfer from a full-annotated small-scale dataset."}, {"id": "scanrefer-dataset", "name": "ScanRefer Dataset", "description": "Contains 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D."}, {"id": "ua-detrac", "name": "UA-DETRAC", "description": "Consists of 100 challenging video sequences captured from real-world traffic scenes (over 140,000 frames with rich annotations, including occlusion, weather, vehicle category, truncation, and vehicle bounding boxes) for object detection, object tracking and MOT system. "}, {"id": "ballroom", "name": "Ballroom", "description": "This data set includes beat and bar annotations of the ballroom dataset, introduced by Gouyon et al. [1]."}, {"id": "sms-spam-collection-data-set", "name": "SMS Spam Collection Data Set", "description": "This corpus has been collected from free or free for research sources at the Internet:"}, {"id": "epinions", "name": "Epinions", "description": "The Epinions dataset is built form a who-trust-whom online social network of a general consumer review site Epinions.com. Members of the site can decide whether to ''trust'' each other. All the trust relationships interact and form the Web of Trust which is then combined with review ratings to determine which reviews are shown to the user. It contains 75,879 nodes and 50,8837 edges."}, {"id": "acdc-automated-cardiac-diagnosis-challenge", "name": "ACDC (Automated Cardiac Diagnosis Challenge)", "description": "The goal of the Automated Cardiac Diagnosis Challenge (ACDC) challenge is to:"}, {"id": "3dmatch", "name": "3DMatch", "description": "The 3DMATCH benchmark evaluates how well descriptors (both 2D and 3D) can establish correspondences between RGB-D frames of different views. The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences. "}, {"id": "vgg-face", "name": "VGG Face", "description": "The VGG Face dataset is face identity recognition dataset that consists of 2,622 identities. It contains over 2.6 million images."}, {"id": "long-video-dataset-3x", "name": "Long Video Dataset (3X)", "description": "We randomly selected three videos from the Internet, that are longer than 1.5K frames and have their main objects continuously appearing. Each video has 20 uniformly sampled frames manually annotated for evaluation. Each video has been played back and forth to generate videos that are three times as long."}, {"id": "discovery-dataset", "name": "Discovery Dataset", "description": "The Discovery datasets consists of adjacent sentence pairs (s1,s2) with a discourse marker (y) that occurred at the beginning of s2. They were extracted from the depcc web corpus."}, {"id": "skycam", "name": "SkyCam", "description": "SkyCam dataset is a collection of sky images from a variety of locations with diverse topological characteristics (Swiss Jura, Plateau and Pre-Alps regions), from both single and stereo camera settings coupled with a high-accuracy pyranometers. The dataset was collected with a high frequency with a data sample every 10 seconds. 13 images with different exposures times are generated along with a post-processed HDR images and a solar radiance values for each of the cameras and locations. We hope that SkyCam dataset will enable researchers to tackle the problem of short-term local camera-based solar radiance prediction."}, {"id": "serv-ct-serv-ct-a-disparity-dataset-from-ct-for-validation-of-endoscopic-3d-reconstruction", "name": "SERV-CT (SERV-CT: A disparity dataset from CT for validation of endoscopic 3D reconstruction)", "description": "Endoscopic stereo reconstruction for surgical scenes gives rise to specific problems, including the lack of clear corner features, highly specular surface properties, and the presence of blood and smoke. These issues present difficulties for both stereo reconstruction itself and also for standardised dataset production. We present a stereo-endoscopic reconstruction validation dataset based on cone-beam CT (SERV-CT). Two ex vivo small porcine full torso cadavers were placed within the view of the endoscope with both the endoscope and target anatomy visible in the CT scan. Subsequent orientation of the endoscope was manually aligned to match the stereoscopic view and benchmark disparities, depths and occlusions are calculated. The requirement of a CT scan limited the number of stereo pairs to 8 from each ex vivo sample. For the second sample an RGB surface was acquired to aid alignment of smooth, featureless surfaces. Repeated manual alignments showed an RMS disparity accuracy of around 2 pixels and a depth accuracy of about 2 mm. A simplified reference dataset is provided consisting of endoscope image pairs with corresponding calibration, disparities, depths, and occlusions covering the majority of the endoscopic image and a range of tissue types, including smooth specular surfaces, as well as significant variation of depth.  The SERV-CT dataset provides an easy-to-use stereoscopic validation for surgical applications with smooth reference disparities and depths covering the majority of the endoscopic image."}, {"id": "multieurlex", "name": "MultiEURLEX", "description": "MultiEURLEX is a multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. The dataset covers 23 official EU languages from 7 language families."}, {"id": "yfcc100m", "name": "YFCC100M", "description": "YFCC100M is a that dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014."}, {"id": "n-imagenet-large-scale-dataset-for-event-based-object-recognition", "name": "N-ImageNet (Large-Scale Dataset for Event-Based Object Recognition)", "description": "The N-ImageNet dataset is an event-camera counterpart for the ImageNet dataset. The dataset is obtained by moving an event camera around a monitor displaying images from ImageNet. N-ImageNet contains approximately 1,300k training samples and 50k validation samples. In addition, the dataset also contains variants of the validation dataset recorded under a wide range of lighting or camera trajectories. Additional details about the dataset are explained in the paper available through this link. Please cite this paper if you make use of the dataset."}, {"id": "pde-dataset-parametric-partial-differential-equation-dataset", "name": "PDE dataset (Parametric Partial Differential Equation dataset)", "description": "Contains data of parametric PDEs"}, {"id": "mimic-iv", "name": "MIMIC-IV", "description": "Retrospectively collected medical data has the opportunity to improve patient care through knowledge discovery and algorithm development. Broad reuse of medical data is desirable for the greatest public good, but data sharing must be done in a manner which protects patient privacy. "}, {"id": "tvc-tv-show-captions", "name": "TVC (TV show Captions)", "description": "TV show Caption is a large-scale multimodal captioning dataset, containing 261,490 caption descriptions paired with 108,965 short video moments. TVC is unique as its captions may also describe dialogues/subtitles while the captions in the other datasets are only describing the visual content."}, {"id": "celebamask-hq", "name": "CelebAMask-HQ", "description": "CelebAMask-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has segmentation mask of facial attributes corresponding to CelebA."}, {"id": "multiviewx", "name": "MultiviewX", "description": "MultiviewX is a synthetic Multiview pedestrian detection dataset. It is build using pedestrian models from PersonX, in Unity. The MultiviewX dataset covers a square of 16 meters by 25 meters. The ground plane is quantized into a 640x1000 grid. There are 6 cameras with overlapping field-of-view in the MultiviewX dataset, each of which outputs a 1080x1920 resolution image. On average, 4.41 cameras are covering the same location."}, {"id": "nyu-symmetry-database", "name": "NYU Symmetry Database", "description": "The NYU Symmetry database contains 176 single-symmetry and 63 multiple-symmetry images (.png files) with accompanying ground-truth annotations (.mat files). Also included are a .m file to visualize the annotations on top of the images, and a .txt file with instructions on how to interpret the .mat annotations."}, {"id": "msmt17", "name": "MSMT17", "description": "MSMT17 is a multi-scene multi-time person re-identification dataset. The dataset consists of 180 hours of videos, captured by 12 outdoor cameras, 3 indoor cameras, and during 12 time slots. The videos cover a long period of time and present complex lighting variations, and it contains a large number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes."}, {"id": "mconala-multilingual-conala", "name": "MCoNaLa (Multilingual CoNaLa)", "description": "MCoNaLa is a multilingual dataset to benchmark code generation from natural language commands extending beyond English. Modeled off of the methodology from the English Code/Natural Language Challenge (CoNALa) dataset, the authors annotated a total of 896 NL-code pairs in three languages: Spanish, Japanese, and Russian. "}, {"id": "tvsum-title-based-video-summarization-dataset", "name": "TVSum (Title-based Video Summarization Dataset)", "description": "Title-based Video Summarization (TVSum) dataset serves as a benchmark to validate video summarization techniques. It contains 50 videos of various genres (e.g., news, how-to, documentary, vlog, egocentric) and 1,000 annotations of shot-level importance scores obtained via crowdsourcing (20 per video). The video and annotation data permits an automatic evaluation of various video summarization techniques, without having to conduct (expensive) user study."}, {"id": "vaw-visual-attributes-in-the-wild", "name": "VAW (Visual Attributes in the Wild)", "description": "VAW is a large scale visual attributes dataset with explicitly labelled positive and negative attributes. "}, {"id": "legal-ner", "name": "legal_NER", "description": "legal_NER is a corpus of 46545 annotated legal named entities mapped to 14 legal entity types. It is designed for named entity recognition in indian court judgement."}, {"id": "darmstadt-noise-dataset-zaid-allal", "name": "Darmstadt Noise Dataset (zaid allal)", "description": "the dataset contains data about hydrogen storage in metal hydrides"}, {"id": "chime-5-chime-speech-separation-and-recognition-challenge", "name": "CHiME-5 (CHiME Speech Separation and Recognition Challenge)", "description": "The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning."}, {"id": "otters", "name": "OTTers", "description": "OTTers is a  dataset of human one-turn topic transitions. In this task, models must connect two topics in a cooperative and coherent manner, by generating a \"bridging\" utterance connecting the new topic tot he topic of the previous conversation turn."}, {"id": "ptl-pedestrian-traffic-lights", "name": "PTL (Pedestrian-Traffic-Lights)", "description": "A dataset of pedestrian traffic lights containing over 5000 photos taken at hundreds of intersections in Shanghai."}, {"id": "konvid-1k-konvid-1k-vqa-database", "name": "KoNViD-1k (KoNViD-1k VQA Database)", "description": "Subjective video quality assessment (VQA) strongly depends on semantics, context, and the types of visual distortions. A lot of existing VQA databases cover small numbers of video sequences with artificial distortions. When testing newly developed Quality of Experience (QoE) models and metrics, they are commonly evaluated against subjective data from such databases, that are the result of perception experiments. However, since the aim of these QoE models is to accurately predict natural videos, these artificially distorted video databases are an insufficient basis for learning. Additionally, the small sizes make them only marginally usable for state-of-the-art learning systems, such as deep learning. In order to give a better basis for development and evaluation of objective VQA methods, we have created a larger datasets of natural, real-world video sequences with corresponding subjective mean opinion scores (MOS) gathered through crowdsourcing. \u200b We took YFCC100m as a baseline database, consisting of 793436 Creative Commons (CC) video sequences, filtered them through multiple steps to ensure that the video sequences are representative of the whole spectrum of available video content, types of distortions, and subjective quality. The resulting 1200 videos are available to download, alongside the subjective data and evaluation of the best-performing techniques available for multiple video attributes. Namely, we have evaluated blur, colorfulness, contrast, spatial information, temporal information and video quality."}, {"id": "shapes-swarm-heuristics-based-adaptive-and-penalized-estimation-of-splines", "name": "SHAPES (Swarm Heuristics based Adaptive and Penalized Estimation of Splines)", "description": "SHAPES is a dataset of synthetic images designed to benchmark systems for understanding of spatial and logical relations among multiple objects. The dataset consists of complex questions about arrangements of colored shapes. The questions are built around compositions of concepts and relations, e.g. Is there a red shape above a circle? or Is a red shape blue?. Questions contain between two and four attributes, object types, or relationships. There are 244 questions and 15,616 images in total, with all questions having a yes and no answer (and corresponding supporting image). This eliminates the risk of learning biases."}, {"id": "fineaction", "name": "FineAction", "description": "FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. FineAction introduces new opportunities and challenges for temporal action localization, thanks to its distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes."}, {"id": "mdd-movie-dialog-dataset", "name": "MDD (Movie Dialog dataset)", "description": "Movie Dialog dataset (MDD) is designed to measure how well models can perform at goal and non-goal orientated dialog centered around the topic of movies (question answering, recommendation and discussion)."}, {"id": "fewsol-a-dataset-for-few-shot-object-learning-in-robotic-environments", "name": "FewSOL (A Dataset for Few-Shot Object Learning in Robotic Environments)", "description": "The Few-Shot Object Learning (FewSOL) dataset can be used for object recognition with a few images per object. It contains 336 real-world objects with 9 RGB-D images per object from different views. Object segmentation masks, object poses and object attributes are provided. In addition, synthetic images generated using 330 3D object models are used to augment the dataset.  FewSOL dataset can be used to study a set of few-shot object recognition problems such as classification, detection and segmentation, shape reconstruction, pose estimation, keypoint correspondences and attribute recognition. "}, {"id": "materials-project", "name": "Materials Project", "description": "The Materials Project is a collection of chemical compounds labelled with different attributes."}, {"id": "cbis-ddsm-curated-breast-imaging-subset-of-digital-database-for-screening-mammography", "name": "CBIS-DDSM (Curated Breast Imaging Subset of Digital Database for Screening Mammography)", "description": "This CBIS-DDSM (Curated Breast Imaging Subset of DDSM) is an updated and standardized version of the  Digital Database for Screening Mammography (DDSM) .  The DDSM is a database of 2,620 scanned film mammography studies. It contains normal, benign, and malignant cases with verified pathology information. The scale of the database along with ground truth validation makes the DDSM a useful tool in the development and testing of decision support systems. The CBIS-DDSM collection includes a subset of the DDSM data selected and curated by a trained mammographer.  The images have been decompressed and converted to DICOM format.  Updated ROI segmentation and bounding boxes, and pathologic diagnosis for training data are also included.  A manuscript describing how to use this dataset in detail is available at https://www.nature.com/articles/sdata2017177."}, {"id": "megadepth", "name": "MegaDepth", "description": "The MegaDepth dataset is a dataset for single-view depth prediction that includes 196 different locations reconstructed from COLMAP SfM/MVS."}, {"id": "algad-andy-lomas-generative-art-dataset", "name": "ALGAD (Andy Lomas Generative Art Dataset)", "description": "Repository of a generative art dataset by computer artist Andy Lomas."}, {"id": "xlent", "name": "XLEnt", "description": "XLEnt consists of parallel entity mentions in 120 languages aligned with English. These entity pairs were constructed by performing named entity recognition (NER) and typing on English sentences from mined sentence pairs. These extracted English entity labels and types were projected to the non-English sentences through word alignment. Word alignment was performed by combining three alignment signals ((1) word co-occurence alignment with FastAlign (2) semantic alignment using LASER embeddings, and (3) phonetic alignment via transliteration) into a unified word-alignment model. This lexical/semantic/phonetic alignment approach yielded more than 160 million aligned entity pairs in 120 languages paired with English. Recognizing that each English is often aligned to mulitple entities in different target languages, we can join on English entities to obtain aligned entity pairs that directly pair two non-English entities (e.g., Arabic-French)"}, {"id": "mimic-iii-the-medical-information-mart-for-intensive-care-iii", "name": "MIMIC-III (The Medical Information Mart for Intensive Care III)", "description": "The Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. Each record in the dataset includes ICD-9 codes, which identify diagnoses and procedures performed. Each code is partitioned into sub-codes, which often include specific circumstantial details. The dataset consists of 112,000 clinical reports records (average length 709.3 tokens) and 1,159 top-level ICD-9 codes. Each report is assigned to 7.6 codes, on average. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. "}, {"id": "nocaps", "name": "nocaps", "description": "The nocaps benchmark consists of 166,100 human-generated captions describing 15,100 images from the OpenImages validation and test sets."}, {"id": "refcoco", "name": "RefCOCO", "description": "This referring expression generation (REG) dataset was collected using the ReferitGame. In this two-player game, the first player is shown an image with a segmented target object and asked to write a natural language expression referring to the target object. The second player is shown only the image and the referring expression and asked to click on the corresponding object. If the players do their job correctly, they receive points and swap roles. If not, they are presented with a new object and image for description. Images in these collections were selected to contain two or more objects of the same object category. In the RefCOCO dataset, no restrictions are placed on the type of language used in the referring expressions. In a version of this dataset called RefCOCO+ players are disallowed from using location words in their referring expressions by adding \u201ctaboo\u201d words to the ReferItGame. This dataset was collected to obtain a referring expression dataset focsed on purely appearance based description, e.g., \u201cthe man in the yellow polka-dotted shirt\u201d rather than \u201cthe second man from the left\u201d, which tend to be more interesting from a computer vision based perspective and are independent of viewer perspective. RefCOCO consists of 142,209 refer expressions for 50,000 objects in 19,994 images, and RefCOCO+ has 141,564 expressions for 49,856 objects in 19,992 images."}, {"id": "raft-realworld-annotated-few-shot-tasks", "name": "RAFT (Realworld Annotated Few-shot Tasks)", "description": "The RAFT benchmark (Realworld Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment."}, {"id": "rs-haze", "name": "RS-Haze", "description": "A large-scale non-homogeneous remote sensing image dehazing dataset"}, {"id": "large-scale-clir-dataset", "name": "Large-Scale CLIR Dataset", "description": "The Large-Scale CLIR Dataset is a retrieval dataset built for Cross-Language Information Retrieval (CLIR). The dataset is derived from Wikipedia and contains more 2.8 million English single-sentence queries with relevant documents from 25 other selected languages."}, {"id": "syrip", "name": "SyRIP", "description": "SyRIP is a hybrid synthetic and real infant pose (SyRIP) dataset with small yet diverse real infant images as well as generated synthetic infant poses and (2) a multi-stage invariant representation learning strategy that could transfer the knowledge from the adjacent domains of adult poses and synthetic infant images into our fine-tuned domain-adapted infant pose"}, {"id": "animeceleb", "name": "AnimeCeleb", "description": "We present a novel Animation CelebHeads dataset (AnimeCeleb) to address an animation head reenactment. Different from previous animation head datasets, we utilize 3D animation models as the controllable image samplers, which can provide a large amount of head images with their corresponding detailed pose annotations. To facilitate a data creation process, we build a semi-automatic pipeline leveraging an open 3D computer graphics software with a developed annotation system. After training with the AnimeCeleb, recent head reenactment models produce high-quality animation head reenactment results, which are not achievable with existing datasets. Furthermore, motivated by metaverse application, we propose a novel pose mapping method and architecture to tackle a cross-domain head reenactment task. During inference, a user can easily transfer one's motion to an arbitrary animation head. Experiments demonstrate the usefulness of the AnimeCeleb to train animation head reenactment models, and the superiority of our cross-domain head reenactment model compared to state-of-the-art methods. Our dataset and code are available at https://github.com/kangyeolk/AnimeCeleb."}, {"id": "voc-mlt", "name": "VOC-MLT", "description": "We construct the long-tailed version of VOC  from its 2012 train-val set. It contains 1,142 images from 20 classes, with a maximum of 775 images per class and a minimum of 4 images per class.  The ratio of head, medium, and tail classes after splitting is 6:6:8. We evaluate the performance on VOC2007 test set with 4952 images."}, {"id": "transcg", "name": "TransCG", "description": "TransCG is the first large-scale real-world dataset for transparent object depth completion and grasping, which contains 57,715 RGB-D images of 51 transparent objects and many opaque objects captured from different perspectives (~240 viewpoints) of 130 scenes under real-world settings. The samples are captured by two different types of cameras (Realsense D435 & L515)."}, {"id": "sig53", "name": "Sig53", "description": "A dataset of 53 complex-valued signal modulation classes."}, {"id": "vehiclex", "name": "VehicleX", "description": "VehicleX is a large-scale synthetic dataset. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes."}, {"id": "bc4chemd-biocreative-iv-chemical-compound-and-drug-name-recognition", "name": "BC4CHEMD (BioCreative IV Chemical compound and drug name recognition)", "description": "Introduced by Krallinger et al. in The CHEMDNER corpus of chemicals and drugs and its annotation principles"}, {"id": "oumvlp", "name": "OUMVLP", "description": "The OU-ISIR Gait Database, Multi-View Large Population Dataset (OU-MVLP) is meant to aid research efforts in the general area of developing, testing and evaluating algorithms for cross-view gait recognition. The Institute of Scientific and Industrial Research (ISIR), Osaka University (OU) has copyright in the collection of gait video and associated data and serves as a distributor of the OU-ISIR Gait Database."}, {"id": "bovtext", "name": "BOVText", "description": "BOVText is a new large-scale benchmark dataset named Bilingual, Open World Video Text(BOVText), the first large-scale and multilingual benchmark for video text spotting in a variety of scenarios. All data are collected from KuaiShou and YouTube"}, {"id": "sun-seg-hard-unseen", "name": "SUN-SEG-Hard (Unseen)", "description": "The SUN-SEG dataset is a high-quality per-frame annotated VPS dataset, which includes 158,690 frames from the famous SUN dataset. It extends the labels with diverse types, i.e., object mask, boundary, scribble, polygon, and visual attribute. It also introduces the pathological information from the original SUN dataset, including pathological classification labels, location information, and shape information."}, {"id": "covid-19-image-data-collection", "name": "COVID-19 Image Data Collection", "description": "Contains hundreds of frontal view X-rays and is the largest public resource for COVID-19 image and prognostic data, making it a necessary resource to develop and evaluate tools to aid in the treatment of COVID-19."}, {"id": "voxforge", "name": "VoxForge", "description": "VoxForge is an open speech dataset that was set up to collect transcribed speech for use with Free and Open Source Speech Recognition Engines (on Linux, Windows and Mac)."}, {"id": "earthnet2021-earthnet2021-earth-surface-forecasting", "name": "EarthNet2021 (EarthNet2021: Earth Surface Forecasting)", "description": "Satellite images are snapshots of the Earth surface. We propose to forecast them. We frame Earth surface forecasting as the task of predicting satellite imagery conditioned on future weather. EarthNet2021 is a large dataset suitable for training deep neural networks on the task. It contains Sentinel~2 satellite imagery at $20$~m resolution, matching topography and mesoscale ($1.28$~km) meteorological variables packaged into $32000$ samples. Additionally we frame EarthNet2021 as a challenge allowing for model intercomparison. Resulting forecasts will greatly improve ($>\\times50$) over the spatial resolution found in numerical models. This allows localized impacts from extreme weather to be predicted, thus supporting downstream applications such as crop yield prediction, forest health assessments or biodiversity monitoring. Find data, code, and how to participate at www.earthnet.tech."}, {"id": "music21", "name": "Music21", "description": "Music21 is an untrimmed video dataset crawled by keyword query from Youtube. It contains music performances belonging to 21 categories. This dataset is relatively clean and collected for the purpose of training and evaluating visual sound source separation models."}, {"id": "ubnormal-university-of-bucharest-abnormal-videos", "name": "UBnormal (University of Bucharest Abnormal Videos)", "description": "UBnormal is a new supervised open-set benchmark composed of multiple virtual scenes for video anomaly detection. Unlike existing data sets, the data set introduces abnormal events annotated at the pixel level at training time, for the first time enabling the use of fully-supervised learning methods for abnormal event detection. To preserve the typical open-set formulation, the data set includes disjoint sets of anomaly types in the training and test collections of videos."}, {"id": "rafd-radboud-faces-database", "name": "RaFD (Radboud Faces Database)", "description": "The Radboud Faces Database (RaFD) is a set of pictures of 67 models (both adult and children, males and females) displaying 8 emotional expressions."}, {"id": "illness-dataset-illness-multi-domain-textual-dataset", "name": "Illness-dataset (Illness multi-domain textual dataset)", "description": "A dataset for evaluating text classification, domain adaptation, and active learning models. The dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer's, Parkinson's, Cancer, and Diabetes."}, {"id": "seadronessee-seadronessee-a-maritime-benchmark-for-detecting-humans-in-open-water", "name": "SeaDronesSee (SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water)", "description": "SeaDronesSee is a large-scale data set aimed at helping develop systems for Search and Rescue (SAR) using Unmanned Aerial Vehicles (UAVs) in maritime scenarios. Building highly complex autonomous UAV systems that aid in SAR missions requires robust computer vision algorithms to detect and track objects or persons of interest. This data set provides three sets of tracks: object detection, single-object tracking and multi-object tracking. Each track consists of its own data set and leaderboard."}, {"id": "zinc", "name": "ZINC", "description": "ZINC is a free database of commercially-available compounds for virtual screening. ZINC contains over 230 million purchasable compounds in ready-to-dock, 3D formats. ZINC also contains over 750 million purchasable compounds that can be searched for analogs."}, {"id": "pattern", "name": "PATTERN", "description": "PATTERN is a node classification tasks generated with Stochastic Block Models, which is widely used to model communities in social networks by modulating the intra- and extra-communities connections, thereby controlling the difficulty of the task. PATTERN tests the fundamental graph task of recognizing specific predetermined subgraphs."}, {"id": "snli-ve", "name": "SNLI-VE", "description": "Visual Entailment (VE) consists of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. SNLI-VE is a dataset for VE which is based on the Stanford Natural Language Inference corpus and Flickr30k dataset."}, {"id": "xquad", "name": "XQuAD", "description": "XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel across 11 languages."}, {"id": "snare", "name": "SNARE", "description": "SNARE, short for ShapeNet Annotated with Referring Expressions, is a benchmark requires a model to choose which of two objects is being referenced by a natural language description."}, {"id": "covid-cq", "name": "COVID-CQ", "description": "COVID-CQ is a stance data set of user-generated content on Twitter in the context of COVID-19."}, {"id": "doors-dataset-for-boulders-segmentation", "name": "DOORS (Dataset fOr bOuldeRs Segmentation)", "description": "DOORS  is a dataset designed for boulders recognition, centroid regression, segmentation, and navigation applications.  The dataset is divided into two sets:"}, {"id": "pathquestion", "name": "PathQuestion", "description": "Adopts two subsets of Freebase (Bollacker et al., 2008) as Knowledge Bases to construct the PathQuestion (PQ) and the PathQuestion-Large (PQL) datasets. Paths are extracted between two entities which span two hops (es \u2192 r1 \u2192 e1 \u2192 r2 \u2192 a, denoted by -2H) or three hops (es\u2192 r1 \u2192 e1 \u2192r2 \u2192 e2\u2192 r3 \u2192 a, denoted by -3H) and then generated natural language questions with templates. To make the generated questions analogical to real-world questions, paraphrasing templates and synonyms for relations are included by searching the Internet and two real-world datasets, WebQuestions (Berant et al., 2013) and WikiAnswers (Fader et al., 2013). In this way, the syntactic structure and surface wording of the generated questions have been greatly enriched."}, {"id": "dronecrowd", "name": "DroneCrowd", "description": "DroneCrowd is a benchmark for object detection, tracking and counting algorithms in drone-captured videos. It is a drone-captured large scale dataset formed by 112 video clips with 33,600 HD frames in various scenarios. Notably, it has annotations for 20,800 people trajectories with 4.8 million heads and several video-level attributes."}, {"id": "tip-2018", "name": "TIP 2018", "description": "The first large demoire dataset. The dataset contains 135,000 image pairs, each containing an image contaminated with moire patterns and its corresponding uncontaminated reference image."}, {"id": "deepmind-control-suite", "name": "DeepMind Control Suite", "description": "The DeepMind Control Suite (DMCS) is a set of simulated continuous control environments with a standardized structure and interpretable rewards. The tasks are written and powered by the MuJoCo physics engine, making them easy to identify. Control Suite tasks include Pendulum, Acrobot, Cart-pole, Cart-k-pole, Ball in cup, Point-mass, Reacher, Finger, Hooper, Fish, Cheetah, Walker, Manipulator, Manipulator extra, Stacker, Swimmer, Humanoid, Humanoid_CMU and LQR."}, {"id": "contactpose", "name": "ContactPose", "description": "ContactPose is a dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. "}, {"id": "casia-iris-complex", "name": "CASIA-Iris-Complex", "description": "Iris is considered one of the most accurate and reliable biometric modality. Iris is more stable and distinctive compared with fingerprint, face, voice, etc, and difficult to be replicated for spoof attacks. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from laboratory to field application is still a challenging task. In practical applications, the iris recognition system must face various unpredictable iris image degraded. For example, recognition of low-quality iris images, non-cooperative iris images, long-range iris images, and moving iris images are all huge problems in iris recognition. We believe that the first step in solving these problems is to design and develop a database of iris images that includes all of these degraded."}, {"id": "ski-pose-ptz-camera", "name": "Ski-Pose PTZ-Camera", "description": "This multi-view pant-tilt-zoom-camera (PTZ) dataset features competitive alpine skiers performing giant slalom runs. It provides labels for the skiers\u2019 3D poses in each frame, their projected 2D pose in all 20k images, and accurate per-frame calibration of the PTZ cameras. The dataset was collected by Sp\u00f6rri and Colleagues within his Habilitation at the Department of Sport Science and Kinesiology of the University of Salzburg [Sp\u00f6rri16], and was previously used as a reference in different methodological studies [Gilgien13, Gilgien14, Gilgien 15, Fasel16, Fasel18, Rhodin18]. Moreover, upon request the dataset would be available to interested researchers for further methodological-orientated research purposes."}, {"id": "big-bench-beyond-the-imitation-game-benchmark", "name": "BIG-bench (Beyond the Imitation Game Benchmark)", "description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. Big-bench include more than 200 tasks."}, {"id": "jnc-japanese-news-corpus", "name": "JNC (Japanese News Corpus)", "description": "The JNC data provides common supervision data for headline generation."}, {"id": "gait3d", "name": "Gait3D", "description": "Gait3D is a large-scale 3D representation-based gait recognition dataset. It contains 4,000 subjects and over 25,000 sequences extracted from 39 cameras in an unconstrained indoor scene."}, {"id": "lol-low-light-dataset", "name": "LOL (LOw-Light dataset)", "description": "The LOL dataset is composed of 500 low-light and normal-light image pairs and divided into 485 training pairs and 15 testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of 400\u00d7600."}, {"id": "tep-tennessee-eastman-process", "name": "TEP (Tennessee Eastman Process)", "description": "The original paper presented a model of the industrial chemical process named Tennessee Eastman Process and a model-based TEP simulator for data generation. The most widely used benchmark consists of 22 datasets, 21 of which (Fault 1\u201321) contain faults and 1 (Fault 0) is fault-free. It is available in repository. All datasets have training (500 samples) and testing (960 samples) parts: training part has healthy state observations, testing part begins right after training, and contains faults which appear after 8 h since the training part. Each dataset has 52 features or observation variables with a 3 min sampling rate for most of all."}, {"id": "pcsod", "name": "PCSOD", "description": "It is a new proposed dataset for point cloud salient object detection that has 2000 training samples and 872 testing samples."}, {"id": "birdvox-full-night", "name": "BirdVox-full-night", "description": "The BirdVox-full-night dataset contains 6 audio recordings, each about ten hours in duration. These recordings come from ROBIN autonomous recording units, placed near Ithaca, NY, USA during the fall 2015. They were captured on the night of September 23rd, 2015, by six different sensors, originally numbered 1, 2, 3, 5, 7, and 10. Andrew Farnsworth used the Raven software to pinpoint every avian flight call in time and frequency. He found 35402 flight calls in total. He estimates that about 25 different species of passerines (thrushes, warblers, and sparrows) are present in this recording. Species are not labeled in BirdVox-full-night, but it is possible to tell apart thrushes from warblers and sparrrows by looking at the center frequencies of their calls. The annotation process took 102 hours."}, {"id": "acronym", "name": "ACRONYM", "description": "A dataset for robot grasp planning based on physics simulation. The dataset contains 17.7M parallel-jaw grasps, spanning 8872 objects from 262 different categories, each labeled with the grasp result obtained from a physics simulator. "}, {"id": "rotowire", "name": "RotoWire", "description": "This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores. Summaries taken from rotowire.com are referred to as the \"rotowire\" data.  There are 4853 distinct rotowire summaries, covering NBA games played between 1/1/2014 and 3/29/2017; some games have multiple summaries. The summaries have been randomly split into training, validation, and test sets consisting of 3398, 727, and 728 summaries, respectively."}, {"id": "pubhealth", "name": "PUBHEALTH", "description": "PUBHEALTH is a comprehensive dataset for explainable automated fact-checking of public health claims. Each instance in the PUBHEALTH dataset has an associated veracity label (true, false, unproven, mixture). Furthermore each instance in the dataset has an explanation text field. The explanation is a justification for which the claim has been assigned a particular veracity label."}, {"id": "aerorit", "name": "AeroRIT", "description": "AeroRIT is a hyperspectral dataset to facilitate aerial hyperspectral scene understanding."}, {"id": "df20-danish-fungi-2020", "name": "DF20 (Danish Fungi 2020)", "description": "Danish Fungi 2020 (DF20) is a fine-grained dataset and benchmark. The dataset, constructed from observations submitted to the Danish Fungal Atlas, is unique in its taxonomy-accurate class labels, small number of errors, highly unbalanced long-tailed class distribution, rich observation metadata, and well-defined class hierarchy. DF20 has zero overlap with ImageNet, allowing unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints. "}, {"id": "advice-seeking-questions", "name": "Advice Seeking Questions", "description": "The Advice-Seeking Questions (ASQ) dataset is a collection of personal narratives with advice-seeking questions. The dataset has been split into train, test, heldout sets, with 8865, 2500, 10000 test instances each. This dataset is used to train and evaluate methods that can infer what is the advice-seeking goal behind a personal narrative. This task is formulated as a cloze test, where the goal is to identify which of two advice-seeking questions was removed from a given narrative."}, {"id": "pds-coco-photometrically-distorted-synthetic-coco", "name": "PDS-COCO (Photometrically Distorted Synthetic COCO)", "description": "Photometrically Distorted Synthetic COCO (PDS-COCO) dataset is a synthetically created dataset for homography estimation learning. The idea is exactly the same as in the Synthetic COCO (S-COCO) dataset with SSD-like image distortion added at the beginning of the whole procedure: the first step involves adjusting the brightness of the image using randomly picked value $\\delta_b \\in \\mathcal{U}(-32, 32)$. Next, contrast, saturation and hue noise is applied with the following values: $\\delta_c \\in \\mathcal{U}(0.5, 1.5)$, $\\delta_s \\in \\mathcal{U}(0.5, 1.5)$ and $\\delta_h \\in \\mathcal{U}(-18, 18)$. Finally, the color channels of the image are randomly swapped with a probability of $0.5$. Such a photometric distortion procedure is applied to the original image independently to create source and target candidates."}, {"id": "humaid-human-annotated-disaster-incidents-data", "name": "HumAID (Human-Annotated Disaster Incidents Data)", "description": "Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. "}, {"id": "buzzfeed-webis-fake-news-corpus-2016", "name": "BuzzFeed-Webis Fake News Corpus 2016", "description": "The BuzzFeed-Webis Fake News Corpus 16 comprises the output of 9 publishers in a week close to the US elections. Among the selected publishers are 6 prolific hyperpartisan ones (three left-wing and three right-wing), and three mainstream publishers (see Table 1). All publishers earned Facebook\u2019s blue checkmark, indicating authenticity and an elevated status within the network. For seven weekdays (September 19 to 23 and September 26 and 27), every post and linked news article of the 9 publishers was fact-checked by professional journalists at BuzzFeed. In total, 1,627 articles were checked, 826 mainstream, 256 left-wing and 545 right-wing. The imbalance between categories results from differing publication frequencies."}, {"id": "pacs-photo-art-cartoon-sketch", "name": "PACS (Photo-Art-Cartoon-Sketch)", "description": "PACS is an image dataset for domain generalization. It consists of four domains, namely Photo (1,670 images), Art Painting (2,048 images), Cartoon (2,344 images) and Sketch (3,929 images). Each domain contains seven categories."}, {"id": "viena2", "name": "VIENA2", "description": "Covers 5 generic driving scenarios, with a total of 25 distinct action classes. It contains more than 15K full HD, 5s long videos acquired in various driving conditions, weathers, daytimes and environments, complemented with a common and realistic set of sensor measurements. This amounts to more than 2.25M frames, each annotated with an action label, corresponding to 600 samples per action class. "}, {"id": "radgraph-radgraph-extracting-clinical-entities-and-relations-from-radiology-reports", "name": "RadGraph (RadGraph: Extracting Clinical Entities and Relations from Radiology Reports)", "description": "RadGraph is a dataset of entities and relations in radiology reports based on our novel information extraction schema, consisting of 600 reports with 30K radiologist annotations and 221K reports with 10.5M automatically generated annotations."}, {"id": "def-outnumbered-parallel-smac-def-outnumbered-parallel-20", "name": "Def_Outnumbered_parallel (SMAC+_Def_Outnumbered_parallel_20)", "description": "smac+ defense outnumbered scenario with parallel episodic buffer"}, {"id": "tqa-textbook-question-answering", "name": "TQA (Textbook Question Answering)", "description": "The TextbookQuestionAnswering (TQA) dataset is drawn from middle school science curricula. It consists of 1,076 lessons from Life Science, Earth Science and Physical Science textbooks. This includes 26,260 questions, including 12,567 that have an accompanying diagram."}, {"id": "csl-daily", "name": "CSL-Daily", "description": "CSL-Daily (Chinese Sign Language Corpus) is a large-scale continuous SLT dataset. It provides both spoken language translations and gloss-level annotations. The topic revolves around people's daily lives (e.g., travel, shopping, medical care), the most likely SLT application scenario."}, {"id": "nne", "name": "NNE", "description": "NNE is a dataset for Nested Named Entity Recognition in English Newswire"}, {"id": "isun", "name": "iSUN", "description": "iSUN is a ground truth of gaze traces on images from the SUN dataset. The collection is partitioned into 6,000 images for training, 926 for validation and 2,000 for test."}, {"id": "topv2-task-oriented-parsing-v2", "name": "TOPv2 (Task Oriented Parsing v2)", "description": "Task Oriented Parsing v2 (TOPv2) representations for intent-slot based dialog systems."}, {"id": "ape-automatic-post-editing", "name": "APE (Automatic Post-Editing)", "description": "APE is useful to evaluate Machine Translation automatic post-editing (APE), which is the task of improving the output of a blackbox MT system by automatically fixing its mistakes. The act of post-editing text can be fully specified as a sequence of delete and insert actions in given positions."}, {"id": "solar-power-solar-power-data-for-integration-studies-alabama", "name": "Solar-Power (Solar Power Data for Integration Studies (Alabama))", "description": "Solar Power Data for Integration Studies NREL's Solar Power Data for Integration Studies are synthetic solar photovoltaic (PV) power plant data points for the United States representing the year 2006."}, {"id": "sims4action", "name": "Sims4Action", "description": "[1] Let's Play for Action: Recognizing Activities of Daily Living by Learning from Life Simulation Video Games. Alina Roitberg, David Schneider, Aulia Djamal, Constantin Seibold, Simon Rei\u00df, Rainer Stiefelhagen, In International Conference on Intelligent Robots and Systems (IROS), 2021 (* denotes equal contribution.)"}, {"id": "spacenet-7-multi-temporal-urban-development-spacenet-dataset", "name": "SpaceNet 7 (Multi-Temporal Urban Development SpaceNet Dataset)", "description": "Satellite imagery analytics have numerous human development and disaster response applications, particularly when time series methods are involved. For example, quantifying population statistics is fundamental to 67 of the 232 United Nations Sustainable Development Goals, but the World Bank estimates that more than 100 countries currently lack effective Civil Registration systems. The SpaceNet 7 Multi-Temporal Urban Development Challenge aims to help address this deficit and develop novel computer vision methods for non-video time series data. In this challenge, participants will identify and track buildings in satellite imagery time series collected over rapidly urbanizing areas. The competition centers around a new open source dataset of Planet satellite imagery mosaics, which includes 24 images (one per month) covering ~100 unique geographies. The dataset will comprise over 40,000 square kilometers of imagery and exhaustive polygon labels of building footprints in the imagery, totaling over 10 million individual annotations. Challenge participants will be asked to track building construction over time, thereby directly assessing urbanization."}, {"id": "logodet-3k", "name": "LogoDet-3K", "description": "A logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. "}, {"id": "appbench-svbrdf-database-bonn", "name": "APPBENCH (SVBRDF Database Bonn)", "description": "A database of 56 high quality fabric material measurements, provided as carefully calibrated rectified HDR images, together with SVBRDF fits. Used in the Fabric Appearance Challange."}, {"id": "duorc", "name": "DuoRC", "description": "DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie."}, {"id": "skipp-d", "name": "SKIPP'D", "description": "Large-scale integration of photovoltaics (PV) into electricity grids is challenged by the intermittent nature of solar power. Sky-image-based solar forecasting using deep learning has been recognized as a promising approach to predicting the short-term fluctuations. However, there are few publicly available standardized benchmark datasets for image-based solar forecasting, which limits the comparison of different forecasting models and the exploration of forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy Images and Photovoltaic Power Generation Dataset. The dataset contains three years (2017-2019) of quality-controlled down-sampled sky images and PV power generation data that is ready-to-use for short-term solar forecasting using deep learning. In addition, to support the flexibility in research, we provide the high resolution, high frequency sky images and PV power generation data as well as the concurrent sky video footage. We also include a code base containing data processing scripts and baseline model implementations for researchers to reproduce our previous work and accelerate their research in solar forecasting."}, {"id": "nlb-neural-latents-benchmark", "name": "NLB (Neural Latents Benchmark)", "description": "Neural Latents is a benchmark for latent variable modeling of neural population activity. It consists of four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas."}, {"id": "ycbineoat-dataset", "name": "YCBInEOAT Dataset", "description": "A new dataset with significant occlusions related to object manipulation."}, {"id": "n-ucla-northwestern-ucla-multiview-action-3d-dataset", "name": "N-UCLA (Northwestern-UCLA Multiview Action 3D Dataset)", "description": "The Multiview 3D event dataset is capture by me and Xiaohan Nie in UCLA. it contains RGB, depth and human skeleton data captured simultaneously by three Kinect cameras. This dataset include 10 action categories: pick up with one hand, pick up with two hands, drop trash, walk around, sit down, stand up, donning, doffing, throw, carry. Each action is performed by 10 actors. This dataset contains data taken from a variety of viewpoints. The dataset can be found in part-1, part-2 part-3, part-4, part-5, part-6, part-7, part-8, part-9, part-10, part-11, part-12, part-13, part-14, part-15, part-16, We also created a version of the dataset that only contains RGB videos: RGB videos only."}, {"id": "pn-summary", "name": "pn-summary", "description": "Pn-summary is a dataset for Persian abstractive text summarization."}, {"id": "coco-wholebody", "name": "COCO-WholeBody", "description": "COCO-WholeBody is an extension of COCO dataset with whole-body annotations. There are 4 types of bounding boxes (person box, face box, left-hand box, and right-hand box) and 133 keypoints (17 for body, 6 for feet, 68 for face and 42 for hands) annotations for each person in the image."}, {"id": "manga109", "name": "Manga109", "description": "Manga109 has been compiled by the Aizawa Yamasaki Matsui Laboratory, Department of Information and Communication Engineering, the Graduate School of Information Science and Technology, the University of Tokyo. The compilation is intended for use in academic research on the media processing of Japanese manga. Manga109 is composed of 109 manga volumes drawn by professional manga artists in Japan. These manga were commercially made available to the public between the 1970s and 2010s, and encompass a wide range of target readerships and genres (see the table in Explore for further details.) Most of the manga in the compilation are available at the manga library \u201cManga Library Z\u201d (formerly the \u201cZeppan Manga Toshokan\u201d library of out-of-print manga)."}, {"id": "a-dataset-of-state-censored-tweets", "name": "A Dataset of State-Censored Tweets", "description": "This is a dataset of 583,437 tweets by 155,715 users that were censored between 2012-2020 July. It also contains 4,301 accounts that were censored in their entirety. Additionally, another set of tweets is related, consisting of 22,083,759 supplemental tweets made up of all tweets by users with at least one censored tweet as well as instances of other users retweeting the censored user."}, {"id": "nyuv2-nyu-depth-v2", "name": "NYUv2 (NYU-Depth V2)", "description": "The NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect. It features:"}, {"id": "nclt-north-campus-long-term-vision-and-lidar", "name": "NCLT (North Campus Long-Term Vision and LiDAR)", "description": "The NCLT dataset is a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan\u2019s North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g., pedestrians, bicyclists, and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g., falling leaves and snow), and long-term structural changes caused by construction projects."}, {"id": "speakingfaces", "name": "SpeakingFaces", "description": "SpeakingFaces is a publicly-available large-scale dataset developed to support multimodal machine learning research in contexts that utilize a combination of thermal, visual, and audio data streams; examples include human-computer interaction (HCI), biometric authentication, recognition systems, domain transfer, and speech recognition. SpeakingFaces is comprised of well-aligned high-resolution thermal and visual spectra image streams of fully-framed faces synchronized with audio recordings of each subject speaking approximately 100 imperative phrases."}, {"id": "ds-1000", "name": "DS-1000", "description": "DS-1000 is a code generation benchmark with a thousand data science questions spanning seven Python libraries that (1) reflects diverse, realistic, and practical use cases, (2) has a reliable metric, (3) defends against memorization by perturbing questions."}, {"id": "multivariate-mobility-paris", "name": "Multivariate-Mobility-Paris", "description": "The original dataset was provided by Orange telecom in France, which contains anonymized and aggregated human mobility data. The Multivariate-Mobility-Paris dataset comprises information from 2020-08-24 to 2020-11-04 (72 days during the COVID-19 pandemic), with time granularity of 30 minutes and spatial granularity of 6 coarse regions in Paris, France. In other words, it represents a multivariate time series dataset."}, {"id": "ford-av-dataset", "name": "Ford AV Dataset", "description": "A challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. "}, {"id": "knowledgenet", "name": "KnowledgeNet", "description": "KnowledgeNet is a benchmark dataset for the task of automatically populating a knowledge base (Wikidata) with facts expressed in natural language text on the web. KnowledgeNet provides text exhaustively annotated with facts, thus enabling the holistic end-to-end evaluation of knowledge base population systems as a whole, unlike previous benchmarks that are more suitable for the evaluation of individual subcomponents (e.g., entity linking, relation extraction)."}, {"id": "adience", "name": "Adience", "description": "The Adience dataset, published in 2014, contains 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups, partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few."}, {"id": "bffhq-gender-biased-ffhq-dataset", "name": "bFFHQ (Gender-biased FFHQ dataset)", "description": "Gender-biased FFHQ dataset (bFFHQ) has age as a target label and gender as a correlated bias, and the images are from the FFHQ dataset. The images include the dominant number of young women (i.e., aged 10-29) and old men (i.e., aged 40-59) in the training data."}, {"id": "wn18rr", "name": "WN18RR", "description": "WN18RR is a link prediction dataset created from WN18, which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples are obtained by inverting triples from the training set. Thus the WN18RR dataset is created to ensure that the evaluation dataset does not have inverse relation test leakage. In summary, WN18RR dataset contains 93,003 triples with 40,943 entities and 11 relation types."}, {"id": "cosmosqa", "name": "CosmosQA", "description": "CosmosQA is a large-scale dataset of 35.6K problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. It focuses on reading between the lines over a diverse collection of people\u2019s everyday narratives, asking questions concerning on the likely causes or effects of events that require reasoning beyond the exact text spans in the context."}, {"id": "isear-international-survey-on-emotion-antecedents-and-reactions", "name": "ISEAR (International Survey on Emotion Antecedents and Reactions)", "description": "Over a period of many years during the 1990s, a large group of psychologists all over the world collected data in the ISEAR project, directed by Klaus R. Scherer and Harald Wallbott. Student respondents, both psychologists and non-psychologists, were asked to report situations in which they had experienced all of 7 major emotions (joy, fear, anger, sadness, disgust, shame, and guilt). In each case, the questions covered the way they had appraised the situation and how they reacted. The final data set thus contained reports on seven emotions each by close to 3000 respondents in 37 countries on all 5 continents."}, {"id": "wmt-2016-news-wmt-2016-news-translation-task", "name": "WMT 2016 News (WMT 2016 News Translation Task)", "description": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Czech, German, Finnish, Romanian, Russian, Turkish) and additional 1500 sentences from each of the 5 languages translated to English. For Romanian a third of the test set were released as a development set instead. For Turkish additional 500 sentence development set was released. The sentences were selected from dozens of news websites and translated by professional translators. The training data consists of parallel corpora to train translation models, monolingual corpora to train language models and development sets for tuning. Some training corpora were identical from WMT 2015 (Europarl, United Nations, French-English 10\u2079 corpus, Common Crawl, Russian-English parallel data provided by Yandex, Wikipedia Headlines provided by CMU) and some were update (CzEng v1.6pre, News Commentary v11, monolingual news data). Additionally, the following new corpora were added: Romanian Europarl, SETIMES2 from OPUS for Romanian-English and Turkish-English, Monolingual data sets from CommonCrawl."}, {"id": "pgdp5k-plane-geometry-diagram-parsing-dataset", "name": "PGDP5K (Plane Geometry Diagram Parsing Dataset)", "description": "PGDP5K is a dataset consisting of 5000 diagram samples composed of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text types, labeled with more fine-grained annotations at primitive level, including primitive classes, locations and relationships, where 1,813 non-duplicated images are selected from the Geometry3K dataset and other 3,187 images are collected from three popular textbooks across grades 6-12 on mathematics curriculum websites by taking screenshots from PDF books."}, {"id": "replica", "name": "Replica", "description": "The Replica Dataset is a dataset of high quality reconstructions of a variety of indoor spaces. Each reconstruction has clean dense geometry, high resolution and high dynamic range textures, glass and mirror surface information, planar segmentation as well as semantic class and instance segmentation. "}, {"id": "esol-scaffold-scaffold-split-of-esol-dataset", "name": "ESOL(scaffold) (Scaffold split of ESOL dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "multiwoz-multi-domain-wizard-of-oz", "name": "MultiWOZ (Multi-domain Wizard-of-Oz)", "description": "The Multi-domain Wizard-of-Oz (MultiWOZ) dataset is a large-scale human-human conversational corpus spanning over seven domains, containing 8438 multi-turn dialogues, with each dialogue averaging 14 turns. Different from existing standard datasets like WOZ and DSTC2, which contain less than 10 slots and only a few hundred values, MultiWOZ has 30 (domain, slot) pairs and over 4,500 possible values. The dialogues span seven domains: restaurant, hotel, attraction, taxi, train, hospital and police."}, {"id": "digits-five", "name": "Digits-Five", "description": "Digits-Five is a collection of five most popular digit datasets, MNIST (mt) (55000 samples), MNIST-M (mm) (55000 samples), Synthetic Digits (syn) (25000 samples), SVHN (sv)(73257 samples), and USPS (up) (7438 samples). Each digit dataset includes a different style of 0-9 digit images."}, {"id": "ap-adversarial-paraphrase", "name": "AP (Adversarial Paraphrase)", "description": "This is a paraphrasing dataset created using the adversarial paradigm. A task was designed called the Adversarial Paraphrasing Task (APT) whose objective was to write sentences that mean the same as a given sentence but have as different syntactical and lexical properties as possible."}, {"id": "quora-question-pairs", "name": "Quora Question Pairs", "description": "Quora Question Pairs (QQP) dataset consists of over 400,000 question pairs, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other."}, {"id": "visual-genome", "name": "Visual Genome", "description": "Visual Genome contains Visual Question Answering data in a multi-choice setting. It consists of 101,174 images from MSCOCO with 1.7 million QA pairs, 17 questions per image on average. Compared to the Visual Question Answering dataset, Visual Genome represents a more balanced distribution over 6 question types: What, Where, When, Who, Why and How. The Visual Genome dataset also presents 108K images with densely annotated objects, attributes and relationships."}, {"id": "dri-corpus-dr-inventor-multi-layer-scientific-corpus", "name": "DRI Corpus (Dr. Inventor Multi-layer Scientific Corpus)", "description": "The Dr. Inventor Multi-Layer Scientific Corpus (DRI Corpus) includes 40 Computer Graphics papers, selected by domain experts. Each paper of the Corpus has been annotated by three annotators by providing the following layers of annotations, each one characterizing a core aspect of scientific publications:"}, {"id": "wic-tsv-words-in-context-target-sense-verification", "name": "WiC-TSV (Words-in-Context: Target Sense Verification)", "description": "WiC-TSV is a new multi-domain evaluation benchmark for Word Sense Disambiguation. More specifically, it is a framework for Target Sense Verification of Words in Context which grounds its uniqueness in the formulation as a binary classification task thus being independent of external sense inventories, and the coverage of various domains. This makes the dataset highly flexible for the evaluation of a diverse set of models and systems in and across domains. WiC-TSV provides three different evaluation settings, depending on the input signals provided to the model."}, {"id": "prid2011-person-re-id-2011", "name": "PRID2011 (Person RE-ID 2011)", "description": "PRID 2011 is a person reidentification dataset that provides multiple person trajectories recorded from two different static surveillance cameras, monitoring crosswalks and sidewalks. The dataset shows a clean background, and the people in the dataset are rarely occluded. In the dataset, 200 people appear in both views. Among the 200 people, 178 people have more than 20 appearances"}, {"id": "fuss-free-universal-sound-separation", "name": "FUSS (Free Universal Sound Separation)", "description": "The Free Universal Sound Separation (FUSS) dataset is a database of arbitrary sound mixtures and source-level references, for use in experiments on arbitrary sound separation. FUSS is based on FSD50K corpus."}, {"id": "ludb-lobachevsky-university-electrocardiography-database", "name": "LUDB (Lobachevsky University Electrocardiography Database)", "description": "Lobachevsky University Electrocardiography Database (LUDB) is an ECG signal database with marked boundaries and peaks of P, T waves and QRS complexes. The database consists of 200 10-second 12-lead ECG signal records representing different morphologies of the ECG signal. The ECGs were collected from healthy volunteers and patients of the Nizhny Novgorod City Hospital No 5 in 2017\u20132018. The patients had various cardiovascular diseases while some of them had pacemakers. The boundaries of P, T waves and QRS complexes were manually annotated by cardiologists for all 200 records. Also, each record is annotated with the corresponding diagnosis. The database can be used for educational purposes as well as for training and testing algorithms for ECG delineation, i.e. for automatic detection of boundaries and peaks of P, T waves and QRS complexes."}, {"id": "sick-sentences-involving-compositional-knowledge", "name": "SICK (Sentences Involving Compositional Knowledge)", "description": "The Sentences Involving Compositional Knowledge (SICK) dataset is a dataset for compositional distributional semantics. It includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson\u2019s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and video caption datasets before being paired up using some algorithm."}, {"id": "oxford5k-oxford-buildings", "name": "Oxford5k (Oxford Buildings)", "description": "Oxford5K is the Oxford Buildings Dataset, which contains 5062 images collected from Flickr. It offers a set of 55 queries for 11 landmark buildings, five for each landmark."}, {"id": "argoverse-2-sensor", "name": "Argoverse 2 Sensor", "description": "The Argoverse 2 Sensor Dataset is a collection of 1,000 scenarios with 3D object tracking annotations. Each sequence in our training and validation sets includes annotations for all objects within five meters of the \u201cdrivable area\u201d \u2014 the area in which it is possible for a vehicle to drive. The HD map for each scenario specifies the driveable area."}, {"id": "dbp15k", "name": "DBP15K", "description": "DBP15k contains four language-specific KGs that are respectively extracted from English (En), Chinese (Zh), French (Fr) and Japanese (Ja) DBpedia, each of which contains around 65k-106k entities. Three sets of 15k alignment labels are constructed to align entities between each of the other three languages and En."}, {"id": "openvidial", "name": "OpenViDial", "description": "OpenViDial is a large-scale open-domain dialogue dataset with visual contexts. The dialogue turns and visual contexts are extracted from movies and TV series, where each dialogue turn is paired with the corresponding visual context in which it takes place. OpenViDial contains a total number of 1.1 million dialogue turns, and thus 1.1 million visual contexts stored in images."}, {"id": "icdar-2013", "name": "ICDAR 2013", "description": "The ICDAR 2013 dataset consists of 229 training images and 233 testing images, with word-level annotations provided. It is the standard benchmark dataset for evaluating near-horizontal text detection."}, {"id": "worldtree", "name": "Worldtree", "description": "Worldtree is a corpus of explanation graphs, explanatory role ratings, and associated tablestore. It contains explanation graphs for 1,680 questions, and 4,950 tablestore rows across 62 semi-structured tables are provided. This data is intended to be paired with the AI2 Mercury Licensed questions."}, {"id": "cooll-controlled-on-off-loads-library", "name": "COOLL (Controlled On/Off Loads Library)", "description": "Controlled On/Off Loads Library (COOLL) is a dataset of high-sampled electrical current and voltage measurements representing individual appliances consumption. The measurements were taken in June 2016 in the PRISME laboratory of the University of Orl\u00e9ans, France. The appliances are mainly controllable appliances (i.e. we can precisely control their turn-on/off time instants). 42 appliances of 12 types were measured at a 100 kHz sampling frequency."}, {"id": "viva-vision-for-intelligent-vehicles-and-applications", "name": "VIVA (Vision for Intelligent Vehicles and Applications)", "description": "The VIVA challenge\u2019s dataset is a multimodal dynamic hand gesture dataset specifically designed with difficult settings of cluttered background, volatile illumination, and frequent occlusion for studying natural human activities in real-world driving settings. This dataset was captured using a Microsoft Kinect device, and contains 885 intensity and depth video sequences of 19 different dynamic hand gestures performed by 8 subjects inside a vehicle."}, {"id": "kitchen-scenes", "name": "Kitchen Scenes", "description": "Kitchen Scenes is a multi-view RGB-D dataset of nine kitchen scenes, each containing several objects in realistic cluttered environments including a subset of objects from the BigBird dataset. The viewpoints of the scenes are densely sampled and objects in the scenes are annotated with bounding boxes and in the 3D point cloud. "}, {"id": "spider-realistic", "name": "Spider-Realistic", "description": "Spider-Realistic dataset is used for evaluation in the paper \"Structure-Grounded Pretraining for Text-to-SQL\". The dataset is created based on the dev split of the Spider dataset (2020-06-07 version from https://yale-lily.github.io/spider). We manually modified the original questions to remove the explicit mention of column names while keeping the SQL queries unchanged to better evaluate the model's capability in aligning the NL utterance and the DB schema. For more details, please check our paper at https://arxiv.org/abs/2010.12773."}, {"id": "multirotor-gym", "name": "Multirotor-Gym", "description": "Multirotor gym environment for learning control policies for various unmanned aerial vehicles."}, {"id": "pie-pedestrian-intention-estimation", "name": "PIE (Pedestrian Intention Estimation)", "description": "PIE is a new dataset for studying pedestrian behavior in traffic. PIE contains over 6 hours of footage recorded in typical traffic scenes with on-board camera. It also provides accurate vehicle information from OBD sensor (vehicle speed, heading direction and GPS coordinates) synchronized with video footage. Rich spatial and behavioral annotations are available for pedestrians and vehicles that potentially interact with the ego-vehicle as well as for the relevant elements of infrastructure (traffic lights, signs and zebra crossings). There are over 300K labeled video frames with 1842 pedestrian samples making this the largest publicly available dataset for studying pedestrian behavior in traffic."}, {"id": "dirha-distant-speech-interaction-for-robust-home-applications", "name": "DIRHA (Distant-speech Interaction for Robust Home Applications)", "description": "DIRHA-English is a multi-microphone database composed of real and simulated sequences of 1-minute. The overall corpus is composed of different types of sequences including: 1) Phonetically-rich sentences; 2) WSJ 5-k utterances; 3) WSJ 20-k utterances; 4) Conversational speech (also including keywords and commands). The sequences are available for both UK and US English at 48 kHz. The DIRHA-English dataset offers the possibility to work with a very large number of microphone channels, to use of microphone arrays having different characteristics and to work considering different speech recognition tasks (e.g., phone-loop, keyword spotting, ASR with small and very large language models)."}, {"id": "taiga-corpus-an-open-source-corpus-for-machine-learning", "name": "Taiga Corpus (An open-source corpus for machine learning.)", "description": "Taiga is a corpus, where text sources and their meta-information are collected according to popular ML tasks."}, {"id": "hcu400", "name": "HCU400", "description": "The dataset consists of the features associated with 402 5-second sound samples. The 402 sounds range from easily identifiable everyday sounds to intentionally obscured artificial ones. The dataset aims to lower the barrier for the study of aural phenomenology as the largest available audio dataset to include an analysis of causal attribution. Each sample has been annotated with crowd-sourced descriptions, as well as familiarity, imageability, arousal, and valence ratings."}, {"id": "salient-object-subitizing-dataset", "name": "Salient Object Subitizing Dataset", "description": "A salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. "}, {"id": "sk-large", "name": "SK-LARGE", "description": "SK-LARGE is a benchmark dataset for object skeleton detection, built on the MS COCO dataset. It contains 1491 images, 746 for training and 745 for testing."}, {"id": "vinli-vietnamese-natural-language-inference-dataset", "name": "ViNLI (Vietnamese Natural Language Inference Dataset)", "description": "A large-scale and high-quality corpus is necessary for studies on NLI for Vietnamese, which can be considered a low-resource language. In this paper, we introduce ViNLI (Vietnamese Natural Language Inference), an open-domain and high-quality corpus for evaluating Vietnamese NLI models, which is created and evaluated with a strict process of quality control. ViNLI comprises over 30,000 human-annotated premise-hypothesis sentence pairs extracted from more than 800 online news articles on 13 distinct topics."}, {"id": "dcase-2018-task-4", "name": "DCASE 2018 Task 4", "description": "DCASE2018 Task 4 is a dataset for large-scale weakly labeled semi-supervised sound event detection in domestic environments. The data are YouTube video excerpts focusing on domestic context which could be used for example in ambient assisted living applications. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events...) and potential industrial applications. Specifically, the task employs a subset of \u201cAudioset: An Ontology And Human-Labeled Dataset For Audio Events\u201d by Google. Audioset consists of an expanding ontology of 632 sound event classes and a collection of 2 million human-labeled 10-second sound clips (less than 21% are shorter than 10-seconds) drawn from 2 million Youtube videos. The ontology is specified as a hierarchical graph of event categories, covering a wide range of human and animal sounds, musical instruments and genres, and common everyday environmental sounds. Task 4 focuses on a subset of Audioset that consists of 10 classes of sound events: speech, dog, cat, alarm bell ringing, dishes, frying, blender, running water, vacuum cleaner, electric shaver toothbrush."}, {"id": "chexpert", "name": "CheXpert", "description": "The CheXpert dataset contains 224,316 chest radiographs of 65,240 patients with both frontal and lateral views available. The task is to do automated chest x-ray interpretation, featuring uncertainty labels and radiologist-labeled reference standard evaluation sets."}, {"id": "cifar10-dvs", "name": "CIFAR10-DVS", "description": "CIFAR10-DVS is an event-stream dataset for object classification. 10,000 frame-based images that come from CIFAR-10 dataset are converted into 10,000 event streams with an event-based sensor, whose resolution is 128\u00d7128 pixels. The dataset has an intermediate difficulty with 10 different classes. The repeated closed-loop smooth (RCLS) movement of frame-based images is adopted to implement the conversion. Due to the transformation, they produce rich local intensity changes in continuous time which are quantized by each pixel of the event-based camera."}, {"id": "fmd-fluorescence-microscopy-denoising", "name": "FMD (Fluorescence Microscopy Denoising)", "description": "The Fluorescence Microscopy Denoising (FMD) dataset is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. Image averaging is used to effectively obtain ground truth images and 60,000 noisy images with different noise levels."}, {"id": "cfq-compositional-freebase-questions", "name": "CFQ (Compositional Freebase Questions)", "description": "A large and realistic natural language question answering dataset."}, {"id": "partial-reid", "name": "Partial-REID", "description": "Partial REID is a specially designed partial person reidentification dataset that includes 600 images from 60 people, with 5 full-body images and 5 occluded images per person. These images were collected on a university campus by 6 cameras from different viewpoints, backgrounds and different types of occlusion. The examples of partial persons in the Partial REID dataset are shown in the Figure."}, {"id": "nycbike1", "name": "NYCBike1", "description": "Bike flow data of New York City with grid 16x8."}, {"id": "3rscan", "name": "3RScan", "description": "A novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. "}, {"id": "wtw-wired-table-in-the-wild", "name": "WTW (Wired Table in the Wild)", "description": "WTW (Wired Table in the Wild)  is a large-scale dataset which includes well-annotated structure parsing of multiple style tables in several scenes like the photo, scanning files, web pages."}, {"id": "weatherbench", "name": "WeatherBench", "description": "A benchmark dataset for data-driven medium-range weather forecasting, a topic of high scientific interest for atmospheric and computer scientists alike. "}, {"id": "aflw2000-3d", "name": "AFLW2000-3D", "description": "AFLW2000-3D is a dataset of 2000 images that have been annotated with image-level 68-point 3D facial landmarks. This dataset is used for evaluation of 3D facial landmark detection models. The head poses are very diverse and often hard to be detected by a CNN-based face detector."}, {"id": "localized-narratives", "name": "Localized Narratives", "description": "We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning."}, {"id": "clinc150", "name": "CLINC150", "description": "This dataset is for evaluating the performance of intent classification systems in the presence of \"out-of-scope\" queries, i.e., queries that do not fall into any of the system-supported intent classes. The dataset includes both in-scope and out-of-scope data."}, {"id": "mnist", "name": "MNIST", "description": "The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field."}, {"id": "msr-actionpairs", "name": "MSR ActionPairs", "description": "This is a 3D action recognition dataset, also known as 3D Action Pairs dataset. The actions in this dataset are selected in pairs such that the two actions of each pair are similar in motion (have similar trajectories) and shape (have similar objects); however, the motion-shape relation is different. "}, {"id": "ava-activespeaker", "name": "AVA-ActiveSpeaker", "description": "Contains temporally labeled face tracks in video, where each face instance is labeled as speaking or not, and whether the speech is audible. This dataset contains about 3.65 million human labeled frames or about 38.5 hours of face tracks, and the corresponding audio. "}, {"id": "webcaricature-dataset", "name": "WebCaricature Dataset", "description": "Aims to facilitate research in caricature recognition. All the caricatures and face images were collected from the Web. Compared with two existing datasets, this dataset is much more challenging, with a much greater number of available images, artistic styles and larger intra-personal variations. "}, {"id": "bcopa-ce-a-balanced-copa-test-set-with-cause-effect-as-alternatives", "name": "BCOPA-CE (A Balanced COPA Test Set with cause-effect as alternatives)", "description": "We provide the BCOPA-CE test set, which has balanced token distribution in the correct and wrong alternatives and increases the difficulty of being aware of cause and effect."}, {"id": "hifimask-casia-surf-hifimask", "name": "HiFiMask (CASIA-SURF HiFiMask)", "description": "HiFiMask is a large-scale High-Fidelity Mask dataset, namely CASIA-SURF HiFiMask (briefly HiFiMask). It contains a total amount of 54,600 videos are recorded from 75 subjects with 225 realistic masks by 7 new kinds of sensors."}, {"id": "germeval-2021-toxic-engaging-fact-claiming-comments-test-set", "name": "GermEval 2021 - Toxic, Engaging, & Fact-Claiming Comments test set", "description": "The data set was provided as part of the GermEval 2021 competition for the identification of toxic, engaging, and fact-claiming comments."}, {"id": "bbbp-scaffold-scaffold-split-of-bbbp-dataset", "name": "BBBP(scaffold) (Scaffold split of BBBP  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "advio", "name": "ADVIO", "description": "Provides a wide range of raw sensor data that is accessible on almost any modern-day smartphone together with a high-quality ground-truth track. "}, {"id": "airsim", "name": "AirSim", "description": "AirSim is a simulator for drones, cars and more, built on Unreal Engine. It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, there exists an experimental version for a Unity plugin."}, {"id": "lani", "name": "Lani", "description": "LANI is a 3D navigation environment and corpus, where an agent navigates between landmarks. Lani contains 27,965 crowd-sourced instructions for navigation in an open environment. Each datapoint includes an instruction, a human-annotated ground-truth demonstration trajectory, and an environment with various landmarks and lakes. The dataset train/dev/test split is 19,758/4,135/4,072. Each environment specification defines placement of 6\u201313 landmarks within a square grass field of size 50m\u00d750m."}, {"id": "light-field-material", "name": "Light-Field Material", "description": "This is a 4D light-field dataset of materials. The dataset contains 12 material categories, each with 100 images taken with a Lytro Illum, from which we extract about 30,000 patches in total."}, {"id": "mengeros", "name": "MengeROS", "description": "MengeROS is an open-source crowd simulation tool for robot navigation that integrates Menge with ROS. It extends Menge to introduce one or more robot agents into a crowd of pedestrians. Each robot agent is controlled by external ROS-compatible controllers. MengeROS has been used to simulate crowds with up to 1000 pedestrians and 20 robots."}, {"id": "5dof-gb-interpolation-five-degree-of-freedom-grain-boundary-interpolation", "name": "5DOF GB Interpolation (Five Degree-of-Freedom Grain Boundary Interpolation)", "description": "These are larger MATLAB .mat files required for reproducing plots from the sgbaird-5DOF/interp repository for grain boundary property interpolation. gitID-0055bee_uuID-475a2dfd_paper-data6.mat contains multiple trials of five degree-of-freedom interpolation model runs for various interpolation schemes. gpr46883_gitID-b473165_puuID-50ffdcf6_kim-rng11.mat contains a Gaussian Process Regression model trained on 46883 Fe simulation GBs. See Five degree-of-freedom property interpolation of arbitrary grain boundaries via Voronoi fundamental zone framework DOI: 10.1016/j.commatsci.2021.110756 for the peer-reviewed, published version of the paper."}, {"id": "auxai", "name": "AuxAI", "description": "AuxAI is a distantly supervised dataset for acronym identification."}, {"id": "arendt", "name": "Arendt", "description": "We have created a NER dataset from the digital edition \"Sechs Essays\" by Hannah Arendt. It consists of 23 documents from the period 1932-1976, which are available as TEI files online (see https://hannah-arendt-edition.net/3p.html?lang=de). "}, {"id": "banking77-oos", "name": "BANKING77-OOS", "description": "A dataset with a single banking domain, includes both general Out-of-Scope (OOD-OOS) queries and In-Domain but Out-of-Scope (ID-OOS) queries, where ID-OOS queries are semantically similar intents/queries with in-scope intents.  BANKING77 originally includes 77 intents. BANKING77-OOS includes 50 in-scope intents in this dataset, and the ID-OOS queries are built up based on 27 held-out in-scope intents."}, {"id": "composable-activities-dataset", "name": "Composable activities dataset", "description": "The Composable activities dataset consists of 693 videos that contain activities in 16 classes performed by 14 actors. Each activity is composed of 3 to 11 atomic actions. RGB-D data for each sequence is captured using a Microsoft Kinect sensor and estimate position of relevant body joints."}, {"id": "monuseg", "name": "MoNuSeg", "description": "The dataset for this challenge was obtained by carefully annotating tissue images of several patients with tumors of different organs and who were diagnosed at multiple hospitals. This dataset was created by downloading H&E stained tissue images captured at 40x magnification from TCGA archive. H&E staining is a routine protocol to enhance the contrast of a tissue section and is commonly used for tumor assessment (grading, staging, etc.). Given the diversity of nuclei appearances across multiple organs and patients, and the richness of staining protocols adopted at multiple hospitals, the training datatset will enable the development of robust and generalizable nuclei segmentation techniques that will work right out of the box."}, {"id": "pile-of-law", "name": "Pile of Law", "description": "Pile of Law is a \u223c256GB (and growing) dataset of legal and administrative data which can be used for assessing norms on data sanitization across legal and administrative settings."}, {"id": "ctpelvic1k", "name": "CTPelvic1K", "description": "Curates a large pelvic CT dataset pooled from multiple sources and different manufacturers, including 1, 184 CT volumes and over 320, 000 slices with different resolutions and a variety of the above-mentioned appearance variations."}, {"id": "behave", "name": "BEHAVE", "description": "BEHAVE is a full body human-object interaction dataset with multi-view RGBD frames and corresponding 3D SMPL and object fits along with the annotated contacts between them. Dataset contains ~15k frames at 5 locations with 8 subjects performing a wide range of interactions with 20 common objects."}, {"id": "itop-invariant-top-view-dataset", "name": "ITOP (Invariant-Top View Dataset)", "description": "The ITOP dataset consists of 40K training and 10K testing depth images for each of the front-view and top-view tracks. This dataset contains depth images with 20 actors who perform 15 sequences each and is recorded by two Asus Xtion Pro cameras. The ground-truth of this dataset is the 3D coordinates of 15 body joints."}, {"id": "relative-size", "name": "Relative Size", "description": "The Relative Size dataset contains 486 object pairs between 41 physical objects. Size comparisons are not available for all pairs of objects (e.g. bird and watermelon) because for some pairs humans cannot determine which object is bigger."}, {"id": "cable-tv-news", "name": "Cable TV News", "description": "Cable TV news is a data set of nearly 24/7 video, audio, and text captions from three U.S. cable TV networks (CNN, FOX, and MSNBC) from January 2010 to July 2019. Using machine learning tools, the authors detect faces in 244,038 hours of video, label each face's presented gender, identify prominent public figures, and align text captions to audio."}, {"id": "psychometric-nlp", "name": "Psychometric NLP", "description": "Psychometric NLP is a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. The dataset aligns user text with their survey-based response items and encompasses survey-based psychometric measures, accompanying user-generated text, and self-reported demographic information, including race, sex, age, income, and education from 8,502 respondents."}, {"id": "cookie", "name": "Cookie", "description": "The dataset is constructed from an Amazon review corpus by integrating both user-agent dialogue and custom knowledge graphs for recommendation."}, {"id": "tas500", "name": "TAS500", "description": "TAS500 is a semantic segmentation dataset for autonomous driving in unstructured environments. TAS500 offers fine-grained vegetation and terrain classes to learn drivable surfaces and natural obstacles in outdoor scenes effectively."}, {"id": "autoencoder-paraphrase-dataset-aepd", "name": "Autoencoder Paraphrase Dataset (AEPD)", "description": "This is a benchmark for neural paraphrase detection, to differentiate between original and machine-generated content."}, {"id": "sparc-semantic-parsing-in-context", "name": "SParC (Semantic Parsing in Context)", "description": "SParC is a large-scale dataset for complex, cross-domain, and context-dependent (multi-turn) semantic parsing and text-to-SQL task (interactive natural language interfaces for relational databases)."}, {"id": "chinese-classifier", "name": "Chinese Classifier", "description": "Classifiers are function words that are used to express quantities in Chinese and are especially difficult for language learners. This dataset of Chinese Classifiers can be used to predict Chinese classifiers from context. The dataset contains a large collection of example sentences for Chinese classifier usage derived from three language corpora (Lancaster Corpus of Mandarin Chinese, UCLA Corpus of Written Chinese and Leiden Weibo Corpus). The data was cleaned and processed for a context-based classifier prediction task."}, {"id": "wisconsin-48-32-20-fixed-splits", "name": "Wisconsin (48%/32%/20% fixed splits)", "description": "Node classification on Wisconsin  with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "3d-front-human", "name": "3D FRONT HUMAN", "description": "3D FRONT HUMAN is a dataset that extends the large-scale synthetic scene dataset 3D-FRONT. Specifically, the 3D scenes with humans, i.e., non-contact humans (a sequence of walking motion and standing humans) as well as contact humans (sitting, touching, and lying humans). 3D FRONT HUMAN contains four room types: 1) 5689 bedrooms, 2) 2987 living rooms, 3) 2549 dining rooms and 4) 679 libraries. We use 21 object categories for the bedrooms, 24 for the living and dining rooms, and 25 for the libraries."}, {"id": "librimix", "name": "LibriMix", "description": "LibriMix is an open-source alternative to wsj0-2mix. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. "}, {"id": "cnn-daily-mail", "name": "CNN/Daily Mail", "description": "CNN/Daily Mail is a dataset for text summarization. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the fill-in the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites."}, {"id": "jarvis-dft", "name": "JARVIS-DFT", "description": "JARVIS-DFT is a repository of density functional theory based calculation data for materials."}, {"id": "techqa-the-techqa-dataset", "name": "TechQA (The TechQA Dataset)", "description": "TECHQA is a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size \u2013 600 training, 310 dev, and 490 evaluation question/answer pairs \u2013 thus reflecting the cost of creating large labeled datasets with actual data. Consequently, TECHQA is meant to stimulate research in domain adaptation rather than being a resource to build QA systems from scratch. The dataset was obtained by crawling the IBM Developer and IBM DeveloperWorks forums for questions with accepted answers that appear in a published IBM Technote\u2014a technical document that addresses a specific technical issue."}, {"id": "superglue", "name": "SuperGLUE", "description": "SuperGLUE is a benchmark dataset designed to pose a more rigorous test of language understanding than GLUE. SuperGLUE has the same high-level motivation as GLUE: to provide a simple, hard-to-game measure of progress toward general-purpose language understanding technologies for English. SuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around eight language understanding tasks, drawing on existing data, accompanied by a single-number performance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:"}, {"id": "places", "name": "Places", "description": "The Places dataset is proposed for scene recognition and contains more than 2.5 million images covering more than 205 scene categories with more than 5,000 images per category."}, {"id": "mcscript", "name": "MCScript", "description": "MCScript is used as the official dataset of SemEval2018 Task11. This dataset constructs a collection of text passages about daily life activities and a series of questions referring to each passage, and each question is equipped with two answer choices. The MCScript comprises 9731, 1411, and 2797 questions in training, development, and test set respectively."}, {"id": "beir-benchmarking-ir", "name": "BEIR (Benchmarking IR)", "description": "BEIR (Benchmarking IR) is an heterogeneous benchmark containing different information retrieval (IR) tasks. Through BEIR, it is possible to systematically study the zero-shot generalization capabilities of multiple neural retrieval approaches."}, {"id": "earnings-call", "name": "Earnings Call", "description": "The Earning Calls dataset consists of processed earning conference calls data (text and audio). It can be used to predict financial risk from both textual and vocal features from conference calls."}, {"id": "suncg", "name": "SUNCG", "description": "SUNCG is a large-scale dataset of synthetic 3D scenes with dense volumetric annotations."}, {"id": "wn18-wordnet18", "name": "WN18 (WordNet18)", "description": "The WN18 dataset has 18 relations scraped from WordNet for roughly 41,000 synsets, resulting in 141,442 triplets. It was found out that a large number of the test triplets can be found in the training set with another relation or the inverse relation. Therefore, a new version of the dataset WN18RR has been proposed to address this issue."}, {"id": "flores-facebook-low-resource-mt-benchmark", "name": "FLoRes (Facebook Low Resource MT Benchmark)", "description": "FLoRes is a benchmark dataset for machine translation between English and four low resource languages, Nepali, Sinhala, Khmer and Pashto, based on sentences translated from Wikipedia."}, {"id": "n-mnist-neuromorphic-mnist", "name": "N-MNIST (Neuromorphic-MNIST)", "description": "Brief Description The Neuromorphic-MNIST (N-MNIST) dataset is a spiking version of the original frame-based MNIST dataset. It consists of the same 60 000 training and 10 000 testing samples as the original MNIST dataset, and is captured at the same visual scale as the original MNIST dataset (28x28 pixels). The N-MNIST dataset was captured by mounting the ATIS sensor on a motorized pan-tilt unit and having the sensor move while it views MNIST examples on an LCD monitor as shown in this video. A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset."}, {"id": "biwi-3d-audiovisual-corpus-of-affective-communication-b3d-ac-2-biwi-3d", "name": "Biwi 3D Audiovisual Corpus of Affective Communication - B3D(AC)^2 (BIWI 3D)", "description": "BIWI 3D corpus comprises a total of 1109 sentences uttered by 14 native English speakers (6 males and 8 females). A real time 3D scanner and a professional microphone were used to capture the facial movements and the speech of the speakers. The dense dynamic face scans were acquired at 25 frames per second and the RMS error in the 3D reconstruction is about 0.5 mm. In order to ease automatic speech segmentation, we carried out the recordings in a anechoic room, with walls covered by sound wave-absorbing materials."}, {"id": "concepticon-concepticon-a-resource-for-the-linking-of-concept-lists", "name": "Concepticon (Concepticon. A Resource for the Linking of Concept Lists)", "description": "This resource, our Concepticon, links concept labels from different conceptlists to concept sets. Each concept set is given a unique identifier, a unique label, and a human-readable definition. Concept sets are further structured by defining different relations between the concepts, as you can see in the graphic to the right, which displays the relations between concept sets linked to the concept set SIBLING. The resource can be used for various purposes. Serving as a rich reference for new and existing databases in diachronic and synchronic linguistics, it allows researchers a quick access to studies on semantic change, cross-linguistic polysemies, and semantic associations."}, {"id": "endoslam-endoscopic-slam-dataset", "name": "EndoSLAM (Endoscopic SLAM dataset)", "description": "The endoscopic SLAM dataset (EndoSLAM) is a dataset for depth estimation approach for endoscopic videos. It consists of both ex-vivo and synthetically generated data. The ex-vivo part of the dataset includes standard as well as capsule endoscopy recordings. The dataset is divided into 35 sub-datasets. Specifically, 18, 5 and 12 sub-datasets exist for colon, small intestine and stomach respectively."}, {"id": "mlb-youtube-dataset", "name": "MLB-YouTube Dataset", "description": "The MLB-YouTube dataset is a new, large-scale dataset consisting of 20 baseball games from the 2017 MLB post-season available on YouTube with over 42 hours of video footage. The dataset consists of two components: segmented videos for activity recognition and continuous videos for activity classification. It is quite challenging as it is created from TV broadcast baseball games where multiple different activities share the camera angle. Further, the motion/appearance difference between the various activities is quite small."}, {"id": "visal", "name": "ViSal", "description": "DataViSal.rar (including the ground truth data) is our new collected dataset for the following paper."}, {"id": "memento10k", "name": "Memento10k", "description": "Memorability dataset with 10000 3-second videos. Each video has upwards of 90 human annotations, and the split-half consistency of this dataset is 0.73 (best in class for video memorabilty datasets)."}, {"id": "tum-rgb-d", "name": "TUM RGB-D", "description": "TUM RGB-D is an RGB-D dataset. It contains the color and depth images of a Microsoft Kinect sensor along the ground-truth trajectory of the sensor. The data was recorded at full frame rate (30 Hz) and sensor resolution (640x480). The ground-truth trajectory was obtained from a high-accuracy motion-capture system with eight high-speed tracking cameras (100 Hz)."}, {"id": "ps-battles", "name": "PS-Battles", "description": "The PS-Battles dataset is gathered from a large community of image manipulation enthusiasts and provides a basis for media derivation and manipulation detection in the visual domain. The dataset consists of 102'028 images grouped into 11'142 subsets, each containing the original image as well as a varying number of manipulated derivatives."}, {"id": "sphere", "name": "SPHERE", "description": "The dataset for the SPHERE challenge consists on a multimodal activity recognition dataset consisting of accelerometer, RGB-D and environmental data. Accelerometer is samplled at 20 Hz and given in its raw format. Raw video is not given in order to preserve anonymity of the participants. Instead, extracted features that relate to the centre of mass and bounding box of the identified persons are provided. Environmental data consists of Passive Infra-Red (PIR) sensors, and these is given in raw format."}, {"id": "flickr-audio-caption-corpus", "name": "Flickr Audio Caption Corpus", "description": "The Flickr 8k Audio Caption Corpus contains 40,000 spoken captions of 8,000 natural images. It was collected in 2015 to investigate multimodal learning schemes for unsupervised speech pattern discovery. For a description of the corpus, see:"}, {"id": "rwth-phoenix-weather-2014", "name": "RWTH-PHOENIX-Weather 2014", "description": "The signing is recorded by a stationary color camera placed in front of the sign language interpreters. Interpreters wear dark clothes in front of an artificial grey background with color transition. All recorded videos are at 25 frames per second and the size of the frames is 210 by 260 pixels. Each frame shows the interpreter box only."}, {"id": "opensubtitles", "name": "OpenSubtitles", "description": "OpenSubtitles is collection of multilingual parallel corpora. The dataset is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages."}, {"id": "visualmrc-visualmrc-machine-reading-comprehension-on-document-images", "name": "VisualMRC (VisualMRC: Machine Reading Comprehension on Document Images)", "description": "VisualMRC is a visual machine reading comprehension dataset that proposes a task: given a question and a document image, a model produces an abstractive answer."}, {"id": "sun09", "name": "SUN09", "description": "The SUN09 dataset consists of 12,000 annotated images with more than 200 object categories. It consists of natural, indoor and outdoor images. Each image contains an average of 7 different annotated objects and the average occupancy of each object is 5% of image size. The frequencies of object categories follow a power law distribution."}, {"id": "comma-2k19", "name": "comma 2k19", "description": "comma 2k19 is a dataset of over 33 hours of commute in California's 280 highway. This means 2019 segments, 1 minute long each, on a 20km section of highway driving between California's San Jose and San Francisco. The dataset was collected using comma EONs that have sensors similar to those of any modern smartphone including a road-facing camera, phone GPS, thermometers and a 9-axis IMU. "}, {"id": "realfacedb", "name": "RealFaceDB", "description": "The dataset contains patches of facial reflectance as described in the paper, namely the diffuse albedo, diffuse normals, specular albedo, specular normals, as well as the shape in UV space. For the shape, reconstructed meshes have been registered to a common topology and the XYZ values of the points have been mapped to the RGB in UV coordinates and interpolated to complete the UV map. From the complete UV maps of 6144x4096 pixels, patches of 512x512 pixels have been sampled. The dataset contains 7500 such patches (1500 of each datatype) that are anonymized, randomized and sampled so that they do not contain identifiable features."}, {"id": "oktoberfest-food-dataset", "name": "Oktoberfest Food Dataset", "description": "A realistic, diverse, and challenging dataset for object detection on images. The data was recorded at a beer tent in Germany and consists of 15 different categories of food and drink items. "}, {"id": "tju-dhd", "name": "TJU-DHD", "description": "TJU-DHD is a high-resolution dataset for object detection and pedestrian detection. The dataset contains 115,354 high-resolution images (52% images have a resolution of 1624\u00d71200 pixels and 48% images have a resolution of at least 2,560\u00d71,440 pixels) and 709,330 labelled objects in total with a large variance in scale and appearance."}, {"id": "uspto-50k", "name": "USPTO-50k", "description": "Subset and preprocessed version of Chemical reactions from US patents (1976-Sep2016) by Daniel Lowe. It includes 50K randomly selected reactions that was later classified into 10 reaction classes by Nadine Schneider et al."}, {"id": "mall-mall-dataset", "name": "Mall (Mall Dataset)", "description": "The Mall is a dataset for crowd counting and profiling research. Its images are collected from publicly accessible webcam. It mainly includes 2,000 video frames, and the head position of every pedestrian in all frames is annotated. A total of more than 60,000 pedestrians are annotated in this dataset."}, {"id": "pems-bay-point-missing", "name": "PEMS-BAY Point Missing", "description": "The original dataset from Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting contains 6 months of traffic readings from 01/01/2017 to 05/31/2017 collected every 5 minutes by 325 traffic sensors in San Francisco Bay Area. The measurements are provided by California Transportation Agencies (CalTrans) Performance Measurement System (PeMS)."}, {"id": "boolq-boolean-questions", "name": "BoolQ (Boolean Questions)", "description": "BoolQ is a question answering dataset for yes/no questions containing 15942 examples. These questions are naturally occurring \u2013 they are generated in unprompted and unconstrained settings. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context."}, {"id": "ailabs1k7-ailabs-tw-pop1k7", "name": "ailabs1k7 (AIlabs.tw Pop1K7)", "description": "https://github.com/YatingMusic/compound-word-transformer"}, {"id": "dsbi-double-sided-braille-image", "name": "DSBI (Double-Sided Braille Image)", "description": "The Double-Sided Braille Image dataset (DSBI) is a large-scale dataset for Braille image recognition. It has detailed Braille recto dots, verso dots and Braille cells annotation."}, {"id": "carer-contextualized-affect-representations-for-emotion-recognition", "name": "CARER (Contextualized Affect Representations for Emotion Recognition)", "description": "CARER is an emotion dataset collected through noisy labels, annotated via distant supervision as in (Go et al., 2009). "}, {"id": "mind-microsoft-news-dataset", "name": "MIND (MIcrosoft News Dataset)", "description": "MIcrosoft News Dataset (MIND) is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of Microsoft News website. The mission of MIND is to serve as a benchmark dataset for news recommendation and facilitate the research in news recommendation and recommender systems area."}, {"id": "exposure-errors", "name": "Exposure-Errors", "description": "A dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image."}, {"id": "kelm", "name": "KELM", "description": "KELM is a large-scale synthetic corpus of Wikidata KG as natural text."}, {"id": "ihdp", "name": "IHDP", "description": "The Infant Health and Development Program (IHDP) is a randomized controlled study designed to evaluate the effect of home visit from specialist doctors on the cognitive test scores of premature infants. The datasets is first used for benchmarking treatment effect estimation algorithms in Hill [35], where selection bias is induced by removing non-random subsets of the treated individuals to create an observational dataset, and the outcomes are generated using the original covariates and treatments. It contains 747 subjects and 25 variables."}, {"id": "kosp2e", "name": "Kosp2e", "description": "Kosp2e (read as `kospi'), is a corpus that allows Korean speech to be translated into English text in an end-to-end manner"}, {"id": "line-coverage-dataset", "name": "Line Coverage Dataset", "description": "The dataset contains road networks taken from 50 most populous cities in the world. The road networks are obtained using OpenStreetMap. These road networks are used to benchmark routing algorithms on graphs."}, {"id": "squirrel-60-20-20-random-splits", "name": "Squirrel (60%/20%/20% random splits)", "description": "Node classification on Squirrel with 60%/20%/20% random splits for training/validation/test."}, {"id": "distinctions-646", "name": "Distinctions-646", "description": "Dinstinctions-646 are composed of 646 foreground images with manually annotated alpha mattes"}, {"id": "viggo", "name": "ViGGO", "description": "The ViGGO corpus is a set of 6,900 meaning representation to natural language utterance pairs in the video game domain. The meaning representations are of 9 different dialogue acts."}, {"id": "amt-objects", "name": "AMT Objects", "description": "AMT Objects is a large dataset of object centric videos suitable for training and benchmarking models for generating 3D models of objects from a small number of photos of the objects. The dataset consists of multiple views of a large collection of object instances."}, {"id": "casia-b", "name": "CASIA-B", "description": "CASIA-B is a large multiview gait database, which is created in January 2005. There are 124 subjects, and the gait data was captured from 11 views. Three variations, namely view angle, clothing and carrying condition changes, are separately considered. Besides the video files, we still provide human silhouettes extracted from video files. The detailed information about Dataset B and an evaluation framework can be found in this paper ."}, {"id": "sd-198", "name": "SD-198", "description": "The SD-198 dataset contains 198 different diseases from different types of eczema, acne and various cancerous conditions. There are 6,584 images in total. A subset include the classes with more than 20 image samples, namely SD-128.\""}, {"id": "pmindia", "name": "PMIndia", "description": "Consists of parallel sentences which pair 13 major languages of India with English. The corpus includes up to 56000 sentences for each language pair."}, {"id": "tdcommons-therapeutics-data-commons", "name": "tdcommons (Therapeutics Data Commons)", "description": "Therapeutics Data Commons is an open-science initiative with AI/ML-ready datasets and AI/ML tasks for therapeutics, spanning the discovery and development of safe and effective medicines. TDC provides an ecosystem of tools, libraries, leaderboards, and community resources, including data functions, strategies for systematic model evaluation, meaningful data splits, data processors, and molecule generation oracles. All resources are integrated via an open Python library."}, {"id": "voxlingua107", "name": "VoxLingua107", "description": "VoxLingua107 is a dataset for spoken language recognition of 6628 hours (62 hours per language on the average) and it is accompanied by an evaluation set of 1609 verified utterances."}, {"id": "global-wheat-global-wheat-head-dataset-2020", "name": "Global Wheat (Global Wheat Head Dataset 2020)", "description": "Global WHEAT Dataset is the first large-scale dataset for wheat head detection from field optical images. It included a very large range of cultivars from differents continents. Wheat is a staple crop grown all over the world and consequently interest in wheat phenotyping spans the globe. Therefore, it is important that models developed for wheat phenotyping, such as wheat head detection networks, generalize between different growing environments around the world."}, {"id": "vist-edit", "name": "VIST-Edit", "description": "The dataset, VIST-Edit, includes 14,905 human-edited versions of 2,981 machine-generated visual stories. The stories were generated by two state-of-the-art visual storytelling models, each aligned to 5 human-edited versions."}, {"id": "nvgaze-nvgaze-an-anatomically-informed-dataset-for-low-latency-near-eye-gaze-estimation", "name": "NVGaze (NVGaze: An Anatomically-Informed Dataset for Low-Latency, Near-Eye Gaze Estimation)", "description": "Quality, diversity, and size of training dataset are critical factors for learning-based gaze estimators. We create two datasets satisfying these criteria for near-eye gaze estimation under infrared illumination: a synthetic dataset using anatomically-informed eye and face models with variations in face shape, gaze direction, pupil and iris, skin tone, and external conditions (two million images at 1280x960), and a real-world dataset collected with 35 subjects (2.5 million images at 640x480). Using our datasets, we train a neural network for gaze estimation, achieving 2.06 (+/- 0.44) degrees of accuracy across a wide 30 x 40 degrees field of view on real subjects excluded from training and 0.5 degrees best-case accuracy (across the same field of view) when explicitly trained for one real subject. We also train a variant of our network to perform pupil estimation, showing higher robustness than previous methods. Our network requires fewer convolutional layers than previous networks, achieving sub-millisecond latency."}, {"id": "chaos-chaos-combined-ct-mr-healthy-abdominal-organ-segmentation", "name": "CHAOS (CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation)", "description": "CHAOS challenge aims the segmentation of abdominal organs (liver, kidneys and spleen)  from CT and MRI data. ONsite section of the CHAOS was held in The IEEE International Symposium on Biomedical Imaging (ISBI) on April 11, 2019, Venice, ITALY.  Online submissions are still welcome!"}, {"id": "orangesum", "name": "OrangeSum", "description": "OrangeSum is a single-document extreme summarization dataset with two tasks: title and abstract. Ground truth summaries are respectively 11.42 and 32.12 words in length on average, for the title and abstract tasks respectively, while document sizes are 315 and 350 words."}, {"id": "urdu-online-reviews", "name": "Urdu Online Reviews", "description": "This corpus was constructed by collecting 10,008 reviews from various domains, including sports, food, software, politics, and entertainment. Human annotators manually tagged the reviews into positive (n = 3662), negative (n = 2619), and neutral (n = 3727) categories."}, {"id": "partnet", "name": "PartNet", "description": "PartNet is a consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. This dataset enables and serves as a catalyst for many tasks such as shape analysis, dynamic 3D scene modeling and simulation, affordance analysis, and others."}, {"id": "nottingham", "name": "Nottingham", "description": "The Nottingham Dataset is a collection of 1200 American and British folk songs."}, {"id": "event-human3-6m", "name": "Event-Human3.6m", "description": "Event-Human3.6m is a challenging dataset for event-based human pose estimation by simulating events from the RGB Human3.6m dataset. It is built by converting the RGB recordings of Human3.6m into events and synchronising raw joints ground-truth with events frames through interpolation."}, {"id": "kolektorsdd-kolektor-surface-defect-dataset", "name": "KolektorSDD (Kolektor Surface-Defect Dataset)", "description": "The dataset is constructed from images of defective production items that were provided and annotated by Kolektor Group d.o.o.. The images were captured in a controlled industrial environment in a real-world case."}, {"id": "minif2f", "name": "MiniF2F", "description": "MiniF2F is a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, and Isabelle and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses."}, {"id": "fewclue", "name": "FewCLUE", "description": "Chinese Few-shot Learning Evaluation Benchmark (FewCLUE) is a comprehensive small sample evaluation benchmark in Chinese. It includes nine tasks, ranging from single-sentence and sentence-pair classification tasks to machine reading comprehension tasks."}, {"id": "codeqa", "name": "CodeQA", "description": "CodeQA is a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. "}, {"id": "deepfake-mnist", "name": "DeepFake MNIST+", "description": "DeepFake MNIST+ is a deepfake facial animation dataset. The dataset is generated by a SOTA image animation generator. It includes 10,000 facial animation videos in ten different actions, which can spoof the recent liveness detectors."}, {"id": "icdcn2019", "name": "ICDCN2019", "description": "This is a dataset consisting of complete network traces comprising benign and malicious traffic, which is feature-rich and publicly available."}, {"id": "handnet", "name": "HandNet", "description": "The HandNet dataset contains depth images of 10 participants' hands non-rigidly deforming in front of a RealSense RGB-D camera. The annotations are generated by a magnetic annotation technique. 6D pose is available for the center of the hand as well as the five fingertips (i.e. position and orientation of each)."}, {"id": "celebv-hq", "name": "CelebV-HQ", "description": "CelebV-HQ is a large-scale video facial attributes dataset with annotations. CelebV-HQ contains 35,666 video clips involving 15,653 identities and 83 manually labeled facial attributes covering appearance, action, and emotion."}, {"id": "cdnet-change-detection", "name": "CDNET (Change Detection)", "description": "A video database for testing change detection algorithms. "}, {"id": "openmeva", "name": "OpenMEVA", "description": "OpenMEVA is a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples."}, {"id": "new3", "name": "New3", "description": "New3, a set of 527 instances from AMR 3.0, whose original source was the LORELEI DARPA project \u2013 not included in the AMR 2.0 training set \u2013 consisting of excerpts from newswires and online forum."}, {"id": "blue-biomedical-language-understanding-evaluation", "name": "BLUE (Biomedical Language Understanding Evaluation)", "description": "The BLUE benchmark consists of five different biomedicine text-mining tasks with ten corpora. These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difficulty and, more importantly, highlight common biomedicine text-mining challenges."}, {"id": "chartqa", "name": "ChartQA", "description": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions."}, {"id": "ipm-nel-derczynski-ipm-named-entity-linking", "name": "IPM NEL (Derczynski IPM Named Entity Linking)", "description": "This data is for the task of named entity recognition and linking/disambiguation over tweets. It comprises the addition of an entity URI layer on top of an NER-annotated tweet dataset. The task is to detect entities and then provide a correct link to them in DBpedia, thus disambiguating otherwise ambiguous entity surface forms; for example, this means linking \"Paris\" to the correct instance of a city named that (e.g. Paris,  France vs. Paris, Texas)."}, {"id": "mini-imagenet", "name": "mini-Imagenet", "description": "mini-Imagenet is proposed by  Matching Networks for One Shot Learning . In NeurIPS, 2016. This dataset consists of 50000 training images and 10000 testing images, evenly distributed across 100 classes."}, {"id": "something-something-v1-20bn-something-something-dataset-v1", "name": "Something-Something V1 (20BN-Something-Something\u00a0Dataset V1)", "description": "The 20BN-SOMETHING-SOMETHING dataset is a large collection of labeled video clips that show humans performing pre-defined basic actions with everyday objects. The dataset was created by a large number of crowd workers. It allows machine learning models to develop fine-grained understanding of basic actions that occur in the physical world. It contains 108,499 videos, with 86,017 in the training set, 11,522 in the validation set and 10,960 in the test set. There are 174 labels."}, {"id": "bookcorpus", "name": "BookCorpus", "description": "BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.)."}, {"id": "biped-barcelona-images-for-perceptual-edge-detection", "name": "BIPED (Barcelona Images for Perceptual Edge Detection)", "description": "It contains 250 outdoor images of 1280$\\times$720 pixels each. These images have been carefully annotated by experts on the computer vision field, hence no redundancy has been considered. In spite of that, all results have been cross-checked several times in order to correct possible mistakes or wrong edges by just one subject. This dataset is publicly available as a benchmark for evaluating edge detection algorithms. The generation of this dataset is motivated by the lack of edge detection datasets, actually, there is just one dataset publicly available for the edge detection task published in 2016 (MDBD: Multicue Dataset for Boundary Detection\u2014the subset for edge detection). The level of details of the edge level annotations in the BIPED\u2019s images can be appreciated looking at the GT, see Figs above."}, {"id": "liveqa", "name": "LiveQA", "description": "A new question answering dataset constructed from play-by-play live broadcast. It contains 117k multiple-choice questions written by human commentators for over 1,670 NBA games, which are collected from the Chinese Hupu (https://nba.hupu.com/games) website."}, {"id": "dhf1k", "name": "DHF1K", "description": "DHF1K is a video saliency dataset which contains a ground-truth map of binary pixel-wise gaze fixation points and a continuous map of the fixation points after being blurred by a gaussian filter. DHF1K contains 1000 videos in total. 700 of the videos are annotated, 600 of which are used for training and 100 for validation. The remaining 300 are the testing set which are to be evaluated on a public server."}, {"id": "findvehicle", "name": "FindVehicle", "description": "The first NER dataset in the field of traffic,  which is to extract the characteristics and attributes of the vehicle on the road."}, {"id": "xlcost-cross-lingual-code-snippet", "name": "XLCoST (Cross-Lingual Code Snippet)", "description": "XLCoST is a benchmark dataset for cross-lingual code intelligence. The dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-language code tasks."}, {"id": "newshead", "name": "NewSHead", "description": "The NewSHead dataset contains 369,940 English stories with 932,571 unique URLs, among which there are 359,940 stories for training, 5,000 for validation, and 5,000 for testing, respectively. Each news story contains at least three (and up to five) articles."}, {"id": "ryanspeech", "name": "RyanSpeech", "description": "RyanSpeech is a speech corpus for research on automated text-to-speech (TTS) systems. This dataset contains textual materials from real-world conversational settings. These materials contain over 10 hours of a professional male voice actor's speech recorded at 44.1 kHz."}, {"id": "endotect-polyp-segmentation-challenge-dataset", "name": "Endotect Polyp Segmentation Challenge Dataset", "description": "A challenge that consists of three tasks, each targeting a different requirement for in-clinic use. The first task involves classifying images from the GI tract into 23 distinct classes. The second task focuses on efficiant classification measured by the amount of time spent processing each image. The last task relates to automatcially segmenting polyps."}, {"id": "yeast", "name": "Yeast", "description": "Yeast dataset consists of a protein-protein interaction network. Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology."}, {"id": "rgb-d-d", "name": "RGB-D-D", "description": "RGB-D-D is a large-scale dataset for depth map super-resolution (SR). It consists of real-world paired low-resolution (LR) and high-resolution (HR) depth maps. The paired LR and HR depth maps are captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes."}, {"id": "rsoc-remote-sensing-object-counting", "name": "RSOC (Remote Sensing Object Counting)", "description": "RSOC is a large-scale object counting dataset with remote sensing images, which contains four important geographic objects: buildings, crowded ships in harbors, large-vehicles and small-vehicles in parking lots."}, {"id": "cross-view-time-dataset", "name": "Cross-View Time Dataset", "description": "The appearance of the world varies dramatically not only from place to place but also from hour to hour and month to month. Every day billions of images capture this complex relationship, many of which are associated with precise time and location metadata. We propose to use these images to construct a global-scale, dynamic map of visual appearance attributes. Such a map enables fine-grained understanding of the expected appearance at any geographic location and time. Our approach integrates dense overhead imagery with location and time metadata into a general framework capable of mapping a wide variety of visual attributes. A key feature of our approach is that it requires no manual data annotation. We demonstrate how this approach can support various applications, including image-driven mapping, image geolocalization, and metadata verification."}, {"id": "fed-fine-grained-evaluation-of-dialog", "name": "FED (Fine-grained Evaluation of Dialog)", "description": "The FED dataset is constructed by annotating a set of human-system and human-human conversations with eighteen fine-grained dialog qualities."}, {"id": "stanfordextra", "name": "StanfordExtra", "description": "An 'in the wild' dataset of 20,580 dog images for which 2D joint and silhouette annotations were collected."}, {"id": "snips-smartlights", "name": "Snips-SmartLights", "description": "The SmartLights benchmark from Snipstests the capability of controlling lights in different rooms. It consists of 1660 requests which are split into five partitions for a 5-fold evaluation.  A sample command could be: \u201cplease change the [bedroom] lights to [red]\u201d or \u201ci\u2019d like the [living room] lights to be at [twelve] percent\u201d"}, {"id": "dada-seg", "name": "DADA-seg", "description": "DADA-seg is a pixel-wise annotated accident dataset, which contains a variety of critical scenarios from traffic accidents. It is used for semantic segmentation."}, {"id": "banfakenews", "name": "BanFakeNews", "description": "An annotated dataset of ~50K news that can be used for building automated fake news detection systems for a low resource language like Bangla. "}, {"id": "coqe-containers-of-liquid-content", "name": "COQE (Containers Of liQuid contEnt)", "description": "Contains more than 5,000 images of 10,000 liquid containers in context labelled with volume, amount of content, bounding box annotation, and corresponding similar 3D CAD models."}, {"id": "photoswitch", "name": "Photoswitch", "description": "A benchmark for molecular machine learning where improvements in model performance can be immediately observed in the throughput of promising molecules synthesized in the lab. Photoswitches are a versatile class of molecule for medical and renewable energy applications where a molecule's efficacy is governed by its electronic transition wavelengths."}, {"id": "meqsum", "name": "MeQSum", "description": "MeQSum is a dataset for medical question summarization. It contains 1,000 summarized consumer health questions."}, {"id": "polyglot-ner", "name": "Polyglot-NER", "description": "Polyglot-NER builds massive multilingual annotators with minimal human expertise and intervention."}, {"id": "query-focused-video-summarization-dataset", "name": "Query-Focused Video Summarization Dataset", "description": "Collects dense per-video-shot concept annotations."}, {"id": "amstertime-amstertime-a-visual-place-recognition-benchmark-dataset-for-severe-domain-shift", "name": "AmsterTime (AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift)", "description": "AmsterTime dataset offers a collection of 2,500 well-curated images matching the same scene from a street view matched to historical archival image data from Amsterdam city. The image pairs capture the same place with different cameras, viewpoints, and appearances. Unlike existing benchmark datasets, AmsterTime is directly crowdsourced in a GIS navigation platform (Mapillary). In turn, all the matching pairs are verified by a human expert to verify the correct matches and evaluate the human competence in the Visual Place Recognition (VPR) task for further references."}, {"id": "camges-cambridge-hand-gesture-dataset", "name": "CamGes (Cambridge Hand Gesture Dataset)", "description": "The size of the data set is about 1GB. The data set consists of 900 image sequences of 9 gesture classes, which are defined by 3 primitive hand shapes and 3 primitive motions. Therefore, the target task for this data set is to classify different shapes as well as different motions at a time."}, {"id": "queryd", "name": "QuerYD", "description": "A large-scale dataset for retrieval and event localisation in video. A unique feature of the dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content."}, {"id": "korean-table-question-answering", "name": "Korean Table Question Answering", "description": "Korean tabular dataset is a collection of 1.4M tables with corresponding descriptions for unsupervised pre-training language models. Korean table question answering corpus consists of 70k pairs of questions and answers created by crowd-sourced workers."}, {"id": "paris-lille-3d", "name": "Paris-Lille-3D", "description": "The Paris-Lille-3D is a Benchmark on Point Cloud Classification. The Point Cloud has been labeled entirely by hand with 50 different classes. The dataset consists of around 2km of Mobile Laser System point cloud acquired in two cities in France (Paris and Lille)."}, {"id": "genwiki", "name": "GenWiki", "description": "GenWiki is a large-scale dataset for knowledge graph-to-text (G2T) and text-to-knowledge graph (T2G) conversion. It is introduced in the paper \"GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\" by Zhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng Zhang at COLING 2020."}, {"id": "smac-the-starcraft-multi-agent-challenge", "name": "SMAC (The StarCraft Multi-Agent Challenge)", "description": "The StarCraft Multi-Agent Challenge (SMAC) is a benchmark that provides elements of partial observability, challenging dynamics, and high-dimensional observation spaces. SMAC is built using the StarCraft II game engine, creating a testbed for research in cooperative MARL where each game unit is an independent RL agent."}, {"id": "helen", "name": "Helen", "description": "The HELEN dataset is composed of 2330 face images of 400\u00d7400 pixels with labeled facial components generated through manually-annotated contours along eyes, eyebrows, nose, lips and jawline."}, {"id": "media", "name": "MEDIA", "description": "The MEDIA French corpus is dedicated to semantic extraction from speech in a context of human/machine dialogues. The corpus has manual transcription and conceptual annotation of dialogues from 250 speakers. It is split into the following three parts : (1) the training set (720 dialogues, 12K sentences), (2) the development set (79 dialogues, 1.3K sentences, and (3) the test set (200 dialogues, 3K sentences)."}, {"id": "ricosca", "name": "RicoSCA", "description": "Rico is a public UI corpus with 72K Android UI screens mined from 9.7K Android apps (Deka et al., 2017). Each screen in Rico comes with a screenshot image and a view hierarchy of a collection of UI objects. Authors manually removed screens whose view hierarchies do not match their screenshots by asking annotators to visually verify whether the bounding boxes of view hierarchy leaves match each UI object on the corresponding screenshot image. This filtering results in 25K unique screens."}, {"id": "strategyqa", "name": "StrategyQA", "description": "StrategyQA is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. It includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies."}, {"id": "milkqa", "name": "MilkQA", "description": "A question answering dataset from the dairy domain dedicated to the study of consumer questions. The dataset contains 2,657 pairs of questions and answers, written in the Portuguese language and originally collected by the Brazilian Agricultural Research Corporation (Embrapa). All questions were motivated by real situations and written by thousands of authors with very different backgrounds and levels of literacy, while answers were elaborated by specialists from Embrapa's customer service. "}, {"id": "timberseg-1-0", "name": "TimberSeg 1.0", "description": "The TimberSeg 1.0 dataset is composed of 220 images showing wood logs in various environments and conditions in Canada. The images are densely annotated with segmentation masks for each log instance, as well as the corresponding bounding box and class label.  This dataset aim towards enabling autonomous forestry forwarders, therefore it contains nearly 2500 instances of wood logs from an operators' point-of-view. Images were taken in the forest, near the roadside, in lumberyards and above timber-filled trailers. The logs were annotated considering a grasping perspective, meaning that only the logs above the piles and accessible are segmented."}, {"id": "instre", "name": "INSTRE", "description": "INSTRE is a benchmark for INSTance-level visual object REtrieval and REcognition (INSTRE). INSTRE has the following major properties: (1) balanced data scale, (2) more diverse intraclass instance variations, (3) cluttered and less contextual backgrounds, (4) object localization annotation for each image, (5) well-manipulated double-labelled images for measuring multiple object (within one image) case."}, {"id": "tmed-tufts-medical-echocardiogram-dataset", "name": "TMED (Tufts Medical Echocardiogram Dataset)", "description": "TMED is a clinically-motivated benchmark dataset for computer vision and machine learning from limited labeled data."}, {"id": "eva-eva-7k-dataset", "name": "EVA (EVA-7K dataset)", "description": "The dataset contains 7000 videos: native, altered and exchanged through social platforms. The altered contents include manipulations with FFmpeg, AVIdemux, Kdenlive and Adobe Premiere. The social platforms used to exchange the native and altered videos are Facebook, Tiktok, Youtube and Weibo. "}, {"id": "flickr30k", "name": "Flickr30k", "description": "The Flickr30k dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators."}, {"id": "dhp19-dynamic-vision-sensor-3d-human-pose-dataset", "name": "DHP19 (Dynamic Vision Sensor 3D Human Pose Dataset)", "description": "DHP19 is the first human pose dataset with data collected from DVS event cameras. "}, {"id": "microsoft-research-multimodal-aligned-recipe-corpus", "name": "Microsoft Research Multimodal Aligned Recipe Corpus", "description": "To construct the MICROSOFT RESEARCH MULTIMODAL ALIGNED RECIPE CORPUS the authors first extract a large number of text and video recipes from the web. The goal is to find joint alignments between multiple text recipes and multiple video recipes for the same dish. The task is challenging, as different recipes vary in their order of instructions and use of ingredients. Moreover, video instructions can be noisy, and text and video instructions include different levels of specificity in their descriptions."}, {"id": "freebaseqa", "name": "FreebaseQA", "description": "FreebaseQA is a data set for open-domain QA over the Freebase knowledge graph. The question-answer pairs in this data set are collected from various sources, including the TriviaQA data set and other trivia websites (QuizBalls, QuizZone, KnowQuiz), and are matched against Freebase to generate relevant subject-predicate-object triples that were further verified by human annotators. As all questions in FreebaseQA are composed independently for human contestants in various trivia-like competitions, this data set shows richer linguistic variation and complexity than existing QA data sets, making it a good test-bed for emerging KB-QA systems."}, {"id": "uea-time-series-datasets-uea-time-series-datasets-for-series-level-anomaly-detection", "name": "UEA time-series datasets (UEA time-series datasets for series-level anomaly detection)", "description": "Five datasets used in NeurTraL-AD paper: \\textit{RacketSports (RS).} Accelerometer and gyroscope recording of players playing four different racket sports. Each sport is designated as a different class. \\textit{Epilepsy (EPSY).} Accelerometer recording of healthy actors simulating four different activity classes, one of them being an epileptic shock. \\textit{Naval air training and operating procedures standardization (NAT).} Positions of sensors mounted on different body parts of a person performing activities. There are six different activity classes in the dataset. \\textit{Character trajectories (CT).} Velocity trajectories of a pen on a WACOM tablet. There are $20$ different characters in this dataset.  \\textit{Spoken Arabic Digits (SAD).} MFCC features of ten arabic digits spoken by $88$ different speakers."}, {"id": "evil", "name": "EVIL", "description": "To automatically generate Python and assembly programs used for security exploits, we curated a large dataset for feeding NMT techniques. A sample in the dataset consists of a snippet of code from these exploits and their corresponding description in the English language. We collected exploits from publicly available databases (exploitdb, shellstorm), public repositories (e.g., GitHub), and programming guidelines. In particular, we focused on exploits targeting Linux, the most common OS for security-critical network services, running on IA-32 (i.e., the 32-bit version of the x86 Intel Architecture). The dataset is stored in the folder EVIL/datasets and consists of two parts: i) Encoders: a Python dataset, which contains Python code used by exploits to encode the shellcode; ii) Decoders: an assembly dataset, which includes shellcode and decoders to revert the encoding."}, {"id": "myops", "name": "MyoPS", "description": "MyoPS is a dataset for myocardial pathology segmentation combining three-sequence cardiac magnetic resonance (CMR) images, which was first proposed in the MyoPS challenge, in conjunction with MICCAI 2020. The challenge provided 45 paired and pre-aligned CMR images, allowing algorithms to combine the complementary information from the three CMR sequences for pathology segment"}, {"id": "wikipediags", "name": "WikipediaGS", "description": "The WikipediaGS dataset was created by extracting Wikipedia tables from Wikipedia pages. It consists of 485,096 tables which were annotated with DBpedia entities for the Cell Entity Annotation (CEA) task."}, {"id": "omnibenchmark", "name": "OmniBenchmark", "description": "Omni-Realm Benchmark (OmniBenchmark) is a diverse (21 semantic realm-wise datasets) and concise (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms, e.g. across mammals to aircraft. "}, {"id": "tacos-multi-level-corpus", "name": "TACoS Multi-Level Corpus", "description": "Augments the video-description dataset TACoS with short and single sentence descriptions."}, {"id": "sewer-ml", "name": "Sewer-ML", "description": "Sewer-ML is a sewer defect dataset. It contains 1.3 million images, from 75,618 videos collected from three Danish water utility companies over nine years. All videos have been annotated by licensed sewer inspectors following the Danish sewer inspection standard, Fotomanualen. This leads to consistent and reliable annotations, and a total of 17 annotated defect classes."}, {"id": "code2seq-java", "name": "Code2Seq (Java)", "description": "Java-Small, Java-Med, Java-Large"}, {"id": "nas-bench-201", "name": "NAS-Bench-201", "description": "NAS-Bench-201 is a benchmark (and search space) for neural architecture search. Each architecture consists of a predefined skeleton with a stack of the searched cell. In this way, architecture search is transformed into the problem of searching a good cell."}, {"id": "gd-gaze-detection", "name": "GD (Gaze-Detection)", "description": "These images were generated using UnityEyes simulator, after including essential eyeball physiology elements and modeling binocular vision dynamics. The images are annotated with head pose and gaze direction information, besides 2D and 3D landmarks of eye's most important features. Additionally, the images are distributed into eight classes denoting the gaze direction of a driver's eyes (TopLeft, TopRight, TopCenter, MiddleLeft, MiddleRight, BottomLeft, BottomRight, BottomCenter). This dataset was used to train a DNN model for estimating the gaze direction. The dataset contains 61,063 training images, 132,630 testing images and additional 72,000 images for improvement."}, {"id": "dsc-10-tasks-task-incremental-document-sentiment-classification", "name": "DSC (10 tasks) (Task Incremental Document Sentiment Classification)", "description": "A set of 10 DSC datasets (reviews of 10 products) to produce sequences of tasks. The products are Sports, Toys, Tools, Video, Pet, Musical, Movies, Garden, Offices, and Kindle. 2500 positive and 2500 negative training reviews per task . The validation reviews are with 250 positive and 250 negative and the test reviews are with 250 positive and 250 negative reviews. The detailed statistic on page https://github.com/ZixuanKe/PyContinual"}, {"id": "rendered-handpose-dataset", "name": "Rendered Handpose Dataset", "description": "Rendered Handpose Dataset contains 41258 training and 2728 testing samples. Each sample provides:"}, {"id": "vist-visual-storytelling", "name": "VIST (Visual Storytelling)", "description": "The Visual Storytelling Dataset (VIST) consists of 210,819 unique photos and 50,000 stories. The images were collected from albums on Flickr. The albums included 10 to 50 images and all the images in an album are taken in a 48-hour span. The stories were created by workers on Amazon Mechanical Turk, where the workers were instructed to choose five images from the album and write a story about them. Every story has five sentences, and every sentence is paired with its appropriate image. The dataset is split into 3 subsets, a training set (80%), a validation set (10%) and a test set (10%). All the words and interpunction signs in the stories are separated by a space character and all the location names are replaced with the word location. All the names of people are replaced with the words male or female depending on the gender of the person."}, {"id": "pascal-part", "name": "PASCAL-Part", "description": "PASCAL-Part is a set of additional annotations for PASCAL VOC 2010. It goes beyond the original PASCAL object detection task by providing segmentation masks for each body part of the object. For categories that do not have a consistent set of parts (e.g., boat), it provides the silhouette annotation. "}, {"id": "tatoeba-translation-challenge", "name": "Tatoeba Translation Challenge", "description": "The Tatoeba Translation Challenge is a benchmark for machine translation that provides training and test data for thousands of language pairs covering over 500 languages."}, {"id": "factify-a-dataset-on-multi-modal-fact-verification", "name": "FACTIFY (a dataset on multi-modal fact verification)", "description": "FACTIFY is a dataset on multi-modal fact verification. It contains images, textual claim, reference textual documenta and image. The task is to classify the claims into support, not-enough-evidence and refute categories with the help of the supporting data. We aim to combat fake news in the social media era by providing this multi-modal dataset. Factify contains 50,000 claims accompanied with 100,000 images, split into training, validation and test sets."}, {"id": "nucls-nucleus-classification-localization-and-segmentation", "name": "NuCLS (Nucleus Classification, Localization and Segmentation)", "description": "The NuCLS dataset contains over 220,000 labeled nuclei from breast cancer images from TCGA. These nuclei were annotated through the collaborative effort of pathologists, pathology residents, and medical students using the Digital Slide Archive. These data can be used in several ways to develop and validate algorithms for nuclear detection, classification, and segmentation, or as a resource to develop and evaluate methods for interrater analysis."}, {"id": "ncbi-disease-corpus", "name": "NCBI Disease Corpus", "description": "NCBI Disease Corpus is a large-scale disease corpus consisting of 6900 disease mentions in 793 PubMed citations, derived from an earlier corpus. The corpus contains rich annotations, was developed by a team of 12 annotators (two people per annotation) and covers all sentences in a PubMed abstract. Disease mentions are categorized into Specific Disease, Disease Class, Composite Mention and Modifier categories."}, {"id": "bdslimset-bangladeshi-sign-language-image-dataset", "name": "BdSLImset (Bangladeshi Sign Language Image Dataset)", "description": "Bangladeshi Sign Language Image Dataset (BdSLImset) is a dataset that contains images of different Bangladeshi sign letters."}, {"id": "diagset", "name": "DiagSet", "description": "DiagSet is a histopathological dataset for prostate cancer detection. The proposed dataset consists of over 2.6 million tissue patches extracted from 430 fully annotated scans, 4675 scans with assigned binary diagnosis, and 46 scans with diagnosis given independently by a group of histopathologists."}, {"id": "foodx-251", "name": "FoodX-251", "description": "FoodX-251 is a dataset of 251 fine-grained classes with 118k training, 12k validation and 28k test images. Human verified labels are made available for the training and test images. The classes are fine-grained and visually similar, for example, different types of cakes, sandwiches, puddings, soups, and pastas."}, {"id": "ait-ldsv2-0-ait-log-data-set-v2-0", "name": "AIT-LDSv2.0 (AIT Log Data Set V2.0)", "description": "Synthetic log data suitable for evaluation of intrusion detection systems, federated learning, and alert aggregation.  Each of the 8 datasets corresponds to a testbed representing a small enterprise network including mail server, file share, WordPress server, VPN, firewall, etc. Normal user behavior is simulated to generate background noise over a time span of 4-6 days. At some point, a sequence of attack steps are launched against the network. Log data is collected from all hosts and includes Apache access and error logs, authentication logs, DNS logs, VPN logs, audit logs, Suricata logs, network traffic packet captures, horde logs, exim logs, syslog, and system monitoring logs. Attacks include scans (nmap, WPScan, dirb), webshell upload, password cracking, privilege escalation, remote command execution, and data exfiltration."}, {"id": "cifar-100n-real-world-human-annotations", "name": "CIFAR-100N (Real-World Human Annotations)", "description": "This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk."}, {"id": "synlidar", "name": "SynLiDAR", "description": "SynLiDAR is a large-scale synthetic LiDAR sequential point cloud dataset with point-wise annotations. 13 sequences of LiDAR point cloud with around 20k scans (over 19 billion points and 32 semantic classes) are collected from virtual urban cities, suburban towns, neighborhood, and harbor."}, {"id": "gad-gene-associations-database", "name": "GAD (Gene Associations Database)", "description": "GAD, or Gene Associations Database, is a corpus of gene-disease associations curated from genetic association studies."}, {"id": "event-camera-dataset", "name": "Event-Camera Dataset", "description": "The Event-Camera Dataset is a collection of datasets with an event-based camera for high-speed robotics. The data also include intensity images, inertial measurements, and ground truth from a motion-capture system. An event-based camera is a revolutionary vision sensor with three key advantages: a measurement rate that is almost 1 million times faster than standard cameras, a latency of 1 microsecond, and a high dynamic range of 130 decibels (standard cameras only have 60 dB). These properties enable the design of a new class of algorithms for high-speed robotics, where standard cameras suffer from motion blur and high latency. All the data are released both as text files and binary (i.e., rosbag) files."}, {"id": "md-gender-multi-dimensional-gender-bias-datasets", "name": "MD Gender (Multi-Dimensional Gender Bias Datasets)", "description": "Provides eight automatically annotated large scale datasets with gender information. "}, {"id": "5k-presetation-slides-5000-presentation-slide-pairs", "name": "5k_presetation_slides (5000 presentation slide pairs)", "description": "We crawled 5000 paper, slide pairs from conference proceeding websites. (e.g. acl.org and usenix.org)."}, {"id": "tau-urban-acoustic-scenes-2019", "name": "TAU Urban Acoustic Scenes 2019", "description": "TAU Urban Acoustic Scenes 2019 development dataset consists of 10-seconds audio segments from 10 acoustic scenes: airport, indoor shopping mall, metro station, pedestrian street, public square, street with medium level of traffic, travelling by a tram, travelling by a bus, travelling by an underground metro and urban park. Each acoustic scene has 1440 segments (240 minutes of audio). The dataset contains in total 40 hours of audio."}, {"id": "semkitti-dvps", "name": "SemKITTI-DVPS", "description": "SemKITTI-DVPS is derived from SemanticKITTI dataset. SemanticKITTI dataset is based on the odometry dataset of the KITTI Vision benchmark. SemanticKITTI dataset provides perspective images and panoptic-labeled 3D point clouds. To convert it for DVPS, we project the 3D point clouds onto the image plane and name the derived dataset as SemKITTI-DVPS. SemKITTI-DVPS is distributed under Creative Commons Attribution-NonCommercial-ShareAlike license."}, {"id": "nips4bplus", "name": "NIPS4Bplus", "description": "NIPS4Bplus is a richly annotated birdsong audio dataset, that is comprised of recordings containing bird vocalisations along with their active species tags plus the temporal annotations acquired for them. It consists of around 687 recordings, 87 classes, species tags, annotations. The total duration of audio is around 1 hour."}, {"id": "3d-printing-data", "name": "3D-Printing-Data", "description": "This is a dataset for anomalies detection in 3D printing."}, {"id": "mmbody", "name": "MMBody", "description": "The MMBody dataset provides human body data with motion capture, GT mesh, Kinect RGBD, and millimeter wave sensor data. See homepage for more details."}, {"id": "marvl-multicultural-reasoning-over-vision-and-language", "name": "MaRVL (Multicultural Reasoning over Vision and Language)", "description": "Multicultural Reasoning over Vision and Language (MaRVL) is a dataset based on an ImageNet-style hierarchy representative of many languages and cultures (Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish). The selection of both concepts and images is entirely driven by native speakers. Afterwards, we elicit statements from native speakers about pairs of images. The task consists in discriminating whether each grounded statement is true or false."}, {"id": "vivos-vivos-corpus", "name": "VIVOS (VIVOS Corpus)", "description": "VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for Automatic Speech Recognition task."}, {"id": "netflix-prize", "name": "Netflix Prize", "description": "Netflix Prize consists of about 100,000,000 ratings for 17,770 movies given by 480,189 users. Each rating in the training dataset consists of four entries: user, movie, date of grade, grade. Users and movies are represented with integer IDs, while ratings range from 1 to 5."}, {"id": "ddi", "name": "DDI", "description": "The DDIExtraction 2013 task relies on the DDI corpus which contains MedLine abstracts on drug-drug interactions as well as documents describing drug-drug interactions from the DrugBank database."}, {"id": "tanks-and-temples", "name": "Tanks and Temples", "description": "We present a benchmark for image-based 3D reconstruction. The benchmark sequences were acquired outside the lab, in realistic conditions. Ground-truth data was captured using an industrial laser scanner. The benchmark includes both outdoor scenes and indoor environments. High-resolution video sequences are provided as input, supporting the development of novel pipelines that take advantage of video input to increase reconstruction fidelity. We report the performance of many image-based 3D reconstruction pipelines on the new benchmark. The results point to exciting challenges and opportunities for future work."}, {"id": "amazon-google", "name": "Amazon-Google", "description": "The Amazon-Google dataset for entity resolution derives from the online retailers Amazon.com and  the product search service of Google accessible through the Google Base Data API. The dataset contains 1363 entities from amazon.com and 3226 google products as well as a gold standard (perfect mapping) with 1300 matching record pairs between the two data sources. The common attributes between the two data sources are: product name, product description, manufacturer and price."}, {"id": "kligler", "name": "Kligler", "description": "Dataset for document shadow removal"}, {"id": "rrs-ranking-test-restoration-200k-for-response-selection-with-ranking-test-set", "name": "RRS Ranking Test (Restoration-200k for Response Selection with Ranking Test Set)", "description": "Ranking test set contains the high-quality responses that selected by some baselines, and their correlation with the conversation context are carefully annotated by 8 professional annotators (the average annotation scores are saved for ranking). For ranking test set, the metrics should be NDCG@3 and NDCG@5, since the correlation scores are provided. More details are available in the Appendix of the paper."}, {"id": "vis30k", "name": "VIS30K", "description": "We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K\u2019s comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers."}, {"id": "spacenet-mvoi-spacenet-multi-view-overhead-imagery-dataset", "name": "SpaceNet MVOI (SpaceNet Multi-View Overhead Imagery Dataset)", "description": "An open source Multi-View Overhead Imagery dataset with 27 unique looks from a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of these images cover the same 665 square km geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance."}, {"id": "wili-2018", "name": "WiLI-2018", "description": "WiLI-2018 is a benchmark dataset for monolingual written natural language identification. WiLI-2018 is a publicly available, free of charge dataset of short text extracts from Wikipedia. It contains 1000 paragraphs of 235 languages, totaling in 23500 paragraphs. WiLI is a classification dataset: Given an unknown paragraph written in one dominant language, it has to be decided which language it is."}, {"id": "wnlampro-wordnet-language-model-probing", "name": "WNLaMPro (WordNet Language Model Probing)", "description": "The WordNet Language Model Probing (WNLaMPro) dataset consists of relations between keywords and words. It contains 4 different kinds of relations: Antonym, Hypernym, Cohyponym and Corruption."}, {"id": "matinf-maternal-and-infant-dataset", "name": "MATINF (Maternal and Infant Dataset)", "description": "Maternal and Infant (MATINF) Dataset is a large-scale dataset jointly labeled for classification, question answering and summarization in the domain of maternity and baby caring in Chinese. An entry in the dataset includes four fields: question (Q), description (D), class (C) and answer (A)."}, {"id": "fin", "name": "FIN", "description": "A dataset of financial agreements made public through U.S. Security and Exchange Commission (SEC) filings. Eight documents (totalling 54,256 words) were randomly selected for manual annotation, based on the four NE types provided in the CoNLL-2003 dataset: LOCATION (LOC), ORGANISATION (ORG), PERSON (PER), and MISCELLANEOUS (MISC)."}, {"id": "dad-driver-anomaly-detection", "name": "DAD (Driver Anomaly Detection)", "description": "Contains normal driving videos together with a set of anomalous actions in its training set. In the test set of the DAD dataset, there are unseen anomalous actions that still need to be winnowed out from normal driving. "}, {"id": "amazon-pqa", "name": "Amazon-PQA", "description": "Amazon-PQA is a product question-answer dataset. The Amazon-PQA dataset includes questions and their answers that are published on Amazon website, along with the public product information and category (Amazon Browse Node name). It contains more than 8M questions from 1M+ products."}, {"id": "cifar-10", "name": "CIFAR-10", "description": "The CIFAR-10 dataset (Canadian Institute for Advanced Research, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck). There are 6000 images per class with 5000 training and 1000 testing images per class."}, {"id": "brain-us", "name": "Brain US", "description": "This brain anatomy segmentation dataset has 1300 2D US scans for training and 329 for testing. A total of 1629 in vivo B-mode US images were obtained from 20 different subjects (age<1 years old) who were treated between 2010 and 2016. The dataset contained subjects with IVH and without (healthy subjects but in risk of developing IVH). The US scans were collected using a Philips US machine with a C8-5 broadband curved array transducer using coronal and sagittal scan planes. For every collected image ventricles and septum pellecudi are manually segmented by an expert ultrasonographer. We split these images randomly into 1300 Training images and 329 Testing images for experiments. Note that these images are of size 512 \u00d7 512. "}, {"id": "xl-bel", "name": "XL-BEL", "description": "XL-BEL is a benchmark for cross-lingual biomedical entity linking (XL-BEL). The benchmark spans 10 typologically diverse languages."}, {"id": "optimizer-2021-data", "name": "!Optimizer 2021 Data", "description": "The data used for !Optimizer 2021 competition, based on seven biological model organisms."}, {"id": "laval-indoor-hdr-dataset", "name": "Laval Indoor HDR Dataset", "description": "This dataset contains 2100+ high resolution indoor panoramas, captured using a Canon 5D Mark III and a robotic panoramic tripod head. Each capture was multi-exposed (22 f-stops) and is fully HDR, without any saturation. Panoramas were stitched from 6 captures (60 degrees azimuth increment) and were captured in a wide variety of indoor environments."}, {"id": "wikimovies", "name": "WikiMovies", "description": "WikiMovies is a dataset for question answering for movies content. It contains ~100k questions in the movie domain, and was designed to be answerable by using either a perfect KB (based on OMDb),"}, {"id": "mpi-sintel", "name": "MPI Sintel", "description": "MPI (Max Planck Institute) Sintel is a dataset for optical flow evaluation that has 1064 synthesized stereo images and ground truth data for disparity. Sintel is derived from open-source 3D animated short film Sintel. The dataset has 23 different scenes. The stereo images are RGB while the disparity is grayscale. Both have resolution of 1024\u00d7436 pixels and 8-bit per channel."}, {"id": "mseg", "name": "MSeg", "description": "A composite dataset that unifies semantic segmentation datasets from different domains. "}, {"id": "flir-aligned", "name": "FLIR-aligned", "description": "This recently released multispectral (multi-)object detection dataset contains around 10k manually-annotated thermal images with their corresponding reference visible images, collected during daytime and nighttime. We only kept the 3 more frequent classes which are \u201cbicycle\u201d, \u201ccar\u201d and \u201cper- son\u201d. We manually removed the misaligned visible-thermal image pairs and ended with 4,129 well-aligned image pairs for training and 1,013 image pairs for test. This new aligned dataset can be downloaded here: http:// shorturl.at/ahAY4"}, {"id": "shemo-sharif-emotional-speech-database", "name": "ShEMO (Sharif Emotional Speech Database)", "description": "The database includes 3000 semi-natural utterances, equivalent to 3 hours and 25 minutes of speech data extracted from online radio plays. The ShEMO covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness and surprise, as well as neutral state."}, {"id": "contract-discovery", "name": "Contract Discovery", "description": "A new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed, where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts."}, {"id": "acoustic-frequency-responses-in-a-conventional-classroom", "name": "Acoustic frequency responses in a conventional classroom", "description": "Hahmann, Manuel; Verburg Riezu, Samuel Arturo (2021): Acoustic frequency responses in a conventional classroom. Technical University of Denmark. Dataset. https://doi.org/10.11583/DTU.13315286"}, {"id": "automated-evolution-of-feature-logging-statement-levels-using-git-histories-and-degree-of-interest", "name": "Automated Evolution of Feature Logging Statement Levels Using Git Histories and Degree of Interest", "description": "Logging\u2014used for system events and security breaches to more informational yet essential aspects of software features\u2014is pervasive. Given the high transactionality of today's software, logging effectiveness can be reduced by information overload. Log levels help alleviate this problem by correlating a priority to logs that can be later filtered. As software evolves, however, levels of logs documenting surrounding feature implementations may also require modification as features once deemed important may have decreased in urgency and vice-versa. We present an automated approach that assists developers in evolving levels of such (feature) logs. The approach, based on mining Git histories and manipulating a degree of interest (DOI) model, transforms source code to revitalize feature log levels based on the \"interestingness\" of the surrounding code. Built upon JGit and Mylyn, the approach is implemented as an Eclipse IDE plug-in and evaluated on 18 Java projects with ~3 million lines of code and ~4K log statements. Our tool successfully analyzes 99.26% of logging statements, increases log level distributions by ~20%, identifies logs manually modified with a recall of ~80% and a level-direction match rate of ~87%, and increases the focus of logs in bug fix contexts ~83% of the time. Moreover, pull (patch) requests were integrated into large and popular open-source projects. The results indicate that the approach is promising in assisting developers in evolving feature log levels."}, {"id": "opereid", "name": "OpeReid", "description": "The OpeReid dataset is a person re-identification dataset that consists of 7,413 images of 200 persons."}, {"id": "trec-news", "name": "TREC-News", "description": "The TREC News Track features modern search tasks in the news domain. In partnership with The Washington Post, we are developing test collections that support the search needs of news readers and news writers in the current news environment. It's our hope that the track will foster research that establishes a new sense for what \"relevance\" means for news search."}, {"id": "pad-purpose-driven-affordance-dataset", "name": "PAD (Purpose-driven Affordance Dataset)", "description": "PAD (Purpose-driven Affordance Dataset) is a dataset for affordance detection, which refers to identifying the potential action possibilities of objects in an image, which is an important ability for robot perception and manipulation. The dataset consists of 4K images from 31 affordance and 72 object categories."}, {"id": "something-something-100", "name": "Something-Something-100", "description": "Something-Something-100 is a dataset split created from Something-Something V2. A total of 100 classes are selected and each comprises 100 samples. The 100 classes were split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively. Link to exactly selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/smsm-100"}, {"id": "a-okvqa", "name": "A-OKVQA", "description": "A-OKVQA is crowdsourced visual question answering dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer."}, {"id": "rich-real-scenes-interaction-contact-and-humans", "name": "RICH (Real scenes, Interaction, Contact and Humans)", "description": "Inferring human-scene contact (HSC) is the first step toward understanding how humans interact with their surroundings. While detecting 2D human-object interaction (HOI) and reconstructing 3D human pose and shape (HPS) have enjoyed significant progress, reasoning about 3D human-scene contact from a single image is still challenging. Existing HSC detection methods consider only a few types of predefined contact, often reduce body and scene to a small number of primitives, and even overlook image evidence. To predict human-scene contact from a single image, we address the limitations above from both data and algorithmic perspectives. We capture a new dataset called RICH for \u201cReal scenes, Interaction, Contact and Humans.\u201d RICH contains multiview outdoor/indoor video sequences at 4K resolution, ground-truth 3D human bodies captured using markerless motion capture, 3D body scans, and high resolution 3D scene scans. A key feature of RICH is that it also contains accurate vertex-level contact labels on the body"}, {"id": "msk", "name": "MSK", "description": "The MSK dataset is a dataset for lesion recognition from the Memorial Sloan-Kettering Cancer Center. It is used as part of the ISIC lesion recognition challenges."}, {"id": "rise", "name": "RISE", "description": "RISE is a large-scale video dataset for Recognizing Industrial Smoke Emissions. A citizen science approach was adopted to collaborate with local community members to annotate whether a video clip has smoke emissions. The dataset contains 12,567 clips from 19 distinct views from cameras that monitored three industrial facilities. These daytime clips span 30 days over two years, including all four seasons."}, {"id": "mqr", "name": "MQR", "description": "A multi-domain question rewriting dataset is constructed from human contributed Stack Exchange question edit histories. The dataset contains 427,719 question pairs which come from 303 domains."}, {"id": "spectrum-challange-2-dataset", "name": "Spectrum Challange 2 Dataset", "description": "The dataset is approved for public release, distribution unlimited."}, {"id": "mutag", "name": "MUTAG", "description": "In particular, MUTAG is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels."}, {"id": "ogb-lsc-ogb-large-scale-challenge", "name": "OGB-LSC (OGB Large-Scale Challenge)", "description": "OGB Large-Scale Challenge (OGB-LSC) is a collection of three real-world datasets for advancing the state-of-the-art in large-scale graph ML. OGB-LSC provides graph datasets that are orders of magnitude larger than existing ones and covers three core graph learning tasks -- link prediction, graph regression, and node classification. "}, {"id": "aisia-vn-review-s-aisia-vn-review-f", "name": "AISIA-VN-Review-S (AISIA-VN-Review-F)", "description": "In AISIA-VN-Review-S and AISIA-VN-Review-F datasets, we first collect 450K customer reviewing comments from various e\u2013commerce websites. Then, we manually label each review to be either positive or negative, resulting in 358,743 positive reviews and 100,699 negative reviews. We named this dataset the sentiment classification from reviews collected by AISIA, the full version (AISIA-VN-Review-F). However, in this work, we are interested in improving the model\u2019s performance when the training data are limited; thus, we only consider a subset of up to 25K training reviews and evaluate the model on another 170K reviews. We refer to this subset from the full dataset as AISIA-VN-Review-S. It is important to emphasize that our team spends a lot of time and effort to manually classify each review into positive or negative sentiments."}, {"id": "university-of-waterloo-skin-cancer-database", "name": "University of Waterloo skin cancer database", "description": "The dataset is maintained by VISION AND IMAGE PROCESSING LAB, University of Waterloo. The images of the dataset were extracted from the public databases DermIS and DermQuest, along with manual segmentations of the lesions."}, {"id": "roof-image-dataset", "name": "Roof-Image Dataset", "description": "We created a building-image paired dataset that contains more than 3K samples using our roof modeling tools."}, {"id": "ind-dataset-intersection-drone-dataset", "name": "inD Dataset (Intersection Drone Dataset)", "description": "The inD dataset is a new dataset of naturalistic vehicle trajectories recorded at German intersections. Using a drone, typical limitations of established traffic data collection methods like occlusions are overcome. Traffic was recorded at four different locations. The trajectory for each road user and its type is extracted. Using state-of-the-art computer vision algorithms, the positional error is typically less than 10 centimetres. The dataset is applicable on many tasks such as road user prediction, driver modeling, scenario-based safety validation of automated driving systems or data-driven development of HAD system components."}, {"id": "icentia11k", "name": "Icentia11K", "description": "Public ECG dataset of continuous raw signals for representation learning containing 11 thousand patients and 2 billion labelled beats."}, {"id": "posetrack", "name": "PoseTrack", "description": "The PoseTrack dataset is a large-scale benchmark for multi-person pose estimation and tracking in videos. It requires not only pose estimation in single frames, but also temporal tracking across frames. It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively. For training videos, 30 frames from the center are annotated. For validation and test videos, besides 30 frames from the center, every fourth frame is also annotated for evaluating long range articulated tracking. The annotations include 15 body keypoints location, a unique person id and a head bounding box for each person instance."}, {"id": "rctw-17-reading-chinese-text-in-the-wild", "name": "RCTW-17 (Reading Chinese Text in the Wild)", "description": "Features a large-scale dataset with 12,263 annotated images. Two tasks, namely text localization and end-to-end recognition, are set up. The competition took place from January 20 to May 31, 2017. 23 valid submissions were received from 19 teams."}, {"id": "arxiv-arxiv-hep-th-high-energy-physics-theory-citation-graph", "name": "arXiv (Arxiv HEP-TH (high energy physics theory) citation graph)", "description": "Arxiv HEP-TH (high energy physics theory) citation graph is from the e-print arXiv and covers all the citations within a dataset of 27,770 papers with 352,807 edges. If a paper i cites paper j, the graph contains a directed edge from i to j. If a paper cites, or is cited by, a paper outside the dataset, the graph does not contain any information about this. The data covers papers in the period from January 1993 to April 2003 (124 months)."}, {"id": "mq2007", "name": "MQ2007", "description": "The MQ2007 dataset consists of queries, corresponding retrieved documents and labels provided by human experts. The possible relevance labels for each document are \u201crelevant\u201d, \u201cpartially relevant\u201d, and \u201cnot relevant\u201d."}, {"id": "samsum-corpus", "name": "SAMSum Corpus", "description": "A new dataset with abstractive dialogue summaries."}, {"id": "sider", "name": "SIDER", "description": "SIDER contains information on marketed medicines and their recorded adverse drug reactions. The information is extracted from public documents and package inserts. The available information include side effect frequency, drug and side effect classifications as well as links to further information, for example drug\u2013target relations."}, {"id": "beeradvocate", "name": "BeerAdvocate", "description": "BeerAdvocate is a dataset that consists of beer reviews from beeradvocate. The data span a period of more than 10 years, including all ~1.5 million reviews up to November 2011. Each review includes ratings in terms of five \"aspects\": appearance, aroma, palate, taste, and overall impression. Reviews include product and user information, followed by each of these five ratings, and a plaintext review."}, {"id": "domainnet", "name": "DomainNet", "description": "DomainNet is a dataset of common objects in six different domain. All domains include 345 categories (classes) of objects such as Bracelet, plane, bird and cello. The domains include clipart: collection of clipart images; real: photos and real world images; sketch: sketches of specific objects; infograph: infographic images with specific object; painting artistic depictions of objects in the form of paintings and quickdraw: drawings of the worldwide players of game \u201cQuick Draw!\u201d."}, {"id": "norec-norwegian-review-corpus", "name": "NoReC (Norwegian Review Corpus)", "description": "The Norwegian Review Corpus (NoReC) was created for the purpose of training and evaluating models for document-level sentiment analysis. More than 43,000 full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1\u20136, as provided by the rating of the original author."}, {"id": "sts-2014", "name": "STS 2014", "description": "STS-2014 is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes."}, {"id": "lila", "name": "Lila", "description": "Lila is a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. The benchmark is constructed by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer."}, {"id": "a2d-sentences-sentences-for-the-actor-action-dataset-a2d", "name": "A2D Sentences (Sentences for the Actor-Action Dataset (A2D))", "description": "The Actor-Action Dataset (A2D) by Xu et al. [29] serves as the largest video dataset for the general actor and action segmentation task. It contains 3,782 videos from YouTube with pixel-level labeled actors and their actions. The dataset includes eight different actions, while a total of seven actor classes are considered to perform those actions. We follow [29], who split the dataset into 3,036 training videos and 746 testing videos. "}, {"id": "stylekqc", "name": "StyleKQC", "description": "StyleKQC is a style-variant paraphrase  corpus for korean questions and commands. It was built with a corpus construction scheme that simultaneously considers the core content and style of directives, namely intent and formality, for the Korean language. Utilizing manually generated natural language queries on six daily topics, the corpus was expanded to formal and informal sentences by human rewriting and transferring."}, {"id": "covert-a-corpus-of-fact-checked-biomedical-covid-19-tweets", "name": "CoVERT (A Corpus of Fact-checked Biomedical COVID-19 Tweets)", "description": "CoVERT is a fact-checked corpus of tweets with a focus on the domain of biomedicine and COVID-19-related (mis)information. The corpus consists of 300 tweets, each annotated with medical named entities and relations. Employs a novel crowdsourcing methodology to annotate all tweets with fact-checking labels and supporting evidence, which crowdworkers search for online. This methodology results in moderate inter-annotator agreement."}, {"id": "ps-plant-dataset", "name": "PS-Plant dataset", "description": "Automated leaf segmentation is a challenging area in computer vision. Recent advances in machine learning approaches allowed to achieve better results than traditional image processing techniques; however, training such systems often require large annotated data sets. To contribute with annotated data sets and help to overcome this bottleneck in plant phenotyping research, here we provide a novel photometric stereo (PS) data set with annotated leaf masks. This data set forms part of the work done in the BBSRC Tools and Resources Development project BB/N02334X/1."}, {"id": "the-game-of-2048", "name": "The Game of 2048", "description": "The 2048 game task involves training an agent to achieve high scores in the game 2048 (Wikipedia)"}, {"id": "diveface", "name": "DiveFace", "description": "A new face annotation dataset with balanced distribution between genders and ethnic origins. "}, {"id": "apartmentour", "name": "ApartmenTour", "description": "Contains a large number of online videos and subtitles. "}, {"id": "m-pccd-mpeg-point-cloud-compression-dataset", "name": "M-PCCD (MPEG Point Cloud Compression Dataset)", "description": "The emerging MPEG point cloud codecs (V-PCC and G-PCC variants) are assessed, and best practices for rate allocation are investigated [1]. For this purpose, three experiments are conducted. In the first experiment, a rigorous evaluation of the codecs is performed, adopting test conditions dictated by experts of the group on a carefully selected set of models, using both subjective and objective quality assessment methodologies. In the other two experiments, different rate allocation schemes for geometry-only and geometry-plus-color encoding are subjectively evaluated, in order to draw conclusions on the best-performing approaches in terms of perceived quality for a given bit rate."}, {"id": "mewsli-9", "name": "Mewsli-9", "description": "A large new multilingual dataset for multilingual entity linking."}, {"id": "wikidata-disamb", "name": "Wikidata-Disamb", "description": "The Wikidata-Disamb dataset is intended to allow a clean and scalable evaluation of NED with Wikidata entries, and to be used as a reference in future research."}, {"id": "aqua-rat-algebra-question-answering-with-rationales", "name": "AQUA-RAT (Algebra Question Answering with Rationales)", "description": "Algebra Question Answering with Rationales (AQUA-RAT) is a dataset that contains algebraic word problems with rationales. The dataset consists of about 100,000 algebraic word problems with natural language rationales. Each problem is a json object consisting of four parts: * question - A natural language definition of the problem to solve * options - 5 possible options (A, B, C, D and E), among which one is correct * rationale - A natural language description of the solution to the problem * correct - The correct option"}, {"id": "ford-campus-vision-and-lidar-data-set", "name": "Ford Campus Vision and Lidar Data Set", "description": "Ford Campus Vision and Lidar Data Set is a dataset collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. "}, {"id": "coma", "name": "COMA", "description": "CoMA contains 17,794 meshes of the human face in various expressions"}, {"id": "flores-101", "name": "FLORES-101", "description": "The FLORES evaluation benchmark consists of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond."}, {"id": "deepcad", "name": "DeepCAD", "description": "DeepCAD is a CAD dataset consisting of 179,133 models and their CAD construction sequences. It can be used to train generative models of 3D shapes."}, {"id": "lsun-large-scale-scene-understanding-challenge", "name": "LSUN (Large-scale Scene UNderstanding Challenge)", "description": "The Large-scale Scene Understanding (LSUN) challenge aims to provide a different benchmark for large-scale scene classification and understanding. The LSUN classification dataset contains 10 scene categories, such as dining room, bedroom, chicken, outdoor church, and so on. For training data, each category contains a huge number of images, ranging from around 120,000 to 3,000,000. The validation data includes 300 images, and the test data has 1000 images for each category."}, {"id": "opa-object-placement-assessment", "name": "OPA (Object Placement Assessment)", "description": "Object-Placement-Assessment (OPA) is a task consisting on verifying whether a composite image is plausible in terms of the object placement. The foreground object should be placed at a reasonable location on the background considering location, size, occlusion, semantics, and etc."}, {"id": "nela-gt-2020", "name": "NELA-GT-2020", "description": "NELA-GT-2020 is an updated version of the NELA-GT-2019 dataset. NELA-GT-2020 contains nearly 1.8M news articles from 519 sources collected between January 1st, 2020 and December 31st, 2020. Just as with NELA-GT-2018 and NELA-GT-2019, these sources come from a wide range of mainstream news sources and alternative news sources. Included in the dataset are source-level ground truth labels from Media Bias/Fact Check (MBFC) covering multiple dimensions of veracity. Additionally, new in the 2020 dataset are the Tweets embedded in the collected news articles, adding an extra layer of information to the data."}, {"id": "cambridge-landmarks", "name": "Cambridge Landmarks", "description": "Cambridge Landmarks, a large scale outdoor visual relocalisation dataset taken around Cambridge University. Contains original video, with extracted image frames labelled with their 6-DOF camera pose and a visual reconstruction of the scene. If you use this data, please cite our paper: Alex Kendall, Matthew Grimes and Roberto Cipolla \"PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization.\" Proceedings of the International Conference on Computer Vision (ICCV), 2015."}, {"id": "rareplanes", "name": "RarePlanes", "description": "RarePlanes is a unique open-source machine learning dataset from CosmiQ Works and AI.Reverie that incorporates both real and synthetically generated satellite imagery. The RarePlanes dataset specifically focuses on the value of AI.Reverie synthetic data to aid computer vision algorithms in their ability to automatically detect aircraft and their attributes in satellite imagery. Although other synthetic/real combination datasets exist, RarePlanes is the largest openly-available very-high resolution dataset built to test the value of synthetic data from an overhead perspective. Previous research has shown that synthetic data can reduce the amount of real training data needed and potentially improve performance for many tasks in the computer vision domain. The real portion of the dataset consists of 253 Maxar WorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700 hand-annotated aircraft. The accompanying synthetic dataset is generated via AI.Reverie\u2019s novel simulation platform and features 50,000 synthetic satellite images with ~630,000 aircraft annotations. Both the real and synthetically generated aircraft feature 10 fine grain attributes including: aircraft length, wingspan, wing-shape, wing-position, wingspan class, propulsion, number of engines, number of vertical-stabilizers, presence of canards, and aircraft role. Finally, we conduct extensive experiments to evaluate the real and synthetic datasets and compare performances. By doing so, we show the value of synthetic data for the task of detecting and classifying aircraft from an overhead perspective."}, {"id": "ost-egocentric-dataset", "name": "OST (Egocentric Dataset)", "description": "Is one of the largest egocentric datasets in the object search task with eyetracking information available"}, {"id": "kennedy-space-center", "name": "Kennedy Space Center", "description": "Kennedy Space Center is a dataset for the classification of wetland vegetation at the Kennedy Space Center, Florida using hyperspectral imagery. Hyperspectral data were acquired over KSC on March 23, 1996 using JPL's Airborne Visible/Infrared Imaging Spectrometer."}, {"id": "a-collection-of-131-ct-datasets-of-pieces-of-modeling-clay-containing-stones", "name": "A collection of 131 CT datasets of pieces of modeling clay containing stones", "description": "This dataset contains a collection of 131 X-ray CT scans of pieces of modeling clay (Play-Doh) with various numbers of stones inserted, retrieved in the FleX-ray lab at CWI. The dataset consists of 5 parts. It is intended as raw supplementary material to reproduce the CT reconstructions and subsequent results in the paper titled \"A tomographic workflow enabling deep learning for X-ray based foreign object detection\". The dataset can be used to set up other CT-based experiments concerning similar objects with variations in shape and composition."}, {"id": "dbp-5l-english", "name": "DBP-5L (English)", "description": "DPB-5L is a Multilingual KG dataset containing 5 KGs in English, French, Japanese, Greek, and Spanish.  The dataset is used for the Knowledge Graph Completion and Entity Alignment task. DPB-5L (English) is a subset of DPB-5L with English KG."}, {"id": "regdb-c", "name": "RegDB-C", "description": "RegDB-C is an evaluation set that consists of algorithmically generated corruptions applied to the RegDB test-set (color images). These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."}, {"id": "celeba-masks", "name": "CelebA+masks", "description": "The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on Spark AR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 196,254 (96.8%) masks for the CelebA data set."}, {"id": "demcare", "name": "DemCare", "description": "Dem@Care is providing the following datasets, which are collected during lab and home experiments. The data collection took place in the Greek Alzheimer\u2019s Association for Dementia and Related Disorders in Thessaloniki, Greece and in participants\u2019 homes. The datasets include video and audio recordings as well as data from physiological sensors. Moreover, they include data from sleep, motion and plug sensors."}, {"id": "stare-structured-analysis-of-the-retina", "name": "STARE (Structured Analysis of the Retina)", "description": "The STARE (Structured Analysis of the Retina) dataset is a dataset for retinal vessel segmentation. It contains 20 equal-sized (700\u00d7605) color fundus images. For each image, two groups of annotations are provided.."}, {"id": "protoqa", "name": "ProtoQA", "description": "ProtoQA is a question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show FAMILY- FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers."}, {"id": "psg-dataset", "name": "PSG Dataset", "description": "PSG dataset has 48749 images with 133 object classes (80 objects and 53 stuff) and 56 predicate classes. It annotates inter-segment relations based on COCO panoptic segmentation."}, {"id": "vcsl-video-copy-segment-localization", "name": "VCSL (Video Copy Segment Localization)", "description": "VCSL (Video Copy Segment Localization) is a new comprehensive segment-level annotated video copy dataset. Compared with existing copy detection datasets restricted by either video-level annotation or small-scale, VCSL not only has two orders of magnitude more segment level labelled data, with 160k realistic video copy pairs containing more than 280k localized copied segment pairs, but also covers a variety of video categories and a wide range of video duration. All the copied segments inside each collected video pair are manually extracted and accompanied by precisely annotated starting and ending timestamps."}, {"id": "cough", "name": "COUGH", "description": "A large challenging dataset, COUGH, for COVID-19 FAQ retrieval. Specifically, similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, User Query Bank and Annotated Relevance Set. FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO)."}, {"id": "eth-py150-open", "name": "ETH Py150 Open", "description": "A massive, deduplicated corpus of 7.4M Python files from GitHub."}, {"id": "orvs-online-retinal-image-for-vessel-segmentation-orvs", "name": "ORVS (Online Retinal image for Vessel Segmentation (ORVS))", "description": "The ORVS dataset has been newly established as a collaboration between the computer science and visual-science departments at the University of Calgary."}, {"id": "re-tacred-revised-tacred", "name": "Re-TACRED (Revised-TACRED)", "description": "The Re-TACRED dataset is a significantly improved version of the TACRED dataset for relation extraction. Using new crowd-sourced labels, Re-TACRED prunes poorly annotated sentences and addresses TACRED relation definition ambiguity, ultimately correcting 23.9% of TACRED labels. This dataset contains over 91 thousand sentences spread across 40 relations. Dataset presented at AAAI 2021."}, {"id": "asr-glue", "name": "ASR-GLUE", "description": "The ASR-GLUE benchmark is a collection of 6 different NLU (Natural Language Understanding) tasks for evaluating the performance of models under automatic speech recognition (ASR) error across 3 different levels of background noise and 6 speakers with various voice characteristics."}, {"id": "kits19-the-2019-kidney-and-kidney-tumor-segmentation-challenge", "name": "KiTS19 (The 2019 Kidney and Kidney Tumor Segmentation Challenge)", "description": "The 2021 Kidney and Kidney Tumor Segmentation challenge (abbreviated KiTS21) is a competition in which teams compete to develop the best system for automatic semantic segmentation of renal tumors and surrounding anatomy."}, {"id": "spectre-v1", "name": "Spectre-v1", "description": "A dataset of assembly functions that are vulnerable to Spectre-V1 attack."}, {"id": "korean-hatespeech-dataset", "name": "Korean HateSpeech Dataset", "description": "Presents 9.4K manually labeled entertainment news comments for identifying Korean toxic speech, collected from a widely used online news platform in Korea."}, {"id": "libri-adapt", "name": "Libri-Adapt", "description": "Libri-Adapt aims to support unsupervised domain adaptation research on speech recognition models."}, {"id": "naturalproofs", "name": "NaturalProofs", "description": "The NaturalProofs Dataset is a large-scale dataset for studying mathematical reasoning in natural language. NaturalProofs consists of roughly 20,000 theorem statements and proofs, 12,500 definitions, and 1,000 additional pages (e.g. axioms, corollaries) derived from ProofWiki, an online compendium of mathematical proofs written by a community of contributors. "}, {"id": "hico-humans-interacting-with-common-objects", "name": "HICO (Humans Interacting with Common Objects)", "description": "HICO is a benchmark for recognizing human-object interactions (HOI). "}, {"id": "ava-speech", "name": "AVA-Speech", "description": "Contains densely labeled speech activity in YouTube videos, with the goal of creating a shared, available dataset for this task. "}, {"id": "core50", "name": "CORe50", "description": "CORe50 is a dataset designed for assessing Continual Learning techniques in an Object Recognition context."}, {"id": "aracovid19-mfh-aracovid19-mfh-arabic-covid-19-multi-label-fake-news-and-hate-speech-detection-dataset", "name": "AraCOVID19-MFH (AraCOVID19-MFH: Arabic COVID-19 Multi-label Fake News and Hate Speech Detection Dataset)", "description": "AraCOVID19-MFH is a manually annotated multi-label Arabic COVID-19 fake news and hate speech detection dataset. The dataset contains 10,828 Arabic tweets annotated with 10 different labels."}, {"id": "cornell-60-20-20-random-splits", "name": "Cornell (60%/20%/20% random splits)", "description": "Node classification on Cornell with 60%/20%/20% random splits for training/validation/test."}, {"id": "sr-raw", "name": "SR-RAW", "description": "Raw sensor dataset where each sequence captures 7 (few contain 6) images (RAW and JPG) taken by different focal lengths."}, {"id": "a-bi-atrial-statistical-shape-model-and-100-volumetric-anatomical-models-of-the-atria", "name": "A Bi-atrial Statistical Shape Model and 100 Volumetric Anatomical Models of the Atria", "description": "This dataset is part of the publication \"A bi-atrial statistical shape model for large-scale in silico studies of human atria: Model development and application to ECG simulations\" by Nagel et al. (https://doi.org/10.1016/j.media.2021.102210). It includes a bi-atrial statistical shape model built based on 47 MR and CT images (Left atrium segmentation challenge (Tobon-Gomez, 2015), Left atrium fibrosis and scar segmentation challenge (Karim, 2013), Left atrial wall thickness challenge (Karim, 2018)). ScalismoLab (https://scalismo.org) was used for parts of the model generation. Further Details are explained in the paper. The SSM is available as an h5 file including information about the mean shape's vertex locations and their triangulation as well as the eigenvectors and -values. "}, {"id": "srrs-snow-removal-in-realistic-scenario", "name": "SRRS (Snow Removal in Realistic Scenario)", "description": "SRRS (Snow Removal in Realistic Scenario) contains 15000 synthesized snow images and 1000 snow images in real scenarios downloaded from the Internet."}, {"id": "wider-web-image-dataset-for-event-recognition", "name": "WIDER (Web Image Dataset for Event Recognition)", "description": "WIDER is a dataset for complex event recognition from static images. As of v0.1, it contains 61 event categories and around 50574 images annotated with event class labels."}, {"id": "toad-gan", "name": "TOAD-GAN", "description": "A procedurally generated jump'n'run game with control over level similarity."}, {"id": "delidata", "name": "DeliData", "description": "DeliData is the first publicly available dataset containing collaborative conversations on solving a cognitive task, consisting of 500 group dialogues and 14k utterances."}, {"id": "bipar", "name": "BiPaR", "description": "BiPaR is a manually annotated bilingual parallel novel-style machine reading comprehension (MRC) dataset, developed to support monolingual, multilingual and cross-lingual reading comprehension on novels. The biggest difference between BiPaR and existing reading comprehension datasets is that each triple (Passage, Question, Answer) in BiPaR is written in parallel in two languages. BiPaR is diverse in prefixes of questions, answer types and relationships between questions and passages. Answering the questions requires reading comprehension skills of coreference resolution, multi-sentence reasoning, and understanding of implicit causality."}, {"id": "empatheticdialogues", "name": "EmpatheticDialogues", "description": "The EmpatheticDialogues dataset is a large-scale multi-turn empathetic dialogue dataset collected on the Amazon Mechanical Turk, containing 24,850 one-to-one open-domain conversations. Each conversation was obtained by pairing two crowd-workers: a speaker and a listener. The speaker is asked to talk about the personal emotional feelings. The listener infers the underlying emotion through what the speaker says and responds empathetically. The dataset provides 32 evenly distributed emotion labels."}, {"id": "toyadmos", "name": "ToyADMOS", "description": "ToyADMOS dataset is a machine operating sounds dataset of approximately 540 hours of normal machine operating sounds and over 12,000 samples of anomalous sounds collected with four microphones at a 48kHz sampling rate, prepared by Yuma Koizumi and members in NTT Media Intelligence Laboratories. The ToyADMOS dataset is designed for anomaly detection in machine operating sounds (ADMOS) research. It is designed for three tasks of ADMOS: product inspection (toy car), fault diagnosis for fixed machine (toy conveyor), and fault diagnosis for moving machine (toy train)."}, {"id": "3d-lane-synthetic-dataset", "name": "3D Lane Synthetic Dataset", "description": "This is a synthetic dataset constructed to stimulate the development and evaluation of 3D lane detection methods."}, {"id": "aom-ctc", "name": "AOM-CTC", "description": "This is the Current Video sequence set from the AOM-CTC."}, {"id": "glasstemp-glass-transition-temperature", "name": "GlassTemp (Glass Transition Temperature)", "description": "The GlassTemp dataset is collected from Polyinfo. It uses monomers as polymer graphs to predict the property of glass transition temperature. The glass transition temperature of the material itself denotes the temperature range over which this glass transition takes place."}, {"id": "brax", "name": "BRAX", "description": "Brax is a differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators. Brax is written in JAX and is designed for use on acceleration hardware. It is both efficient for single-device simulation, and scalable to massively parallel simulation on multiple devices, without the need for pesky datacenters."}, {"id": "nmed-t-naturalistic-music-eeg-dataset-tempo", "name": "NMED-T (Naturalistic Music EEG Dataset - Tempo)", "description": "Losorelli, Steven, Nguyen, Duc T., Dmochowski, Jacek P., and Kaneshiro, Blair"}, {"id": "us-4", "name": "US-4", "description": "The US-4 is a dataset of Ultrasound (US) images. It is a video-based image dataset that contains over 23,000 high-resolution images from four US video sub-datasets, where two sub-datasets are newly collected by experienced doctors for this dataset."}, {"id": "bengali-hate-speech", "name": "Bengali Hate Speech", "description": "Introduces three datasets of expressing hate, commonly used topics, and opinions for hate speech detection, document classification, and sentiment analysis, respectively. "}, {"id": "voice-conversion-challenge-2018", "name": "Voice Conversion Challenge 2018", "description": "Voice conversion (VC) is a technique to transform a speaker identity included in a source speech waveform into a different one while preserving linguistic information of the source speech waveform. The Voice Conversion Challenge (VCC) 2016 was launched in 2016 at Interspeech 2016. The objective of the 2016 challenge was to better understand different VC techniques built on a freely-available common dataset to look at a common goal, and to share views about unsolved problems and challenges faced by the current VC techniques. The VCC 2016 focused on the most basic VC task, that is, the construction of VC models that automatically transform the voice identity of a source speaker into that of a target speaker using a parallel clean training database where source and target speakers read out the same set of utterances in a professional recording studio. 17 research groups had participated in the 2016 challenge. The challenge was successful and it established new standard evaluation methodology and protocols for bench-marking the performance of VC systems. The second edition of VCC was launched in 2018, the VCC 2018. In this second edition, three aspects of the challenge were revised. First, the amount of speech data used for the construction of participant's VC systems was reduced to half. This is based on feedback from participants in the previous challenge and this is also essential for practical applications. Second, a more challenging task refereed to a Spoke task in addition to a similar task to the 1st edition was introduced, which we call a Hub task. In the Spoke task, participants need to build their VC systems using a non-parallel database in which source and target speakers read out different sets of utterances. Both parallel and non-parallel voice conversion systems are evaluated via the same large-scale crowdsourcing listening test. Third, bridging the gap between the ASV and VC communities was also attempted. Since new VC systems developed for the VCC 2018 may be strong candidates for enhancing the ASVspoof 2015 database, spoofing performance of the VC systems based on anti-spoofing scores was assessed."}, {"id": "sbic-social-bias-inference-corpus", "name": "SBIC (Social Bias Inference Corpus)", "description": "To support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups."}, {"id": "softattributes-softattributes-relative-movie-attribute-dataset-for-soft-attributes", "name": "SoftAttributes (SoftAttributes: Relative movie attribute dataset for soft attributes)", "description": "The dataset consists of sets of movie titles, with each set annotated with a single English soft attribute (subjective descriptive property, such as 'confusing' or 'romantic') and a reference movie. For each set, a crowd worker has placed the movies into three sets: more, equally, and less than the reference movie. There are 5,991 such sets, from which one can infer approximately 250,000 pairwise preferences over movies for the 60 distinct soft attributes studied."}, {"id": "humaneval", "name": "HumanEval", "description": "This is an evaluation harness for the HumanEval problem solving dataset described in the paper \"Evaluating Large Language Models Trained on Code\". It used to measure functional correctness for synthesizing programs from docstrings. It consists of 164 original programming problems, assessing language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions."}, {"id": "toyadmos2", "name": "ToyADMOS2", "description": "ToyADMOS2 is a dataset of miniature-machine operating sounds for anomalous sound detection under domain shift conditions."}, {"id": "skillspan-hard-and-soft-skill-extraction-from-english-job-postings", "name": "SkillSpan (Hard and Soft Skill Extraction from English Job Postings)", "description": "SkillSpan is a dataset for Skill Extraction (SE). It is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, the authors introduce SkillSpan, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans."}, {"id": "bughunter", "name": "BugHunter", "description": "The BugHunter dataset is an automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information."}, {"id": "libricount", "name": "LibriCount", "description": "LibriCount is a synthetic dataset for speaker count estimation. The dataset contains a simulated cocktail party environment of 0 to 10 speakers, mixed with 0dB SNR from random utterances of different speakers from the LibriSpeech CleanTest dataset. All recordings are of 5s durations, and all speakers are active for the most part of the recording."}, {"id": "xtreme-cross-lingual-transfer-evaluation-of-multilingual-encoders", "name": "XTREME (Cross-Lingual Transfer Evaluation of Multilingual Encoders)", "description": "The Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark was introduced to encourage more research on multilingual transfer learning,. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics."}, {"id": "scan-cad-object-similarity-dataset", "name": "Scan-CAD Object Similarity Dataset", "description": "A dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. "}, {"id": "diml-cvl-rgb-d-dataset", "name": "DIML/CVl RGB-D Dataset", "description": "This dataset contains synchronized RGB-D frames from both Kinect v2 and Zed stereo camera. For the outdoor scene, the authors first generate disparity maps using an accurate stereo matching method and convert them using calibration parameters. A per-pixel confidence map of disparity is also provided. The scenes are captured at various places, e.g., offices, rooms, dormitory, exhibition center, street, road etc., from Yonsei University and Ewha University."}, {"id": "wmt19-metrics-task", "name": "WMT19 Metrics Task", "description": "This shared task will examine automatic evaluation metrics for machine translation. The goals of the shared metrics task are:"}, {"id": "phase-physically-grounded-abstract-social-events", "name": "PHASE (PHysically-grounded Abstract Social Events)", "description": "PHASE is a dataset of physically-grounded abstract social events, that resemble a wide range of real-life social interactions by including social concepts such as helping another agent. PHASE consists of 2D animations of pairs of agents moving in a continuous space generated procedurally using a physics engine and a hierarchical planner. Agents have a limited field of view, and can interact with multiple objects, in an environment that has multiple landmarks and obstacles. Using PHASE, we design a social recognition task and a social prediction task. PHASE is validated with human experiments demonstrating that humans perceive rich interactions in the social events, and that the simulated agents behave similarly to humans."}, {"id": "metaphorical-connections", "name": "Metaphorical Connections", "description": "The Metaphorical Connections dataset is a poetry dataset that contains annotations between metaphorical prompts and short poems. Each poem is annotated whether or not it successfully communicates the idea of the metaphorical prompt."}, {"id": "retinal-lesions", "name": "Retinal-Lesions", "description": "Over 1.5K images selected from the public Kaggle DR Detection dataset; Five DR grades (DR0 / DR1 / DR2 / DR3 / DR4), re-labeled by a panel of 45 experienced ophthalmologists; Eight retinal lesion classes, including microaneurysm, intraretinal hemorrhage, hard exudate, cotton-wool spot, vitreous hemorrhage, preretinal hemorrhage, neovascularization and fibrous proliferation; Over 34K expert-labeled pixel-level lesion segments; Multi-task, i.e., lesion segmentation, lesion classification, and DR grading."}, {"id": "cova-cova-dataset-for-webpage-object-detection-information-extraction", "name": "CoVA (CoVA dataset for Webpage Object Detection / Information Extraction)", "description": "We labeled 7,740 webpage screenshots spanning 408 domains (Amazon, Walmart, Target, etc.). Each of these webpages contains exactly one labeled price, title, and image. All other web elements are labeled as background. On average, there are 90 web elements in a webpage."}, {"id": "semantic3d", "name": "Semantic3D", "description": "Semantic3D is a point cloud dataset of scanned outdoor scenes with over 3 billion points. It contains 15 training and 15 test scenes annotated with 8 class labels. This large labelled 3D point cloud data set of natural covers a range of diverse urban scenes: churches, streets, railroad tracks, squares, villages, soccer fields, castles to name just a few. The point clouds provided are scanned statically with state-of-the-art equipment and contain very fine details."}, {"id": "semantic-textual-similarity-2012-2016-sts", "name": "Semantic Textual Similarity (2012 - 2016) (STS)", "description": "Semantic Textual Similarity (2012 - 2016) involves a set of semantic textual similarity datasets that were part of previous shared tasks (2012-2016):"}, {"id": "wikineural", "name": "WikiNEuRal", "description": "WikiNEuRal is a high-quality automatically-generated dataset for Multilingual Named Entity Recognition."}, {"id": "ost300", "name": "OST300", "description": "OST300 is an outdoor scene dataset with 300 test images of outdoor scenes, and a training set of 7 categories of images with rich textures."}, {"id": "motfront", "name": "MOTFront", "description": "MOTFront provides photo-realistic RGB-D images with their corresponding instance segmentation masks, class labels, 2D & 3D bounding boxes, 3D geometry, 3D poses and camera parameters. The MOTFront dataset comprises 2,381 unique indoor sequences with a total of 60,000 images and is based on the 3D-FRONT dataset."}, {"id": "tum-gaid", "name": "TUM-GAID", "description": "TUM-GAID (TUM Gait from Audio, Image and Depth) collects 305 subjects performing two walking trajectories in an indoor environment. The first trajectory is traversed from left to right and the second one from right to left. Two recording sessions were performed, one in January, where subjects wore heavy jackets and mostly winter boots, and another one in April, where subjects wore lighter clothes. The action is captured by a Microsoft Kinect sensor which provides a video stream with a resolution of 640\u00d7480 pixels and a frame rate around 30 FPS."}, {"id": "amharic-error-corpus", "name": "Amharic Error Corpus", "description": "Amharic Error Corpus is a manually annotated spelling error corpus for Amharic, lingua franca in Ethiopia. The corpus is designed to be used for the evaluation of spelling error detection and correction. The misspellings are tagged as non-word and real-word errors. In addition, the contextual information available in the corpus makes it useful in dealing with both types of spelling errors."}, {"id": "multibench", "name": "MultiBench", "description": "MultiBench, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers evaluation methodology to study (1) generalization, (2) time and space complexity, and (3) modality robustness."}, {"id": "dblp-citation-network-dataset", "name": "DBLP (Citation Network Dataset)", "description": "The DBLP is a citation network dataset. The citation data is extracted from DBLP, ACM, MAG (Microsoft Academic Graph), and other sources. The first version contains 629,814 papers and 632,752 citations. Each paper is associated with abstract, authors, year, venue, and title. The data set can be used for clustering with network and side information, studying influence in the citation network, finding the most influential papers, topic modeling analysis, etc."}, {"id": "hkr-handwritten-kazakh-and-russian-hkr-database-for-text-recognition", "name": "HKR (Handwritten Kazakh and Russian (HKR) Database for Text Recognition)", "description": "The database is written in Cyrillic and shares the same 33 characters. Besides these characters, the Kazakh alphabet also contains 9 additional specific characters. This dataset is a collection of forms. The sources of all the forms in the datasets were generated by LATEX which subsequently was filled out by persons with their handwriting. The database consists of more than 1400 filled forms. There are approximately 63000 sentences, more than 715699 symbols produced by approximately 200 diferent writers. We utilized three different datasets described as following:"}, {"id": "adaptiope", "name": "Adaptiope", "description": "Adaptiope is a domain adaptation dataset with 123 classes in the three domains synthetic, product and real life. One of the main goals of Adaptiope is to offer a clean and well curated set of images for domain adaptation. This was necessary as many other common datasets in the area suffer from label noise and low quality images. Additionally, Adaptiope's class set was chosen in a way that minimizes the overlap with the class set of the commonly used ImageNet pretraining, therefore preventing information leakage in a domain adaptation setup."}, {"id": "wikisection", "name": "WikiSection", "description": "A publicly available dataset with 242k labeled sections in English and German from two distinct domains: diseases and cities."}, {"id": "pastrie-prepositions-annotated-with-supersense-tags-in-reddit-international-english", "name": "PASTRIE (Prepositions Annotated with Supersense Tags in Reddit International English)", "description": "Prepositions Annotated with Supersense Tags in Reddit International English (PASTRIE) is a new corpus containing manually annotated preposition supersenses of English data from presumed speakers of four L1s: English, French, German, and Spanish"}, {"id": "fire-fundus-image-registration-dataset", "name": "FIRE (Fundus Image Registration Dataset)", "description": "Fundus Image Registration Dataset (FIRE) is a dataset consisting of 129 retinal images forming 134 image pairs. These image pairs are split into 3 different categories depending on their characteristics. The images were acquired with a Nidek AFC-210 fundus camera, which acquires images with a resolution of 2912x2912 pixels and a FOV of 45\u00b0 both in the x and y dimensions. Images were acquired at the Papageorgiou Hospital, Aristotle University of Thessaloniki, Thessaloniki from 39 patients."}, {"id": "compguesswhat", "name": "CompGuessWhat?!", "description": "CompGuessWhat?! extends the original GuessWhat?! datasets with a rich semantic representations in the form of scene graphs associated with every image used as reference scene for the guessing games."}, {"id": "separated-coco", "name": "Separated COCO", "description": "Separated COCO is automatically generated subsets of COCO val dataset, collecting separated objects for a large variety of categories in real images in a scalable manner, where target object segmentation mask is separated into distinct regions by the occluder."}, {"id": "acoustic-extinguisher-fire-dataset", "name": "Acoustic Extinguisher Fire Dataset", "description": "Yavuz Selim TASPINAR, Murat KOKLU and Mustafa ALTIN"}, {"id": "digestpath", "name": "DigestPath", "description": "Introduced by Da et al. in DigestPath: a Benchmark Dataset with Challenge Review for the Pathological Detection and Segmentation of Digestive-System"}, {"id": "swag-situations-with-adversarial-generations", "name": "SWAG (Situations With Adversarial Generations)", "description": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). SWAG (Situations With Adversarial Generations) is a large-scale dataset for this task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning."}, {"id": "vizwiz-qualityissues", "name": "VizWiz-QualityIssues", "description": "A large-scale dataset that links the assessment of image quality issues to two practical vision tasks: image captioning and visual question answering."}, {"id": "retrieval-sfm", "name": "Retrieval-SfM", "description": "The Retrieval-SFM dataset is used for instance image retrieval. The dataset contains 28559 images from 713 locations in the world. Each image has a label indicating the location it belongs to. Most locations are famous man-made architectures such as palaces and towers, which are relatively static and positively contribute to visual place recognition. The training dataset contains various perceptual changes including variations in viewing angles, occlusions and illumination conditions, etc."}, {"id": "plittersdorf", "name": "Plittersdorf", "description": "A set of 221 stereo videos captured by the SOCRATES stereo camera trap in a wildlife park in Bonn, Germany between February and July of 2022. A subset of frames is labeled with instance annotations in the COCO format."}, {"id": "robonet", "name": "RoboNet", "description": "An open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation."}, {"id": "photoshape", "name": "PhotoShape", "description": "The PhotoShape dataset consists of photorealistic, relightable, 3D shapes produced by the work proposed in the work of Park et al. (2021)."}, {"id": "freihand", "name": "FreiHAND", "description": "FreiHAND is a 3D hand pose dataset which records different hand actions performed by 32 people. For each hand image, MANO-based 3D hand pose annotations are provided. It currently contains 32,560 unique training samples and 3960 unique samples for evaluation. The training samples are recorded with a green screen background allowing for background removal. In addition, it applies three different post processing strategies to training samples for data augmentation. However, these post processing strategies are not applied to evaluation samples."}, {"id": "stanford-cars", "name": "Stanford Cars", "description": "The Stanford Cars dataset consists of 196 classes of cars with a total of 16,185 images, taken from the rear. The data is divided into almost a 50-50 train/test split with 8,144 training images and 8,041 testing images. Categories are typically at the level of Make, Model, Year. The images are 360\u00d7240."}, {"id": "libritts", "name": "LibriTTS", "description": "LibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, prepared by Heiga Zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is designed for TTS research. It is derived from the original materials (mp3 audio files from LibriVox and text files from Project Gutenberg) of the LibriSpeech corpus. The main differences from the LibriSpeech corpus are listed below:"}, {"id": "wmt-2014", "name": "WMT 2014", "description": "WMT 2014 is a collection of datasets used in shared tasks of the Ninth Workshop on Statistical Machine Translation. The workshop featured four tasks:"}, {"id": "lvis", "name": "LVIS", "description": "LVIS is a dataset for long tail instance segmentation. It has annotations for over 1000 object categories in 164k images."}, {"id": "stanford-background-standford-background-dataset", "name": "Stanford Background (Standford Background Dataset)", "description": "The Stanford Background dataset contains 715 RGB images and the corresponding label images. Images are approximately 240\u00d7320 pixels in size and pixels are classified into eight different categories"}, {"id": "airs", "name": "AIRS", "description": "The AIRS (Aerial Imagery for Roof Segmentation) dataset provides a wide coverage of aerial imagery with 7.5 cm resolution and contains over 220,000 buildings. The task posed for AIRS is defined as roof segmentation. "}, {"id": "elfw-extended-labeled-faces-in-the-wild", "name": "ELFW (Extended Labeled Faces in-the-Wild)", "description": "Extended Labeled Faces in-the-Wild (ELFW) is a dataset supplementing with additional face-related categories \u2014and also additional faces\u2014 the originally released semantic labels in the vastly used Labeled Faces in-the-Wild (LFW) dataset. Additionally, two object-based data augmentation techniques are deployed to synthetically enrich under-represented categories which, in benchmarking experiments, reveal that not only segmenting the augmented categories improves, but also the remaining ones benefit."}, {"id": "griddly", "name": "Griddly", "description": "Griddly is an environment for grid-world based research.  Griddly provides a highly optimized game state and rendering engine with a flexible high-level interface for configuring environments. Not only does Griddly offer simple interfaces for single, multi-player and RTS games, but also multiple methods of rendering, configurable partial observability and interfaces for procedural content generation."}, {"id": "totalcapture", "name": "TotalCapture", "description": "The TotalCapture dataset consists of 5 subjects performing several activities such as walking, acting, a range of motion sequence (ROM) and freestyle motions, which are recorded using 8 calibrated, static HD RGB cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet, however the IMU data is not required for our experiments. The dataset has publicly released foreground mattes and RGB images. Ground-truth poses are obtained using a marker-based motion capture system, with the markers are <5mm in size. All data is synchronised and operates at a framerate of 60Hz, providing ground truth poses as joint positions."}, {"id": "grasping-dataset-suction-based-suction-based-grasping-dataset", "name": "Grasping dataset: suction-based (suction-based-grasping-dataset)", "description": "A small and simple dataset featuring RGB-D images and heightmaps of various objects in a bin with manually annotated suctionable regions"}, {"id": "oie2016", "name": "OIE2016", "description": "OIE2016 is the first large-scale OpenIE benchmark. It is created by automatic conversion from QA-SRL [He et al., 2015], a semantic role labeling dataset. The sentences are from news (e.g., WSJ) and encyclopedia (e.g., WIKI) domains. Since there are no restrictions on the elements of OpenIE extractions, partial-matching criteria instead of exact-matching is typically used. Hence, the evaluation script can tolerate the extractions that are slightly different from the gold annotation. "}, {"id": "conan-counter-narratives-through-nichesourcing", "name": "CONAN (COunter NArratives through Nichesourcing)", "description": "COunter NArratives through Nichesourcing (CONAN) is a dataset that consists of 4,078 pairs over the 3 languages. Additionally, 3 types of metadata are provided: expert demographics, hate speech sub-topic and counter-narrative type. The dataset is augmented through translation (from Italian/French to English) and paraphrasing, which brought the total number of pairs to 14.988."}, {"id": "chfinann", "name": "ChFinAnn", "description": "Ten years (2008-2018) ChFinAnn documents and human-summarized event knowledge bases to conduct the DS-based event labeling. Five event types included: Equity Freeze (EF), Equity Repurchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), which belong to major events required to be disclosed by the regulator and may have a huge impact on the company value. To ensure the labeling quality, the authors set constraints for matched document-record pairs."}, {"id": "gazefollow", "name": "GazeFollow", "description": "GazeFollow is a large-scale dataset annotated with the location of where people in images are looking. It uses several major datasets that contain people as a source of images: 1, 548 images from SUN, 33, 790 images from MS COCO, 9, 135 images from Actions 40, 7, 791 images from PASCAL, 508 images from the ImageNet detection challenge and 198, 097 images from the Places dataset. This concatenation results in a challenging and large image collection of people performing diverse activities in many everyday scenarios."}, {"id": "h-dibco-2014", "name": "H-DIBCO 2014", "description": "H-DIBCO 2014 is the International Document Image Binarization Competition which is dedicated to handwritten document images organized in conjunction with ICFHR 2014 conference. The objective of the contest is to identify current advances in handwritten document image binarization using meaningful evaluation performance measures."}, {"id": "clear-weather-dense", "name": "Clear Weather (DENSE)", "description": "We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are: - We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions. - The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects. - In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods. - Please check out our paper for more information."}, {"id": "natural-hazards-twitter-dataset", "name": "Natural Hazards Twitter Dataset", "description": "Natural Hazards is a natural disaster dataset with sentiment labels, which contains nearly 50,00 Twitter data about different natural disasters in the United States (e.g., a tornado in 2011, a hurricane named Sandy in 2012, a series of floods in 2013, a hurricane named Matthew in 2016, a blizzard in 2016, a hurricane named Harvey in 2017, a hurricane named Michael in 2018, a series of wildfires in 2018, and a hurricane named Dorian in 2019)."}, {"id": "a-dataset-of-neonatal-eeg-recordings-with-seizures-annotations", "name": "A dataset of neonatal EEG recordings with seizures annotations", "description": "Neonatal seizures are a common emergency in the neonatal intensive care unit (NICU). There are many questions yet to be answered regarding the temporal/spatial characteristics of seizures from different pathologies, response to medication, effects on neurodevelopment and optimal detection. This dataset contains EEG recordings from human neonates and the visual interpretation of the EEG by the human expert. Multi-channel EEG was recorded from 79 term neonates admitted to the neonatal intensive care unit (NICU) at the Helsinki University Hospital. The median recording duration was 74 minutes (IQR: 64 to 96 minutes). EEGs were annotated by three experts for the presence of seizures. An average of 460 seizures were annotated per expert in the dataset, 39 neonates had seizures by consensus and 22 were seizure free by consensus. The dataset can be used as a reference set of neonatal seizures, for the development of automated methods of seizure detection and other EEG analysis, as well as for the analysis of inter-observer agreement."}, {"id": "1dsfm-1dsfm-landmarks", "name": "1DSfM (1DSfM Landmarks)", "description": "The 1DSfM Landmarks is a collection of community-based image reconstruction by Kyle Wilson and is comprised of 14 datasets with comparison to bundler ground truth. Notredame is provided separately. Datasets (tar.gz, 642 MB) Alamo images (tar, 2.0 GB) Ellis Island images (tar, 1.6 GB) Madrid Metropolis images (tar, 0.7 GB) Montreal Notre Dame images (tar, 1.6 GB) NYC_Library images (tar, 1.6 GB) Piazza del Popolo images (tar, 1.5 GB) Piccadilly images (tar, 3.7 GB) Roman Forum images (tar, 1.5 GB) Tower of London images (tar, 1.1 GB) Trafalgar images (tar, 8.5 GB) Union Square images (tar, 3.6 GB) Vienna Cathedral images (tar, 3.3 GB) Yorkminster images (tar, 2.2 GB) Gendarmenmarkt images (tar, 1.0 GB) References Robust Global Translations with 1DSfM Kyle Wilson and Noah Snavely, ECCV 2014"}, {"id": "reviewqa", "name": "ReviewQA", "description": "ReviewQA is a question-answering dataset based on hotel reviews. The questions of this dataset are linked to a set of relational understanding competencies that a model is expected to master. Indeed, each question comes with an associated type that characterizes the required competency."}, {"id": "imagenet-w-imagenet-watermark", "name": "ImageNet-W (ImageNet-Watermark)", "description": "ImageNet-W(atermark) is a test set to evaluate models\u2019 reliance on the newly found watermark shortcut in ImageNet, which is used to predict the carton class. ImageNet-W is created by overlaying transparent watermarks on the ImageNet validation set. Two metrics are used to evaluate watermark shortcut reliance: (1) IN-W Gap: the top-1 accuracy drop from ImageNet to ImageNet-W, (2) Carton Gap: carton class accuracy increase from ImageNet to ImageNet-W. Combining ImageNet-W with previous out-of-distribution variants of ImageNet (e.g., Stylized ImageNet, ImageNet-R, ImageNet-9) forms a comprehensive suite of multi-shortcut evaluation on ImageNet."}, {"id": "aesvqa", "name": "AesVQA", "description": "AesVQA is a dataset that contains 72168 high-quality images and 324756 pairs of aesthetic questions. This dataset addresses the task of aesthetic VQA and introduces subjectiveness into VQA tasks."}, {"id": "scierc", "name": "SciERC", "description": "SciERC dataset is a collection of 500 scientific abstract annotated with scientific entities, their relations, and coreference clusters. The abstracts are taken from 12 AI conference/workshop proceedings in four AI communities, from the Semantic Scholar Corpus. SciERC extends previous datasets in scientific articles SemEval 2017 Task 10 and SemEval 2018 Task 7 by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links."}, {"id": "inria-aerial-image-labeling", "name": "INRIA Aerial Image Labeling", "description": "The INRIA Aerial Image Labeling dataset is comprised of 360 RGB tiles of 5000\u00d75000px with a spatial resolution of 30cm/px on 10 cities across the globe. Half of the cities are used for training and are associated to a public ground truth of building footprints. The rest of the dataset is used only for evaluation with a hidden ground truth. The dataset was constructed by combining public domain imagery and public domain official building footprints."}, {"id": "ho-3d", "name": "HO-3D", "description": "A hand-object interaction dataset with 3D pose annotations of hand and object. The dataset contains 66,034 training images and 11,524 test images from a total of 68 sequences. The sequences are captured in multi-camera and single-camera setups and contain 10 different subjects manipulating 10 different objects from YCB dataset. The annotations are automatically obtained using an optimization algorithm. The hand pose annotations for the test set are withheld and the accuracy of the algorithms on the test set can be evaluated with standard metrics using the CodaLab challenge submission(see project page). The object pose annotations for the test and train set are provided along with the dataset."}, {"id": "video2gif", "name": "Video2GIF", "description": "The Video2GIF dataset contains over 100,000 pairs of GIFs and their source videos. The GIFs were collected from two popular GIF websites (makeagif.com, gifsoup.com) and the corresponding source videos were collected from YouTube in Summer 2015. IDs and URLs of the GIFs and the videos are provided, along with temporal alignment of GIF segments to their source videos. The dataset shall be used to evaluate GIF creation and video highlight techniques."}, {"id": "policyqa", "name": "PolicyQA", "description": "A dataset that contains 25,017 reading comprehension style examples curated from an existing corpus of 115 website privacy policies. PolicyQA provides 714 human-annotated questions written for a wide range of privacy practices."}, {"id": "germanquad", "name": "GermanQuAD", "description": "GermanQuAD is a Question Answering (QA) dataset of 13,722 extractive question/answer pairs in German."}, {"id": "rtc-reddit-time-corpus", "name": "RTC (Reddit Time Corpus)", "description": "RTC is a benchmark corpus of social media comments sampled over three years. The corpus consists of 36.36m unlabelled comments for adaptation and evaluation on an upstream masked language modelling task as well as 0.9m labelled comments for finetuning and evaluation on a downstream document classification task.  The Reddit Time Corpus (RTC) covers three years between March 2017 and February 2020 and is split into 36 evenly-sized monthly subsets based on comment timestamps. RTC is sampled from the Pushshift Reddit dataset."}, {"id": "casia-face-africa", "name": "CASIA-Face-Africa", "description": "CASIA-Face-Africa is a face image database which contains 38,546 images of 1,183 African subjects. Multi-spectral cameras are utilized to capture the face images under various illumination settings. Demographic attributes and facial expressions of the subjects are also carefully recorded. For landmark detection, each face image in the database is manually labeled with 68 facial keypoints. A group of evaluation protocols are constructed according to different applications, tasks, partitions and scenarios. The proposed database along with its face landmark annotations, evaluation protocols and preliminary results form a good benchmark to study the essential aspects of face biometrics for African subjects, especially face image preprocessing, face feature analysis and matching, facial expression recognition, sex/age estimation, ethnic classification, face image generation, etc."}, {"id": "persian-atis", "name": "Persian-ATIS", "description": "The PATIS is a Persian language dataset for intent detection and slot filling."}, {"id": "wiki-web-traffic-time-series-forecasting", "name": "Wiki (Web Traffic Time Series Forecasting)", "description": "There's a story behind every dataset and here's your opportunity to share yours."}, {"id": "dawn", "name": "DAWN", "description": "DAWN emphasizes a diverse traffic environment (urban, highway and freeway) as well as a rich variety of traffic flow. The DAWN dataset comprises a collection of 1000 images from real-traffic environments, which are divided into four sets of weather conditions: fog, snow, rain and sandstorms. The dataset is annotated with object bounding boxes for autonomous driving and video surveillance scenarios. This data helps interpreting effects caused by the adverse weather conditions on the performance of vehicle detection systems."}, {"id": "pendigits", "name": "Pendigits", "description": "We create a digit database by collecting 250 samples from 44 writers. The samples written by 30 writers are used for training, cross-validation and writer dependent testing, and the digits written by the other 14 are used for writer independent testing. This database is also available in the UNIPEN format."}, {"id": "clevr-dialog", "name": "CLEVR-Dialog", "description": "CLEVR-Dialog is a large diagnostic dataset for studying multi-round reasoning in visual dialog. Specifically, that authors construct a dialog grammar that is grounded in the scene graphs of the images from the CLEVR dataset. This combination results in a dataset where all aspects of the visual dialog are fully annotated. In total, CLEVR-Dialog contains 5 instances of 10-round dialogs for about 85k CLEVR images, totaling to 4.25M question-answer pairs."}, {"id": "houston", "name": "Houston", "description": "Houston is a hyperspectral image classification dataset. The hyperspectral imagery consists of 144 spectral bands in the 380 nm to 1050 nm region and has been calibrated to at-sensor spectral radiance units, SRU =$ \\mu \\text{W} /( \\text{cm}^2 \\text{ sr nm})$. The corresponding co-registered DSM consists of elevation in meters above sea level (per the Geoid 2012A model)."}, {"id": "gcdc-grammarly-corpus-of-discourse-coherence", "name": "GCDC (Grammarly Corpus of Discourse Coherence)", "description": "A corpus of real-world texts."}, {"id": "fquad-french-question-answering-dataset", "name": "FQuAD (French Question Answering Dataset)", "description": "A French Native Reading Comprehension dataset of questions and answers on a set of Wikipedia articles that consists of 25,000+ samples for the 1.0 version and 60,000+ samples for the 1.1 version."}, {"id": "youtube-vos-2018-youtube-video-object-segmentation", "name": "YouTube-VOS 2018 (Youtube Video Object Segmentation)", "description": "Youtube-VOS is a Video Object Segmentation dataset that contains 4,453 videos - 3,471 for training, 474 for validation, and 508 for testing. The training and validation videos have pixel-level ground truth annotations for every 5th frame (6 fps). It also contains Instance Segmentation annotations. It has more than 7,800 unique objects, 190k high-quality manual annotations and more than 340 minutes in duration."}, {"id": "sim10k", "name": "Sim10k", "description": "SIM10k is a synthetic dataset containing 10,000 images, which is rendered from the video game Grand Theft Auto V (GTA5)."}, {"id": "mobilityaids", "name": "MobilityAids", "description": "MobilityAids is a dataset for perception of people and their mobility aids. The annotated dataset contains five classes: pedestrian, person in wheelchair, pedestrian pushing a person in a wheelchair, person using crutches and person using a walking frame. In total the hospital dataset has over 17, 000 annotated RGB-D images, containing people categorized according to the mobility aids they use. The images were collected in the facilities of the Faculty of Engineering of the University of Freiburg and in a hospital in Frankfurt."}, {"id": "labelme", "name": "LabelMe", "description": "LabelMe database is a large collection of images with ground truth labels for object detection and recognition. The annotations come from two different sources, including the LabelMe online annotation tool."}, {"id": "clevr-x", "name": "CLEVR-X", "description": "CLEVR-X is a dataset that extends the CLEVR dataset with natural language explanations in the context of  VQA. It consists of 3.6 million natural language explanations for 850k question-image pairs."}, {"id": "urbansound8k", "name": "UrbanSound8K", "description": "Urban Sound 8K is an audio dataset that contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All excerpts are taken from field recordings uploaded to www.freesound.org."}, {"id": "risawoz", "name": "RiSAWOZ", "description": "RiSAWOZ is a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn semantically annotated dialogues, with more than 150K utterances spanning over 12 domains, which is larger than all previous annotated H2H conversational datasets. Both single- and multi-domain dialogues are constructed, accounting for 65% and 35%, respectively. Each dialogue is labelled with comprehensive dialogue annotations, including dialogue goal in the form of natural language description, domain, dialogue states and acts at both the user and system side. In addition to traditional dialogue annotations, it also includes linguistic annotations on discourse phenomena, e.g., ellipsis and coreference, in dialogues, which are useful for dialogue coreference and ellipsis resolution tasks."}, {"id": "retweet", "name": "RETWEET", "description": "RETWEET is a dataset of tweets and overall predominant sentiment of their replies."}, {"id": "reuters-21578", "name": "Reuters-21578", "description": "The Reuters-21578 dataset is a collection of documents with news articles. The original corpus has 10,369 documents and a vocabulary of 29,930 words."}, {"id": "tox21-tox21-machine-learning-data-set", "name": "Tox21 (Tox21 Machine Learning Data Set)", "description": "The Tox21 data set comprises 12,060 training samples and 647 test samples that represent chemical compounds. There are 801 \"dense features\" that represent chemical descriptors, such as molecular weight, solubility or surface area, and 272,776 \"sparse features\" that represent chemical substructures (ECFP10, DFS6, DFS8; stored in Matrix Market Format ). Machine learning methods can either use sparse or dense data or combine them. For each sample there are 12 binary labels that represent the outcome (active/inactive) of 12 different toxicological experiments. Note that the label matrix contains many missing values (NAs). The original data source and Tox21 challenge site is https://tripod.nih.gov/tox21/challenge/."}, {"id": "psi-iupui-csrc-pedestrian-situated-intent", "name": "PSI (IUPUI-CSRC Pedestrian Situated Intent)", "description": "The IUPUI-CSRC Pedestrian Situated Intent (PSI) benchmark dataset has two innovative labels besides comprehensive computer vision annotations. The first novel label is the dynamic intent changes for the pedestrians to cross in front of the ego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one is the text-based explanations of the driver reasoning process when estimating pedestrian intents and predicting their behaviors during the interaction period."}, {"id": "dailytalk", "name": "DailyTalk", "description": "DailyTalk is a high-quality conversational speech dataset designed for Text-to-Speech. We sampled, modified, and recorded 2,541 dialogues from the open-domain dialogue dataset DailyDialog which are adequately long to represent context of each dialogue."}, {"id": "pop909", "name": "POP909", "description": "POP909 is a dataset which contains multiple versions of the piano arrangements of 909 popular songs created by professional musicians. The main body of the dataset contains the vocal melody, the lead instrument melody, and the piano accompaniment for each song in MIDI format, which are aligned to the original audio files. Furthermore, annotations are provided of tempo, beat, key, and chords, where the tempo curves are hand-labelled and others are done by MIR algorithms."}, {"id": "general-100", "name": "General-100", "description": "The General-100 dataset is a dataset for image super-resolution. It contains 100 bmp format images with no compression) The size of the 100 images ranges from 710 x 704 (large) to 131 x 112 (small)."}, {"id": "d4rl", "name": "D4RL", "description": "D4RL is a collection of environments for offline reinforcement learning. These environments include Maze2D, AntMaze, Adroit, Gym, Flow, FrankKitchen and CARLA."}, {"id": "fakeddit", "name": "Fakeddit", "description": "Fakeddit is a novel multimodal dataset for fake news detection consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision."}, {"id": "sstem", "name": "ssTEM", "description": "We provide two image stacks where each contains 20 sections from serial section Transmission Electron Microscopy (ssTEM) of the Drosophila melanogaster third instar larva ventral nerve cord. Both stacks measure approx. 4.7 x 4.7 x 1 microns with a resolution of 4.6 x 4.6 nm/pixel and section thickness of 45-50 nm."}, {"id": "animerun", "name": "AnimeRun", "description": "AnimeRun is a 2D animation visual correspondence dataset. It is designed for tasks converting open source three-dimensional (3D) movies to full scenes in 2D style, including simultaneous moving background and interactions of multiple subjects."}, {"id": "bdd100k", "name": "BDD100K", "description": "Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue. More detail is at the dataset home page."}, {"id": "smc-text-corpus", "name": "SMC Text Corpus", "description": "The text corpus contains running text from various free licensed sources. - The whole content of Malayalam Wikipedia extracted on January 1, 2019 - News/Article from various sources, source mentioned in respective files:  - 251 Mb - 8,60,159 lines - 98,15,533 words - 10,11,11,885 characters"}, {"id": "wikigraphs", "name": "WikiGraphs", "description": "WikiGraphs is a dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. "}, {"id": "wikipedia-title", "name": "Wikipedia Title", "description": "Wikipedia Title is a dataset for learning character-level compositionality from the character visual characteristics. It consists of a collection of Wikipedia titles in Chinese, Japanese or Korean labelled with the category to which the article belongs."}, {"id": "mir-1k", "name": "MIR-1K", "description": "MIR-1K (Multimedia Information Retrieval lab, 1000 song clips) is a dataset designed for singing voice separation. It contains:"}, {"id": "carpk-car-parking-lot-dataset", "name": "CARPK (car parking lot dataset)", "description": "The Car Parking Lot Dataset (CARPK) contains nearly 90,000 cars from 4 different parking lots collected by means of drone (PHANTOM 3 PROFESSIONAL). The images are collected with the drone-view at approximate 40 meters height. The image set is annotated by bounding box per car. All labeled bounding boxes have been well recorded with the top-left points and the bottom-right points. It is supporting object counting, object localizing, and further investigations with the annotation format in bounding boxes."}, {"id": "holopix50k", "name": "Holopix50k", "description": "An in-the-wild stereo image dataset, comprising 49,368 image pairs contributed by users of the Holopix mobile social platform."}, {"id": "vizdoom", "name": "VizDoom", "description": "ViZDoom is an AI research platform based on the classical First Person Shooter game Doom. The most popular game mode is probably the so-called Death Match, where several players join in a maze and fight against each other. After a fixed time, the match ends and all the players are ranked by the FRAG scores defined as kills minus suicides. During the game, each player can access various observations, including the first-person view screen pixels, the corresponding depth-map and segmentation-map (pixel-wise object labels), the bird-view maze map, etc. The valid actions include almost all the keyboard-stroke and mouse-control a human player can take, accounting for moving, turning, jumping, shooting, changing weapon, etc. ViZDoom can run a game either synchronously or asynchronously, indicating whether the game core waits until all players\u2019 actions are collected or runs in a constant frame rate without waiting."}, {"id": "autonomous-driving-streaming-perception-benchmarrk", "name": "Autonomous-driving Streaming Perception Benchmarrk", "description": "The Autonomous-driving StreAming Perception (ASAP) benchmark is a benchmark to evaluate the online performance of vision-centric perception in autonomous driving. It extends the 2Hz annotated nuScenes dataset by generating high-frame-rate labels for the 12Hz raw images."}, {"id": "turing-change-point-dataset", "name": "Turing Change Point Dataset", "description": "Specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains. "}, {"id": "coin", "name": "COIN", "description": "The COIN dataset (a large-scale dataset for COmprehensive INstructional video analysis) consists of 11,827 videos related to 180 different tasks in 12 domains (e.g., vehicles, gadgets, etc.) related to our daily life. The videos are all collected from YouTube. The average length of a video is 2.36 minutes. Each video is labelled with 3.91 step segments, where each segment lasts 14.91 seconds on average. In total, the dataset contains videos of 476 hours, with 46,354 annotated segments."}, {"id": "smhd-self-reported-mental-health-diagnoses", "name": "SMHD (Self-reported Mental Health Diagnoses)", "description": "A novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users."}, {"id": "vidstg", "name": "VidSTG", "description": "The VidSTG dataset is a spatio-temporal video grounding dataset constructed based on the video relation dataset VidOR. VidOR contains 7,000, 835 and 2,165 videos for training, validation and testing, respectively. The goal of the Spatio-Temporal Video Grounding task (STVG) is to localize the spatio-temporal section of an untrimmed video that matches a given sentence depicting an object."}, {"id": "nuscenes-lidar-only", "name": "nuScenes LiDAR only", "description": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online."}, {"id": "pred18-pred18-predator-prey-davis-dataset", "name": "PRED18 (PRED18: Predator/Prey DAVIS Dataset)", "description": "Twenty DAVIS recordings with a total duration of about 1.25 hour were obtained by driving the two robots in the robot arena of the University of Ulster in Londonderry."}, {"id": "subjqa", "name": "SubjQA", "description": "SubjQA is a question answering dataset that focuses on subjective (as opposed to factual) questions and answers. The dataset consists of roughly 10,000 questions over reviews from 6 different domains: books, movies, grocery, electronics, TripAdvisor (i.e. hotels), and restaurants. Each question is paired with a review and a span is highlighted as the answer to the question (with some questions having no answer). Moreover, both questions and answer spans are assigned a subjectivity label by annotators. Questions such as \"How much does this product weigh?\" is a factual question (i.e., low subjectivity), while \"Is this easy to use?\" is a subjective question (i.e., high subjectivity)."}, {"id": "utd-mhad", "name": "UTD-MHAD", "description": "The UTD-MHAD dataset consists of 27 different actions performed by 8 subjects. Each subject repeated the action for 4 times, resulting in 861 action sequences in total. The RGB, depth, skeleton and the inertial sensor signals were recorded."}, {"id": "fever-fact-extraction-and-verification", "name": "FEVER (Fact Extraction and VERification)", "description": "FEVER is a publicly available dataset for fact extraction and verification against textual sources."}, {"id": "collective-activity", "name": "Collective Activity", "description": "The Collective Activity Dataset contains 5 different collective activities: crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point."}, {"id": "asos-digital-experiments-dataset", "name": "ASOS Digital Experiments Dataset", "description": "A novel dataset that can support the end-to-end design and running of Online Controlled Experiments (OCE) with adaptive stopping."}, {"id": "eyecar", "name": "EyeCar", "description": "EyeCar is a dataset of driving videos of vehicles involved in rear-end collisions paired with eye fixation data captured from human subjects. It contains 21 front-view videos that were captured in various traffic, weather, and day light conditions. Each video is 30sec in length and contains typical driving tasks (e.g., lanekeeping, merging-in, and braking) ending to rear-end collisions."}, {"id": "emovo-emovo-corpus-an-italian-emotional-speech-database", "name": "EMOVO (EMOVO Corpus: an Italian emotional speech database)", "description": "This article describes the first emotional corpus, named EMOVO, applicable to Italian language,. It is a database built from the voices of up to 6 actors who played 14 sentences simulating 6 emotional states (disgust, fear, anger, joy, surprise, sadness) plus the neutral state. These emotions are the well-known Big Six found in most of the literature related to emotional speech. The recordings were made with professional equipment in the Fondazione Ugo Bordoni laboratories. The paper also describes a subjective validation test of the corpus, based on emotion-discrimination of two sentences carried out by two different groups of 24 listeners. The test was successful because it yielded an overall recognition accuracy of 80%. It is observed that emotions less easy to recognize are joy and disgust, whereas the most easy to detect are anger, sadness and the neutral state."}, {"id": "skab-skoltech-anomaly-benchmark", "name": "SKAB (Skoltech Anomaly Benchmark)", "description": "SKAB is designed for evaluating algorithms for anomaly detection. The benchmark currently includes 30+ datasets plus Python modules for algorithms\u2019 evaluation. Each dataset represents a multivariate time series collected from the sensors installed on the testbed. All instances are labeled for evaluating the results of solving outlier detection and changepoint detection problems."}, {"id": "logo-net", "name": "LOGO-Net", "description": "A large-scale logo image database for logo detection and brand recognition from real-world product images. "}, {"id": "sc-burst-smartphone-burst-dataset", "name": "SC_burst (Smartphone burst Dataset)", "description": "Contains16 burst images using smartphones for burst/video denoising, restoration, and enhancement tasks. The raw format are unified and saved SC_burst in \".MAT\", where the raw data and metadata are stored."}, {"id": "transnas-bench-101", "name": "TransNAS-Bench-101", "description": "TransNAS-Bench-101 is a Neural Architecture Search (NAS) benchmark dataset containing network performance across seven tasks, covering classification, regression, pixel-level prediction, and self-supervised tasks. This diversity provides opportunities to transfer NAS methods among tasks and allows for more complex transfer schemes to evolve. We explore two fundamentally different types of search space: cell-level search space and macro-level search space. With 7,352 backbones evaluated on seven tasks, 51,464 trained models with detailed training information are provided. With TransNAS-Bench-101, we hope to encourage the advent of exceptional NAS algorithms that raise cross-task search efficiency and generalizability to the next level."}, {"id": "japanese-word-similarity", "name": "Japanese Word Similarity", "description": "This dataset contains information about Japanese word similarity including rare words. The dataset is constructed following the Stanford Rare Word Similarity Dataset. 10 annotators annotated word pairs with 11 levels of similarity."}, {"id": "rst-dt-rst-discourse-treebank", "name": "RST-DT (RST Discourse Treebank)", "description": "The Rhetorical Structure Theory (RST) Discourse Treebank consists of 385 Wall Street Journal articles from the Penn Treebank annotated with discourse structure in the RST framework along with human-generated extracts and abstracts associated with the source documents."}, {"id": "ipac-icelandic-parallel-abstracts-corpus", "name": "IPAC (Icelandic Parallel Abstracts Corpus)", "description": "IPAC (Icelandic Parallel Abstracts Corpus ) is a new Icelandic-English parallel corpus, composed of abstracts from student theses and dissertations. The texts were collected from the Skemman repository which keeps records of all theses, dissertations and final projects from students at Icelandic universities. The corpus was aligned based on sentence-level BLEU scores, in both translation directions, from NMT models using Bleualign. The result is a corpus of 64k sentence pairs from over 6 thousand parallel abstracts."}, {"id": "veri-wild", "name": "VeRi-Wild", "description": "Veri-Wild is the largest vehicle re-identification dataset (as of CVPR 2019). The dataset is captured from a large CCTV surveillance system consisting of 174 cameras across one month (30\u00d7 24h) under unconstrained scenarios. This dataset comprises 416,314 vehicle images of 40,671 identities. Evaluation on this dataset is split across three subsets: small, medium and large; comprising 3000, 5000 and 10,000 identities respectively (in probe and gallery sets)."}, {"id": "deft-corpus", "name": "DEFT Corpus", "description": "A SemEval shared task in which participants must extract definitions from free text using a term-definition pair corpus that reflects the complex reality of definitions in natural language. "}, {"id": "klej", "name": "KLEJ", "description": "The KLEJ benchmark (Kompleksowa Lista Ewaluacji J\u0119zykowych) is a set of nine evaluation tasks for the Polish language understanding task."}, {"id": "msu-video-upscalers-quality-enhancement", "name": "MSU Video Upscalers: Quality Enhancement", "description": "The dataset aims to find the algorithms that produce the most visually pleasant image possible and generalize well to a broad range of content. It consists of 30 clips and contains 15 2D-animated segments losslessly recorded from various video games and 15 camera-shot segments from high-bitrate YUV444 sources. The complexity of clips varies significantly in terms of spatial and temporal indexes. Multiple bicubic downscaling mixed with sharpening is used to simulate complex real-world camera degradation. The authors used slight compression and YUV420 conversion to simulate a practical use case. 1920\u00d71080 sources were downscaled to 480\u00d7270 input."}, {"id": "hateful-memes", "name": "Hateful Memes", "description": "The Hateful Memes data set is a multimodal dataset for hateful meme detection (image + text) that contains 10,000+ new multimodal examples created by Facebook AI. Images were licensed from Getty Images so that researchers can use the data set to support their work."}, {"id": "emobank", "name": "EmoBank", "description": "EmoBank is a corpus of 10k English sentences balancing multiple genres, annotated with dimensional emotion metadata in the Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a bi-perspectival and bi-representational design. "}, {"id": "noaa-atmospheric-temperature-dataset", "name": "NOAA Atmospheric Temperature Dataset", "description": "This dataset contains meteorological observations (temperature) at the land-based weather stations located in the United States, collected from the Online Climate Data Directory of the National Oceanic and Atmospheric Administration (NOAA). The weather stations are sampled from the Western and Southeastern states that have actively measured meteorological observations during 2015. The 1-year sequential data of hourly temperature records are divided into small sequences of 24 hours. For training, validation, and test a sequential 8-2-2 (months) split is used."}, {"id": "t2dv2", "name": "T2Dv2", "description": "The T2Dv2 dataset consists of 779 tables originating from the English-language subset of the WebTables corpus. 237 tables are annotated for the Table Type Detection task, 236 for the Columns Property Annotation (CPA) task and 235 for the Row Annotation task. The annotations that are used are DBpedia types, properties and entities."}, {"id": "adobevfr-syn-adobe-visual-font-recognition-synthetic-dataset", "name": "AdobeVFR syn (Adobe Visual Font Recognition synthetic dataset)", "description": "Subset of AdobeVFR. The dataset contains images depicting English text and consists of 1000 synthetic images for training and 100 for testing, for each of 2383 font classes. The training and test sets are called VFR_syn_train and VFR_syn_val, respectively."}, {"id": "peerread", "name": "PeerRead", "description": "PearRead is a dataset of scientific peer reviews. The dataset consists of over 14K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR, as well as over 10K textual peer reviews written by experts for a subset of the papers."}, {"id": "machine-number-sense", "name": "Machine Number Sense", "description": "Consists of visual arithmetic problems automatically generated using a grammar model--And-Or Graph (AOG). These visual arithmetic problems are in the form of geometric figures: each problem has a set of geometric shapes as its context and embedded number symbols. "}, {"id": "duo-detecting-underwater-objects", "name": "DUO (Detecting Underwater Objects)", "description": "DUO is a dataset for Underwater object detection for robot picking. The dataset contains a collection of diverse underwater images with more rational annotations."}, {"id": "capes", "name": "capes", "description": "Approximately 240,000 documents were collected and aligned using the Hunalign tool."}, {"id": "16s-rdna-sequencing-of-feces-from-c9orf72-loss-of-function-mice", "name": "16s rDNA sequencing of feces from C9orf72 loss of function mice", "description": "In one round of sequencing, 5 fecal pellets from 2 pro-inflammatory environments (Harvard BRI/Johns Hopkins) and 2 pro-survival environments (Broad Institute/Jackson Labs) were sequenced at the 16s rDNA locus. In a second round of sequencing, 9 fecal pellets from Harvard BRI, 9 fecal pellets from Broad Institute, 6 fecal pellets from Harvard BRI mice transplanted with Harvard BRI feces, and 6 pellets from Harvard BRI mice transplanted with Broad feces were sequenced at the 16S rDNA locus"}, {"id": "wikilingua", "name": "WikiLingua", "description": "WikiLingua includes ~770k article and summary pairs in 18 languages from WikiHow. Gold-standard article-summary alignments across languages are extracted by aligning the images that are used to describe each how-to step in an article."}, {"id": "streetstyle", "name": "StreetStyle", "description": "StreetStyle is a large-scale dataset of photos of people annotated with clothing attributes, and use this dataset to train attribute classifiers via deep learning."}, {"id": "mlm", "name": "MLM", "description": "A new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. "}, {"id": "voxclamantis", "name": "VoxClamantis", "description": "A large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. "}, {"id": "colorectal-adenoma", "name": "Colorectal Adenoma", "description": "Colorectal Adenoma contains 177 whole slide images (156 contain adenoma) gathered and labelled by pathologists from the Department of Pathology, The Chinese PLA General Hospital."}, {"id": "washington-rgb-d", "name": "Washington RGB-D", "description": "Washington RGB-D is a widely used testbed in the robotic community, consisting of 41,877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors, cereal box, keyboard etc). Each object instance was positioned on a turntable and captured from three different viewpoints while rotating."}, {"id": "fetoscopy-placenta-data", "name": "Fetoscopy Placenta Data", "description": "The fetoscopy placenta dataset is associated with our MICCAI2020 publication titled \u201cDeep Placental Vessel Segmentation for Fetoscopic Mosaicking\u201d. The dataset contains 483 frames with ground-truth vessel segmentation annotations taken from six different in vivo fetoscopic procedure videos. The dataset also includes six unannotated in vivo continuous fetoscopic video clips (950 frames) with predicted vessel segmentation maps obtained from the leave-one-out cross-validation of our method."}, {"id": "reccon", "name": "RECCON", "description": "RECCON is a dataset for the task of recognizing emotion cause in conversations."}, {"id": "geoqa-geometric-question-answering", "name": "GeoQA (Geometric Question Answering)", "description": "GeoQA is a dataset for automatic geometric problem solving containing 5,010 geometric problems with corresponding annotated programs, which illustrate the solving process of the given problems"}, {"id": "webis-touche-2020", "name": "Webis-Touch\u00e9-2020", "description": "This paper is a condensed report on the second year of the Touch\u00e9 shared task on argument retrieval held at CLEF 2021. With the goal to provide a collaborative platform for researchers, we organized two tasks: (1) supporting individuals in finding arguments on controversial topics of social importance and (2) supporting individuals with arguments in personal everyday comparison situations."}, {"id": "kvqa-knowledge-aware-vqa", "name": "KVQA (Knowledge-aware VQA)", "description": "It contains manually verified 183K question-answer pairs about more than 18K persons and 24K images. The questions in this dataset require multi-entity, multi-relation and multi-hop reasoning over KG to arrive at an answer. To enable visual named entity linking, it also provides a support set containing reference images of 69K persons harvested from Wikidata as part of the dataset."}, {"id": "splash", "name": "SPLASH", "description": "A dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback."}, {"id": "tamil-memes", "name": "Tamil Memes", "description": "Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes, which in most cases combines an image with a concept or catchphrase. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. To facilitate the computational modelling of trolling in the memes for Indian languages, we created a meme dataset for Tamil (TamilMemes). We annotated and released the dataset containing suspected trolls and not-troll memes. In this paper, we use the a image classification to address the difficulties involved in the classification of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score."}, {"id": "rgb-stacking", "name": "RGB-Stacking", "description": "RGB-Stacking is a benchmark for vision-based robotic manipulation. The robot is trained to learn how to grasp objects and balance them on top of one another."}, {"id": "6impose-synthetic-rgbd-dataset-for-6d-pose-estimation", "name": "6IMPOSE (Synthetic RGBD dataset for 6D pose estimation)", "description": "The dataset includes the synthetic data generated from rendering the 3D meshes of LM objects and several household objects in Blender for training 6D pose estimation algorithms. The whole dataset contains synthetic data for 18 objects (13 from LM and 5 from household objects), with 20,000 data samples for each object. Each data sample includes an RGB image in .png format and a depth image in .exr format. Each sample has the annotations of mask labels in .png format and the ground truth pose labels saved in .json files. Apart from the training data, the 3D meshes of the objects and the pre-trained models of the 6D pose estimation algorithm are also included. The whole dataset takes approximately ~1T of storage memory."}, {"id": "usps", "name": "USPS", "description": "USPS is a digit dataset automatically scanned from envelopes by the U.S. Postal Service containing a total of 9,298 16\u00d716 pixel grayscale samples; the images are centered, normalized and show a broad range of font styles."}, {"id": "australian-statlog-australian-credit-approval-data-set", "name": "australian (Statlog (Australian Credit Approval) Data Set)", "description": "Data Set Information:"}, {"id": "pubmedqa", "name": "PubMedQA", "description": "The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts."}, {"id": "pano3d", "name": "Pano3D", "description": "Pano3D  is a new benchmark for depth estimation from spherical panoramas. Its goal is to drive progress for this task in a consistent and holistic manner.  The Pano3D 360 depth estimation benchmark provides a standard Matterport3D train and test split, as well as a secondary GibsonV2 partioning for testing and training as well. The latter is used for zero-shot cross dataset transfer performance assessment and decomposes it into 3 different splits, each one focusing on a specific generalization axis."}, {"id": "kvasir-the-kvasir-dataset", "name": "Kvasir (The Kvasir Dataset)", "description": "The KVASIR Dataset was released as part of the medical multimedia challenge presented by MediaEval. It is based on images obtained from the GI tract via an endoscopy procedure. The dataset is composed of images that are annotated and verified by medical doctors, and captures 8 different classes. The classes are based on three anatomical landmarks (z-line, pylorus, cecum), three pathological findings (esophagitis, polyps, ulcerative colitis) and two other classes (dyed and lifted polyps, dyed resection margins) related to the polyp removal process. Overall, the dataset contains 8,000 endoscopic images, with 1,000 image examples per class."}, {"id": "lfpw-labeled-face-parts-in-the-wild", "name": "LFPW (Labeled Face Parts in the Wild)", "description": "The Labeled Face Parts in-the-Wild (LFPW) consists of 1,432 faces from images downloaded from the web using simple text queries on sites such as google.com, flickr.com, and yahoo.com.   Each image was labeled by three MTurk workers, and 29 fiducial points, shown below, are included in dataset."}, {"id": "ddd17-davis-driving-dataset-2017", "name": "DDD17 (DAVIS Driving Dataset 2017)", "description": "DDD17 has over 12 h of a 346x260 pixel DAVIS sensor recording highway and city driving in daytime, evening, night, dry and wet weather conditions, along with vehicle speed, GPS position, driver steering, throttle, and brake captured from the car's on-board diagnostics interface. "}, {"id": "help", "name": "HELP", "description": "The HELP dataset is an automatically created natural language inference (NLI) dataset that embodies the combination of lexical and logical inferences focusing on monotonicity (i.e., phrase replacement-based reasoning). The HELP (Ver.1.0) has 36K inference pairs consisting of upward monotone, downward monotone, non-monotone, conjunction, and disjunction."}, {"id": "matterport3d", "name": "Matterport3D", "description": "The Matterport3D dataset is a large RGB-D dataset for scene understanding in indoor environments. It contains 10,800 panoramic views inside 90 real building-scale scenes, constructed from 194,400 RGB-D images. Each scene is a residential building consisting of multiple rooms and floor levels, and is annotated with surface construction, camera poses, and semantic segmentation."}, {"id": "fer2013-facial-expression-recognition-2013-dataset", "name": "FER2013 (Facial Expression Recognition 2013 Dataset)", "description": "Fer2013 contains approximately 30,000 facial RGB images of different expressions with size restricted to 48\u00d748, and the main labels of it can be divided into 7 types: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral. The Disgust expression has the minimal number of images \u2013 600, while other labels have nearly 5,000 samples each."}, {"id": "lener-br", "name": "LeNER-Br", "description": "LeNER-Br is a dataset for named entity recognition (NER) in Brazilian Legal Text."}, {"id": "cinemairsim", "name": "CinemAirSim", "description": "CinemAirSim is an extension of the well-known drone simulator, AirSim, with a cinematic camera as well as extended its API to control all of its parameters in real time, including various filming lenses and common cinematographic properties."}, {"id": "stpls3d", "name": "STPLS3D", "description": "Our project (STPLS3D) aims to provide a large-scale aerial photogrammetry dataset with synthetic and real annotated 3D point clouds for semantic and instance segmentation tasks."}, {"id": "vlog-dataset", "name": "VLOG Dataset", "description": "A large collection of interaction-rich video data which are annotated and analyzed."}, {"id": "penn-action", "name": "Penn Action", "description": "The Penn Action Dataset contains 2326 video sequences of 15 different actions and human joint annotations for each sequence."}, {"id": "xalign", "name": "XAlign", "description": "It consists of an extensive collection of a high quality cross-lingual fact-to-text dataset in 11 languages: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Malayalam (ml), Marathi (mr), Oriya (or), Punjabi (pa), Tamil (ta), Telugu (te), and monolingual dataset in English (en). This is the Wikipedia text <--> Wikidata KG aligned corpus used to train the data-to-text generation model. The Train & validation splits are created using distant supervision methods and Test data is generated through human annotations."}, {"id": "fingerprint-dataset-neural-audio-fingerprint-dataset", "name": "Fingerprint Dataset (Neural Audio Fingerprint Dataset)", "description": "This dataset includes all music sources, background noises and impulse-reponses (IR) samples and conversation speech  that have been used in the work \"Neural Audio Fingerprint for High-specific Audio Retrieval based on Contrastive Learning\" ICASSP 2021 (https://arxiv.org/abs/2010.11910)."}, {"id": "artdl", "name": "ArtDL", "description": "ArtDL is a novel painting data set for iconography classification composed of images collected from online sources. Most of the paintings are from the Renaissance period and depict scenes or characters of Christian art. The data set is annotated with classes representing specific characters belonging to the Iconclass classification system."}, {"id": "ck-extended-cohn-kanade-dataset", "name": "CK+ (Extended Cohn-Kanade dataset)", "description": "The Extended Cohn-Kanade (CK+) dataset contains 593 video sequences from a total of 123 different subjects, ranging from 18 to 50 years of age with a variety of genders and heritage. Each video shows a facial shift from the neutral expression to a targeted peak expression, recorded at 30 frames per second (FPS) with a resolution of either 640x490 or 640x480 pixels. Out of these videos, 327 are labelled with one of seven expression classes: anger, contempt, disgust, fear, happiness, sadness, and surprise. The CK+ database is widely regarded as the most extensively used laboratory-controlled facial expression classification database available, and is used in the majority of facial expression classification methods."}, {"id": "kleister-nda", "name": "Kleister NDA", "description": "Kleister NDA is a dataset for Key Information Extraction (KIE). The dataset contains a mix of scanned and born-digital long formal English-language documents. For this datasets, an NLP system is expected to find or infer various types of entities by employing both textual and structural layout features.  The Kleister NDA dataset has 540 Non-disclosure Agreements, with 3,229 unique pages and 2,160 entities to extract."}, {"id": "navigation-turing-test", "name": "Navigation Turing Test", "description": "Replay data from human players and AI agents navigating in a 3D game environment."}, {"id": "videolt", "name": "VideoLT", "description": "VideoLT is a large-scale long-tailed video recognition dataset that contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution."}, {"id": "imdb-multi", "name": "IMDB-MULTI", "description": "IMDB-MULTI is a relational dataset that consists of a network of 1000 actors or actresses who played roles in movies in IMDB. A node represents an actor or actress, and an edge connects two nodes when they appear in the same movie. In IMDB-MULTI, the edges are collected from three different genres: Comedy, Romance and Sci-Fi."}, {"id": "anoshift-anoshift-a-distribution-shift-benchmark-for-unsupervised-anomaly-detection", "name": "AnoShift (AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection)", "description": "AnoShift is a large-scale anomaly detection benchmark, which focuses on splitting the test data based on its temporal distance to the training set, introducing three testing splits: IID, NEAR, and FAR. This testing scenario proves to capture the in-time performance degradation of anomaly detection methods for classical to masked language models. "}, {"id": "ham10000", "name": "HAM10000", "description": "HAM10000 is a dataset of 10000 training images for detecting pigmented skin lesions. The authors collected dermatoscopic images from different populations, acquired and stored by different modalities."}, {"id": "ukiyo-e-faces", "name": "Ukiyo-e Faces", "description": "The ukiyo-e faces dataset comprises of 5209 images of faces from ukiyo-e prints. The images are 1024x1024 pixels in jpeg format and have been aligned using the procedure used for the FFHQ dataset"}, {"id": "octagon-octagon-dataset", "name": "OCTAGON (OCTAGON Dataset)", "description": "The OCTAGON dataset is a set of Angiography by Octical Coherence Tomography images (OCT-A) used to the segmentation of the Foveal Avascular Zone (FAZ). The dataset includes 144 healthy OCT-A images and 69 diabetic OCT-A images, divided into four groups, each one with 36 and about 17 OCT-A images, respectively. These groups are: 3x3 superficial, 3x3 deep, 6x6 superficial and 6x6 deep, where 3x3 and 6x6 are the zoom of the image and superficial/deep are the depth level of the extracted image. The healthy dataset includes OCT-A images from people classified in 6 age ranges: 10-19 years, 20-29 years, 30-39 years, 40-49 years, 50-59 years and 60-69 years. Each age range includes 3 different patients with information of left and right eyes for each one. Finally, for each eye, there are four different images: one 3x3 superficial image, one 3x3 deep image, one 6x6 superficial image and one 6x6 deep image. Each image have two manual labelled of expert clinicians of the FAZ and their quantification in the healthy OCT-A images, and one manual labelled in the diabetic OCT-A images."}, {"id": "retinal-microsurgery", "name": "Retinal Microsurgery", "description": "The Retinal Microsurgery dataset is a dataset for surgical instrument tracking. It consists of 18 in-vivo sequences, each with 200 frames of resolution 1920 \u00d7 1080 pixels. The dataset is further classified into four instrument-dependent subsets. The annotated tool joints are n=3 and semantic classes c=2 (tool and background)."}, {"id": "nci1", "name": "NCI1", "description": "The NCI1 dataset comes from the cheminformatics domain, where each input graph is used as representation of a chemical compound: each vertex stands for an atom of the molecule, and edges between vertices represent bonds between atoms. This dataset is relative to anti-cancer screens where the chemicals are assessed as positive or negative to cell lung cancer. Each vertex has an input label representing the corresponding atom type, encoded by a one-hot-encoding scheme into a vector of 0/1 elements."}, {"id": "dast-danish-stance", "name": "DAST (Danish Stance)", "description": "This is an SDQC stance-annotated Reddit dataset for the Danish language generated within a thesis project. The dataset consists of over 5000 comments structured as comment trees and linked to 33 source posts."}, {"id": "gazecapture-eye-tracking-for-everyone", "name": "GazeCapture (Eye Tracking for Everyone)", "description": "From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost $2.5M$ frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10 - 15fps) on a modern mobile device. Our model achieves a prediction error of 1.7cm and 2.5cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.3cm and 2.1cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results."}, {"id": "codesyntax", "name": "CodeSyntax", "description": "CodeSyntax is a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. It contains 18,701 code samples annotated with 1,342,050 relation edges in 43 relation types for Python, and 13,711 code samples annotated with 864,411 relation edges in 39 relation types for Java. It is designed to  evaluate the performance of language models on code syntax understanding."}, {"id": "mit-states", "name": "MIT-States", "description": "The MIT-States dataset has 245 object classes, 115 attribute classes and \u223c53K images. There is a wide range of objects (e.g., fish, persimmon, room) and attributes (e.g., mossy, deflated, dirty). On average, each object instance is modified by one of the 9 attributes it affords."}, {"id": "uasol-a-large-scale-high-resolution-outdoor-stereo-dataset", "name": "UASOL (A large-scale high-resolution outdoor stereo dataset)", "description": "The UASOL an RGB-D stereo dataset, that contains 160902 frames, filmed at 33 different scenes, each with between 2\u2009k and 10\u2009k frames. The frames show different paths from the perspective of a pedestrian, including sidewalks, trails, roads, etc. The images were extracted from video files with 15\u2009fps at HD2K resolution with a size of 2280\u2009\u00d7\u20091282 pixels. The dataset also provides a GPS geolocalization tag for each second of the sequences and reflects different climatological conditions. It also involved up to 4 different persons filming the dataset at different moments of the day."}, {"id": "jung", "name": "Jung", "description": "Dataset for document shadow removal"}, {"id": "online-cryptocurrency-topic-diffusion-on-twitter-telegram-and-discord", "name": "Online Cryptocurrency-topic diffusion on Twitter, Telegram, and Discord", "description": "This Dataset is described in Charting the Landscape of Online Cryptocurrency Manipulation. IEEE Access (2020), a study that aims to map and assess the extent of cryptocurrency manipulations within and across the online ecosystems of Twitter, Telegram, and Discord. Starting from tweets mentioning cryptocurrencies, we leveraged and followed invite URLs from platform to platform, building the invite-link network, in order to study the invite link diffusion process."}, {"id": "egobody", "name": "EgoBody", "description": "EgoBody dataset is a novel large-scale dataset for egocentric 3D human pose, shape and motions under interactions in complex 3D scenes."}, {"id": "ruddit", "name": "Ruddit", "description": "Ruddit is a dataset of English language Reddit comments that has fine-grained, real-valued scores for offensive language detection between -1 (maximally supportive) and 1 (maximally offensive)."}, {"id": "argoverse-2-map-change", "name": "Argoverse 2 Map Change", "description": "The Argoverse 2 Map Change Dataset is a collection of 1,000 scenarios with ring camera imagery, lidar, and HD maps. Two hundred of the scenarios include changes in the real-world environment that are not yet reflected in the HD map, such as new crosswalks or repainted lanes. By sharing a map dataset that labels the instances in which there are discrepancies with sensor data, we encourage the development of novel methods for detecting out-of-date map regions."}, {"id": "evidence-inference", "name": "Evidence Inference", "description": "Evidence Inference is a corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. "}, {"id": "kinships", "name": "Kinships", "description": "The Kinships dataset describes relationships between members of the Australian tribe Alyawarra and consists of 10,686 triples. It contains 104 entities representing members of the tribe and 26 relationship types that represent kinship terms such as Adiadya or Umbaidya."}, {"id": "meld-multimodal-emotionlines-dataset", "name": "MELD (Multimodal EmotionLines Dataset)", "description": "Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset. MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -- Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. MELD also has sentiment (positive, negative and neutral) annotation for each utterance."}, {"id": "cais-chinese-artificial-intelligence-speakers", "name": "CAIS (Chinese Artificial Intelligence Speakers)", "description": "We collect utterances from the Chinese Artificial Intelligence Speakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme (Ratinov and Roth, 2009) in the sequence labeling field"}, {"id": "atari-100k", "name": "Atari 100k", "description": "Atari Games for only 100k environment steps. (400k frames with frame-skip=4)."}, {"id": "skeletics-152", "name": "Skeletics 152", "description": "A curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset."}, {"id": "dihard-ii", "name": "DIHARD II", "description": "The DIHARD II development and evaluation sets draw from a diverse set of sources exhibiting wide variation in recording equipment, recording environment, ambient noise, number of speakers, and speaker demographics. The development set includes reference diarization and speech segmentation and may be used for any purpose including system development or training."}, {"id": "docred", "name": "DocRED", "description": "DocRED (Document-Level Relation Extraction Dataset) is a relation extraction dataset constructed from Wikipedia and Wikidata. Each document in the dataset is human-annotated with named entity mentions, coreference information, intra- and inter-sentence relations, and supporting evidence. DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document. Along with the human-annotated data, the dataset provides large-scale distantly supervised data."}, {"id": "ghosts", "name": "GHOSTS", "description": "GHOSTS is the first natural-language dataset made and curated by working researchers in mathematics that (1) aims to cover graduate-level mathematics and (2) provides a holistic overview of the mathematical capabilities of language models. It a collection of multiple datasets of prompts, totalling 728 prompts, for which ChatGPT\u2019s output was manually rated by experts."}, {"id": "lsoie-large-scale-dataset-for-supervised-open-information-extraction", "name": "LSOIE (Large-Scale dataset for Supervised Open Information Extraction)", "description": "LSOIE is a large-scale OpenIE data converted from QA-SRL 2.0 in two domains, i.e., Wikipedia and Science. It is 20 times larger than the next largest human-annotated OpenIE data, and thus is reliable for fair evaluation. LSOIE provides n-ary OpenIE annotations and gold tuples are in the \u3008ARG0, Relation, ARG1, . . . , ARGn\u3009 format. The dataset has two subsets ... namely LSOIE-wiki and LSOIE-sci, for comprehensive evaluation. LSOIE-wiki has 24,251 sentences and LSOIE-sci has 47,919 sentences."}, {"id": "whu-rs19", "name": "WHU-RS19", "description": "WHU-RS19 is a set of satellite images exported from Google Earth, which provides high-resolution satellite images up to 0.5 m. Some samples of the database are displayed in the following picture. It contains 19 classes of meaningful scenes in high-resolution satellite imagery, including airport, beach, bridge, commercial, desert, farmland, footballfield, forest, industrial, meadow, mountain, park, parking, pond, port, railwaystation, residential, river, and viaduct. For each class, there are about 50 samples. It\u2019s worth noticing that the image samples of the same class are collected from different regions in satellite images of different resolutions and then might have different scales, orientations and illuminations."}, {"id": "panocontext", "name": "PanoContext", "description": "The PanoContext dataset contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms."}, {"id": "hybridqa", "name": "HybridQA", "description": "A new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. "}, {"id": "bimcv-covid-19", "name": "BIMCV COVID-19", "description": "BIMCV-COVID19+ dataset is a large dataset with chest X-ray images CXR (CR, DX) and computed tomography (CT) imaging of COVID-19 patients along with their radiographic findings, pathologies, polymerase chain reaction (PCR), immunoglobulin G (IgG) and immunoglobulin M (IgM) diagnostic antibody tests and radiographic reports from Medical Imaging Databank in Valencian Region Medical Image Bank (BIMCV). The findings are mapped onto standard Unified Medical Language System (UMLS) terminology and they cover a wide spectrum of thoracic entities, contrasting with the much more reduced number of entities annotated in previous datasets. Images are stored in high resolution and entities are localized with anatomical labels in a Medical Imaging Data Structure (MIDS) format. In addition, 23 images were annotated by a team of expert radiologists to include semantic segmentation of radiographic findings. Moreover, extensive information is provided, including the patient\u2019s demographic information, type of projection and acquisition parameters for the imaging study, among others. These iterations of the database include 7,377 CR, 9,463 DX and 6,687 CT studies."}, {"id": "duc-2007-document-understanding-conferences", "name": "DUC 2007 (Document Understanding Conferences)", "description": "There is currently much interest and activity aimed at building powerful multi-purpose information systems. The agencies involved include DARPA, ARDA and NIST. Their programmes, for example DARPA's TIDES (Translingual Information Detection Extraction and Summarization) programme, ARDA's Advanced Question & Answering Program and NIST's TREC (Text Retrieval Conferences) programme cover a range of subprogrammes. These focus on different tasks requiring their own evaluation designs."}, {"id": "tlp-track-long-and-prosper", "name": "TLP (Track Long and Prosper)", "description": "A new long video dataset and benchmark for single object tracking. The dataset consists of 50 HD videos from real world scenarios, encompassing a duration of over 400 minutes (676K frames), making it more than 20 folds larger in average duration per sequence and more than 8 folds larger in terms of total covered duration, as compared to existing generic datasets for visual tracking."}, {"id": "herbarium-2021-half-earth", "name": "Herbarium 2021 Half\u2013Earth", "description": "The Herbarium Half-Earth dataset is a large and diverse dataset of herbarium specimens to date for automatic taxon recognition. The Herbarium 2021: Half-Earth Challenge dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list."}, {"id": "dip-imu", "name": "DIP-IMU", "description": "Dataset consisting of IMU measurements and corresponding SMPL poses. Participants were wearing 17 IMU sensors and reference SMPL poses were obtained by running the SIP optimization with all 17 sensors."}, {"id": "para-quality", "name": "Para-Quality", "description": "Used to investigate common crowdsourced paraphrasing issues and for detecting the quality issues."}, {"id": "tusimple", "name": "TuSimple", "description": "The TuSimple dataset consists of 6,408 road images on US highways. The resolution of image is 1280\u00d7720. The dataset is composed of 3,626 for training, 358 for validation, and 2,782 for testing called the TuSimple test set of which the images are under different weather conditions."}, {"id": "2d-3d-s-2d-3d-semantic", "name": "2D-3D-S (2D-3D-Semantic)", "description": "The 2D-3D-S dataset provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. It covers over 6,000 m2 collected in 6 large-scale indoor areas that originate from 3 different buildings. It contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360\u00b0 equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces."}, {"id": "clonedperson", "name": "ClonedPerson", "description": "The ClonedPerson dataset is a large-scale synthetic person re-identification dataset introduced in the paper \"Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification\" in CVPR 2022. It is generated by MakeHuman and Unity3D. Characters in this dataset use an automatic approach to directly clone the whole outfits from real-world person images to virtual 3D characters, such that any virtual person thus created will appear very similar to its real-world counterpart. The dataset contains 887,766 synthesized person images of 5,621 identities."}, {"id": "topical-chat", "name": "Topical-Chat", "description": "A knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don\u2019t have explicitly defined roles."}, {"id": "ch-sims", "name": "CH-SIMS", "description": "CH-SIMS is a Chinese single- and multimodal sentiment analysis dataset which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis."}, {"id": "quarel", "name": "QuaRel", "description": "QuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms."}, {"id": "a2d-actor-action-dataset", "name": "A2D (Actor-Action Dataset)", "description": "A2D (Actor-Action Dataset) is a dataset for simultaneously inferring actors and actions in videos. A2D has seven actor classes (adult, baby, ball, bird, car, cat, and dog) and eight action classes (climb, crawl, eat, fly, jump, roll, run, and walk) not including the no-action class, which we also consider. The A2D has 3,782 videos with at least 99 instances per valid actor-action tuple and videos are labeled with both pixel-level actors and actions for sampled frames. The A2D dataset serves as a large-scale testbed for various vision problems: video-level single- and multiple-label actor-action recognition, instance-level object segmentation/co-segmentation, as well as pixel-level actor-action semantic segmentation to name a few."}, {"id": "conceptual-captions", "name": "Conceptual Captions", "description": "Automatic image captioning is the task of producing a natural-language utterance (usually a sentence) that correctly reflects the visual content of an image. Up to this point, the resource most used for this task was the MS-COCO dataset, containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators)."}, {"id": "zeshel", "name": "ZESHEL", "description": "ZESHEL is a zero-shot entity linking dataset, which places more emphasis on understanding the unstructured descriptions of entities to resolve the ambiguity of mentions on four unseen domains."}, {"id": "realsrset", "name": "RealSRSet", "description": "20 real low-resolution images selected from existing datasets or downloaded from internet"}, {"id": "codexglue", "name": "CodeXGLUE", "description": "CodeXGLUE is a benchmark dataset and open challenge for code intelligence. It includes a collection of code intelligence tasks and a platform for model evaluation and comparison. CodeXGLUE stands for General Language Understanding Evaluation benchmark for CODE. It includes 14 datasets for 10 diversified code intelligence tasks covering the following scenarios:"}, {"id": "timetravel", "name": "TimeTravel", "description": "TimeTravel contains 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. "}, {"id": "labpics-labpics-dataset-for-computer-vision-for-autonomous-chemistry-labs-and-medical-labs", "name": "LabPics (LabPics Dataset for computer vision for autonomous chemistry labs and medical labs)", "description": "LabPics Chemistry Dataset"}, {"id": "tut-acoustic-scenes-2017", "name": "TUT Acoustic Scenes 2017", "description": "The TUT Acoustic Scenes 2017 dataset is a collection of recordings from various acoustic scenes all from distinct locations. For each recording location 3-5 minute long audio recordings are captured and are split into 10 seconds which act as unit of sample for this task. All the audio clips are recorded with 44.1 kHz sampling rate and 24 bit resolution."}, {"id": "edface-celeb-1m", "name": "EDFace-Celeb-1M", "description": "EDFace-Celeb-1M is a public Ethnically Diverse Face dataset which is used to benchmark the task of face hallucination. The dataset includes 1.7 million photos that cover different countries, with balanced race composition."}, {"id": "map-maybe-ambiguous-pronoun", "name": "MAP (Maybe Ambiguous Pronoun)", "description": "Maybe Ambiguous Pronoun is a dataset similar to GAP dataset, but without binary gender constraints."}, {"id": "wpc-waterloo-point-cloud", "name": "WPC (Waterloo Point Cloud)", "description": "The WPC (Waterloo Point Cloud) database is a dataset for subjective and objective quality assessment of point clouds."}, {"id": "ai2-thor", "name": "AI2-THOR", "description": "AI2-Thor is an interactive environment for embodied AI. It contains four types of scenes, including kitchen, living room, bedroom and bathroom, and each scene includes 30 rooms, where each room is unique in terms of furniture placement and item types. There are over 2000 unique objects for AI agents to interact with."}, {"id": "culane", "name": "CULane", "description": "CULane is a large scale challenging dataset for academic research on traffic lane detection. It is collected by cameras mounted on six different vehicles driven by different drivers in Beijing. More than 55 hours of videos were collected and 133,235 frames were extracted. The dataset is divided into 88880 images for training set, 9675 for validation set, and 34680 for test set. The test set is divided into normal and 8 challenging categories."}, {"id": "laptop-acos", "name": "Laptop-ACOS", "description": "Laptop-ACOS is a brand new Laptop dataset collected from the Amazon platform in the years 2017 and 2018 (covering ten types of laptops under six brands such as ASUS, Acer, Samsung, Lenovo, MBP, MSI, and so on). It contains 4,076 review sentences, much larger than the SemEval Laptop datasets. For Laptop-ACOS, we annotate the four elements and their corresponding quadruples all by ourselves. We employ the aspect categories defined in the SemEval 2016 Laptop dataset. The Laptop-ACOS dataset contains 4076 sentences with 5758 quadruples. As we have mentioned, a large percentage of the quadruples contain implicit aspects or implicit opinions .  By comparing two datasets, it can be observed that Laptop-ACOS has a higher percentage of implicit opinions than Restaurant-ACOS . It is worth noting that the Laptop-ACOS is available for all subtasks in ABSA, including aspect-based sentiment classification, aspect-sentiment pair extraction, aspect-opinion pair extraction, aspect-opinion sentiment triple extraction, aspect-category-sentiment triple extraction, etc."}, {"id": "s2orc", "name": "S2ORC", "description": "A large corpus of 81.1M English-language academic papers spanning many academic disciplines. Rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. Aggregated papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date."}, {"id": "upfd-user-preference-aware-fake-news-detection", "name": "UPFD (User Preference-aware Fake News Detection)", "description": "For benchmarking, please refer to its variant UPFD-POL and UPFD-GOS."}, {"id": "afhq-animal-faces-hq", "name": "AFHQ (Animal Faces-HQ)", "description": "Animal FacesHQ (AFHQ) is a dataset of animal faces consisting of 15,000 high-quality images at 512 \u00d7 512 resolution. The dataset includes three domains of cat, dog, and wildlife, each providing 5000 images. By having multiple (three) domains and diverse images of various breeds (\u2265 eight) per each domain, AFHQ sets a more challenging image-to-image translation problem.  All images are vertically and horizontally aligned to have the eyes at the center. The low-quality images were discarded by human effort."}, {"id": "ted-gesture-dataset", "name": "TED Gesture Dataset", "description": "Co-speech gestures are everywhere. People make gestures when they chat with others, give a public speech, talk on a phone, and even think aloud. Despite this ubiquity, there are not many datasets available. The main reason is that it is expensive to recruit actors/actresses and track precise body motions. There are a few datasets available (e.g., MSP AVATAR [17] and Personality Dyads Corpus [18]), but their sizes are limited to less than 3 h, and they lack diversity in speech content and speakers. The gestures also could be unnatural owing to inconvenient body tracking suits and acting in a lab environment."}, {"id": "flightmare-simulator", "name": "Flightmare Simulator", "description": "Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation. Those two components are totally decoupled and can run independently from each other. Flightmare comes with several desirable features: (i) a large multi-modal sensor suite, including an interface to extract the 3D point-cloud of the scene; (ii) an API for reinforcement learning which can simulate hundreds of quadrotors in parallel; and (iii) an integration with a virtual-reality headset for interaction with the simulated environment. Flightmare can be used for various applications, including path-planning, reinforcement learning, visual-inertial odometry, deep learning, human-robot interaction, etc."}, {"id": "nas-bench-1shot1", "name": "NAS-Bench-1Shot1", "description": "NAS-Bench-1Shot1 draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods. "}, {"id": "pcc-potsdam-commentary-corpus", "name": "PCC (Potsdam Commentary Corpus)", "description": "The Potsdam Commentary Corpus (PCC) is a corpus of 220 German newspaper commentaries (2.900 sentences, 44.000 tokens) taken from the online issues of the M\u00e4rkische Allgemeine Zeitung (MAZ subcorpus) and Tagesspiegel (ProCon subcorpus) and is annotated with a range of different types of linguistic information."}, {"id": "openwebtext", "name": "OpenWebText", "description": "OpenWebText is an open-source recreation of the WebText corpus. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB)."}, {"id": "spatialsense-benchmark", "name": "SpatialSense Benchmark", "description": "SpatialSense Benchmark is a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. "}, {"id": "cocodoom", "name": "CocoDoom", "description": "CocoDoom is a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format."}, {"id": "ijb-s-iarpa-janus-benchmark-s", "name": "IJB-S (IARPA Janus Benchmark-S)", "description": "Paper Abstract"}, {"id": "watercolor2k", "name": "Watercolor2k", "description": "Watercolor2k is a dataset used for cross-domain object detection which contains 2k watercolor images with image and instance-level annotations."}, {"id": "classical-conditioning", "name": "Classical conditioning", "description": "The paper introduces three benchmarking tasks inspired by animal learning."}, {"id": "iowarain", "name": "IowaRain", "description": "IowaRain is a dataset of rainfall events for the state of Iowa (2016-2019) acquired from the National Weather Service Next Generation Weather Radar (NEXRAD) system and processed by a quantitative precipitation estimation system. The dataset presented in this study could be used for better disaster monitoring, response and recovery by paving the way for both predictive and prescriptive modeling"}, {"id": "morph-call", "name": "Morph Call", "description": "Morph Call is a suite of 46 probing tasks for four Indo-European languages that fall under different morphology: Russian, French, English, and German. The tasks are designed to explore the morphosyntactic content of multilingual transformers which is a less studied aspect at the moment."}, {"id": "tsac-tunisian-sentiment-analysis-corpus", "name": "TSAC (Tunisian Sentiment Analysis Corpus)", "description": "Tunisian Sentiment Analysis Corpus (TSAC) is a Tunisian Dialect corpus of 17.000 comments from Facebook. "}, {"id": "pcmsp", "name": "PcMSP", "description": "PcMSP is a dataset annotated  from 305 open access scientific articles for material science information extraction that simultaneously contains the synthesis sentences extracted from the experimental paragraphs, as well as the entity mentions and intra-sentence relations."}, {"id": "foodlogodet-1500", "name": "FoodLogoDet-1500", "description": "FoodLogoDet-1500 is a new large-scale publicly available food logo dataset, which has 1,500 categories, about 100,000 images and about 150,000 manually annotated food logo objects."}, {"id": "hate-speech-and-offensive-language", "name": "Hate Speech and Offensive Language", "description": "HSOL is a dataset for hate speech detection. The authors begun with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API they searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. They extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus they took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech."}, {"id": "matterportlayout", "name": "MatterportLayout", "description": "MatterportLayout extends the Matterport3D dataset with general Manhattan layout annotations. It has 2,295 RGBD panoramic images from Matterport3D which are extended with ground truth 3D layouts."}, {"id": "acm-association-for-computing-machinery-active-contour-model-algebraic-collective-model-and-compare-module-active-contour-models", "name": "ACM (Association for Computing Machinery\nActive Contour Model\nalgebraic collective model\nand-Compare Module\nActive Contour Models)", "description": "The ACM dataset contains papers published in KDD, SIGMOD, SIGCOMM, MobiCOMM, and VLDB and are divided into three classes (Database, Wireless Communication, Data Mining). An heterogeneous graph is constructed, which comprises 3025 papers, 5835 authors, and 56 subjects. Paper features correspond to elements of a bag-of-words represented of keywords."}, {"id": "maskedface-net", "name": "MaskedFace-Net", "description": "Proposes three types of masked face detection dataset; namely, the Correctly Masked Face Dataset (CMFD), the Incorrectly Masked Face Dataset (IMFD) and their combination for the global masked face detection (MaskedFace-Net)."}, {"id": "petraw-peg-transfer-workflow-recognition-by-different-modalities", "name": "PETRAW (PEg TRAnsfer Workflow recognition by different modalities)", "description": "PETRAW data set was composed of 150 sequences of peg transfer training sessions. The objective of the peg transfer session is to transfer 6 blocks from the left to the right and back. Each block must be extracted from a peg with one hand, transferred to the other hand, and inserted in a peg at the other side of the board. All cases were acquired by a non-medical expert on the LTSI Laboratory from the University of Rennes. The data set was divided into a training data set composed of 90 cases and a test data set composed of 60 cases. A case was composed of kinematic data, a video, semantic segmentation of each frame, and workflow annotation."}, {"id": "yago-yet-another-great-ontology", "name": "YAGO (Yet Another Great Ontology)", "description": "Yet Another Great Ontology (YAGO) is a Knowledge Graph that augments WordNet with common knowledge facts extracted from Wikipedia, converting WordNet from a primarily linguistic resource to a common knowledge base. YAGO originally consisted of more than 1 million entities and 5 million facts describing relationships between these entities. YAGO2 grounded entities, facts, and events in time and space, contained 446 million facts about 9.8 million entities, while YAGO3 added about 1 million more entities from non-English Wikipedia articles. YAGO3-10 a subset of YAGO3, containing entities which have a minimum of 10 relations each."}, {"id": "l3das22", "name": "L3DAS22", "description": "This dataset supports the L3DAS22 IEEE ICASSP Gand Challenge. The challenge is supported by a Python API that facilitates the dataset download and preprocessing, the training and evaluation of the baseline models and the results submission."}, {"id": "santesteban-vto", "name": "Santesteban VTO", "description": "Physics-based simulated garments on top of SMPL bodies. The data is generated used a modified version of ARCSim and sequences from the CMU Motion Capture Database converted to SMPL format in SURREAL. Each simulated sequence is stored as a .pkl file that contains the following data:"}, {"id": "wnut-2017-wnut-2017-emerging-and-rare-entity-recognition", "name": "WNUT 2017 (WNUT 2017 Emerging and Rare entity recognition)", "description": "This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named entities form the basis of many modern approaches to other tasks (like event clustering and summarisation), but recall on them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms. Take for example the tweet \u201cso.. kktny in 30 mins?\u201d - even human experts find entity kktny hard to detect and resolve. This task will evaluate the ability to detect and classify novel, emerging, singleton named entities in noisy text."}, {"id": "hspace-human-space", "name": "HSPACE (Human-SPACE)", "description": "HSPACE (Human-SPACE) is a large-scale photo-realistic dataset of animated humans placed in complex synthetic indoor and outdoor environments. For all frames the dataset provides 3d pose and shape ground truth, as well as other rich image annotations including human segmentation, body part localisation semantics, and temporal correspondences."}, {"id": "avsd-audio-visual-scene-aware-dialog", "name": "AVSD (Audio-Visual Scene-Aware Dialog)", "description": "The Audio Visual Scene-Aware Dialog (AVSD) dataset, or DSTC7 Track 3, is a audio-visual dataset for dialogue understanding. The goal with the dataset and track was to design systems to generate responses in a dialog about a video, given the dialog history and audio-visual content of the video."}, {"id": "omniart", "name": "OmniArt", "description": "Presents half a million samples and structured meta-data to encourage further research and societal engagement."}, {"id": "slue-spoken-language-understanding-evaluation", "name": "SLUE (Spoken Language Understanding Evaluation)", "description": "Spoken Language Understanding Evaluation (SLUE) is a suite of benchmark tasks  for spoken language understanding evaluation. It consists of limited-size labeled training sets and corresponding evaluation sets. This resource would allow the research community to track progress, evaluate pre-trained representations for higher-level tasks, and study open questions such as the utility of pipeline versus end-to-end approaches. The first phase of the SLUE benchmark suite consists of named entity recognition (NER), sentiment analysis (SA), and ASR on the corresponding datasets."}, {"id": "lemgorl", "name": "LemgoRL", "description": "LemgoRL is an open-source benchmark tool for traffic signal control designed to train reinforcement learning agents in a highly realistic simulation scenario with the aim to reduce Sim2Real gap. In addition to the realistic simulation model, LemgoRL encompasses a traffic signal logic unit that ensures compliance with all regulatory and safety requirements. LemgoRL offers the same interface as the well-known OpenAI gym toolkit to enable easy deployment in existing research work."}, {"id": "vot2020", "name": "VOT2020", "description": "VOT2020 is a Visual Object Tracking benchmark for short-term tracking in RGB."}, {"id": "uav-human", "name": "UAV-Human", "description": "UAV-Human is a large dataset for human behavior understanding with UAVs. It contains 67,428 multi-modal video sequences and 119 subjects for action recognition, 22,476 frames for pose estimation, 41,290 frames and 1,144 identities for person re-identification, and 22,263 frames for attribute recognition. The dataset was collected by a flying UAV in multiple urban and rural districts in both daytime and nighttime over three months, hence covering extensive diversities w.r.t subjects, backgrounds, illuminations, weathers, occlusions, camera motions, and UAV flying attitudes. This dataset can be used for UAV-based human behavior understanding, including action recognition, pose estimation, re-identification, and attribute recognition."}, {"id": "ti1k-dataset-thumb-index-1000-hand-fingertip-detection-dataset", "name": "TI1K Dataset (Thumb Index 1000 Hand & Fingertip Detection Dataset)", "description": "Thumb Index 1000 (TI1K) is a dataset of 1000 hand images with the hand bounding box, and thumb and index fingertip positions. The dataset includes the natural movement of the thumb and index fingers making it suitable for mixed reality (MR) applications."}, {"id": "billsum", "name": "BillSum", "description": "BillSum is the first dataset for summarization of US Congressional and California state bills."}, {"id": "proba-v-proba-v-super-resolution-dataset", "name": "PROBA-V (PROBA-V Super-Resolution dataset)", "description": "The PROBA-V Super-Resolution dataset is the official dataset of ESA's Kelvins competition for \"PROBA-V Super Resolution\". It contains satellite data from 74 hand-selected regions around the globe at different points in time. The data is composed of radiometrically and geometrically corrected Top-Of-Atmosphere (TOA) reflectances for the RED and NIR spectral bands at 300m and 100m resolution in Plate Carr\u00e9e projection. The 300m resolution data is delivered as 128x128 grey-scale pixel images, the 100m resolution data as 384x384 grey-scale pixel images. Additionally, a quality map is provided for each pixel, indicating whether the pixels are concealed (i.e. by clouads, ice, water, missing information, etc.)."}, {"id": "pidray", "name": "PIDray", "description": "PIDray is a large-scale dataset which covers various cases in real-world scenarios for prohibited item detection, especially for deliberately hidden items. The dataset contains 12 categories of prohibited items in 47, 677 X-ray images with high-quality annotated segmentation masks and bounding boxes."}, {"id": "artimage", "name": "ArtImage", "description": "ArtImage is a synthetic dataset of articulated object models of 5 categories from PartNet-Mobility for articulated object tasks in category level."}, {"id": "mod-meme-incorporated-open-domain-dialogue", "name": "MOD (Meme incorporated Open-domain Dialogue)", "description": "MOD is a large-scale open-domain multimodal dialogue dataset incorporating abundant Internet memes into utterances. The dataset consists of \u223c45K Chinese conversations with \u223c606K utterances. Each conversation contains about 13 utterances with about 4 Internet memes on average and each utterance equipped with an Internet meme is annotated with the corresponding emotion."}, {"id": "senticap", "name": "SentiCap", "description": "The SentiCap dataset contains several thousand images with captions with positive and negative sentiments. These sentimental captions are constructed by the authors by re-writing factual descriptions. In total there are 2000+ sentimental captions."}, {"id": "rcv1-reuters-corpus-volume-1", "name": "RCV1 (Reuters Corpus Volume 1)", "description": "The RCV1 dataset is a benchmark dataset on text categorization. It is a collection of newswire articles producd by Reuters in 1996-1997. It contains 804,414 manually labeled newswire documents, and categorized with respect to three controlled vocabularies: industries, topics and regions."}, {"id": "vqa-e", "name": "VQA-E", "description": "VQA-E is a dataset for Visual Question Answering with Explanation, where the models are required to generate and explanation with the predicted answer. The VQA-E dataset is automatically derived from the VQA v2 dataset by synthesizing a textual explanation for each image-question-answer triple."}, {"id": "happydb", "name": "HappyDB", "description": "HappyDB is a corpus of 100,000 crowdsourced happy moments."}, {"id": "hrsc2016-high-resolution-ship-collections-2016", "name": "HRSC2016 (High resolution ship collections 2016)", "description": "High-resolution ship collections 2016 (HRSC2016) is a data set used for scientific research. Currently, all of the images in HRSC2016 were collected from Google Earth."}, {"id": "ibm-rank-30k-ibm-argq-rank-30kargs", "name": "IBM-Rank-30k (IBM-ArgQ-Rank-30kArgs)", "description": "The IBM-Rank-30k is a dataset for the task of argument quality ranking. It is a corpus of 30,497 arguments carefully annotated for point-wise quality."}, {"id": "fgadr", "name": "FGADR", "description": "This dataset has 1,842 images with pixel-level DR-related lesion annotations, and 1,000 images with image-level labels graded by six board-certified ophthalmologists with intra-rater consistency. The proposed dataset will enable extensive studies on DR diagnosis."}, {"id": "meta-world-benchmark", "name": "Meta-World Benchmark", "description": "An open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks."}, {"id": "common-objects-in-3d", "name": "Common Objects in 3D", "description": "Common Objects in 3D is a large-scale dataset with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects."}, {"id": "newsroom-cornell-newsroom", "name": "NEWSROOM (CORNELL NEWSROOM)", "description": "CORNELL NEWSROOM is a large dataset for training and evaluating summarization systems. It contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017 and use a variety of summarization strategies combining extraction and abstraction."}, {"id": "pastel", "name": "PASTEL", "description": "PASTEL is a parallelly annotated stylistic language dataset. The dataset consists of ~41K parallel sentences and 8.3K parallel stories annotated across different personas."}, {"id": "mass", "name": "MaSS", "description": "MaSS (Multilingual corpus of Sentence-aligned Spoken utterances) is an extension of the CMU Wilderness Multilingual Speech Dataset, a speech dataset based on recorded readings of the New Testament."}, {"id": "opusparcus", "name": "Opusparcus", "description": "Opusparcus is a paraphrase corpus for six European languages: German, English, Finnish, French, Russian, and Swedish. The paraphrases are extracted from the OpenSubtitles2016 corpus, which contains subtitles from movies and TV shows."}, {"id": "mpii-mpii-human-pose", "name": "MPII (MPII Human Pose)", "description": "The MPII Human Pose Dataset for single person pose estimation is composed of about 25K images of which 15K are training samples, 3K are validation samples and 7K are testing samples (which labels are withheld by the authors). The images are taken from YouTube videos covering 410 different human activities and the poses are manually annotated with up to 16 body joints."}, {"id": "lrs3-ted", "name": "LRS3-TED", "description": "LRS3-TED is a multi-modal dataset for visual and audio-visual speech recognition. It includes face tracks from over 400 hours of TED and TEDx videos, along with the corresponding subtitles and word alignment boundaries. The new dataset is substantially larger in scale compared to other public datasets that are available for general research. "}, {"id": "psi-ava", "name": "PSI-AVA", "description": "PSI-AVA is a dataset designed for holistic surgical scene understanding. It contains approximately 20.45 hours of the surgical procedure performed by three expert surgeons and annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos."}, {"id": "antique", "name": "ANTIQUE", "description": "ANTIQUE is a collection of 2,626 open-domain non-factoid questions from a diverse set of categories. The dataset  contains 34,011 manual relevance annotations. The questions were asked by real users in a community question answering service, i.e., Yahoo! Answers. Relevance judgments for all the answers to each question were collected through crowdsourcing."}, {"id": "deezer-user-networks", "name": "Deezer User Networks", "description": "The data was collected from the music streaming service Deezer (November 2017). These datasets represent friendship networks of users from 3 European countries. Nodes represent the users and edges are the mutual friendships. We reindexed the nodes in order to achieve a certain level of anonimity. The csv files contain the edges -- nodes are indexed from 0. The json files contain the genre preferences of users -- each key is a user id, the genres loved are given as lists. Genre notations are consistent across users. In each dataset users could like 84 distinct genres. Liked genre lists were compiled based on the liked song lists. The countries included are Romania, Croatia and Hungary. For each dataset we listed the number of nodes an edges."}, {"id": "real-m", "name": "REAL-M", "description": "Real-M is a crowd-sourced speech-separation corpus of real-life mixtures. The mixtures are recorded in different acoustic environments using a wide variety of recording devices such as laptops and smartphones, thus reflecting more closely potential application scenarios."}, {"id": "svhn-street-view-house-numbers", "name": "SVHN (Street View House Numbers)", "description": "Street View House Numbers (SVHN) is a digit classification benchmark dataset that contains 600,000 32\u00d732 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images are centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and an extra set with 530,000 images that are less difficult and can be used for helping with the training process."}, {"id": "coco-cn", "name": "COCO-CN", "description": "COCO-CN is a bilingual image description dataset enriching MS-COCO with manually written Chinese sentences and tags. The new dataset can be used for multiple tasks including image tagging, captioning and retrieval, all in a cross-lingual setting."}, {"id": "brazilian-coffee-scenes-dataset", "name": "Brazilian Coffee Scenes Dataset", "description": "This dataset is a composition of scenes taken by SPOT sensor in 2005 over four counties in the State of Minas Gerais, Brazil: Arceburgo, Guaranesia, Guaxup\u00e9 and Monte Santo. It has multispectral high-resolution scenes of coffee crops and non-coffee areas. It has many intraclass variance caused by different crop management technique, as well as scenes with different plant ages and/or with spectral distortions caused by shadows."}, {"id": "e2e-end-to-end-nlg-challenge", "name": "E2E (End-to-End NLG Challenge)", "description": "End-to-End NLG Challenge (E2E) aims to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena."}, {"id": "feret-morphs", "name": "FERET-Morphs", "description": "FERET-Morphs is a dataset of morphed faces selected from the publicly available FERET dataset [1]."}, {"id": "olivetti-face", "name": "Olivetti face", "description": "This dataset contains a set of face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge."}, {"id": "durlar-a-high-fidelity-128-channel-lidar-dataset-with-panoramic-ambient-and-reflectivity-imagery", "name": "DurLAR (A High-Fidelity 128-Channel LiDAR Dataset with Panoramic Ambient and Reflectivity Imagery)", "description": "DurLAR is a high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery for multi-modal autonomous driving applications. Compared to existing autonomous driving task datasets, DurLAR has the following novel features:  "}, {"id": "amr3-0-abstract-meaning-representation-amr-annotation-release-3-0", "name": "AMR3.0 (Abstract Meaning Representation (AMR) Annotation Release 3.0)", "description": "Abstract Meaning Representation (AMR) Annotation Release 3.0 was developed by the Linguistic Data Consortium (LDC), SDL/Language Weaver, Inc., the University of Colorado's Computational Language and Educational Research group and the Information Sciences Institute at the University of Southern California. It contains a sembank (semantic treebank) of over 59,255 English natural language sentences from broadcast conversations, newswire, weblogs, web discussion forums, fiction and web text."}, {"id": "orcas", "name": "ORCAS", "description": "ORCAS is a click-based dataset. It covers 1.4 million of the TREC DL documents, providing 18 million connections to 10 million distinct queries."}, {"id": "norec-fine", "name": "NoReC_fine", "description": "NoReC_fine is a dataset for fine-grained sentiment analysis in Norwegian, annotated with respect to polar expressions, targets and holders of opinion. "}, {"id": "cronquestions", "name": "CronQuestions", "description": "CRONQUESTIONS, the Temporal KGQA dataset consists of two parts: a KG with temporal annotations, and a set of natural language questions requiring temporal reasoning."}, {"id": "imagecode-image-retrieval-from-contextual-descriptions", "name": "ImageCoDe (Image Retrieval from Contextual Descriptions)", "description": "Given 10 minimally contrastive (highly similar) images and a complex description for one of them, the task is to retrieve the correct image. The source of most images are videos and descriptions as well as retrievals come from human."}, {"id": "gun-violence-corpus", "name": "Gun Violence Corpus", "description": "The Gun Violence Corpus (GVC) consists of 241 unique incidents for which we have structured data on a) location, b) time c) the name, gender and age of the victims and d) the status of the victims after the incident: killed or injured.  For these data, 510 news articles were gathered following the 'data to text' approach. The structured data and articles report on a variety of gun violence incidents, such as drive-by shootings, murder-suicides, hunting accidents, involuntary gun discharges, etcetera. The documents have been manually annotated for all mentions that make reference to the gun violence incident at hand."}, {"id": "contour-drawing-dataset", "name": "Contour Drawing Dataset", "description": "A new dataset of contour drawings."}, {"id": "sf-xl-san-francisco-extra-large", "name": "SF-XL (San Francisco eXtra Large)", "description": "Large scale dataset for visual geo-localization / visual place recognition. It provides images from the city of San Francisco, labeled with GPS coordinates and heading."}, {"id": "homebreweddb", "name": "HomebrewedDB", "description": "HomebrewedDB is a dataset for 6D pose estimation mainly targeting training from 3D models (both textured and textureless), scalability, occlusions, and changes in light conditions and object appearance. The dataset features 33 objects (17 toy, 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty. It also consists of a set of benchmarks to test various desired detector properties, particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions, occlusions and clutter."}, {"id": "egad-evolved-grasping-analysis-dataset", "name": "EGAD (Evolved Grasping Analysis Dataset)", "description": "The Evolved Grasping Analysis Dataset (EGAD) comprises over 2000 generated objects aimed at training and evaluating robotic visual grasp detection algorithms. The objects in EGAD are geometrically diverse, filling a space ranging from simple to complex shapes and from easy to difficult to grasp, compared to other datasets for robotic grasping, which may be limited in size or contain only a small number of object classes."}, {"id": "drop-discrete-reasoning-over-paragraphs", "name": "DROP (Discrete Reasoning Over Paragraphs)", "description": "Discrete Reasoning Over Paragraphs DROP is a crowdsourced, adversarially-created, 96k-question benchmark, in which a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. The questions consist of passages extracted from Wikipedia articles. The dataset is split into a training set of about 77,000 questions, a development set of around 9,500 questions and a hidden test set similar in size to the development set."}, {"id": "cats-color-and-thermal-stereo-benchmark", "name": "CATS (Color and Thermal Stereo Benchmark)", "description": "A dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth (< 2mm) generated from a LiDAR. The authors scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes."}, {"id": "fnc-1-fake-news-challenge-stage-1", "name": "FNC-1 (Fake News Challenge Stage 1)", "description": "FNC-1 was designed as a stance detection dataset and it contains 75,385 labeled headline and article pairs. The pairs are labelled as either agree, disagree, discuss, and unrelated. Each headline in the dataset is phrased as a statement"}, {"id": "celeba-hq", "name": "CelebA-HQ", "description": "The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024\u00d71024 resolution."}, {"id": "igibson-2-0", "name": "iGibson 2.0", "description": "iGibson 2.0 is an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations."}, {"id": "emopia-a-multi-modal-pop-piano-dataset-for-emotion-recognition-and-emotion-based-music-generation", "name": "EMOPIA (A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation)", "description": "EMOPIA (pronounced \u2018yee-m\u00f2-pi-uh\u2019) dataset is a shared multi-modal (audio and MIDI) database focusing on perceived emotion in pop piano music, to facilitate research on various tasks related to music emotion. The dataset contains 1,087 music clips from 387 songs and clip-level emotion labels annotated by four dedicated annotators."}, {"id": "uk-dale", "name": "UK-DALE", "description": "UK-DALE is an open-access dataset from the UK recording Domestic Appliance-Level Electricity to conduct research on disaggregation algorithms, with data describing not just the aggregate demand per building but also the `ground truth' demand of individual appliances. It was built at a sample rate of 16 kHz for the whole-house and at 1/6 Hz for individual appliances. This is the first open access UK dataset at this temporal resolution. It wAS recorded from five houses, one of which was recorded for 655 days."}, {"id": "tum-visual-inertial-dataset", "name": "TUM Visual-Inertial Dataset", "description": "A novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024x1024 resolution at 20 Hz, high dynamic range and photometric calibration. "}, {"id": "how2", "name": "How2", "description": "The How2 dataset contains 13,500 videos, or 300 hours of speech, and is split into 185,187 training, 2022 development (dev), and 2361 test utterances. It has subtitles in English and crowdsourced Portuguese translations."}, {"id": "eracond-error-annotated-conversational-dialog-dataset-for-grammatical-error-correction", "name": "ErAConD (Error Annotated Conversational Dialog Dataset for Grammatical Error Correction)", "description": "ErAConD is a novel GEC dataset consisting of parallel original and corrected utterances drawn from open-domain chatbot conversations."}, {"id": "clovacall", "name": "ClovaCall", "description": "ClovaCall is a new large-scale Korean call-based speech corpus under a goal-oriented dialog scenario from more than 11,000 people. The raw dataset of ClovaCall includes approximately 112,000 pairs of a short sentence and its corresponding spoken utterance in a restaurant reservation domain."}, {"id": "mit-traffic", "name": "MIT Traffic", "description": "MIT Traffic is a dataset for research on activity analysis and crowded scenes. It includes a traffic video sequence of 90 minutes long. It is recorded by a stationary camera. The size of the scene is 720 by 480 and it is divided into 20 clips."}, {"id": "who-did-what-who-did-what", "name": "Who-did-What (Who did What)", "description": "Who-did-What collects its corpus from news and provides options for questions similar to CBT. Each question is formed from two independent articles: an article is treated as context to be read and a separate article on the same event is used to form the query."}, {"id": "4d-or", "name": "4D-OR", "description": "4D-OR includes a total of 6734 scenes, recorded by six calibrated RGB-D Kinect sensors 1 mounted to the ceiling of the OR, with one frame-per-second, providing synchronized RGB and depth images. We provide fused point cloud sequences of entire scenes, automatically annotated human 6D poses and 3D bounding boxes for OR objects. Furthermore, we provide SSG annotations for each step of the surgery together with the clinical roles of all the humans in the scenes, e.g., nurse, head surgeon, anesthesiologist."}, {"id": "rfw-racial-faces-in-the-wild", "name": "RFW (Racial Faces in-the-Wild)", "description": "To validate the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. "}, {"id": "uoftped50", "name": "UofTPed50", "description": "UofTPed50 is an object detection and tracking dataset which uses GPS to ground truth the position and velocity of a pedestrian."}, {"id": "jfleg-jhu-fluency-extended-gug-corpus", "name": "JFLEG (JHU FLuency-Extended GUG corpus)", "description": "JFLEG is for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. "}, {"id": "semeval-2018-task-1-semeval-2018-task-1-affect-in-tweets-ait-2018", "name": "SemEval-2018 Task 1 (SemEval-2018 Task 1: Affect in Tweets (AIT-2018))", "description": "POST-COMPETITION: The official competition is now over, but you are welcome to develop and test new solutions on this website. All data with gold labels (training, developing, and test) are available here. The test data in this archive do not include the instances from the Equity Evaluation Corpus (EEC) used for bias evaluation. The EEC corpus is available here."}, {"id": "deformingthings4d", "name": "DeformingThings4D", "description": "DeformingThings4D is a synthetic dataset containing 1,972 animation sequences spanning 31 categories of humanoids and animals. It provides 200 animations for humanoids and 1772 animations for animals. "}, {"id": "cplfw-cross-pose-lfw", "name": "CPLFW (Cross-Pose LFW)", "description": "A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. "}, {"id": "wikiann", "name": "WikiAnn", "description": "WikiAnn is a dataset for cross-lingual name tagging and linking based on Wikipedia articles in 295 languages."}, {"id": "uieb-underwater-image-enhancement-benchmark-dataset", "name": "UIEB (Underwater Image Enhancement Benchmark Dataset)", "description": "Includes 950 real-world underwater images, 890 of which have the corresponding reference images."}, {"id": "partialspoof", "name": "PartialSpoof", "description": "PartialSpoof is a dataset of partially-spoofed data to evaluate detection of partially-spoofed speech data. It has been built based on the ASVspoof 2019 LA database since the latter covers 17 types of spoofed data produced by advanced speech synthesizers, voice converters, and hybrids. The authors used the same set of bona fide data from the ASVspoof 2019 LA database but created partially spoofed audio from the ASVspoof 2019 LA data."}, {"id": "reascan-reascan-compositional-reasoning-in-language-grounding", "name": "ReaSCAN (ReaSCAN: Compositional Reasoning in Language Grounding)", "description": "ReaSCAN is a synthetic navigation task that requires models to reason about surroundings over syntactically difficult languages."}, {"id": "sbd-semantic-boundaries-dataset", "name": "SBD (Semantic Boundaries Dataset)", "description": "The Semantic Boundaries Dataset (SBD) is a dataset for predicting pixels on the boundary of the object (as opposed to the inside of the object with semantic segmentation). The dataset consists of 11318 images from the trainval set of the PASCAL VOC2011 challenge, divided into 8498 training and 2820 test images. This dataset has object instance boundaries with accurate figure/ground masks that are also labeled with one of 20 Pascal VOC classes."}, {"id": "coda-19", "name": "CODA-19", "description": "CODA-19 is a human-annotated dataset that denotes the Background, Purpose, Method, Finding/Contribution, and Other for 10,966 English abstracts in the COVID-19 Open Research Dataset."}, {"id": "casino", "name": "CaSiNo", "description": "CaSiNo is a dataset of 1030 negotiation dialogues in English. To create the dataset, two participates take the role of campsite neighbors and negotiate for Food, Water, and Firewood packages, based on their individual preferences and requirements. This design keeps the task tractable, while still facilitating linguistically rich and personal conversations."}, {"id": "streetlearn", "name": "StreetLearn", "description": "An interactive, first-person, partially-observed visual environment that uses Google Street View for its photographic content and broad coverage, and give performance baselines for a challenging goal-driven navigation task. "}, {"id": "msrdailyactivity3d", "name": "MSRDailyActivity3D", "description": "DailyActivity3D dataset is a daily activity dataset captured by a Kinect device. There are 16 activity types: drink, eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand up, sit down. If possible, each subject performs an activity in two different poses: \u201csitting on sofa\u201d and \u201cstanding\u201d. The total number of the activity samples is 320. This dataset is designed to cover human\u2019s daily activities in the living room. When the performer stands close to the sofa or sits on the sofa, the 3D joint positions extracted by the skeleton tracker are very noisy. Moreover, most of the activities involve the humans-object interactions. Thus this dataset is more challenging."}, {"id": "viton-viton-zalando-dataset", "name": "VITON (VITON-Zalando Dataset)", "description": "VITON was a dataset for virtual try-on of clothing items. It consisted of 16,253 pairs of images of a person and a clothing item ."}, {"id": "freibrug-cars", "name": "Freibrug Cars", "description": "An object-centric dataset consiting of 52 RGB sequences of cars"}, {"id": "scirepeval", "name": "SciRepEval", "description": "SciRepEval is a comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search."}, {"id": "text8", "name": "Text8", "description": "Desc: About of Text8"}, {"id": "medmentions", "name": "MedMentions", "description": "MedMentions is a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. "}, {"id": "comp6-comprehensive-machine-learning-potential", "name": "COMP6 (COmprehensive Machine-learning Potential)", "description": "COMP6 is a benchmark for evaluating the extensibility of machine-learning based molecular potentials. It contains a diverse set of organic molecules."}, {"id": "insuranceqa", "name": "InsuranceQA", "description": "InsuranceQA is a question answering dataset for the insurance domain, the data stemming from the website Insurance Library. There are 12,889 questions and 21,325 answers in the training set. There are 2,000 questions and 3,354 answers in the validation set. There are 2,000 questions and 3,308 answers in the test set."}, {"id": "msu-fr-vqa-database-msu-full-reference-video-quality-assessment-database", "name": "MSU FR VQA Database (MSU Full-Reference Video Quality Assessment Database)", "description": "The dataset was created for video quality assessment problem. It was formed with 36 clips from Vimeo, which were selected from 18,000+ open-source clips with high bitrate (license CCBY or CC0)."}, {"id": "subj-subjectivity-dataset", "name": "SUBJ (Subjectivity dataset)", "description": "Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity."}, {"id": "pioner", "name": "pioNER", "description": "The pioNER corpus provides gold-standard and automatically generated named-entity datasets for the Armenian language. The automatically generated corpus is generated from Wikipedia. The gold-standard set is a collection of over 250 news articles from iLur.am with manual named-entity annotation. It includes sentences from political, sports, local and world news, and is comparable in size with the test sets of other languages."}, {"id": "pubmed-cognitive-control-abstracts-cogtext", "name": "PubMed Cognitive Control Abstracts (CogText)", "description": "A collection of 385,705 scientific abstracts about Cognitive Control and their GPT-3 embeddings."}, {"id": "def-infantry-parallel-smac-def-infantry-parallel-20", "name": "Def_Infantry_parallel (SMAC+_Def_Infantry_parallel_20)", "description": "smac+ defense infantry scenario with parallel episodic buffer"}, {"id": "mumu", "name": "MuMu", "description": "MuMu is a new dataset of more than 31k albums classified into 250 genre classes."}, {"id": "ucfrep", "name": "UCFRep", "description": "The UCFRep dataset contains 526 annotated repetitive action videos. This dataset is built from the action recognition dataset UCF101."}, {"id": "kitti360-ex", "name": "KITTI360-EX", "description": "KITTI360-EX is a dataset for outer- and inner FoV expansion. It contains 76k pinhole images as well as 76k spherical images and is used for beyond-FoV estimation."}, {"id": "stocknet", "name": "StockNet", "description": "The StockNet dataset is a comprehensive dataset for stock movement prediction from tweets and historical stock prices. It consists of two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks, coming from all the 8 stocks in the Conglomerates sector and the top 10 stocks in capital size in each of the other 8 sectors."}, {"id": "sfew-static-facial-expression-in-the-wild", "name": "SFEW (Static Facial Expression in the Wild)", "description": "The Static Facial Expressions in the Wild (SFEW) dataset is a dataset for facial expression recognition. It was created by selecting static frames from the AFEW database by computing key frames based on facial point clustering. The most commonly used version, SFEW 2.0, was the benchmarking data for the SReco sub-challenge in EmotiW 2015. SFEW 2.0 has been divided into three sets: Train (958 samples), Val (436 samples) and Test (372 samples). Each of the images is assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happiness, sadness, and surprise. The expression labels of the training and validation sets are publicly available, whereas those of the testing set are held back by the challenge organizer."}, {"id": "must-c", "name": "MuST-C", "description": "MuST-C currently represents the largest publicly available multilingual corpus (one-to-many) for speech translation. It covers eight language directions, from English to German, Spanish, French, Italian, Dutch, Portuguese, Romanian and Russian. The corpus consists of audio, transcriptions and translations of English TED talks, and it comes with a predefined training, validation and test split."}, {"id": "wikitext-2", "name": "WikiText-2", "description": "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License."}, {"id": "l3das21", "name": "L3DAS21", "description": "L3DAS21 is a dataset for 3D audio signal processing. It consists of a 65 hours 3D audio corpus, accompanied with a Python API that facilitates the data usage and results submission stage. "}, {"id": "scicite", "name": "SciCite", "description": "SciCite is a dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL-ARC."}, {"id": "pipal-perceptual-image-processing-algorithms-iqa-dataset", "name": "PIPAL (Perceptual Image Processing ALgorithms IQA Dataset)", "description": "PIPAL training set contains 200 reference images, 40 distortion types, 23k distortion images, and more than one million human ratings. Especially, we include GAN-based algorithms\u2019 outputs as a new GAN-based distortion type. We employ the Elo rating system to assign the Mean Opinion Scores (MOS)."}, {"id": "exdark-exclusively-dark-image-dataset", "name": "ExDark (Exclusively Dark Image Dataset)", "description": "The Exclusively Dark (ExDARK) dataset is a collection of 7,363 low-light images from very low-light environments to twilight (i.e 10 different conditions) with 12 object classes (similar to PASCAL VOC) annotated on both image class level and local object bounding boxes."}, {"id": "middlebury-2001", "name": "Middlebury 2001", "description": "The Middlebury 2001 is a stereo dataset of indoor scenes with multiple handcrafted layouts."}, {"id": "masakhaner", "name": "MasakhaNER", "description": "MasakhaNER is a collection of Named Entity Recognition (NER) datasets for 10 different African languages. The languages forming this dataset are: Amharic, Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian-Pidgin, Swahili, Wolof, and Yor\u00f9b\u00e1."}, {"id": "dukemtmc-videoreid", "name": "DukeMTMC-VideoReID", "description": "The DukeMTMC-VideoReID (Duke Multi-Tracking Multi-Camera Video-based ReIDentification) dataset is a subset of the DukeMTMC for video-based person re-ID. The dataset is created from high-resolution videos from 8 different cameras. It is one of the largest pedestrian video datasets wherein images are cropped by hand-drawn bounding boxes. The dataset consists 4832 tracklets of 1812 identities in total, and each tracklet has 168 frames on average."}, {"id": "usr-personachat", "name": "USR-PersonaChat", "description": "This dataset was collected with the goal of assessing dialog evaluation metrics. In the paper, USR: An Unsupervised and Reference Free Evaluation Metric for Dialog (Mehri and Eskenazi, 2020), the authors collect this data to measure the quality of several existing word-overlap and embedding-based metrics, as well as their newly proposed USR metric."}, {"id": "stacked-mnist", "name": "Stacked MNIST", "description": "The Stacked MNIST dataset is derived from the standard MNIST dataset with an increased number of discrete modes. 240,000 RGB images in the size of 32\u00d732 are synthesized by stacking three random digit images from MNIST along the color channel, resulting in 1,000 explicit modes in a uniform distribution corresponding to the number of possible triples of digits."}, {"id": "ufpr-amr", "name": "UFPR-AMR", "description": "This dataset contains 2,000 images taken from inside a warehouse of the Energy Company of Paran\u00e1 (Copel), which directly serves more than 4 million consuming units in the Brazilian state of Paran\u00e1."}, {"id": "hockey-fight-detection-dataset", "name": "Hockey Fight Detection Dataset", "description": "Whereas the action recognition community has focused mostly on detecting simple actions like clapping, walking or jogging, the detection of fights or in general aggressive behaviors has been comparatively less studied. Such capability may be extremely useful in some video surveillance scenarios like in prisons, psychiatric or elderly centers or even in camera phones. After an analysis of previous approaches we test the well-known Bag-of-Words framework used for action recognition in the specific problem of fight detection, along with two of the best action descriptors currently available: STIP and MoSIFT. For the purpose of evaluation and to foster research on violence detection in video we introduce a new video database containing 1000 sequences divided in two groups: fights and non-fights. Experiments on this database and another one with fights from action movies show that fights can be detected with near 90% accuracy."}, {"id": "acdc-scribbles", "name": "ACDC Scribbles", "description": "We release expert-made scribble annotations for the medical ACDC dataset [1]. The released data must be considered as extending the original ACDC dataset. The ACDC dataset contains cardiac MRI images, paired with hand-made segmentation masks. It is possible to use the segmentation masks provided in the ACDC dataset to evaluate the performance of methods trained using only scribble supervision. "}, {"id": "a3d-anan-accident-detection", "name": "A3D (AnAn Accident Detection)", "description": "A new dataset of diverse traffic accidents."}, {"id": "pubtables-1m-pubmed-tables-one-million", "name": "PubTables-1M (PubMed Tables One Million)", "description": "The goal of PubTables-1M is to create a large, detailed, high-quality dataset for training and evaluating a wide variety of models for the tasks of table detection, table structure recognition, and functional analysis. It contains:"}, {"id": "egok360", "name": "EGOK360", "description": "Contains annotations of human activity with different sub-actions, e.g., activity Ping-Pong with four sub-actions which are pickup-ball, hit, bounce-ball and serve. "}, {"id": "wenetspeech", "name": "WenetSpeech", "description": "WenetSpeech is a multi-domain Mandarin corpus consisting of 10,000+ hours high-quality labeled speech, 2,400+ hours weakly labelled speech, and about 10,000 hours unlabeled speech, with 22,400+ hours in total. The authors collected the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics, and noisy conditions. An optical character recognition (OCR) based method is introduced to generate the audio/text segmentation candidates for the YouTube data on its corresponding video captions."}, {"id": "hollywood-3d-dataset", "name": "Hollywood 3D dataset", "description": "A dataset for benchmarking action recognition algorithms in natural environments, while making use of 3D information. The dataset contains around 650 video clips, across 14 classes. In addition, two state of the art action recognition algorithms are extended to make use of the 3D data, and five new interest point detection strategies are also proposed, that extend to the 3D data. "}, {"id": "vlep-video-and-language-event-prediction", "name": "VLEP (Video-and-Language Event Prediction)", "description": "VLEP contains 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips. Each example (see Figure 1) consists of a Premise Event (a short video clip with dialogue), a Premise Summary (a text summary of the premise event), and two potential natural language Future Events (along with Rationales) written by people. These clips are on average 6.1 seconds long and are harvested from diverse event-rich sources, i.e., TV show and YouTube Lifestyle Vlog videos."}, {"id": "stereoset", "name": "StereoSet", "description": "A large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion."}, {"id": "nus-wide", "name": "NUS-WIDE", "description": "The NUS-WIDE dataset contains 269,648 images with a total of 5,018 tags collected from Flickr. These images are manually annotated with 81 concepts, including objects and scenes."}, {"id": "nlmaps", "name": "NLmaps", "description": "There are two versions of the NLmaps corpus. NLmaps (v1) and its extension NLmaps v2. Both versions of the NLmaps corpus consist of questions about geographical facts that can be answered with the OpenStreetMap (OSM) database (available under the Open Database Licence). The questions are in English and have a corresponding Machine Readable Language (MRL) parse. Gold answers can be obtained by executing the gold parses against the OSM database using the NLmaps backend, which is based on the Overpass-API (available under the Affero GPL v3)."}, {"id": "rite-retinal-images-vessel-tree-extraction", "name": "RITE (Retinal Images vessel Tree Extraction)", "description": "The RITE (Retinal Images vessel Tree Extraction) is a database that enables comparative studies on segmentation or classification of arteries and veins on retinal fundus images, which is established based on the public available DRIVE database (Digital Retinal Images for Vessel Extraction)."}, {"id": "cosal2015", "name": "CoSal2015", "description": "Cosal2015 is a large-scale dataset for co-saliency detection which consists of 2,015 images of 50 categories, and each group suffers from various challenging factors such as complex environments, occlusion issues, target appearance variations and background clutters, etc. All these increase the difficulty for accurate co-saliency detection."}, {"id": "cmeee-chinese-medical-named-entity-recognition-dataset", "name": "CMeEE (Chinese Medical Named Entity Recognition Dataset)", "description": "Chinese Medical Named Entity Recognition, a dataset first released in CHIP20204, is used for CMeEE task. Given a pre-defined schema, the task is to identify and extract entities from the given sentence and classify them into nine categories: disease, clinical manifestations, drugs, medical equipment, medical procedures, body, medical examinations, microorganisms, and department."}, {"id": "quoref", "name": "Quoref", "description": "Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems. In this span-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard coreferences before selecting the appropriate span(s) in the paragraphs for answering questions."}, {"id": "dialogue-state-tracking-challenge", "name": "Dialogue State Tracking Challenge", "description": "The Dialog State Tracking Challenges 2 & 3 (DSTC2&3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. State tracking, sometimes called belief tracking, refers to accurately estimating the user's goal as a dialog progresses. Accurate state tracking is desirable because it provides robustness to errors in speech recognition, and helps reduce ambiguity inherent in language within a temporal process like dialog. In these challenges, participants were given labelled corpora of dialogs to develop state tracking algorithms. The trackers were then evaluated on a common set of held-out dialogs, which were released, un-labelled, during a one week period."}, {"id": "st-vqa-scene-text-visual-question-answering", "name": "ST-VQA (Scene Text Visual Question Answering)", "description": "ST-VQA aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the VQA process. "}, {"id": "place-pulse-2-0", "name": "Place Pulse 2.0", "description": "Place Pulse is a crowdsourcing effort that aims to map which areas of a city are perceived as safer, livelier, wealthier, more active, beautiful and friendly. By asking users to select images from a pair, Place Pulse collected more than 1.5 million reports that evaluate more than 100,000 images from 56 cities."}, {"id": "mutual", "name": "MuTual", "description": "MuTual is a retrieval-based dataset for multi-turn dialogue reasoning, which is modified from Chinese high school English listening comprehension test data. It tests dialogue reasoning via next utterance prediction."}, {"id": "matres-multi-axis-temporal-relations-for-start-points", "name": "MATRES (Multi-Axis Temporal RElations for Start-points)", "description": "This is the Multi-Axis Temporal RElations for Start-points (i.e., MATRES) dataset"}, {"id": "fvi-free-form-video-inpainting", "name": "FVI (Free-form Video Inpainting)", "description": "The Free-Form Video Inpainting dataset is a dataset used for training and evaluation video inpainting models. It consists of 1940 videos from the YouTube-VOS dataset and 12,600 videos from the YouTube-BoundingBoxes."}, {"id": "bs-rscd", "name": "BS-RSCD", "description": "BS-RSCD is a dataset for rolling shutter correction and deblurring (RSCD). The dataset includes both ego-motion and object-motion in dynamic scenes. Real distorted and blurry videos with corresponding ground truth are recorded simultaneously via a beam-splitter-based acquisition system."}, {"id": "irs-indoor-robotics-stereo", "name": "IRS (Indoor Robotics Stereo)", "description": "IRS is an open dataset for indoor robotics vision tasks, especially disparity and surface normal estimation. It contains totally 103,316 samples covering a wide range of indoor scenes, such as home, office, store and restaurant."}, {"id": "acfr-orchard-fruit-dataset", "name": "ACFR Orchard Fruit Dataset", "description": "ACFR Orchard Fruit Dataset is an agricultural dataset containing images and annotations for different fruits, collected at different farms across Australia. The dataset was gathered by the agriculture team at the Australian Centre for Field Robotics, The University of Sydney, Australia."}, {"id": "egocap", "name": "EgoCap", "description": "EgoCap is a dataest of 100,000 egocentric images of eight people in different clothing, with 75,000 images from six people used for training. The images have been captured with two fisheye cameras."}, {"id": "fsdd-free-spoken-digit-dataset", "name": "FSDD (Free Spoken Digit Dataset)", "description": "Free Spoken Digit Dataset (FSDD) is a simple audio/speech dataset consisting of recordings of spoken digits in wav files at 8kHz. The recordings are trimmed so that they have near minimal silence at the beginnings and ends. It contains data from 6 speakers, 3,000 recordings (50 of each digit per speaker), and English pronunciations."}, {"id": "guitarset", "name": "GuitarSet", "description": "GuitarSet is a dataset of high-quality guitar recordings and rich annotations. It contains 360 excerpts 30 seconds in length. The 360 excerpts are the result of the following combinations:"}, {"id": "iitb-corridor", "name": "IITB Corridor", "description": "An abnormal activity data-set for research use that contains 4,83,566 annotated frames."}, {"id": "camerafusion", "name": "CameraFusion", "description": "We present a novel approach to reference-based super-resolution (RefSR) with the focus on real-world dual-camera super-resolution (DCSR). This dataset currently consists of 143 pairs of telephoto and wide-angle images in 4K resolution captured by smartphone dual-cameras. See our paper for more details: Dual-Camera Super-Resolution with Aligned Attention Modules."}, {"id": "phomt", "name": "PhoMT", "description": "PhoMT is a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs for machine translation."}, {"id": "cater", "name": "CATER", "description": "Rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning."}, {"id": "objaverse", "name": "Objaverse", "description": "Objaverse is a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category."}, {"id": "jobs", "name": "Jobs", "description": "The Jobs dataset by LaLonde [36] is a widely used benchmark in the causal inference community, where the treatment is job training and the outcomes are income and employment status after training. The dataset includes 8 covariates such as age, education, and previous earnings. Our goal is to predict unemployment, using the feature set of Dehejia and Wahba [37]. Following Shalit et al. [8], we combined the LaLonde experimental sample (297 treated, 425 control) with the PSID comparison group (2490 control)."}, {"id": "learning-to-rank-challenge-yahoo-learning-to-rank-challenge", "name": "Learning to Rank Challenge (Yahoo! Learning to Rank Challenge)", "description": "The Yahoo! Learning to Rank Challenge dataset consists of 709,877 documents encoded in 700 features and sampled from query logs of the Yahoo! search engine, spanning 29,921 queries."}, {"id": "advising-corpus", "name": "Advising Corpus", "description": "Advising Corpus is a dataset based on an entirely new collection of dialogues in which university students are being advised which classes to take. These were collected at the University of Michigan with IRB approval. They were released as part of DSTC 7 track 1 and used again in DSTC 8 track 2."}, {"id": "thingi10k", "name": "Thingi10K", "description": "Thingi10K is a dataset of 3D-Printing Models. Specifically there are 10,000 models from featured \u201cthings\u201d on thingiverse.com, suitable for testing 3D printing techniques such as structural analysis , shape optimization, or solid geometry operations."}, {"id": "ipinyou-ipinyou-global-rtb-bidding-algorithm-competition-dataset", "name": "iPinYou (iPinYou Global RTB Bidding Algorithm Competition Dataset)", "description": "The iPinYou Global RTB(Real-Time Bidding) Bidding Algorithm Competition is organized by iPinYou from April 1st, 2013 to December 31st, 2013.The competition has been divided into three seasons. For each season, a training dataset is released to the competition participants, the testing dataset is reserved by iPinYou. The complete testing dataset is randomly divided into two parts: one part is the leaderboard testing dataset to score and rank the participating teams on the leaderboard, and the other part is reserved for the final offline evaluation. The participant's last offline submission is evaluated by the reserved testing dataset to get a team's offline final score. This dataset contains all three seasons training datasets and leaderboard testing datasets.The reserved testing datasets are withheld by iPinYou. The training dataset includes a set of processed iPinYou DSP bidding, impression, click, and conversion logs."}, {"id": "pwdb-pulse-wave-database", "name": "PWDB (Pulse Wave Database)", "description": "This database of simulated arterial pulse waves is designed to be representative of a sample of pulse waves measured from healthy adults. It contains pulse waves for 4,374 virtual subjects, aged from 25-75 years old (in 10 year increments). The database contains a baseline set of pulse waves for each of the six age groups, created using cardiovascular properties (such as heart rate and arterial stiffness) which are representative of healthy subjects at each age group. It also contains 728 further virtual subjects at each age group, in which each of the cardiovascular properties are varied within normal ranges. This allows for extensive in silico analyses of haemodynamics and the performance of pulse wave analysis algorithms."}, {"id": "clear", "name": "CLEAR", "description": "CLEAR is a continual image classification benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2004-2014). CLEAR is built from existing large-scale image collections (YFCC100M) through a novel and scalable low-cost approach to visio-linguistic dataset curation. The pipeline makes use of pretrained vision language models (e.g. CLIP) to interactively build labeled datasets, which are further validated with crowd-sourcing to remove errors and even inappropriate images (hidden in original YFCC100M). The major strength of CLEAR over prior CL benchmarks is the smooth temporal evolution of visual concepts with real-world imagery, including both high-quality labeled data along with abundant unlabeled samples per time period for continual semi-supervised learning."}, {"id": "vgg-ss-vgg-sound-source", "name": "VGG-SS (VGG-Sound Source)", "description": "VGG-SS (VGG Sound Source) is a benchmark for evaluating sound source localisation in videos. The dataset consists on a new set of annotations for the recently-introduced VGG-Sound dataset, where the sound sources visible in each video clip are explicitly marked with bounding box annotations. This dataset is 20 times larger than analogous existing ones, contains 5K videos spanning over 200 categories, and, differently from Flickr SoundNet, is video-based."}, {"id": "definite-pronoun-resolution-dataset", "name": "Definite Pronoun Resolution Dataset", "description": "Composes sentence pairs (i.e., twin sentences)."}, {"id": "wikiconv", "name": "WikiConv", "description": "A corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations---including not only comments and replies, but also their modifications, deletions and restorations---this data offers an unprecedented view of online conversation."}, {"id": "quickdraw-extended", "name": "QuickDraw-Extended", "description": "Consists of 330,000 sketches and 204,000 photos spanning across 110 categories."}, {"id": "bird-bigram-relatedness-dataset", "name": "BiRD (Bigram Relatedness Dataset)", "description": "Bigram Relatedness Dataset (BiRD) is a large, fine-grained, bigram relatedness dataset, using a comparative annotation technique called Best Worst Scaling. Each of BiRD's 3,345 English term pairs involves at least one bigram. BiRD is made freely available to foster further research on how meaning can be represented and how meaning can be composed."}, {"id": "dsd100", "name": "DSD100", "description": "The dsd100 is a dataset of 100 full lengths of music tracks of different styles along with their isolated drums, bass, vocals, and other stems."}, {"id": "ldc2020t02-abstract-meaning-representation-amr-annotation-release-3-0", "name": "LDC2020T02 (Abstract Meaning Representation (AMR) Annotation Release 3.0)", "description": "Abstract Meaning Representation (AMR) Annotation Release 3.0 was developed by the Linguistic Data Consortium (LDC), SDL/Language Weaver, Inc., the University of Colorado's Computational Language and Educational Research group and the Information Sciences Institute at the University of Southern California. It contains a sembank (semantic treebank) of over 59,255 English natural language sentences from broadcast conversations, newswire, weblogs, web discussion forums, fiction and web text. This release adds new data to, and updates material contained in, Abstract Meaning Representation 2.0 (LDC2017T10), specifically: more annotations on new and prior data, new or improved PropBank-style frames, enhanced quality control, and multi-sentence annotations."}, {"id": "stylegan-human", "name": "StyleGAN-Human", "description": "A large-scale human image dataset with over 230K samples capturing diverse poses and textures."}, {"id": "scrolls-standardized-comparison-over-long-language-sequences", "name": "SCROLLS (Standardized CompaRison Over Long Language Sequences)", "description": " SCROLLS (Standardized CompaRison Over Long Language Sequences) is an NLP benchmark consisting of a suite of tasks that require reasoning over long texts. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. The dataset is made available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods."}, {"id": "europarl-european-parliament-proceedings-parallel-corpus", "name": "Europarl (European Parliament Proceedings Parallel Corpus)", "description": "A corpus of parallel text in 21 European languages from the proceedings of the European Parliament."}, {"id": "dstc7-task-2-dialog-system-technology-challenges-task-2", "name": "DSTC7 Task 2 (Dialog System Technology Challenges Task 2)", "description": "DSTC Task 2 is a dataset and task for end-to-end conversation modeling. The goal is to generate conversational responses that go beyond trivial chitchat by injecting informative responses that are grounded in external knowledge. The data consists of conversational data from Reddit, and contextually-relevant \u201cfacts\u201d taken from the website that started the Reddit conversation. That is the setup is grounded, as each conversation in the data is about a specific web page that was linked at the start of the conversation."}, {"id": "movi-large-multipurpose-motion-and-video-dataset", "name": "MoVi (Large Multipurpose Motion and Video Dataset)", "description": "Contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement."}, {"id": "crackseg9k", "name": "Crackseg9k", "description": "The dataset published here is the largest, most diverse and consistent crack segmentation dataset constructed so far. It contains 9255 images that combine different smaller open source datasets. It consists of 10 sub datasets preprocessed and resized to 400x400 namely, Crack500, Deepcrack, Sdnet, Cracktree, Gaps, Volker, Rissbilder, Noncrack, Masonry and Ceramic."}, {"id": "200-people-chinese-speech-data-by-mobile-phone", "name": "200 People-Chinese Speech Data by Mobile Phone", "description": "Description: The dataset contains 200 Chinese native speakers, covering main dialect zones. It is recorded in both noisy and quiet environment and more suitable for the actual application scenario for speech recognition. The recordings are commonly used spoken sentences. Texts are transcribed by professional annotators. It can be used for speech recognition and machine translation."}, {"id": "preco", "name": "PreCo", "description": "A large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. "}, {"id": "videoset", "name": "VideoSet", "description": "VideoSet is a large-scale compressed video quality dataset based on just-noticeable-difference (JND) measurement."}, {"id": "sciq", "name": "SciQ", "description": "The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided."}, {"id": "buaa-mihr-dataset-large-scale-multi-illumination-hr-database", "name": "BUAA-MIHR dataset (Large-scale-Multi-illumination-HR-Database)", "description": "BUAA-MIHR dataset is a remote photoplethysmography (rPPG) dataset. BUAA-MIHR dataset for evaluation of remote photoplethysmography pipeline under multi-illumination situations. We recruited 15 healthy subjects (12 male, 3 female, 18 to 30 years old) in this experiment and a total number of 165 video sequences were recorded under various illuminations. The experiments were conducted in a darkroom in order to isolate from ambient light."}, {"id": "epinion", "name": "Epinion", "description": "The Epinions dataset is trust network dataset. For each user, it contains his profile, his ratings and his trust relations. For each rating, it has the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating."}, {"id": "rr-review-rebuttal", "name": "RR (Review-Rebuttal)", "description": "Review-Rebuttal (RR) dataset is introduced to facilitate the study of argument pair extraction in the peer review and rebuttal domain."}, {"id": "autsl-ankara-university-turkish-sign-language-dataset", "name": "AUTSL (Ankara University Turkish Sign Language Dataset)", "description": "The Ankara University Turkish Sign Language Dataset (AUTSL) is a large-scale, multimode dataset that contains isolated Turkish sign videos. It contains 226 signs that are performed by 43 different signers. There are 38,336 video samples in total. The samples are recorded using Microsoft Kinect v2 in RGB, depth and skeleton formats. The videos are provided at a resolution of 512\u00d7512. The skeleton data contains spatial coordinates, i.e. (x, y), of the 25 junction points on the signer body that are aligned with 512\u00d7512 data."}, {"id": "metaqa-movie-text-audio-qa", "name": "MetaQA (MoviE Text Audio QA)", "description": "The MetaQA dataset consists of a movie ontology derived from the WikiMovies Dataset and three sets of question-answer pairs written in natural language: 1-hop, 2-hop, and 3-hop queries."}, {"id": "kitti-road", "name": "KITTI Road", "description": "KITTI Road is road and lane estimation benchmark that consists of 289 training and 290 test images. It contains three different categories of road scenes: * uu - urban unmarked (98/100) * um - urban marked (95/96) * umm - urban multiple marked lanes (96/94) * urban - combination of the three above Ground truth has been generated by manual annotation of the images and is available for two different road terrain types: road - the road area, i.e, the composition of all lanes, and lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category \"um\"). Ground truth is provided for training images only."}, {"id": "malimg", "name": "Malimg", "description": "The Malimg Dataset contains 9,339 malware byteplot images from 25 different families."}, {"id": "mediaspeech", "name": "MediaSpeech", "description": "MediaSpeech is a media speech dataset (you might have guessed this) built with the purpose of testing Automated Speech Recognition (ASR) systems performance. The dataset consists of short speech segments automatically extracted from media videos available on YouTube and manually transcribed, with some pre- and post-processing. The dataset contains 10 hours of speech for each language provided. This release contains audio datasets in French, Arabic, Turkish and Spanish, and is a part of a larger private dataset."}, {"id": "retouch-retouch-the-retinal-oct-fluid-detection-and-segmentation-benchmark-and-challenge", "name": "RETOUCH (RETOUCH -The Retinal OCT Fluid Detection and Segmentation Benchmark and Challenge)", "description": "The goal of the challenge is to compare automated algorithms that are able to detect and segment various types of fluids on a common dataset of optical coherence tomography (OCT) volumes representing different retinal diseases, acquired with devices from different manufacturers. We made available a dataset of OCT volumes containing a wide variety of retinal fluid lesions with accompanying reference annotations. We invite the medical imaging community to participate by developing and testing existing and novel automated retinal OCT segmentation methods."}, {"id": "ucf-crime", "name": "UCF-Crime", "description": "The UCF-Crime dataset is a large-scale dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies including Abuse, Arrest, Arson, Assault, Road Accident, Burglary, Explosion, Fighting, Robbery, Shooting, Stealing, Shoplifting, and Vandalism. These anomalies are selected because they have a significant impact on public safety. "}, {"id": "mot20", "name": "MOT20", "description": "MOT20 is a dataset for multiple object tracking. The dataset contains 8 challenging video sequences (4 train, 4 test) in unconstrained environments, from crowded places such as train stations, town squares and a sports stadium."}, {"id": "squad-stanford-question-answering-dataset", "name": "SQuAD (Stanford Question Answering Dataset)", "description": "The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones."}, {"id": "elpv-a-dataset-of-functional-and-defective-solar-cells-extracted-from-el-images-of-solar-modules", "name": "ELPV (A dataset of functional and defective solar cells extracted from EL images of solar modules)", "description": "The dataset contains 2,624 samples of $300\\times300$ pixels 8-bit grayscale images of functional and defective solar cells with varying degree of degradations extracted from 44 different solar modules. The defects in the annotated images are either of intrinsic or extrinsic type and are known to reduce the power efficiency of solar modules."}, {"id": "comphy-compositional-physical-reasoning-dataset", "name": "ComPhy (Compositional Physical Reasoning Dataset)", "description": "**Compositional Physical Reasoning is a dataset for understanding object-centric and relational physics properties hidden from visual appearances. For a given set of objects, the dataset includes few videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions posted on one of the videos."}, {"id": "im2latex-100k", "name": "im2latex-100k", "description": "A prebuilt dataset for OpenAI's task for image-2-latex system. Includes total of ~100k formulas and images splitted into train, validation and test sets. Formulas were parsed from LaTeX sources provided here: http://www.cs.cornell.edu/projects/kddcup/datasets.html(originally from  arXiv)"}, {"id": "clevr-hans", "name": "CLEVR-Hans", "description": "The CLEVR-Hans data set is a novel confounded visual scene data set, which captures complex compositions of different objects. This data set consists of CLEVR images divided into several classes. "}, {"id": "twitter-sentiment-analysis-entity-level-twitter-sentiment-analysis-dataset", "name": "Twitter Sentiment Analysis (Entity-Level Twitter Sentiment Analysis Dataset)", "description": "This is an entity-level Twitter Sentiment Analysis dataset. For each message, the task is to judge the sentiment of the entire sentence towards a given entity. For example, A outperforms B is positive for entity A but negative for entity B. The dataset contains ~70K labeled training messages and 1K labeled validation messages. It is available online for free on Kaggle."}, {"id": "3dpeople-dataset", "name": "3DPeople Dataset", "description": "A large-scale synthetic dataset with 2.5 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits."}, {"id": "chilean-waiting-list", "name": "Chilean Waiting List", "description": "The Chilean Waiting List corpus comprises de-identified referrals from the waiting list in Chilean public hospitals. A subset of 10,000 referrals (including medical and dental notes) was manually annotated with ten entity types with clinical relevance, keeping 1,000 annotations for a future shared task. A trained medical doctor or dentist annotated these referrals and then, together with three other researchers, consolidated each of the annotations. The annotated corpus has more than 48% of entities embedded in other entities or containing another. This corpus can be a useful resource to build new models for Nested Named Entity Recognition (NER). This work constitutes the first annotated corpus using clinical narratives from Chile and one of the few in Spanish."}, {"id": "kit-motion-language", "name": "KIT Motion-Language", "description": "The KIT Motion-Language is a dataset linking human motion and natural language."}, {"id": "visa-visual-anomaly-dataset", "name": "VisA (Visual Anomaly Dataset)", "description": "The VisA dataset contains 12 subsets corresponding to 12 different objects as shown in the above figure. There are 10,821 images with 9,621 normal and 1,200 anomalous samples. Four subsets are different types of printed circuit boards (PCB) with relatively complex structures containing transistors, capacitors, chips, etc. For the case of multiple instances in a view, we collect four subsets: Capsules, Candles, Macaroni1 and Macaroni2. Instances in Capsules and Macaroni2 largely differ in locations and poses. Moreover, we collect four subsets including Cashew, Chewing gum, Fryum and Pipe fryum, where objects are roughly aligned. The anomalous images contain various flaws, including surface defects such as scratches, dents, color spots or crack, and structural defects like misplacement or missing parts."}, {"id": "mju-waste", "name": "MJU-Waste", "description": "MJU-Waste is an RGBD waste object segmentation dataset that is made public to facilitate future research in this area."}, {"id": "mctest", "name": "MCTest", "description": "MCTest is a freely available set of stories and associated questions intended for research on the machine comprehension of text. "}, {"id": "grocery-store", "name": "Grocery Store", "description": "Grocery Store is a dataset of natural images of grocery items. All natural images were taken with a smartphone camera in different grocery stores. It contains 5,125 natural images from 81 different classes of fruits, vegetables, and carton items (e.g. juice, milk, yoghurt). The 81 classes are divided into 42 coarse-grained classes, where e.g. the fine-grained classes 'Royal Gala' and 'Granny Smith' belong to the same coarse-grained class 'Apple'. Additionally, each fine-grained class has an associated iconic image and a product description of the item."}, {"id": "dada-2000", "name": "DADA-2000", "description": "DADA-2000 is a large-scale benchmark with 2000 video sequences (named as DADA-2000) is contributed with laborious annotation for driver attention (fixation, saccade, focusing time), accident objects/intervals, as well as the accident categories, and superior performance to state-of-the-arts are provided by thorough evaluations. "}, {"id": "redweb-relative-depth-from-web", "name": "ReDWeb (Relative Depth from Web)", "description": "The ReDWeb dataset consists of 3600 RGB-RD image pairs collected from the Web. This dataset covers a wide range of scenes and features various non-rigid objects."}, {"id": "argkp-2021", "name": "ArgKP-2021", "description": "Data set covering a set of debatable topics, where for each topic and stance, a set of triplets of the form <argument, KP, label> is provided. The data set is based on the ArgKP data set, which contains arguments contributed by the crowd on 28 debatable topics, split by their stance towards the topic, and KPs written by an expert for those topics. Crowd annotations were collected to determine whether a KP represents an argument, i.e., is a match for an argument. The arguments in ArgKP are a subset of the IBM-ArgQ-Rank-30kArgs data set. For a test set, we extended ArgKP, adding three new debatable topics, that were also not part of IBM-ArgQ-Rank-30kArgs. The test set was collected specifically for KPA-2021, and was carefully designed to be similar in various aspects to the training data 2 . For each topic, crowd sourced arguments were collected, expert KPs generated, and match/no match annotations for argument/KP pairs obtained, resulting in a data set compatible with the ArgKP format. Arguments collection strictly adhered to the guidelines, quality measures, and post processing used for the collection of arguments in IBM-ArgQ-Rank-30kArgs, while the generation of expert KPs, collection of match annotations, and final data set creation strictly adhered to the manner in which ArgKP was created."}, {"id": "accidental-turntables", "name": "Accidental Turntables", "description": "Accidental Turntables contains a challenging set of 41,212 images of cars in cluttered backgrounds, motion blur and illumination changes that serves as a benchmark for 3D pose estimation."}, {"id": "oasis-1-open-access-series-of-imaging-studies", "name": "OASIS-1 (Open Access Series of Imaging Studies)", "description": "The Open Access Series of Imaging Studies (OASIS) is a project aimed at making neuroimaging data sets of the brain freely available to the scientific community. By compiling and freely distributing neuroimaging data sets, we hope to facilitate future discoveries in basic and clinical neuroscience."}, {"id": "chickenpox-cases-in-hungary", "name": "Chickenpox Cases in Hungary", "description": "Chickenpox Cases in Hungary is a spatio-temporal dataset of weekly chickenpox (childhood disease) cases from Hungary. It can be used as a longitudinal dataset for benchmarking the predictive performance of spatiotemporal graph neural network architectures. The dataset consists of a county-level adjacency matrix and time series of the county-level reported cases between 2005 and 2015. There are 2 specific related tasks:"}, {"id": "abstrct-neoplasm", "name": "AbstRCT - Neoplasm", "description": "The AbstRCT dataset consists of randomized controlled trials retrieved from the MEDLINE database via PubMed search. The trials are annotated with argument components and argumentative relations."}, {"id": "glucose", "name": "GLUCOSE", "description": "GLUCOSE is a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement."}, {"id": "3dseg-8", "name": "3DSeg-8", "description": "The 3DSeg-8 is a collection of several publicly available 3D segmentation datasets from different medical imaging modalities, e.g. magnetic resonance imaging (MRI) and computed tomography (CT), with various scan regions, target organs and pathologies."}, {"id": "ego4d", "name": "Ego4D", "description": "Ego4D is a massive-scale egocentric video dataset and benchmark suite. It offers 3,025 hours of daily life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, a host of new benchmark challenges are presented, centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, the aim is to push the frontier of first-person perception."}, {"id": "textvqa", "name": "TextVQA", "description": "TextVQA is a dataset to benchmark visual reasoning based on text in images. TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions."}, {"id": "xor-tydi-qa", "name": "XOR-TYDI QA", "description": "A large-scale dataset built on questions from TyDi QA lacking same-language answers."}, {"id": "ldc2017t10-abstract-meaning-representation-amr-annotation-release-2-0", "name": "LDC2017T10 (Abstract Meaning Representation (AMR) Annotation Release 2.0)", "description": "Abstract Meaning Representation (AMR) Annotation Release 2.0 was developed by the Linguistic Data Consortium (LDC), SDL/Language Weaver, Inc., the University of Colorado's Computational Language and Educational Research group and the Information Sciences Institute at the University of Southern California. It contains a sembank (semantic treebank) of over 39,260 English natural language sentences from broadcast conversations, newswire, weblogs and web discussion forums."}, {"id": "apolloscape-inpainting", "name": "Apolloscape Inpainting", "description": "The Inpainting dataset consists of synchronized Labeled image and LiDAR scanned point clouds. It's captured by HESAI Pandora All-in-One Sensing Kit. It is collected under various lighting conditions and traffic densities in Beijing, China."}, {"id": "ppi-protein-protein-interactions-ppi", "name": "PPI (Protein-Protein Interactions (PPI))", "description": "protein roles\u2014in terms of their cellular functions from gene ontology\u2014in various protein-protein interaction (PPI) graphs, with each graph corresponding to a different human tissue [41]. positional gene sets are used, motif gene sets and immunological signatures as features and gene ontology sets as labels (121 in total), collected from the Molecular Signatures Database [34]. The average graph contains 2373 nodes, with an average degree of 28.8."}, {"id": "clotho", "name": "Clotho", "description": "Clotho is an audio captioning dataset, consisting of 4981 audio samples, and each audio sample has five captions (a total of 24 905 captions). Audio samples are of 15 to 30 s duration and captions are eight to 20 words long."}, {"id": "brats-2016", "name": "BraTS 2016", "description": "BRATS 2016 is a brain tumor segmentation dataset. It shares the same training set as BRATS 2015, which consists of 220 HHG and 54 LGG. Its testing dataset consists of 191 cases with unknown grades."}, {"id": "pec-persona-based-empathetic-conversational", "name": "PEC (Persona-Based Empathetic Conversational)", "description": "A novel large-scale multi-domain dataset for persona-based empathetic conversations. "}, {"id": "southern-california-seismic-network-data-generalized-seismic-phase-detection-with-deep-learning", "name": "Southern California Seismic Network Data (Generalized Seismic Phase Detection with Deep Learning)", "description": "These files are supplementary material for \u201cGeneralized Seismic Phase Detection with Deep Learning\u201d by Ross et al. (2018), BSSA (doi.org/10.1785/0120180080). The models were trained using keras and TensorFlow, and can be used with these libraries. The training dataset contains 4.5 million seismograms evenly split between P-waves, S-waves, and pre-event noise classes. We encourage the use of this hdf5 dataset for training deep learning models, and hope that it and the model architecture in the paper can serve as a benchmark for future studies. For additional information please contact Zachary Ross (zross@caltech.edu)."}, {"id": "flickr30k-cna-flickr30k-chinese-all", "name": "Flickr30k-CNA (Flickr30k-Chinese All)", "description": "Former Flickr30k-CN translates the training and validation sets of Flickr30k using machine translation and manually translates the test set. We check the machine-translated results and find two kinds of problems. (1) Some sentences have language problems and translation errors. (2) Some sentences have poor semantics. In addition, the different translation ways between the training set and test set prevent the model from achieving accurate performance. We gather 6 professional English and Chinese linguists to meticulously re-translate all data of Flickr30k and double-check each sentence."}, {"id": "tapos", "name": "TAPOS", "description": "TAPOS is a new dataset developed on sport videos with manual annotations of sub-actions, and conduct a study on temporal action parsing on top. A sport activity usually consists of multiple sub-actions and that the awareness of such temporal structures is beneficial to action recognition."}, {"id": "mechanical-mnist", "name": "Mechanical MNIST", "description": "Each dataset in the Mechanical MNIST collection contains the results of 70,000 (60,000 training examples + 10,000 test examples) finite element simulation of a heterogeneous material subject to large deformation. Mechanical MNIST is generated by first converting the MNIST bitmap images (http://www.pymvpa.org/datadb/mnist.html) to 2D heterogeneous blocks of material. Consistent with the MNIST bitmap ($28 \\times 28$ pixels), the material domain is a $28 \\times 28$ unit square. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/elejeune11/Mechanical-MNIST/tree/master/generate_dataset)."}, {"id": "wld-wildlife-documentary", "name": "WLD (WildLife Documentary)", "description": "WildLife Documentary is an animal object detection dataset. It contains 15 documentary films that are downloaded from YouTube. The videos vary between 9 minutes to as long as 50 minutes, with resolution ranging from 360p to 1080p. A unique property of this dataset is that all videos are accompanied with subtitles that are automatically generated from speech by YouTube. The subtitles are revised manually to correct obvious spelling mistakes. All the animals in the videos are annotated, resulting in more than 4098 object tracklets of 60 different visual concepts, e.g., \u2018tiger\u2019, \u2018koala\u2019, \u2018langur\u2019, and \u2018ostrich\u2019."}, {"id": "i-haze", "name": "I-HAZE", "description": "The I-Haze dataset contains 25 indoor hazy images (size 2833\u00d74657 pixels) training. It has 5 hazy images for validation along with their corresponding ground truth images."}, {"id": "youtube-8m", "name": "YouTube-8M", "description": "The YouTube-8M dataset is a large scale video dataset, which includes more than 7 million videos with 4716 classes labeled by the annotation system. The dataset consists of three parts: training set, validate set, and test set. In the training set, each class contains at least 100 training videos. Features of these videos are extracted by the state-of-the-art popular pre-trained models and released for public use. Each video contains audio and visual modality. Based on the visual information, videos are divided into 24 topics, such as sports, game, arts & entertainment, etc"}, {"id": "3d-ken-burns-dataset", "name": "3D Ken Burns Dataset", "description": "Provides a large-scale synthetic dataset which contains accurate ground truth depth of various photo-realistic scenes."}, {"id": "podcastfillers", "name": "PodcastFillers", "description": "The PodcastFillers dataset consists of 199 full-length podcast episodes in English with manually annotated filler words and automatically generated transcripts. The podcast audio recordings, sourced from SoundCloud, are CC-licensed, gender-balanced, and total 145 hours of audio from over 350 speakers. The annotations are provided under a non-commercial license and consist of 85,803 manually annotated audio events including approximately 35,000 filler words (\u201cuh\u201d and \u201cum\u201d) and 50,000 non-filler events such as breaths, music, laughter, repeated words, and noise. The annotated events are also provided as pre-processed 1-second audio clips. The dataset also includes automatically generated speech transcripts from a speech-to-text system. A detailed description is provided in Dataset."}, {"id": "deepnets-1m", "name": "DeepNets-1M", "description": "The DeepNets-1M dataset is composed of neural network architectures represented as graphs where nodes are operations (convolution, pooling, etc.) and edges correspond to the forward pass flow of data through the network. DeepNets-1M has 1 million training architectures and 1402 in-distribution (ID) and out-of-distribution (OOD) evaluation architectures:  500 validation and 500 testing ID architectures,  100 wide OOD architectures,  100 deep OOD architectures,  100 dense OOD architectures,  100 OOD archtectures without batch normalization, and  2 predefined architectures (ResNet-50 and 12 layer Visual Transformer)."}, {"id": "ambiguous-vqa", "name": "Ambiguous VQA", "description": "The Ambiguous VQA dataset is a dataset of ambiguous questions about images. It consists of a set of ambiguous images and their answers. It is used to train and evaluate question generation models in English."}, {"id": "meddialog", "name": "MedDialog", "description": "The MedDialog dataset (Chinese) contains conversations (in Chinese) between doctors and patients. It has 1.1 million dialogues and 4 million utterances. The data is continuously growing and more dialogues will be added. The raw dialogues are from haodf.com. All copyrights of the data belong to haodf.com."}, {"id": "off-superhard-sequential", "name": "Off_Superhard_sequential", "description": "SMAC+ offensive superhard scenario with sequential episodic buffer"}, {"id": "androidhowto", "name": "AndroidHowTo", "description": "AndroidHowTo contains 32,436 data points from 9,893 unique How-To instructions and split into training (8K), validation (1K) and test (900). All test examples have perfect agreement across all three annotators for the entire sequence. In total, there are 190K operation spans, 172K object spans, and 321 input spans labeled. The lengths of the instructions range from 19 to 85 tokens, with median of 59. They describe a sequence of actions from one to 19 steps, with a median of 5."}, {"id": "chairs", "name": "Chairs", "description": "The Chairs dataset contains rendered images of around 1000 different three-dimensional chair models."}, {"id": "animal-pose-dataset", "name": "Animal-Pose Dataset", "description": "Animal-Pose Dataset is an animal pose dataset to facilitate training and evaluation. This dataset provides animal pose annotations on five categories are provided: dog, cat, cow, horse, sheep, with in total 6,000+ instances in 4,000+ images. Besides, the dataset also contains bounding box annotations for other 7 animal categories."}, {"id": "grb-graph-robustness-benchmark", "name": "GRB (Graph Robustness Benchmark)", "description": "Graph Robustness Benchmark (GRB) provides scalable, unified, modular, and reproducible evaluation on the adversarial robustness of graph machine learning models. GRB has elaborated datasets, unified evaluation pipeline, modular coding framework, and reproducible leaderboards, which facilitate the developments of graph adversarial learning, summarizing existing progress and generating insights into future research."}, {"id": "pku-mmd", "name": "PKU-MMD", "description": "The PKU-MMD dataset is a large skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view."}, {"id": "wads-winter-adverse-driving-dataset", "name": "WADS (Winter  Adverse  Driving  dataSet)", "description": "Collected in the snow belt region of Michigan's Upper Peninsula, WADS is the first multi-modal dataset featuring dense point-wise labeled sequential LiDAR scans collected in severe winter weather."}, {"id": "pirm-perceptual-image-restoration-and-manipulation", "name": "PIRM (Perceptual Image Restoration and Manipulation)", "description": "The PIRM dataset consists of 200 images, which are divided into two equal sets for validation and testing. These images cover diverse contents, including people, objects, environments, flora, natural scenery, etc. Images vary in size, and are typically ~300K pixels in resolution."}, {"id": "wi-locness-cambridge-english-write-improve-locness", "name": "WI-LOCNESS (Cambridge English Write & Improve & LOCNESS)", "description": "WI-LOCNESS is part of the Building Educational Applications 2019 Shared Task for Grammatical Error Correction. It consists of two datasets:"}, {"id": "sharc-shaping-answers-with-rules-through-conversation", "name": "ShARC (Shaping Answers with Rules through Conversation)", "description": "ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules."}, {"id": "twitter100k", "name": "Twitter100k", "description": "Twitter100k is a large-scale dataset for weakly supervised cross-media retrieval."}, {"id": "infotabs", "name": "InfoTabS", "description": "InfoTabS comprises of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes."}, {"id": "perkey", "name": "PerKey", "description": "A corpus of 553k news articles from six Persian news websites and agencies with relatively high quality author extracted keyphrases, which is then filtered and cleaned to achieve higher quality keyphrases. "}, {"id": "socnav1", "name": "SocNav1", "description": "SocNav1 is a dataset for social navigation conventions. The aims of SocNav1 are two-fold: a) enabling comparison of the algorithms that robots use to assess the convenience of their presence in a particular position when navigating; b) providing a sufficient amount of data so that modern machine learning algorithms such as deep neural networks can be used. Because of the structured nature of the data, SocNav1 is particularly well-suited to be used to benchmark non-Euclidean machine learning algorithms such as Graph Neural Networks"}, {"id": "new-york-times-annotated-corpus", "name": "New York Times Annotated Corpus", "description": "The New York Times Annotated Corpus contains over 1.8 million articles written and published by the New York Times between January 1, 1987 and June 19, 2007 with article metadata provided by the New York Times Newsroom, the New York Times Indexing Service and the online production staff at nytimes.com. The corpus includes:"}, {"id": "adult-data-set", "name": "Adult Data Set", "description": "Data Set Information: Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))"}, {"id": "liputan6", "name": "Liputan6", "description": "A large-scale Indonesian summarization dataset consisting of harvested articles from Liputan6.com, an online news portal, resulting in 215,827 document-summary pairs."}, {"id": "muse", "name": "MUSE", "description": "The MUSE dataset contains bilingual dictionaries for 110 pairs of languages. For each language pair, the training seed dictionaries contain approximately 5000 word pairs while the evaluation sets contain 1500 word pairs."}, {"id": "gittables", "name": "GitTables", "description": "GitTables is a corpus of currently 1M relational tables extracted from CSV files in GitHub covering 96 topics. Table columns in GitTables have been annotated with more than 2K different semantic types from Schema.org and DBpedia. The column annotations consist of semantic types, hierarchical relations, range types, table domain and descriptions. "}, {"id": "parus-choice-of-plausible-alternatives-for-russian-language", "name": "PARus (Choice of Plausible Alternatives for Russian language)", "description": "Choice of Plausible Alternatives for Russian language (PARus) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. Each question in PARus is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%."}, {"id": "draper-vdisc-dataset", "name": "Draper VDisc Dataset", "description": "Draper VDISC Dataset - Vulnerability Detection in Source Code"}, {"id": "archival-bundle-of-the-data-used-for-predictive-auto-scaling-with-openstack-monasca-ucc-2021", "name": "Archival bundle of the data used for \"Predictive Auto-scaling with OpenStack Monasca\" (UCC 2021)", "description": "Follow the instructions provided in the companion repo to automatically download and decompress the archive. The following files are included:"}, {"id": "msu-video-alignment-and-retrieval-benchmark-suite", "name": "MSU Video Alignment and Retrieval Benchmark Suite", "description": "Frame-to-frame video alignment/synchronization"}, {"id": "polyu-dataset", "name": "PolyU Dataset", "description": "PolyU Dataset is a large dataset of real-world noisy images with reasonably obtained corresponding \u201cground truth\u201d images. The basic idea is to capture the same and unchanged scene for many (e.g., 500) times and compute their mean image, which can be roughly taken as the \u201cground truth\u201d image for the real-world noisy images. The rational of this strategy is that for each pixel, the noise is generated randomly larger or smaller than 0. Sampling the same pixel many times and computing the average value will approximate the truth pixel value and alleviate significantly the noise."}, {"id": "irc-disentanglement", "name": "irc-disentanglement", "description": "This is a dataset for disentangling conversations on IRC, which is the task of identifying separate conversations in a single stream of messages. It contains disentanglement information for 77,563 messages or IRC."}, {"id": "vizwiz-answer-grounding", "name": "VizWiz Answer Grounding", "description": "Visual Question Answering (VQA) is the task of returning the answer to a question about an image. While most VQA services only return a natural language answer, we believe it is also valuable for a VQA service to return the region in the image used to arrive at the answer. We call this task of locating the relevant visual evidence answer grounding. We publicly share the VizWiz-VQA-Grounding dataset, the first dataset that visually grounds answers to visual questions asked by people with visual impairments, to encourage community progress in developing algorithmic frameworks.."}, {"id": "gmeg-wiki", "name": "GMEG-wiki", "description": "Grammatical error correction dataset for text from Wikipedia."}, {"id": "arramon", "name": "ArraMon", "description": "A dataset (in English; and also extended to Hindi) with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. "}, {"id": "vatex-video-and-text", "name": "VATEX (Video And TEXt)", "description": "VATEX is multilingual, large, linguistically complex, and diverse dataset in terms of both video and natural language descriptions. It has two tasks for video-and-language research: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context."}, {"id": "putemg", "name": "putEMG", "description": "putEMG and putEMG-Force datasets are databases of surface electromyographic activity recorded from forearm. Datasets allows for development of algorithms for gesture recognition and grasp force recognition. Experiment was conducted on 44 participants, with two repetitions separated by, minimum of one week. The dataset includes 7 active gestures (like hand flexion, extension, etc.) + idle and a set of trials with isometric contractions. sEMG was recorded using a 24-electrode matrix."}, {"id": "avcaffe-a-large-scale-audio-visual-dataset-of-cognitive-load-and-affect-for-remote-work", "name": "AVCAffe (A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work)", "description": "We introduce AVCAffe, the first Audio-Visual dataset consisting of Cognitive load and Affect attributes. We record AVCAffe by simulating remote work scenarios over a video-conferencing platform, where subjects collaborate to complete a number of cognitively engaging tasks. AVCAffe is the largest originally collected (not collected from the Internet) affective dataset in English language. We recruit 106 participants from 18 different countries of origin, spanning an age range of 18 to 57 years old, with a balanced male-female ratio. AVCAffe comprises a total of 108 hours of video, equivalent to more than 58,000 clips along with task-based self-reported ground truth labels for arousal, valence, and cognitive load attributes such as mental demand, temporal demand, effort, and a few others. We believe AVCAffe would be a challenging benchmark for the deep learning research community given the inherent difficulty of classifying affect and cognitive load in particular. Moreover, our dataset fills an existing timely gap by facilitating the creation of learning systems for better self-management of remote work meetings, and further study of hypotheses regarding the impact of remote work on cognitive load and affective states."}, {"id": "weibo-cov", "name": "Weibo-COV", "description": "Weibo-COV is a large-scale COVID-19 social media dataset from Weibo, covering more than 30 million posts from 1 November 2019 to 30 April 2020. Moreover, the field information of the dataset is very rich, including basic posts information, interactive information, location information and retweet network."}, {"id": "msra-hand", "name": "MSRA Hand", "description": "MSRA Hands is a dataset for hand tracking. In total 6 subjects' right hands are captured using Intel's Creative Interactive Gesture Camera. Each subject is asked to make various rapid gestures in a 400-frame video sequence. To account for different hand sizes, a global hand model scale is specified for each subject: 1.1, 1.0, 0.9, 0.95, 1.1, 1.0 for subject 1~6, respectively. The camera intrinsic parameters are: principle point = image center(160, 120), focal length = 241.42. The depth image is 320x240, each .bin file stores the depth pixel values in row scanning order, which are 320240 floats. The unit is millimeters. The bin file is binary and needs to be opened with std::ios::binary flag. joint.txt file stores 400 frames x 21 hand joints per frame. Each line has 3 * 21 = 63 floats for 21 3D points in (x, y, z) coordinates. The 21 hand joints are: wrist, index_mcp, index_pip, index_dip, index_tip, middle_mcp, middle_pip, middle_dip, middle_tip, ring_mcp, ring_pip, ring_dip, ring_tip, little_mcp, little_pip, little_dip, little_tip, thumb_mcp, thumb_pip, thumb_dip, thumb_tip. The corresponding *.jpg file is just for visualization of depth and ground truth joints."}, {"id": "rwsd-the-winograd-schema-challenge-russian", "name": "RWSD (The Winograd Schema Challenge (Russian))", "description": "A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution. The schema takes its name from a well-known example by Terry Winograd."}, {"id": "kitti", "name": "KITTI", "description": "KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute) is one of the most popular datasets for use in mobile robotics and autonomous driving. It consists of hours of traffic scenarios recorded with a variety of sensor modalities, including high-resolution RGB, grayscale stereo cameras, and a 3D laser scanner. Despite its popularity, the dataset itself does not contain ground truth for semantic segmentation. However, various researchers have manually annotated parts of the dataset to fit their necessities. \u00c1lvarez et al. generated ground truth for 323 images from the road detection challenge with three classes: road, vertical, and sky. Zhang et al. annotated 252 (140 for training and 112 for testing) acquisitions \u2013 RGB and Velodyne scans \u2013 from the tracking challenge for ten object categories: building, sky, road, vegetation, sidewalk, car, pedestrian, cyclist, sign/pole, and fence. Ros et al. labeled 170 training images and 46 testing images (from the visual odometry challenge) with 11 classes: building, tree, sky, car, sign, road, pedestrian, fence, pole, sidewalk, and bicyclist."}, {"id": "goemotions", "name": "GoEmotions", "description": "GoEmotions is a corpus of 58k carefully curated comments extracted from Reddit, with human annotations to 27 emotion categories or Neutral."}, {"id": "idda", "name": "IDDA", "description": "IDDA is a large scale, synthetic dataset for semantic segmentation with more than 100 different source visual domains. The dataset has been created to explicitly address the challenges of domain shift between training and test data in various weather and view point conditions, in seven different city types."}, {"id": "bamboo", "name": "Bamboo", "description": "Bamboo Dataset is a mega-scale and information-dense dataset for both classification and detection pre-training. It is built upon integrating 24 public datasets (e.g. ImagenNet, Places365, Object365, OpenImages) and added new annotations through active learning.  Bamboo has 69M image classification annotations and 32M object bounding boxes."}, {"id": "string", "name": "STRING", "description": "STRING is a collection of protein-protein interaction (PPI) networks."}, {"id": "aqa-7", "name": "AQA-7", "description": "Consists of 1106 action samples from seven actions with quality scores as measured by expert human judges."}, {"id": "objectnet3d", "name": "ObjectNet3D", "description": "ObjectNet3D is a large scale database for 3D object recognition, named, that consists of 100 categories, 90,127 images, 201,888 objects in these images and 44,147 3D shapes. Objects in the images in the database are aligned with the 3D shapes, and the alignment provides both accurate 3D pose annotation and the closest 3D shape annotation for each 2D object. Consequently, the database is useful for recognizing the 3D pose and 3D shape of objects from 2D images. Authors also provide baseline experiments on four tasks: region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval, which can serve as baselines for future research."}, {"id": "hindi-msr-vtt-hindi-microsoft-reseacrh-video-to-text", "name": "Hindi MSR-VTT (Hindi Microsoft reseacrh video to text)", "description": "This dataset is the Hindi version of standard English MSR-VTT dataset."}, {"id": "world-mortality-dataset", "name": "World Mortality Dataset", "description": "The World Mortality Dataset contains weekly, monthly, or quarterly all-cause mortality data from 103 countries and territories. It contains country-level data on all-cause mortality in 2015\u20132021 collected from various sources."}, {"id": "aminer", "name": "AMiner", "description": "The AMiner Dataset is a collection of different relational datasets. It consists of a set of relational networks such as citation networks, academic social networks or topic-paper-autor networks among others."}, {"id": "kptimes", "name": "KPTimes", "description": "KPTimes is a large-scale dataset of news texts paired with editor-curated keyphrases. "}, {"id": "asnq-answer-sentence-natural-questions", "name": "ASNQ (Answer Sentence Natural Questions)", "description": "A large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. "}, {"id": "card-660", "name": "CARD-660", "description": "An expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques. "}, {"id": "lits17-liver-tumor-segmentation-challenge-2017", "name": "LiTS17 (Liver Tumor Segmentation Challenge 2017)", "description": "LiTS17 is a liver tumor segmentation benchmark. The data and segmentations are provided by various clinical sites around the world. The training data set contains 130 CT scans and the test data set 70 CT scans."}, {"id": "inloc", "name": "InLoc", "description": "InLoc is a dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario."}, {"id": "upfd-gos-user-preference-aware-fake-news-detection", "name": "UPFD-GOS (User Preference-aware Fake News Detection)", "description": "The Gossipcop variant of the UPFD dataset for benchmarking."}, {"id": "fluo-n2dl-hela", "name": "Fluo-N2DL-HeLa", "description": "HeLa cells stably expressing H2b-GFP"}, {"id": "natural-stories", "name": "Natural Stories", "description": "The Natural Stories dataset consists of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected parse trees and includes self-paced reading time data."}, {"id": "yaso", "name": "YASO", "description": "YASO is a crowd-sourced TSA evaluation dataset, collected using a new annotation scheme for labeling targets and their sentiments. The dataset contains 2,215 English sentences from movie, business and product reviews, and 7,415 terms and their corresponding sentiments annotated within these sentences. "}, {"id": "middlebury-2005", "name": "Middlebury 2005", "description": "Middlebury 2005 is a stereo dataset of indoor scenes."}, {"id": "nell-995", "name": "NELL-995", "description": "NELL-995 KG Completion Dataset"}, {"id": "heichole-benchmark-surgical-workflow-and-skill-analysis-challenge-heichole-benchmark", "name": "HeiChole Benchmark (Surgical Workflow and Skill Analysis Challenge (HeiChole Benchmark))", "description": "Analyzing the surgical workflow is a prerequisite for many applications in computer assisted surgery (CAS), such as context-aware visualization of navigation information, specifying the most probable tool required next by the surgeon or determining the remaining duration of surgery. Since laparoscopic surgeries are performed using an endoscopic camera, a video stream is always available during surgery, making it the obvious choice as input sensor data for workflow analysis. Moreover, this offers the opportunity for structured assessment of surgical skill for safety, teaching and quality management."}, {"id": "hpatches-homography-patches-dataset", "name": "HPatches (Homography-patches dataset)", "description": "The HPatches is a recent dataset for local patch descriptor evaluation that consists of 116 sequences of 6 images with known homography. The dataset is split into two parts: viewpoint - 59 sequences with significant viewpoint change and illumination - 57 sequences with significant illumination change, both natural and artificial."}, {"id": "g3d-gaming-3d-dataset", "name": "G3D (Gaming 3D Dataset)", "description": "The Gaming 3D Dataset (G3D) focuses on real-time action recognition in a gaming scenario. It contains 10 subjects performing 20 gaming actions: \u201cpunch right\u201d, \u201cpunch left\u201d, \u201ckick right\u201d, \u201ckick left\u201d, \u201cdefend\u201d, \u201cgolf swing\u201d, \u201ctennis swing forehand\u201d, \u201ctennis swing backhand\u201d, \u201ctennis serve\u201d, \u201cthrow bowling ball\u201d, \u201caim and fire gun\u201d, \u201cwalk\u201d, \u201crun\u201d, \u201cjump\u201d, \u201cclimb\u201d, \u201ccrouch\u201d, \u201csteer a car\u201d, \u201cwave\u201d, \u201cflap\u201d and \u201cclap\u201d."}, {"id": "terms-of-service", "name": "Terms of Service", "description": "The Terms of Service dataset is a law dataset corresponding to the task of identifying whether contractual terms are potentially unfair. This is a binary classification task, where positive examples are potentially unfair contractual terms (clauses) from the terms of service in consumer contracts. Article 3 of the Directive 93/13 on Unfair Terms in Consumer Contracts defines an unfair contractual term as follows. A contractual term is unfair if: (1) it has not been individually negotiated; and (2) contrary to the requirement of good faith, it causes a significant imbalance in the parties rights and obligations, to the detriment of the consumer. The Terms of Service dataset consists of 9,414 examples."}, {"id": "50statesimulations-50-state-redistricting-simulations", "name": "50stateSimulations (50-State Redistricting Simulations)", "description": "Every decade following the Census, states and municipalities must redraw districts for Congress, state houses, city councils, and more. The goal of the 50-State Simulation Project is to enable researchers, practitioners, and the general public to use cutting-edge redistricting simulation analysis to evaluate enacted congressional districts."}, {"id": "tinyperson", "name": "TinyPerson", "description": "TinyPerson is a benchmark for tiny object detection in a long distance and with massive backgrounds. The images in TinyPerson are collected from the Internet. First, videos with a high resolution are collected from different websites. Second, images from the video are sampled every 50 frames. Then images with a certain repetition (homogeneity) are deleted, and the resulting images are annotated with 72,651 objects with bounding boxes by hand."}, {"id": "concode", "name": "CONCODE", "description": "A new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment."}, {"id": "binarized-mnist", "name": "Binarized MNIST", "description": "A binarized version of MNIST."}, {"id": "lamem", "name": "LaMem", "description": "An annotated image memorability dataset to date (with 60,000 labeled images from a diverse array of sources)."}, {"id": "isia-food-500", "name": "ISIA Food-500", "description": "Includes 500 categories from the list in the Wikipedia and 399,726 images, a more comprehensive food dataset that surpasses existing popular benchmark datasets by category coverage and data volume."}, {"id": "l-bird-large-bird", "name": "L-Bird (Large-Bird)", "description": "The L-Bird (Large-Bird) dataset contains nearly 4.8 million images which are obtained by searching images of a total of 10,982 bird species from the Internet."}, {"id": "mvor-multi-view-operating-room-2", "name": "mvor (Multi-View Operating Room)", "description": "Multi-View Operating Room (MVOR) dataset consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR during real clinical interventions. Each multi-view frame consists of three color and three depth images. The MVOR dataset was sampled from four days of recording in an interventional room at the University Hospital of Strasbourg during procedures such as vertebroplasty and lung biopsy. There are in total 4699 bounding boxes, 2926 2D keypoint annotations, and 1061 3D keypoint annotations."}, {"id": "readingbank", "name": "ReadingBank", "description": "ReadingBank is a benchmark dataset for reading order detection built with weak supervision from WORD documents, which contains 500K document images with a wide range of document types as well as the corresponding reading order information."}, {"id": "algonauts-2021-how-the-human-brain-makes-sense-of-a-world-in-motion", "name": "Algonauts 2021 (How the Human Brain Makes Sense of a World in Motion)", "description": "The Algonauts dataset provides human brain responses to a set of 1,102 3-s long video clips of everyday events. The brain responses are measured with functional magnetic resonance imaging (fMRI). fMRI is a widely used brain imaging technique with high spatial resolution that measures blood flow changes associated with neural responses. "}, {"id": "phc-c2dh-u373", "name": "PhC-C2DH-U373", "description": "Glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate"}, {"id": "twitter-abusive-behavior", "name": "Twitter Abusive Behavior", "description": "80k tweets annotated concerning Inappropriate Speech (more particularly in matters of Abusive and Hateful speech) as well as Normal and Spam."}, {"id": "wikipedia-generation", "name": "Wikipedia Generation", "description": "Wikipedia Generation is a dataset for article generation from Wikipedia from references at the end of Wikipedia page and the top 10 search results for the Wikipedia topic."}, {"id": "3dmad-3d-mask-attack-dataset", "name": "3DMAD (3D Mask Attack Dataset)", "description": "The 3D Mask Attack Database (3DMAD) is a biometric (face) spoofing database. It currently contains 76500 frames of 17 persons, recorded using Kinect for both real-access and spoofing attacks. Each frame consists of:"}, {"id": "uta-rldd-university-of-texas-at-arlington-real-life-drowsiness-dataset", "name": "UTA-RLDD (University of Texas at Arlington Real-Life Drowsiness Dataset)", "description": "Consists of around 30 hours of video, with contents ranging from subtle signs of drowsiness to more obvious ones."}, {"id": "sleep-edf-sleep-edf-expanded", "name": "Sleep-EDF (Sleep-EDF Expanded)", "description": "The sleep-edf database contains 197 whole-night PolySomnoGraphic sleep recordings, containing EEG, EOG, chin EMG, and event markers. Some records also contain respiration and body temperature. Corresponding hypnograms (sleep patterns) were manually scored by well-trained technicians according to the Rechtschaffen and Kales manual, and are also available."}, {"id": "3d-hand-pose", "name": "3D Hand Pose", "description": "3D Hand Pose is a multi-view hand pose dataset consisting of color images of hands and different kind of annotations for each: the bounding box and the 2D and 3D location on the joints in the hand. "}, {"id": "armanemo", "name": "ArmanEmo", "description": "ArmanEmo is a human-labeled emotion dataset of more than 7000 Persian sentences labeled for seven categories. The dataset has been collected from different resources, including Twitter, Instagram, and Digikala (an Iranian e-commerce company) comments. Labels are based on Ekman's six basic emotions (Anger, Fear, Happiness, Hatred, Sadness, Wonder) and another category (Other) to consider any other emotion not included in Ekman's model. "}, {"id": "padchest", "name": "PadChest", "description": "PadChest is a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score."}, {"id": "tobacco800", "name": "Tobacco800", "description": "Tobacco800 is a public subset of the complex document image processing (CDIP) test collection constructed by Illinois Institute of Technology, assembled from 42 million pages of documents (in 7 million multi-page TIFF images) released by tobacco companies under the Master Settlement Agreement and originally hosted at UCSF."}, {"id": "real-blur-dataset", "name": "Real Blur Dataset", "description": "The dataset consists of 4,738 pairs of images of 232 different scenes including reference pairs. All images were captured both in the camera raw and JPEG formats, hence generating two datasets: RealBlur-R from the raw images, and RealBlur-J from the JPEG images. Each training set consists of 3,758 image pairs, while each test set consists of 980 image pairs."}, {"id": "wmt-2018-news-wmt-2018-news-translation-task", "name": "WMT 2018 News (WMT 2018 News Translation Task)", "description": "News translation is a recurring WMT task. The test set is a collection of parallel corpora consisting of about 1500 English sentences translated into 5 languages (Chinese, Czech, Estonian, German, Finnish, Russian, Turkish) and additional 1500 sentences from each of the 7 languages translated to English. The sentences were selected from dozens of news websites and translated by professional translators."}, {"id": "semeval-2010-task-8", "name": "SemEval-2010 Task 8", "description": "The dataset for the SemEval-2010 Task 8 is a dataset for multi-way classification of mutually exclusive semantic relations between pairs of nominals."}, {"id": "acre-abstract-causal-reasoning", "name": "ACRE (Abstract Causal REasoning)", "description": "Abstract Causal REasoning (ACRE) is a dataset for the systematic evaluation of current vision systems in causal induction, i.e., identifying unobservable mechanisms that lead to the observable relations among variables."}, {"id": "crossmoda-cross-modality-domain-adaptation", "name": "CrossMoDA (Cross-Modality Domain Adaptation)", "description": "**CrossMoDA is a large and multi-class benchmark for unsupervised cross-modality Domain Adaptation. The goal of the challenge is to segment two key brain structures involved in the follow-up and treatment planning of vestibular schwannoma (VS): the VS and the cochleas. Currently, the diagnosis and surveillance in patients with VS are commonly performed using contrast-enhanced T1 (ceT1) MR imaging."}, {"id": "annomi", "name": "AnnoMI", "description": "Research on natural language processing approaches to analysing counselling dialogues has seen substantial development in recent years, but access to this area remains extremely limited, due to the lack of publicly available expert-annotated therapy conversations. In this paper, we introduce AnnoMI, the first publicly and freely accessible dataset of professionally transcribed dialogues demonstrating high- and low-quality motivational interviewing (MI), an effective counselling technique, with annotations on key MI aspects by domain experts."}, {"id": "adversarialqa", "name": "AdversarialQA", "description": "We have created three new Reading Comprehension datasets constructed using an adversarial model-in-the-loop."}, {"id": "perspectrum", "name": "Perspectrum", "description": "Perspectrum is a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify the dataset. Crowd-sourcing was used to filter out noise and ensure high-quality data. The dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively."}, {"id": "fivr-200k", "name": "FIVR-200K", "description": "The FIVR-200K dataset has been collected to simulate the problem of Fine-grained Incident Video Retrieval (FIVR). The dataset comprises 225,960 videos associated with 4,687 Wikipedia events and 100 selected video queries."}, {"id": "mediqa-ans-mediqa-answer-summarization", "name": "MEDIQA-AnS (MEDIQA-Answer Summarization)", "description": "The first summarization collection containing question-driven summaries of answers to consumer health questions. This dataset can be used to evaluate single or multi-document summaries generated by algorithms using extractive or abstractive approaches. "}, {"id": "minc-materials-in-context-database", "name": "MINC (Materials in Context Database)", "description": "MINC is a large-scale, open dataset of materials in the wild."}, {"id": "rgb-davis-dataset", "name": "RGB-DAVIS Dataset", "description": "Used to show systematic performance improvement in applications such as high frame-rate video synthesis, feature/corner detection and tracking, as well as high dynamic range image reconstruction."}, {"id": "codex-large", "name": "CoDEx Large", "description": "CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. "}, {"id": "dublincity", "name": "DublinCity", "description": "A novel benchmark dataset that includes a manually annotated point cloud for over 260 million laser scanning points into 100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects are labelled into 13 classes using hierarchical levels of detail from large (i.e., building, vegetation and ground) to refined (i.e., window, door and tree) elements. "}, {"id": "cosmoflow", "name": "CosmoFlow", "description": "The latest CosmoFlow dataset includes around 10,000 cosmological N-body dark matter simulations. The simulations are run using MUSIC to generate the initial conditions, and are evolved with pyCOLA, a multithreaded Python/Cython N-body code. The output of these simulations is then binned into a 3D histogram of particle counts in a cube of size 512x512x512, which is sampled at 4 different redshifts."}, {"id": "multidex", "name": "MultiDex", "description": "We collect a large-scale synthetic dataset for robotic hands with Differentiable Force Closure(DFC). It covers 436,000 diverse and stable grasps for 58 household objects from ContactDB and YCB datasets across 5 robotic hands including EZGripper, Barrett Hand, Robotiq-3Finger, Allegro Hand and Shadowhand."}, {"id": "cmrc-2019-chinese-machine-reading-comprehension-2019", "name": "CMRC 2019 (Chinese Machine Reading Comprehension 2019)", "description": "CMRC 2019 is a Chinese Machine Reading Comprehension dataset that was used in The Third Evaluation Workshop on Chinese Machine Reading Comprehension. Specifically, CMRC 2019 is a sentence cloze-style machine reading comprehension dataset that aims to evaluate the sentence-level inference ability."}, {"id": "spo-funcat", "name": "Spo Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "sysu-30k", "name": "SYSU-30k", "description": "SYSU-30k contains 30k categories of persons, which is about 20 times larger than CUHK03 (1.3k categories) and Market1501 (1.5k categories), and 30 times larger than ImageNet (1k categories). SYSU-30k contains 29,606,918 images. Moreover, SYSU-30k provides not only a large platform for the weakly supervised ReID problem but also a more challenging test set that is consistent with the realistic setting for standard evaluation."}, {"id": "xbd", "name": "xBD", "description": "The xBD dataset contains over 45,000KM2 of polygon labeled pre and post disaster imagery. The dataset provides the post-disaster imagery with transposed polygons from pre over the buildings, with damage classification labels."}, {"id": "fashion-144k", "name": "Fashion 144K", "description": "Fashion 144K is a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information."}, {"id": "nyu-vp", "name": "NYU-VP", "description": "NYU-VP is a new dataset for multi-model fitting, vanishing point (VP) estimation in this case. Each image is annotated with up to eight vanishing points, and pre-extracted line segments are provided which act as data points for a robust estimator. Due to its size, the dataset is the first to allow for supervised learning of a multi-model fitting task."}, {"id": "reco", "name": "ReCO", "description": "A human-curated ChineseReading Comprehension dataset on Opinion. The questions in ReCO are opinion based queries issued to the commercial search engine. The passages are provided by the crowdworkers who extract the support snippet from the retrieved documents. "}, {"id": "gooaq", "name": "GooAQ", "description": "GooAQ is a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to the collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections."}, {"id": "tydiqa-goldp", "name": "TyDiQA-GoldP", "description": "TyDiQA is the gold passage version of the Typologically Diverse Question Answering (TyDiWA) dataset, a benchmark for information-seeking question answering, which covers nine languages. The gold passage version is a simplified version of the primary task, which uses only the gold passage as context and excludes unanswerable questions. It is thus similar to XQuAD and MLQA, while being more challenging as questions have been written without seeing the answers, leading to 3\u00d7 and 2\u00d7 less lexical overlap compared to XQuAD and MLQA respectively."}, {"id": "crema-d", "name": "CREMA-D", "description": "CREMA-D is an emotional multimodal actor data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified)."}, {"id": "logiqa", "name": "LogiQA", "description": "LogiQA consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. The dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting."}, {"id": "snap-stanford-large-network-dataset-collection", "name": "SNAP (Stanford Large Network Dataset Collection)", "description": "SNAP is a collection of large network datasets. It includes graphs representing social networks, citation networks, web graphs, online communities, online reviews and more."}, {"id": "hindencorp", "name": "HindEnCorp", "description": "A parallel corpus of Hindi and English, and HindMonoCorp, a monolingual corpus of Hindi in their release version 0.5. Both corpora were collected from web sources and preprocessed primarily for the training of statistical machine translation systems. HindEnCorp consists of 274k parallel sentences (3.9 million Hindi and 3.8 million English tokens). HindMonoCorp amounts to 787 million tokens in 44 million sentences."}, {"id": "converse", "name": "CONVERSE", "description": "A novel dataset that represents complex conversational interactions between two individuals via 3D pose. 8 pairwise interactions describing 7 separate conversation based scenarios were collected using two Kinect depth sensors."}, {"id": "eth-xgaze", "name": "ETH-XGaze", "description": "Consists of over one million high-resolution images of varying gaze under extreme head poses. The dataset is collected from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. "}, {"id": "foggy-cityscapes", "name": "Foggy Cityscapes", "description": "Foggy Cityscapes is a synthetic foggy dataset which simulates fog on real scenes. Each foggy image is rendered with a clear image and depth map from Cityscapes. Thus the annotations and data split in Foggy Cityscapes are inherited from Cityscapes."}, {"id": "image-description-sequences", "name": "Image Description Sequences", "description": "A dataset of description sequences, a sequence of expressions that together are meant to single out one image from an (imagined) set of other similar images. These sequences were produced in a monological setting, but with the instruction to imagine they were provided to a partner who successively asked for more information (hence, tell me more)."}, {"id": "a2d2-audi-autonomous-driving-dataset", "name": "A2D2 (Audi Autonomous Driving Dataset)", "description": "Audi Autonomous Driving Dataset (A2D2) consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus."}, {"id": "europarl-st", "name": "Europarl-ST", "description": "Europarl-ST is a multilingual Spoken Language Translation corpus containing paired audio-text samples for SLT from and into 9 European languages, for a total of 72 different translation directions. This corpus has been compiled using the debates held in the European Parliament in the period between 2008 and 2012. "}, {"id": "news-interactions-on-globo-com-news-portal-user-interactions-by-globo-com-a-large-dataset-for-news-recommendations-offline-evaluation-and-analytics", "name": "News Interactions on Globo.com (News Portal User Interactions by Globo.com - A large dataset for news recommendations offline evaluation and analytics)", "description": "This large dataset with users interactions logs (page views) from a news portal was kindly provided by Globo.com, the most popular news portal in Brazil, for reproducibility of the experiments with CHAMELEON - a meta-architecture for contextual hybrid session-based news recommender systems. The source code was made available at GitHub."}, {"id": "worldstrat-the-worldstrat-dataset-open-high-resolution-satellite-imagery-with-paired-multi-temporal-low-resolution", "name": "WorldStrat (The WorldStrat Dataset: Open High-Resolution Satellite Imagery With Paired Multi-Temporal Low-Resolution)", "description": "Nearly 10,000 km\u00b2 of free high-resolution and paired multi-temporal low-resolution satellite imagery of unique locations which ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. \u200b"}, {"id": "midgard", "name": "MIDGARD", "description": "MIDGARD is an open-source simulator for autonomous robot navigation in outdoor unstructured environments. It is designed to enable the training of autonomous agents (e.g., unmanned ground vehicles) in photorealistic 3D environments, and support the generalization skills of learning-based agents thanks to the variability in training scenarios."}, {"id": "oulu-casia-oulu-casia-nir-vis-facial-expression-database", "name": "Oulu-CASIA (Oulu-CASIA NIR&VIS facial expression database)", "description": "The Oulu-CASIA NIR&VIS facial expression database consists of six expressions (surprise, happiness, sadness, anger, fear and disgust) from 80 people between 23 and 58 years old. 73.8% of the subjects are males. The subjects were asked to sit on a chair in the observation room in a way that he/ she is in front of camera. Camera-face distance is about 60 cm. Subjects were asked to make a facial expression according to an expression example shown in picture sequences. The imaging hardware works at the rate of 25 frames per second and the image resolution is 320 \u00d7 240 pixels."}, {"id": "risec-recipe-instruction-semantics-corpus", "name": "RISeC (Recipe Instruction Semantics Corpus)", "description": "We propose a newly annotated dataset for information extraction on recipes. Unlike previous approaches to machine comprehension of procedural texts, we avoid a priori pre-defining domain-specific predicates to recognize (e.g., the primitive instructionsin MILK) and focus on basic understanding of the expressed semantics rather than directly reduce them to a simplified state representation."}, {"id": "digital-peter", "name": "Digital Peter", "description": "Digital Peter is a dataset of Peter the Great's manuscripts annotated for segmentation and text recognition. The dataset may be useful for researchers to train handwriting text recognition models as a benchmark for comparing different models. It consists of 9,694 images and text files corresponding to lines in historical documents. The dataset includes Peter\u2019s handwritten materials covering the period from 1709 to 1713. "}, {"id": "wikibio-wikipedia-biography-dataset", "name": "WikiBio (Wikipedia Biography Dataset)", "description": "This dataset gathers 728,321 biographies from English Wikipedia. It aims at evaluating text generation algorithms. For each article, we provide the first paragraph and the infobox (both tokenized)."}, {"id": "electricity-individual-household-electric-power-consumption-data-set", "name": "Electricity (Individual household electric power consumption Data Set)", "description": "Abstract: Measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. Different electrical quantities and some sub-metering values are available."}, {"id": "deepwriting", "name": "DeepWriting", "description": "A new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation."}, {"id": "b-pref", "name": "B-Pref", "description": "B-Pref is a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities."}, {"id": "scut-head", "name": "SCUT-HEAD", "description": "Includes 4405 images with 111251 heads annotated."}, {"id": "jglue", "name": "JGLUE", "description": "JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese."}, {"id": "pathbased", "name": "pathbased", "description": "pathbased is a 3-cluster data set. The data set consists of a circular cluster with an opening near the bottom and two Gaussian distributed clusters inside. Each cluster contains 100 data points."}, {"id": "film-48-32-20-fixed-splits", "name": "Film(48%/32%/20% fixed splits)", "description": "Node classification on Film with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "levir-cd", "name": "LEVIR-CD", "description": "LEVIR-CD is a new large-scale remote sensing building Change Detection dataset. The introduced dataset would be a new benchmark for evaluating change detection (CD) algorithms, especially those based on deep learning."}, {"id": "dress-code", "name": "Dress Code", "description": "Dress Code is a new dataset for image-based virtual try-on composed of image pairs coming from different catalogs of YOOX NET-A-PORTER. The dataset contains more than 50k high resolution model clothing images pairs divided into three different categories (i.e. dresses, upper-body clothes, lower-body clothes)."}, {"id": "watch-n-patch", "name": "Watch-n-Patch", "description": "The Watch-n-Patch dataset was created with the focus on modeling human activities, comprising multiple actions in a completely unsupervised setting. It is collected with Microsoft Kinect One sensor for a total length of about 230 minutes, divided in 458 videos. 7 subjects perform human daily activities in 8 offices and 5 kitchens with complex backgrounds. Moreover, skeleton data are provided as ground truth annotations."}, {"id": "omg-emotion-one-minute-gradual-emotional-behavior", "name": "OMG-Emotion (One-Minute Gradual-Emotional Behavior)", "description": "The One-Minute Gradual-Emotional Behavior dataset (OMG-Emotion) dataset is composed of Youtube videos which are around a minute in length and are annotated taking into consideration a continuous emotional behavior. The videos were selected using a crawler technique that uses specific keywords based on long-term emotional behaviors such as \"monologues\", \"auditions\", \"dialogues\" and \"emotional scenes\"."}, {"id": "covost", "name": "CoVoST", "description": "CoVoST is a large-scale multilingual speech-to-text translation corpus. Its latest 2nd version covers translations from 21 languages into English and from English into 15 languages. It has total 2880 hours of speech and is diversified with 78K speakers and 66 accents."}, {"id": "content4all", "name": "Content4All", "description": "Content4All is a collection of six open research datasets aimed at automatic sign language translation research."}, {"id": "apolloscape-trajectory", "name": "Apolloscape Trajectory", "description": "Our trajectory dataset consists of camera-based images, LiDAR scanned point clouds, and manually annotated trajectories. It is collected under various lighting conditions and traffic densities in Beijing, China. More specifically, it contains highly complicated traffic flows mixed with vehicles, riders, and pedestrians."}, {"id": "citypersons", "name": "CityPersons", "description": "The CityPersons dataset is a subset of Cityscapes which only consists of person annotations. There are 2975 images for training, 500 and 1575 images for validation and testing. The average of the number of pedestrians in an image is 7. The visible-region and full-body annotations are provided."}, {"id": "1078-people-3d-faces-collection-data", "name": "1,078 People 3D Faces Collection Data", "description": "Description: 1,078 People 3D Faces Collection Data. The collection device is Realsense SR300. Each subject was collected once a week, 6 times in total, so the time span is 6 weeks. The number of videos collected for one subject is 16. The dataset can be used for tasks such as 3D face recognition."}, {"id": "lip-look-into-person", "name": "LIP (Look into Person)", "description": "The LIP (Look into Person) dataset is a large-scale dataset focusing on semantic understanding of a person. It contains 50,000 images with elaborated pixel-wise annotations of 19 semantic human part labels and 2D human poses with 16 key points. The images are collected from real-world scenarios and the subjects appear with challenging poses and view, heavy occlusions, various appearances and low resolution."}, {"id": "reddit-conversation-corpus", "name": "Reddit Conversation Corpus", "description": "Reddit Conversation Corpus (RCC) consists of conversations, scraped from Reddit, for a 20 month period from November 2016 until August 2018. To ensure the quality and diversity of topics, 95 subreddits are selected from which conversations are collected. In total, RCC contains 9.2 million 3-turn conversations."}, {"id": "echr", "name": "ECHR", "description": "ECHR is an English legal judgment prediction dataset of cases from the European Court of Human Rights (ECHR). The dataset contains ~11.5k cases, including the raw text."}, {"id": "figureqa", "name": "FigureQA", "description": "FigureQA is a visual reasoning corpus of over one million question-answer pairs grounded in over 100,000 images. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. "}, {"id": "smallnorb", "name": "smallNORB", "description": "The smallNORB dataset is a datset for 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees). The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5)."}, {"id": "rare-diseases-mentions-in-mimic-iii-rare-disease-mention-annotations-from-a-sample-of-mimic-iii-clinical-notes", "name": "Rare Diseases Mentions in MIMIC-III (Rare disease mention annotations from a sample of MIMIC-III clinical notes)", "description": "The 1,073 full rare disease mention annotations (from 312 MIMIC-III discharge summaries) are in full_set_RD_ann_MIMIC_III_disch.csv."}, {"id": "holist", "name": "HOList", "description": "The official HOList benchmark for automated theorem proving consists of all theorem statements in the core, complex, and flyspeck corpora. The goal of the benchmark is to prove as many theorems as possible in the HOList environment in the order they appear in the database. That is, only theorems that occur before the current theorem are supposed to be used as premises (lemmata) in its proof."}, {"id": "wgisd-embrapa-wine-grape-instance-segmentation-dataset", "name": "WGISD (Embrapa Wine Grape Instance Segmentation Dataset)", "description": "Embrapa Wine Grape Instance Segmentation Dataset (WGISD) contains grape clusters properly annotated in 300 images and a novel annotation methodology for segmentation of complex objects in natural images."}, {"id": "physionet-challenge-2016", "name": "PhysioNet Challenge 2016", "description": "Introduction The 2016 PhysioNet/CinC Challenge aims to encourage the development of algorithms to classify heart sound recordings collected from a variety of clinical or nonclinical (such as in-home visits) environments. The aim is to identify, from a single short recording (10-60s) from a single precordial location, whether the subject of the recording should be referred on for an expert diagnosis."}, {"id": "synthia-synthetic-collection-of-imagery-and-annotations", "name": "SYNTHIA (SYNTHetic Collection of Imagery and Annotations)", "description": "The SYNTHIA dataset is a synthetic dataset that consists of 9400 multi-viewpoint photo-realistic frames rendered from a virtual city and comes with pixel-level semantic annotations for 13 classes. Each frame has resolution of 1280 \u00d7 960."}, {"id": "tao-tracking-any-object-dataset", "name": "TAO (Tracking Any Object Dataset)", "description": "TAO is a federated dataset for Tracking Any Object, containing 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. A bottom-up approach was used for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. "}, {"id": "bstc-baidu-speech-translation-corpus", "name": "BSTC (Baidu Speech Translation Corpus)", "description": "BSTC (Baidu Speech Translation Corpus) is a large-scale dataset for automatic simultaneous interpretation. BSTC version 1.0 contains 50 hours of real speeches, including three parts, the audio files, the transcripts, and the translations. The corpus can be used to build automatic simultaneous interpretation system. The corpus is collected from the Chinese mandarin talks and reports, including science, technology, culture, economy, etc.,. The utterances in talks and reports are carefully transcribed into Chinese text, and further translated into English text. The sentence boundary is determined by the English text instead of the Chinese text which is analogous to the previous related corpus (TED and Translation Augmented LibriSpeech Corpus)."}, {"id": "holl-e", "name": "Holl-E", "description": "Holl-E is a dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie."}, {"id": "software-heritage-graph-dataset", "name": "Software Heritage Graph Dataset", "description": "Software Heritage is the largest existing public archive of software source code and accompanying development history. It spans more than five billion unique source code files and one billion unique commits , coming from more than 80 million software projects. These software artifacts were retrieved from major collaborative development platforms (e.g., GitHub, GitLab) and package repositories (e.g., PyPI, Debian, NPM), and stored in a uniform representation linking together source code files, directories, commits, and full snapshots of version control systems (VCS) repositories as observed by Software Heritage during periodic crawls. This dataset is unique in terms of accessibility and scale, and allows to explore a number of research questions on the long tail of public software development, instead of solely focusing on ''most starred'' repositories as it often happens."}, {"id": "kaggle-credit-card-fraud-dataset", "name": "Kaggle-Credit Card Fraud Dataset", "description": "The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.  "}, {"id": "1025-hours-mandarin-strong-accent-speech-data-by-mobile-phone", "name": "1,025 Hours - Mandarin Strong Accent Speech Data by Mobile Phone", "description": "Description: More than 2,000 Chinese native speakers participated in the recording with equal gender. Speakers are mainly from the southern China, and some of them are from the provinces of northern China with Strong accents. The recording content is rich, covering mobile phone voice assistant interaction, smart home command and control, In-car command and control, numbers and other fields, which is accurately matching the smart home, intelligent car and other practical application scenarios."}, {"id": "mtop-multilingual-task-oriented-semantic-parsing", "name": "MTOP (Multilingual Task-Oriented Semantic Parsing)", "description": "A multilingual task-oriented semantic parsing dataset covering 6 languages and 11 domains."}, {"id": "shrec-shape-retrieval-contest", "name": "SHREC (SHape REtrieval Contest)", "description": "The SHREC dataset contains 14 dynamic gestures performed by 28 participants (all participants are right handed) and captured by the Intel RealSense short range depth camera. Each gesture is performed between 1 and 10 times by each participant in two way: using one finger and the whole hand. Therefore, the dataset is composed by 2800 sequences captured. The depth image, with a resolution of 640x480, and the coordinates of 22 joints (both in the 2D depth image space and in the 3D world space) are saved for each frame of each sequence in the dataset."}, {"id": "gvgai-general-video-game-ai", "name": "GVGAI (General Video Game AI)", "description": "The General Video Game AI (GVGAI) framework is widely used in research which features a corpus of over 100 single-player games and 60 two-player games. These are fairly small games, each focusing on specific mechanics or skills the players should be able to demonstrate, including clones of classic arcade games such as Space Invaders, puzzle games like Sokoban, adventure games like Zelda or game-theory problems such as the Iterative Prisoners Dilemma. All games are real-time and require players to make decisions in only 40ms at every game tick, although not all games explicitly reward or require fast reactions; in fact, some of the best game-playing approaches add up the time in the beginning of the game to run Breadth-First Search in puzzle games in order to find an accurate solution. However, given the large variety of games (many of which are stochastic and difficult to predict accurately), scoring systems and termination conditions, all unknown to the players, highly-adaptive general methods are needed to tackle the diverse challenges proposed."}, {"id": "permuted-mnist", "name": "Permuted MNIST", "description": "Permuted MNIST is an MNIST variant that consists of 70,000 images of handwritten digits from 0 to 9, where 60,000 images are used for training, and 10,000 images for test. The difference of this dataset from the original MNIST is that each of the ten tasks is the multi-class classification of a different random permutation of the input pixels."}, {"id": "meltingtemp-melting-temperature", "name": "MeltingTemp (Melting Temperature)", "description": "The MeltingTemp dataset is collected from Polyinfo. It uses monomers as polymer graphs to predict the property of polymer melting temperature."}, {"id": "geocov19", "name": "GeoCoV19", "description": "GeoCoV19 is a large-scale Twitter dataset containing more than 524 million multilingual tweets. The dataset contains around 378K geotagged tweets and 5.4 million tweets with Place information. The annotations include toponyms from the user location field and tweet content and resolve them to geolocations such as country, state, or city level. In this case, 297 million tweets are annotated with geolocation using the user location field and 452 million tweets using tweet content."}, {"id": "wikipedia-person-and-animal-dataset", "name": "Wikipedia Person and Animal Dataset", "description": "This dataset gathers 428,748 person and 12,236 animal infobox with descriptions based on Wikipedia dump (2018/04/01) and Wikidata (2018/04/12)."}, {"id": "bobsl-bbc-oxford-british-sign-language", "name": "BOBSL (BBC-Oxford British Sign Language)", "description": "BOBSL is a large-scale dataset of British Sign Language (BSL). It comprises 1,962 episodes (approximately 1,400 hours) of BSL-interpreted BBC broadcast footage accompanied by written English subtitles. From horror, period and medical dramas, history, nature and science documentaries, sitcoms, children\u2019s shows and programs covering cooking, beauty, business and travel, BOBSL covers a wide range of topics. The dataset features a total of 39 signers. Distinct signers appear in the training, validation and test sets for signer-independent evaluation."}, {"id": "cifar-10n-real-world-human-annotations", "name": "CIFAR-10N (Real-World Human Annotations)", "description": "This work presents two new benchmark datasets (CIFAR-10N, CIFAR-100N), equipping the training dataset of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk."}, {"id": "btad-beantech-anomaly-detection", "name": "BTAD (beanTech Anomaly Detection)", "description": "The BTAD ( beanTech Anomaly Detection) dataset is a real-world industrial anomaly dataset. The dataset contains a total of 2830 real-world images of 3 industrial products showcasing body and surface defects."}, {"id": "cdr-biocreative-v-cdr-task-corpus", "name": "CDR (BioCreative V CDR Task Corpus)", "description": "The BioCreative V CDR task corpus is manually annotated for chemicals, diseases and chemical-induced disease (CID) relations. It contains the titles and abstracts of 1500 PubMed articles and is split into equally sized train, validation and test sets. It is common to first tune a model on the validation set and then train on the combination of the train and validation sets before evaluating on the test set. It is also common to filter negative relations with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract (see Appendix C of this paper for details)."}, {"id": "vmsmo", "name": "VMSMO", "description": "The Video-based Multimodal Summarization with Multimodal Output (VMSMO) corpus consists of 184,920 document-summary pairs, with 180,000 training pairs, 2,460 validation and test pairs. The task for this dataset is generating and appropriate textual summary of an article and choosing a proper cover frame from a video accompanying the article."}, {"id": "atlantis", "name": "ATLANTIS", "description": "ATLANTIS is a benchmark for semantic segmentation of waterbody images. This dataset covers a wide range of natural waterbodies such as sea, lake, river and man-made (artificial) water-related structures such as dam, reservoir, canal, and pier. ATLANTIS includes 5,195 pixel-wise annotated images split to 3,364 training, 535 validation, and 1,296 testing images. In addition to 35 waterbodies, this dataset covers 21 general labels such as person, car, road and building."}, {"id": "aid-aerial-image-dataset", "name": "AID (Aerial Image Dataset)", "description": "AID is a new large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although the Google Earth images are post-processed using RGB renderings from the original optical aerial images, it has proven that there is no significant difference between the Google Earth images with the real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can also be used as aerial images for evaluating scene classification algorithms."}, {"id": "asap-automated-student-assessment-prize", "name": "ASAP (Automated Student Assessment Prize)", "description": "There are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities."}, {"id": "ethics", "name": "ETHICS", "description": "A new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality."}, {"id": "cityflow", "name": "CityFlow", "description": "CityFlow is a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. "}, {"id": "alg514", "name": "ALG514", "description": "514 algebra word problems and associated equation systems gathered from Algebra.com."}, {"id": "coco-mlt", "name": "COCO-MLT", "description": "The COCO-MLT is created from MS COCO-2017, containing 1,909 images from 80 classes. The maximum of training number per class is 1,128 and the minimum is 6. We use the test set of COCO2017 with 5,000 for evaluation. The ratio of head, medium, and tail classes is 22:33:25 in COCO-MLT."}, {"id": "inspired", "name": "Inspired", "description": "A new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations."}, {"id": "reddit-tifu", "name": "Reddit TIFU", "description": "Reddit TIFU dataset is a newly collected Reddit dataset, where TIFU denotes the name of /r/tifu subbreddit. There are 122,933 text-summary pairs in total."}, {"id": "emocontext", "name": "EmoContext", "description": "EmoContext consists of three-turn English Tweets. The emotion labels include happiness, sadness, anger and other."}, {"id": "dut-omron", "name": "DUT-OMRON", "description": "The DUT-OMRON dataset is used for evaluation of Salient Object Detection task and it contains 5,168 high quality images. The images have one or more salient objects and relatively cluttered background."}, {"id": "arqmath2-task-1-the-second-answer-retrieval-for-questions-on-math-lab-task-1", "name": "ARQMath2 - Task 1 (The second Answer Retrieval for Questions on Math lab - Task 1)", "description": "The goal of ARQMath is to advance techniques for mathematical information retrieval, in particular, retrieving answers to mathematical questions (Task 1), and formula retrieval (Task 2). Using the question posts from Math Stack Exchange, participating systems are given a question or a formula from a question and asked to return a ranked list of either potential answers to the question or potentially useful formulae (in the case of a formula query). Relevance is determined by the expected utility of each returned item. These tasks allow participating teams to explore leveraging math notation together with text to improve the quality of retrieval results."}, {"id": "fat-falling-things", "name": "FAT (Falling Things)", "description": "Falling Things (FAT) is a dataset for advancing the state-of-the-art in object detection and 3D pose estimation in the context of robotics. It consists of generated photorealistic images with accurate 3D pose annotations for all objects in 60k images."}, {"id": "mathmlben-formula-semantics-benchmark", "name": "MathMLben (Formula semantics benchmark)", "description": "MathMLben is a benchmark to the evaluate tools for mathematical format conversion (LaTeX \u2194 MathML \u2194 CAS). It comprises semantically annotated and linked formulae extracted from the NTCIR 11/12 arXiv and Wikipedia task / dataset, NIST Digital Library of Mathematical Functions (DLMF) and annotations using the AnnoMathTeX formula and identifier name recommender system (https://annomathtex.wmflabs.org)."}, {"id": "wdc-lspm", "name": "WDC LSPM", "description": "Many e-shops have started to mark-up product data within their HTML pages using the schema.org vocabulary. The Web Data Commons project regularly extracts such data from the Common Crawl, a large public web crawl. The Web Data Commons Training and Test Sets for Large-Scale Product Matching contain product offers from different e-shops in the form of binary product pairs (with corresponding label \"match\" or \"no match\") for four product categories, computers, cameras, watches and shoes. "}, {"id": "map2seq", "name": "map2seq", "description": "7,672 human written natural language navigation instructions for routes in OpenStreetMap with a focus on visual landmarks. Validated in Street View."}, {"id": "wikiatomicedits", "name": "WikiAtomicEdits", "description": "WikiAtomicEdits is a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. "}, {"id": "sorel-20m-sophos-reversinglabs-20-million", "name": "SOREL-20M (Sophos/ReversingLabs-20 Million)", "description": "SOREL-20M is a large-scale dataset consisting of nearly 20 million files with pre-extracted features and metadata, high-quality labels derived from multiple sources, information about vendor detections of the malware samples at the time of collection, and additional \u201ctags\u201d related to each malware sample to serve as additional targets."}, {"id": "muse-car-multimodal-sentiment-analysis-in-car-reviews", "name": "MuSe-CaR (Multimodal Sentiment Analysis in Car Reviews)", "description": "The MuSe-CAR database is a large, multimodal (video, audio, and text) dataset which has been gathered in-the-wild with the intention of further understanding Multimodal Sentiment Analysis in-the-wild, e.g., the emotional engagement that takes place during product reviews (i.e., automobile reviews) where a sentiment is linked to a topic or entity. "}, {"id": "sod-small-obstacle-detection", "name": "SOD (small obstacle detection)", "description": "Detect small obstacles, like lost and found."}, {"id": "russe-russian-words-in-context-based-on-russe", "name": "RUSSE (Russian Words in Context (based on RUSSE))", "description": "WiC: The Word-in-Context Dataset A reliable benchmark for the evaluation of context-sensitive word embeddings."}, {"id": "avalon", "name": "Avalon", "description": "Avalon is a benchmark for generalization in Reinforcement Learning (RL). The benchmark consists of a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This benchmark setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks."}, {"id": "bg-20k-background-dataset-20k", "name": "BG-20k (Background Dataset - 20k)", "description": "BG-20k contains 20,000 high-resolution background images excluded salient objects, which can be used to help generate high quality synthetic data."}, {"id": "winogavil", "name": "WinoGAViL", "description": "This dataset is collected via the WinoGAViL game to collect challenging vision-and-language associations. Inspired by the popular card game Codenames, a \u201cspymaster\u201d gives a textual cue related to several visual candidates, and another player has to identify them."}, {"id": "voxceleb2", "name": "VoxCeleb2", "description": "VoxCeleb2 is a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected \u2018in the wild\u2019, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example \u2013 visual speech synthesis, speech separation, cross-modal transfer from face to voice or vice versa and training face recognition from video to complement existing face recognition datasets."}, {"id": "calms21-caltech-mouse-social-interactions", "name": "CalMS21 (Caltech Mouse Social Interactions)", "description": "The Caltech Mouse Social Interactions (CalMS21) dataset is a multi-agent dataset from behavioral neuroscience. The dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. The CalMS21 dataset is part of the Multi-Agent Behavior Challenge 2021."}, {"id": "meccano", "name": "MECCANO", "description": "The MECCANO dataset is the first dataset of egocentric videos to study human-object interactions in industrial-like settings. The MECCANO dataset has been acquired in an industrial-like scenario in which subjects built a toy model of a motorbike. We considered 20 object classes which include the 16 classes categorizing the 49 components, the two tools (screwdriver and wrench), the instructions booklet and a partial_model class."}, {"id": "v2x-sim", "name": "V2X-SIM", "description": "V2X-Sim, short for vehicle-to-everything simulation, is the a synthetic collaborative perception dataset in autonomous driving developed by AI4CE Lab at NYU and MediaBrain Group at SJTU to facilitate collaborative perception between multiple vehicles and roadside infrastructure. Data is collected from both roadside and vehicles when they are presented near the same intersection. With information from both the roadside infrastructure and vehicles, the dataset aims to encourage research on collaborative perception tasks."}, {"id": "geowebnews", "name": "GeoWebNews", "description": "GeoWebNews provides test/train examples and enable fine-grained Geotagging and Toponym Resolution (Geocoding). This dataset is also suitable for prototyping and evaluating machine learning NLP models."}, {"id": "mmse-hr-multimodal-spontaneous-expression-heart-rate-dataset", "name": "MMSE-HR (Multimodal Spontaneous Expression-Heart Rate dataset)", "description": "The MMSE-HR benchmark consists of a dataset of 102 videos from 40 subjects recorded at 1040x1392 raw resolution at 25fps. During the recordings, various stimuli such as videos, sounds, and smells are introduced to induce different emotional states in the subjects. The ground truth waveform for MMSE-HR is the blood pressure signal sampled at 1000Hz. The dataset contains a diverse distribution of skin colors in the Fitzpatrick scale (II=8, III=11, IV=17, V+VI=4)."}, {"id": "mrs-multilingual-reply-suggestion", "name": "MRS (Multilingual Reply Suggestion)", "description": "MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence labeling tasks."}, {"id": "bdd-x-berkeley-deep-drive-x-explanation", "name": "BDD-X (Berkeley Deep Drive-X (eXplanation))", "description": "Berkeley Deep Drive-X (eXplanation) is a dataset is composed of over 77 hours of driving within 6,970 videos. The videos are taken in diverse driving conditions, e.g. day/night, highway/city/countryside, summer/winter etc. On average 40 seconds long, each video contains around 3-4 actions, e.g. speeding up, slowing down, turning right etc., all of which are annotated with a description and an explanation. Our dataset contains over 26K activities in over 8.4M frames."}, {"id": "reddit-corpus", "name": "Reddit Corpus", "description": "Reddit Corpus is part of a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using '1-of-100 accuracy'. The Reddit Corpus contains 726 million multi-turn dialogues from the Reddit board. "}, {"id": "sepehr-rumtel01", "name": "Sepehr_RumTel01", "description": "The expansion of social networks has accelerated the transmission of information and news at every communities. Over the past few years, the number of users, audiences and social networking publishers, are increased dramatically too. Among the massive amounts of information and news reported on these networks, we are faced with issues that have not been verified which is called \u201crumors\u201d. Identifying rumors on social networks is carried out in the form of rumor detection approaches; the massive amount of these news and information force to use the machine learning techniques. The most important problem with auto-detection approaches is the lack of a database of rumors. For that matter, in this article, a collection of rumors published on the social network \u201ctelegrams\u201d have been collected. These data are gathered from five Persian-language channels that have specially reviewed this issue. The collected data set contains 3283 messages with 2829 attachments, having a volume of over 1.6 gigabytes. This dataset can also be used for different purposes of natural language processing."}, {"id": "prosocialdialog", "name": "ProsocialDialog", "description": "Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. "}, {"id": "csaw-s", "name": "CSAW-S", "description": "CSAW-S is a dataset of mammography images which includes expert annotations of tumors and non-expert annotations of breast anatomy and artifacts in the image."}, {"id": "focus-call-for-customized-conversation-customized-conversation-grounding-persona-and-knowledge", "name": "FoCus (Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge)", "description": "We introduce a new dataset, called FoCus, that supports knowledge-grounded answers that reflect user\u2019s persona. One of the situations in which people need different types of knowledge, based on their preferences, occurs when they travel around the world."}, {"id": "7-point-criteria-evaluation-database", "name": "7-point criteria evaluation Database", "description": "\"We provide a database for evaluating computerized image-based prediction of the 7-point skin lesion malignancy checklist. The dataset includes over 2000 clinical and dermoscopy color images, along with corresponding structured metadata tailored for training and evaluating computer aided diagnosis (CAD) systems. \""}, {"id": "hrf-high-resolution-fundus", "name": "HRF (High-Resolution Fundus)", "description": "The HRF dataset is a dataset for retinal vessel segmentation which comprises 45 images and is organized as 15 subsets. Each subset contains one healthy fundus image, one image of patient with diabetic retinopathy and one glaucoma image. The image sizes are 3,304 x 2,336, with a training/testing image split of 22/23."}, {"id": "3dnet", "name": "3DNet", "description": "The 3DNet dataset is a free resource for object class recognition and 6DOF pose estimation from point cloud data. 3DNet provides a large-scale hierarchical CAD-model databases with increasing numbers of classes and difficulty with 10, 60 and 200 object classes together with evaluation datasets that contain thousands of scenes captured with an RGB-D sensor."}, {"id": "cholect45", "name": "CholecT45", "description": "CholecT45 is a subset of CholecT50 consisting of 45 videos from the Cholec80 dataset. It is the first public release of part of CholecT50 dataset. CholecT50 is a dataset of 50 endoscopic videos of laparoscopic cholecystectomy surgery introduced to enable research on fine-grained action recognition in laparoscopic surgery. It is annotated with 100 triplet classes in the form of <instrument, verb, target>. "}, {"id": "movieplotevents-cmu-movie-summary-corpus-with-events", "name": "MoviePlotEvents (CMU Movie Summary Corpus with Events)", "description": "A version of the CMU Movie Summary Corpus (http://www.cs.cmu.edu/~ark/personas/), which was originally scraped from plot summaries from Wikipedia, with some cleaning and sentences turned into events & sorted into \"genres\" (via LDA)."}, {"id": "vanilla", "name": "VANiLLa", "description": "VANiLLa is a dataset for Question Answering over Knowledge Graphs (KGQA) offering answers in natural language sentences. The answer sentences in this dataset are syntactically and semantically closer to the question than to the triple fact. The dataset consists of over 100k simple questions adapted from the CSQA and SimpleQuestionsWikidata datasets and generated using a semi-automatic framework."}, {"id": "a-robot-dataset-of-successful-and-failed-placement-executions", "name": "A robot dataset of successful and failed placement executions", "description": "The dataset contains the following data from successful and failed executions of the Toyota HSR robot placing a book on a shelf."}, {"id": "fod-a", "name": "FOD-A", "description": "FOD in Airports (FOD-A) is an image dataset of FOD, Foreign Object Degris, which consists of 31 object categories and over 30,000 annotation instances. The object categories have been selected based on guidance from prior documentation and related research by the Federal Aviation Administration (FAA)."}, {"id": "ogtd-offensive-greek-tweet-dataset", "name": "OGTD (Offensive Greek Tweet Dataset)", "description": "A manually annotated dataset containing 4,779 posts from Twitter annotated as offensive and not offensive. "}, {"id": "mvp-multi-view-partial-point-cloud", "name": "MVP (Multi-View Partial point cloud)", "description": "MVP is a multi-view partial point cloud dataset (MVP) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model."}, {"id": "mvsec-multi-vehicle-stereo-event-camera", "name": "MVSEC (Multi Vehicle Stereo Event Camera)", "description": "The Multi Vehicle Stereo Event Camera (MVSEC) dataset is a collection of data designed for the development of novel 3D perception algorithms for event based cameras. Stereo event data is collected from car, motorbike, hexacopter and handheld data, and fused with lidar, IMU, motion capture and GPS to provide ground truth pose and depth images. "}, {"id": "lyra", "name": "Lyra", "description": "Lyra is a dataset for code generation that consists on Python code with embedded SQL. This dataset contains 2,000 carefully annotated database manipulation programs from real usage projects. Each program is paired with both a Chinese comment and an English comment."}, {"id": "pot-210-planar-object-tracking-in-the-wild-a-benchmark", "name": "POT-210 (Planar Object Tracking in the Wild: A Benchmark)", "description": "Planar object tracking is an actively studied problem in vision-based robotic applications. While several benchmarks have been constructed for evaluating state-of-theart algorithms, there is a lack of video sequences captured in the wild rather than in constrained laboratory environment. In this paper, we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. The ground truth is carefully annotated semi-manually to ensure the quality. Moreover, eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics, with detailed analysis provided for the evaluation results. We expect the proposed benchmark to benefit future studies on planar object tracking."}, {"id": "europarl-asr", "name": "Europarl-ASR", "description": "Europarl-ASR (EN) is a 1300-hour English-language speech and text corpus of parliamentary debates for (streaming) Automatic Speech Recognition training and benchmarking, speech data filtering and speech data verbatimization, based on European Parliament speeches and their official transcripts (1996-2020). Includes dev-test sets for streaming ASR benchmarking, made up of 18 hours of manually revised speeches. The availability of manual non-verbatim and verbatim transcripts for dev-test speeches makes this corpus also useful for the assessment of automatic filtering and verbatimization techniques. The corpus is released under an open licence at https://www.mllp.upv.es/europarl-asr/"}, {"id": "bci-breast-cancer-immunohistochemical-image-generation", "name": "BCI (Breast Cancer Immunohistochemical Image Generation)", "description": "The evaluation of human epidermal growth factor receptor 2 (HER2) expression is essential to formulate a precise treatment for breast cancer. The routine evaluation of HER2 is conducted with immunohistochemical techniques (IHC), which is very expensive. Therefore, we propose a breast cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data directly with the paired hematoxylin and eosin (HE) stained images. The dataset contains 4870 registered image pairs, covering a variety of HER2 expression levels (0, 1+, 2+, 3+)."}, {"id": "cadp", "name": "CADP", "description": "A novel dataset for traffic accidents analysis. "}, {"id": "hq-ytvis", "name": "HQ-YTVIS", "description": "While Video Instance Segmentation (VIS) has seen rapid progress, current approaches struggle to predict high-quality masks with accurate boundary details. To tackle this issue, we identify that the coarse boundary annotations of the popular YouTube-VIS dataset constitute a major limiting factor. To benchmark high-quality mask predictions for VIS, we introduce the HQ-YTVIS dataset as well as Tube-Boundary AP in ECCV 2022. HQ-YTVIS consists of a manually re-annotated test set and our automatically refined training data, which provides training, validation and testing support to facilitate future development of VIS methods aiming at higher mask quality."}, {"id": "oxuva", "name": "OxUva", "description": "OxUva is a dataset and benchmark for evaluating single-object tracking algorithms."}, {"id": "tsinghua-daimler-cyclist-benchmark", "name": "Tsinghua-Daimler Cyclist Benchmark", "description": "The Tsinghua-Daimler Cyclist Benchmark provides a benchmark dataset for cyclist detection. Bounding Box based labels are provided for the classes: (\"pedestrian\", \"cyclist\", \"motorcyclist\", \"tricyclist\", \"wheelchairuser\", \"mopedrider\")."}, {"id": "mlfw-masked-lfw", "name": "MLFW (Masked LFW)", "description": "The Masked LFW (MLFW), based on Cross-Age LFW (CALFW) database, is built using a simple but effective tool that generates masked faces from unmasked faces automatically."}, {"id": "iiit-ilst", "name": "IIIT-ILST", "description": "IIIT-ILST is a dataset and benchmark for scene text recognition for three Indic scripts - Devanagari, Telugu and Malayalam. IIIT-ILST contains nearly 1000 real images per each script which are annotated for scene text bounding boxes and transcriptions."}, {"id": "gwa-geometric-wave-acoustic", "name": "GWA (Geometric-Wave Acoustic)", "description": "GWA is a large-scale audio dataset of over 2 million synthetic room impulse responses (IRs) and their corresponding detailed geometric and simulation configurations. Our dataset samples acoustic environments from over 6.8K high-quality diverse and professionally designed houses represented as semantically labeled 3D meshes"}, {"id": "au-air", "name": "AU-AIR", "description": "The AU-AIR is a multi-modal aerial dataset captured by a UAV. Having visual data, object annotations, and flight data (time, GPS, altitude, IMU sensor data, velocities), AU-AIR meets vision and robotics for UAVs."}, {"id": "fb15k-237-low", "name": "FB15k-237-low", "description": "The FB15k-237-low dataset is a variation of the FB15k-237 dataset where relations with a low number of triplets are kept."}, {"id": "cvgl-camera-calibration-dataset", "name": "CVGL Camera Calibration Dataset", "description": "The dataset has been generated using Town 1 and Town 2 of CARLA Simulator. The dataset consists of 50 camera configurations with each town having 25 configurations. The parameters modified for generating the configurations include f ov, x, y, z, pitch, yaw, and roll. Here, f ov is the field of view, (x, y, z) is the translation while (pitch, yaw, and roll) is the rotation between the cameras. The total number of image pairs is 1,23,017, out of which 58,596 belong to Town 1 while 64,421 belong to Town 2, the difference in the number of images is due to the length of the tracks."}, {"id": "mmhs150k-multimodal-hate-speech", "name": "MMHS150k (Multimodal Hate Speech)", "description": "Existing hate speech datasets contain only textual data. We create a new manually annotated multimodal hate speech dataset formed by 150,000 tweets, each one of them containing text and an image. We call the dataset MMHS150K."}, {"id": "wikihop", "name": "WikiHop", "description": "WikiHop is a multi-hop question-answering dataset. The query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WikiReading. A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WikiHop is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided. The task is to predict the correct answer given a query and multiple supporting documents."}, {"id": "med-node-dermatology-database-used-in-med-node", "name": "MED-NODE (Dermatology database used in MED-NODE)", "description": "\"Our dataset consists of 70 melanoma and 100 naevus images from the digital image archive of the Department of Dermatology of the University Medical Center Groningen (UMCG) used for the development and testing of the MED-NODE system for skin cancer detection from macroscopic images. The file - complete_mednode_dataset.zip 24KB - contains 170 images (70 melanoma and 100 nevi cases).\""}, {"id": "imagenet-sketch", "name": "ImageNet-Sketch", "description": "ImageNet-Sketch data set consists of 50000 images, 50 images for each of the 1000 ImageNet classes. The data set is constructed with Google Image queries \"sketch of \", where  is the standard class name. Only within the \"black and white\" color scheme is searched. 100 images are initially queried for every class, and the pulled images are cleaned by deleting the irrelevant images and images that are for similar but different classes. For some classes, there are less than 50 images after manually cleaning, and then the data set is augmented by flipping and rotating the images."}, {"id": "wos-web-of-science-dataset", "name": "WOS (Web of Science Dataset)", "description": "Web of Science (WOS) is a document classification dataset that contains 46,985 documents with 134 categories which include 7 parents categories."}, {"id": "pascal-panoptic-parts", "name": "Pascal Panoptic Parts", "description": "The Pascal Panoptic Parts dataset consists of annotations for the part-aware panoptic segmentation task on the PASCAL VOC 2010 dataset. It is created by merging scene-level labels from PASCAL-Context with part-level labels from PASCAL-Part"}, {"id": "akb-48", "name": "AKB-48", "description": "AKB-48 is a large-scale Articulated object Knowledge Base which consists of 2,037 real-world 3D articulated object models of 48 categories."}, {"id": "novelcraft", "name": "NovelCraft", "description": "Scene-focused, multi-modal, episodic data of the images and symbolic world-states seen by an agent completing a pogo-stick assembly task within a video game world. Classes consist of episodes with novel objects inserted. A subset of these novel objects can impact gameplay and agent behavior. Novelty objects can vary in size, position, and occlusion within the images. Usable for novelty detection, generalized category discovery, and class-imbalanced classification."}, {"id": "facescape", "name": "FaceScape", "description": "FaceScape dataset provides 3D face models, parametric models and multi-view images in large-scale and high-quality. The camera parameters, the age and gender of the subjects are also included. The data have been released to public for non-commercial research purpose."}, {"id": "tartanair", "name": "TartanAir", "description": "A dataset for robot navigation task and more. The data is collected in photo-realistic simulation environments in the presence of various light conditions, weather and moving objects."}, {"id": "wiki-zsl", "name": "Wiki-ZSL", "description": "The Wiki-ZSL (Wiki Zero-Shot Learning) dataset contains 113 relations and 94,383 instances from Wikipedia. The dataset is divided into three subsets: training set (98 relations), validation set (5 relations) and test set (10 relations)."}, {"id": "anonymized-keystrokes-dataset", "name": "Anonymized Keystrokes Dataset", "description": "Includes two datasets for this task, one for English-French (En-Fr) and another for English-German (En-De). For each dataset, the action sequences for full documents are provided, along with an editor identifier. The dataset contains document-level post-editing action sequences, including edit operations from keystrokes, mouse actions, and waiting times."}, {"id": "streethazards", "name": "StreetHazards", "description": "StreetHazards is a synthetic dataset for anomaly detection, created by inserting a diverse array of foreign objects into driving scenes and re-render the scenes with these novel objects."}, {"id": "arc-ai2-reasoning-challenge", "name": "ARC (AI2 Reasoning Challenge)", "description": "The AI2\u2019s Reasoning Challenge (ARC) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning. Most of the questions have 4 answer choices, with <1% of all the questions having either 3 or 5 answer choices. ARC includes a supporting KB of 14.3M unstructured text passages."}, {"id": "action-recognition-in-the-dark-arid", "name": "Action Recognition in the Dark (ARID)", "description": "ARID is a dataset for action recognition in dark videos. It consists of over 3,780 video clips with 11 action categories."}, {"id": "fakeavceleb", "name": "FakeAVCeleb", "description": "FakeAVCeleb is a novel Audio-Video Deepfake dataset that not only contains deepfake videos but respective synthesized cloned audios as well. "}, {"id": "dota-dataset-for-object-detection-in-aerial-images", "name": "DOTA (Dataset for Object deTection in Aerial Images)", "description": "DOTA is a large-scale dataset for object detection in aerial images. It can be used to develop and evaluate object detectors in aerial images. The images are collected from different sensors and platforms. Each image is of the size in the range from 800 \u00d7 800 to 20,000 \u00d7 20,000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. The instances in DOTA images are annotated by experts in aerial image interpretation by arbitrary (8 d.o.f.) quadrilateral. We will continue to update DOTA, to grow in size and scope to reflect evolving real-world conditions. Now it has three versions:"}, {"id": "bkai-igh-neopolyp-small", "name": "BKAI-IGH NeoPolyp-Small", "description": "This dataset contains 1200 images (1000 WLI images and 200 FICE images) with fine-grained segmentation annotations. The training set consists of 1000 images, and the test set consists of 200 images. All polyps are classified into neoplastic or non-neoplastic classes denoted by red and green colors, respectively.  This dataset is a part of a bigger dataset called NeoPolyp."}, {"id": "cc12m-conceptual-12m", "name": "CC12M (Conceptual 12M)", "description": "Conceptual 12M (CC12M) is a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training."}, {"id": "clinical-admission-notes-from-mimic-iii", "name": "Clinical Admission Notes from MIMIC-III", "description": "This dataset is created from MIMIC-III (Medical Information Mart for Intensive Care III) and contains simulated patient admission notes. The clinical notes contain information about a patient at admission time to the ICU and are labelled for four outcome prediction tasks: Diagnoses at discharge, procedures performed, in-hospital mortality and length-of-stay."}, {"id": "instaorder", "name": "InstaOrder", "description": "InstaOrder can be used to understand the geometrical relationships of instances in an image. The dataset consists of 2.9M annotations of geometric orderings for class-labeled instances in 101K natural scenes. The scenes were annotated by 3,659 crowd-workers regarding (1) occlusion order that identifies occluder/occludee and (2) depth order that describes ordinal relations that consider relative distance from the camera."}, {"id": "cosqa-code-search-and-question-answering", "name": "CoSQA (Code Search and Question Answering)", "description": "CoSQA (Code Search and Question Answering) It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators."}, {"id": "hummingbird", "name": "Hummingbird", "description": "Hummingbird is a dataset to examine stylistic lexical cues from human perception and BERT used to characterize their discrepancy. In HUMMINGBIRD crowd-workers relabeled benchmarking datasets for style classification tasks."}, {"id": "oreba-objectively-recognizing-eating-behavior-and-associated-intake", "name": "OREBA (Objectively Recognizing Eating Behavior and Associated Intake)", "description": "The OREBA dataset aims to provide a comprehensive multi-sensor recording of communal intake occasions for researchers interested in automatic detection of intake gestures. Two scenarios are included, with 100 participants for a discrete dish and 102 participants for a shared dish, totalling 9069 intake gestures. Available sensor data consists of synchronized frontal video and IMU with accelerometer and gyroscope for both hands."}, {"id": "daisee", "name": "DAiSEE", "description": "DAiSEE is a multi-label video classification dataset comprising of 9,068 video snippets captured from 112 users for recognizing the user affective states of boredom, confusion, engagement, and frustration \"in the wild\". The dataset has four levels of labels namely - very low, low, high, and very high for each of the affective states, which are crowd annotated and correlated with a gold standard annotation created using a team of expert psychologists. "}, {"id": "resume-ner", "name": "Resume NER", "description": "Resume contains eight fine-grained entity categories -score from 74.5% to 86.88%."}, {"id": "sysu-mm01", "name": "SYSU-MM01", "description": "The SYSU-MM01 is a dataset collected for the Visible-Infrared Re-identification problem. The images in the dataset were obtained from 491 different persons by recording them using 4 RGB and 2 infrared cameras. Within the dataset, the persons are divided into 3 fixed splits to create training, validation and test sets. In the training set, there are 20284 RGB and 9929 infrared images of 296 persons. The validation set contains 1974 RGB and 1980 infrared images of 99 persons. The testing set consists of the images of 96 persons where 3803 infrared images are used as query and 301 randomly selected RGB images are used as gallery."}, {"id": "chestx-ray8", "name": "ChestX-ray8", "description": "ChestX-ray8 is a medical imaging dataset which comprises 108,948 frontal-view X-ray images of 32,717 (collected from the year of 1992 to 2015) unique patients with the text-mined eight common disease labels, mined from the text radiological reports via NLP techniques."}, {"id": "climart-climate-atmospheric-radiative-transfer", "name": "ClimART (Climate Atmospheric Radiative Transfer)", "description": "Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. This has made them a popular target for neural network-based emulators. However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking. To fill this gap, we build a large dataset, ClimART, with more than \\emph{10 million samples from present, pre-industrial, and future climate conditions}, based on the Canadian Earth System Model. ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed."}, {"id": "otb", "name": "OTB", "description": "Object Tracking Benchmark (OTB) is a visual tracking benchmark that is widely used to evaluate the performance of a visual tracking algorithm. The dataset contains a total of 100 sequences and each is annotated frame-by-frame with bounding boxes and 11 challenge attributes. OTB-2013 dataset contains 51 sequences and the OTB-2015 dataset contains all 100 sequences of the OTB dataset."}, {"id": "mmkg", "name": "MMKG", "description": "MMKG is a collection of three knowledge graphs for link prediction and entity matching research. Contrary to other knowledge graph datasets, these knowledge graphs contain both numerical features and images for all entities as well as entity alignments between pairs of KGs. While MMKG is intended to perform relational reasoning across different entities and images, previous resources are intended to perform visual reasoning within the same image."}, {"id": "ufdd-unconstrained-face-detection-dataset", "name": "UFDD (Unconstrained Face Detection Dataset)", "description": "Unconstrained Face Detection Dataset (UFDD) aims to fuel further research in unconstrained face detection."}, {"id": "birdsnap", "name": "Birdsnap", "description": "Birdsnap is a large bird dataset consisting of 49,829 images from 500 bird species with 47,386 images used for training and 2,443 images used for testing."}, {"id": "tnl2k-tracking-by-natural-language", "name": "TNL2K (Tracking by natural language)", "description": "Tracking by Natural Language (TNL2K) is constructed for the evaluation of tracking by natural language specification. TNL2K features:"}, {"id": "crossner", "name": "CrossNER", "description": "CrossNER is a cross-domain NER (Named Entity Recognition) dataset, a fully-labeled collection of NER data spanning over five diverse domains (Politics, Natural Science, Music, Literature, and Artificial Intelligence) with specialized entity categories for different domains. Additionally, CrossNER also includes unlabeled domain-related corpora for the corresponding five domains. "}, {"id": "cped-chinese-personalized-and-emotional-dialogue", "name": "CPED (Chinese Personalized and Emotional Dialogue)", "description": "We construct a dataset named CPED from 40 Chinese TV shows. CPED consists of multisource knowledge related to empathy and personal characteristic. This knowledge covers 13 emotions, gender, Big Five personality traits, 19 dialogue acts and other knowledge. "}, {"id": "cometa", "name": "COMETA", "description": "Consists of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph."}, {"id": "finqa", "name": "FinQA", "description": "FinQA is a new large-scale dataset with Question-Answering pairs over Financial reports, written by financial experts. The dataset contains 8,281 financial QA pairs, along with their numerical reasoning processes."}, {"id": "openttgames", "name": "OpenTTGames", "description": "OSAI introduces OpenTTGames - an open dataset aimed at evaluation of different computer vision tasks in Table Tennis: ball detection, semantic segmentation of humans, table and scoreboard and fast in-game events spotting."}, {"id": "cid-campus-image-dataset", "name": "CID (Campus Image Dataset)", "description": "The CID (Campus Image Dataset) is a dataset captured in low-light env with the help of Android programming. Its basic unit is group, which is named by capture time and contains 8 exposure-time-varying raw image shot in a burst."}, {"id": "amass", "name": "AMASS", "description": "AMASS is a large database of human motion unifying different optical marker-based motion capture datasets by representing them within a common framework and parameterization. AMASS is readily useful for animation, visualization, and generating training data for deep learning."}, {"id": "diffusiondb", "name": "DiffusionDB", "description": "DiffusionDB is a large-scale text-to-image prompt dataset. It contains 2 million images generated by Stable Diffusion using prompts and hyperparameters specified by real users."}, {"id": "verse-large-scale-vertebrae-segmentation-challenge", "name": "VerSe (Large Scale Vertebrae Segmentation Challenge)", "description": "Spine or vertebral segmentation is a crucial step in all applications regarding automated quantification of spinal morphology and pathology. With the advent of deep learning, for such a task on computed tomography (CT) scans, a big and varied data is a primary sought-after resource. However, a large-scale, public dataset is currently unavailable."}, {"id": "qmsum", "name": "QMSum", "description": "QMSum is a new human-annotated benchmark for query-based multi-domain meeting summarisation task, which consists of 1,808 query-summary pairs over 232 meetings in multiple domains."}, {"id": "ccnet", "name": "CCNet", "description": "CCNet is a dataset extracted from Common Crawl with a different filtering process than for OSCAR. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables. CCNet contains longer documents on average compared to OSCAR with smaller\u2014and often noisier\u2014documents weeded out."}, {"id": "bipedal-skills-bipedal-skills-benchmark-for-reinforcement-learning", "name": "bipedal-skills (Bipedal Skills Benchmark for Reinforcement Learning)", "description": "The bipedal skills benchmark is a suite of reinforcement learning environments implemented for the MuJoCo physics simulator. It aims to provide a set of tasks that demand a variety of motor skills beyond locomotion, and is intended for evaluating skill discovery and hierarchical learning methods. The majority of tasks exhibit a sparse reward structure."}, {"id": "gsv-cities", "name": "GSV-Cities", "description": "GSV-Cities is a large-scale dataset for training deep neural network for the task of Visual Place Recognition."}, {"id": "isprs-potsdam-2d-semantic-labeling-contest-potsdam", "name": "ISPRS Potsdam (2D Semantic Labeling Contest - Potsdam)", "description": "The data set contains 38 patches (of the same size), each consisting of a true orthophoto (TOP) extracted from a larger TOP mosaic."}, {"id": "ahp-amodal-human-perception", "name": "AHP (Amodal Human Perception)", "description": "The AHP dataset consists of 56,599 images in total which are collected from several large-scale instance segmentation and detection datasets, including COCO, VOC (w/ SBD), LIP, Objects365 and OpenImages. Each image is annotated with a pixel-level segmentation mask of a single integrated human."}, {"id": "senseval-2", "name": "Senseval-2", "description": "There are now many computer programs for automatically determining the sense of a word in context (Word Sense Disambiguation or WSD).  The purpose of SENSEVAL is to evaluate the strengths and weaknesses of such programs with respect to different words, different varieties of language, and different languages."}, {"id": "sip-salient-person", "name": "SIP (Salient Person)", "description": "The Salient Person dataset (SIP) contains 929 salient person samples with different poses and illumination conditions."}, {"id": "raw-c", "name": "RAW-C", "description": "Relatedness judgments of ambiguous English words, in experimentally controlled sentential contexts."}, {"id": "aebad-aero-engine-blade-anomaly-detection-dataset", "name": "AeBAD (Aero-engine Blade Anomaly Detection Dataset)", "description": "The aim of AeBAD is to automatically detect abnormalities in the blades of aero-engines, ensuring their stable operation. AeBAD consists of two sub-datasets: the single-blade dataset (AeBAD-S) and the video anomaly detection of blades (AeBAD-V). AeBAD-S comprises images of single blades of different scales, with a primary feature being that the samples are not aligned. Furthermore, there is a domain shift between the distribution of normal samples in the test set and the training set, where the domain shifts are mainly caused by the changes in illumination and view. AeBAD-V, on the other hand, includes videos of blades assembled on the blisks of aero-engines, with the aim of detecting blade anomalies during blisk rotation. A distinctive feature of AeBAD-V is that the shooting view in the test set differs from that in the training set."}, {"id": "winobias", "name": "WinoBias", "description": "WinoBias contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenarios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately."}, {"id": "crd3-critical-role-dungeons-and-dragons-dataset", "name": "CRD3 (Critical Role Dungeons and Dragons Dataset)", "description": "The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction."}, {"id": "biosses-biomedical-semantic-similarity-estimation-system", "name": "BIOSSES (Biomedical Semantic Similarity Estimation System)", "description": "The BIOSSES data set comprises total 100 sentence pairs all of which were selected from the \"TAC2 Biomedical Summarization Track Training Data Set\" ."}, {"id": "peyma", "name": "PEYMA", "description": "Peyma is a Persian NER dataset to train and test NER systems. It is constructed by collecting documents from ten news websites."}, {"id": "20000-utterances", "name": "20000 utterances", "description": "20000 utterances"}, {"id": "deep-fakes-dataset-inamibora", "name": "Deep Fakes Dataset (inamibora)", "description": "The Deep Fakes Dataset is a collection of \"in the wild\" portrait videos for deepfake detection. The videos in the dataset are diverse real-world samples in terms of the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context. They originate from various sources such as news articles, forums, apps, and research presentations; totalling up to 142 videos, 32 minutes, and 17 GBs. Synthetic videos are matched with their original counterparts when possible. "}, {"id": "forest-covertype", "name": "Forest CoverType", "description": "Predicting forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types)."}, {"id": "pascal-voc-pascal-visual-object-classes-challenge", "name": "PASCAL VOC (PASCAL Visual Object Classes Challenge)", "description": "The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set."}, {"id": "pwc-leaderboards-papers-with-code-leaderboards", "name": "PWC Leaderboards (Papers with Code Leaderboards)", "description": "The Papers with Code Leaderboards dataset is a collection of over 5,000 results capturing performance of machine learning models. Each result is a tuple of form (task, dataset, metric name, metric value). The data was collected using the Papers with Code review interface."}, {"id": "dadagp", "name": "DadaGP", "description": "DadaGP is a new symbolic music dataset comprising 26,181 song scores in the GuitarPro format covering 739 musical genres, along with an accompanying tokenized format well-suited for generative sequence models such as the Transformer. The tokenized format is inspired by event-based MIDI encodings, often used in symbolic music generation models. The dataset is released with an encoder/decoder which converts GuitarPro files to tokens and back."}, {"id": "visuelle", "name": "VISUELLE", "description": "VISUELLE is a repository build upon the data of a real fast fashion company, Nunalie, and is composed of 5577 new products and about 45M sales related to fashion seasons from 2016-2019. Each product in VISUELLE is equipped with multimodal information: its image, textual metadata, sales after the first release date, and three related Google Trends describing category, color and fabric popularity."}, {"id": "cmu-motion-capture-cmu-graphics-lab-motion-capture-database", "name": "CMU Motion Capture (CMU Graphics Lab Motion Capture Database)", "description": "This dataset of motions is free for all uses."}, {"id": "llamas-labeled-lane-markers", "name": "LLAMAS (Labeled Lane Markers)", "description": "The unsupervised Labeled Lane MArkerS dataset (LLAMAS) is a dataset for lane detection and segmentation. It contains over 100,000 annotated images, with annotations of over 100 meters at a resolution of 1276 x 717 pixels. The Unsupervised Llamas dataset was annotated by creating high definition maps for automated driving including lane markers based on Lidar. "}, {"id": "mtvr", "name": "mTVR", "description": "mTVR is a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations."}, {"id": "msu-nr-vqa-database-msu-no-reference-video-quality-assessment-database", "name": "MSU NR VQA Database (MSU No-Reference Video Quality Assessment Database)", "description": "The dataset was created for video quality assessment problem. It was formed with 36 clips from Vimeo, which were selected from 18,000+ open-source clips with high bitrate (license CCBY or CC0). "}, {"id": "universal-dependencies", "name": "Universal Dependencies", "description": "The Universal Dependencies (UD) project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for multiple languages. The first version of the dataset was released in 2015 and consisted of 10 treebanks over 10 languages. Version 2.7 released in 2020 consists of 183 treebanks over 104 languages. The annotation consists of UPOS (universal part-of-speech tags), XPOS (language-specific part-of-speech tags), Feats (universal morphological features), Lemmas, dependency heads and universal dependency labels."}, {"id": "llff-local-light-field-fusion", "name": "LLFF (Local Light Field Fusion)", "description": "Local Light Field Fusion (LLFF) is a practical and robust deep learning solution for capturing and rendering novel views of complex real-world scenes for virtual exploration. The dataset consists of both renderings and real images of natural scenes. The synthetic images are rendered from the SUNCG and UnrealCV where SUNCG contains 45000 simplistic house and room environments with texture-mapped surfaces and low geometric complexity. UnrealCV contains a few large-scale environments modeled and rendered with extreme detail. The real images are 24 scenes captured from a handheld cellphone."}, {"id": "arkittrack", "name": "ARKitTrack", "description": "ARKitTrack is a new RGB-D tracking dataset for both static and dynamic scenes captured by consumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack contains 300 RGBD sequences, 455 targets, and 229.7K video frames in total. This dataset has 123.9K pixel-level target masks along with the bounding box annotations and frame-level attributes."}, {"id": "lcqmc-large-scale-chinese-question-matching-corpus", "name": "LCQMC (Large-scale Chinese Question Matching Corpus)", "description": "LCQMC is a large-scale Chinese question matching corpus. LCQMC is more general than paraphrase corpus as it focuses on intent matching rather than paraphrase. The corpus contains 260,068 question pairs with manual annotation."}, {"id": "mnist-large-scale-dataset", "name": "MNIST Large Scale dataset", "description": "The MNIST Large Scale dataset is based on the classic MNIST dataset, but contains large scale variations up to a factor of 16. The motivation behind creating this dataset was to enable testing the ability of different algorithms to learn in the presence of large scale variability and specifically the ability to generalise to new scales not present in the training set over wide scale ranges."}, {"id": "letter-letter-recognition-data-set", "name": "Letter (Letter Recognition Data Set)", "description": "Letter Recognition Data Set is a handwritten digit dataset. The task is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15."}, {"id": "cal500-computer-audition-lab-500", "name": "CAL500 (Computer Audition Lab 500)", "description": "CAL500 (Computer Audition Lab 500) is a dataset aimed for evaluation of music information retrieval systems. It consists of 502 songs picked from western popular music. The audio is represented as a time series of the first 13 Mel-frequency cepstral coefficients (and their first and second derivatives) extracted by sliding a 12 ms half-overlapping short-time window over the waveform of each song. Each song has been annotated by at least 3 people with 135 musically-relevant concepts spanning six semantic categories:"}, {"id": "shapestacks", "name": "ShapeStacks", "description": "A simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability. "}, {"id": "mid-air-dataset", "name": "Mid-Air Dataset", "description": "Mid-Air, The Montefiore Institute Dataset of Aerial Images and Records, is a multi-purpose synthetic dataset for low altitude drone flights. It provides a large amount of synchronized data corresponding to flight records for multi-modal vision sensors and navigation sensors mounted on board of a flying quadcopter. Multi-modal vision sensors capture RGB pictures, relative surface normal orientation, depth, object semantics and stereo disparity."}, {"id": "korsts", "name": "KorSTS", "description": "KorSTS is a dataset for semantic textural similarity (STS) in Korean. The dataset is constructed by automatically the STS-B dataset. To ensure translation quality, two professional translators with at least seven years of experience who specialize in academic papers/books as well as business contracts post-edited a half of the dataset each and cross-checked each other\u2019s translation afterward. The KorSTS dataset comprises 5,749 training examples translated automatically and 2,879 evaluation examples translated manually."}, {"id": "long-range-graph-benchmark-lrgb", "name": "Long Range Graph Benchmark (LRGB)", "description": "The Long Range Graph Benchmark (LRGB) is a collection of 5 graph learning datasets that arguably require long-range reasoning to achieve strong performance in a given task. The 5 datasets in this benchmark can be used to prototype new models that can capture long range dependencies in graphs."}, {"id": "multimodal-emoji-prediction", "name": "Multimodal Emoji Prediction", "description": "The twitter emoji dataset obtained from CodaLab comprises of 50 thousand tweets along with the associated emoji label. Each tweet in the dataset has a corresponding numerical label which maps to a specific emoji. The emojis are of the 20 most frequent emojis and hence the labels range from 0 to 19"}, {"id": "diabla", "name": "DiaBLa", "description": "A new English-French test set for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue. The test set contains 144 spontaneous dialogues (5,700+ sentences) between native English and French speakers, mediated by one of two neural MT systems in a range of role-play settings. The dialogues are accompanied by fine-grained sentence-level judgments of MT quality, produced by the dialogue participants themselves, as well as by manually normalised versions and reference translations produced a posteriori. "}, {"id": "bc7-nlm-chem-biocreative-vii-nlm-chem", "name": "BC7 NLM-Chem (BioCreative VII NLM-Chem)", "description": "Full-text chemical identification and indexing in PubMed articles."}, {"id": "msvd-microsoft-research-video-description-corpus", "name": "MSVD (Microsoft Research Video Description Corpus)", "description": "The Microsoft Research Video Description Corpus (MSVD) dataset consists of about 120K sentences collected during the summer of 2010. Workers on Mechanical Turk were paid to watch a short video snippet and then summarize the action in a single sentence. The result is a set of roughly parallel descriptions of more than 2,000 video snippets. Because the workers were urged to complete the task in the language of their choice, both paraphrase and bilingual alternations are captured in the data."}, {"id": "faviq-fact-verification-from-information-seeking-questions", "name": "FaVIQ (Fact Verification from Information-seeking Questions)", "description": "FaVIQ (Fact Verification from Information-seeking Questions) is a challenging and realistic fact verification dataset that reflects confusions raised by real users. We use the ambiguity in information-seeking questions and their disambiguation, and automatically convert them to true and false claims. These claims are natural, and require a complete understanding of the evidence for verification. FaVIQ serves as a challenging benchmark for natural language understanding, and improves performance in professional fact checking."}, {"id": "wavefake", "name": "WaveFake", "description": "WaveFake is a dataset for audio deepfake detection. The dataset consists of a large-scale dataset of over 100K generated audio clips."}, {"id": "ntu-x", "name": "NTU-X", "description": "NTU-X is an extended version of popular NTU dataset."}, {"id": "ptr", "name": "PTR", "description": "PTR is a new large-scale diagnostic visual reasoning dataset for research around part-based conceptual, relational and physical reasoning. PTR contains around 70k RGBD synthetic images with ground truth object and part level annotations regarding semantic instance segmentation, color attributes, spatial and geometric relationships, and certain physical properties such as stability. These images are paired with 700k machine-generated questions covering various types of reasoning types."}, {"id": "viper-viewpoint-invariant-pedestrian-recognition", "name": "VIPeR (Viewpoint Invariant Pedestrian Recognition)", "description": "The Viewpoint Invariant Pedestrian Recognition (VIPeR) dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128\u00d748 pixels. It provides the pose angle of each person as 0\u00b0 (front), 45\u00b0, 90\u00b0 (right), 135\u00b0, and 180\u00b0 (back)."}, {"id": "xia-and-ding-2019", "name": "Xia and Ding, 2019", "description": "Emotion-cause pair extraction (ECPE) aims to extract the potential pairs of emotions and corresponding causes in a document. This dataset consists of 1,945 Chinese documents from SINA NEWS website."}, {"id": "orgaze", "name": "ORGaze", "description": "A new video dataset for OR, with 30, 000 objects over 5, 000 stereo video sequences annotated for their descriptions and gaze."}, {"id": "fashion-mnist", "name": "Fashion-MNIST", "description": "Fashion-MNIST is a dataset comprising of 28\u00d728 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST shares the same image size, data format and the structure of training and testing splits with the original MNIST."}, {"id": "wsvd-web-stereo-video-dataset", "name": "WSVD (Web Stereo Video Dataset)", "description": "The Web Stereo Video Dataset consists of 553 stereoscopic videos from YouTube. This dataset has a wide variety of scene types, and features many nonrigid objects."}, {"id": "harpervalleybank", "name": "HarperValleyBank", "description": "The data simulate simple consumer banking interactions, containing about 23 hours of audio from 1,446 human-human conversations between 59 unique speakers."}, {"id": "cadis-cataract-dataset-for-image-segmentation", "name": "CaDIS (Cataract Dataset for Image Segmentation)", "description": "CaDIS: a Cataract Dataset for Image Segmentation is a dataset for semantic segmentation created by Digital Surgery Ltd. on top of the CATARACTS dataset. CaDIS consists of 4670 images sampled from the 25 videos on CATARACTS' training set. Each pixel in each image is labeled with its respective instrument or anatomical class from a set of 36 identified classes. More details about the dataset could be found in the paper (https://arxiv.org/pdf/1906.11586.pdf)."}, {"id": "multi-news", "name": "Multi-News", "description": "Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited."}, {"id": "mlfp-multispectral-latex-mask-based-video-face-presentation-attack", "name": "MLFP (Multispectral Latex Mask based Video Face Presentation Attack)", "description": "The MLFP dataset consists of face presentation attacks captured with seven 3D latex masks and three 2D print attacks. The dataset contains videos captured from color, thermal and infrared channels."}, {"id": "eurekaalert-eureka-alert", "name": "EurekaAlert (Eureka Alert)", "description": "This dataset contains around 5000 scholarly articles and their corresponding easy summary from eureka alert blog, the dataset can be used for the combined task of summarization and simplification."}, {"id": "semart", "name": "SemArt", "description": "SemArt is a multi-modal dataset for semantic art understanding. SemArt is a collection of fine-art painting images in which each image is associated to a number of attributes and a textual artistic comment, such as those that appear in art catalogues or museum collections. It contains 21,384 samples that provides artistic comments along with fine-art paintings and their attributes for studying semantic art understanding."}, {"id": "car-cityscapes-attributes-recognition", "name": "CAR (Cityscapes Attributes Recognition)", "description": "CAR contains visual attributes for objects in the Cityscapes dataset. For each object in an image, we have a list of attributes that depend on the category of the object. For instance, a vehicle category has a visibility attribute while a pedestrian has an activity attribute (walking, standing, etc.). "}, {"id": "celex", "name": "CELEX", "description": "CELEX database comprises three different searchable lexical databases, Dutch, English and German. The lexical data contained in each database is divided into five categories: orthography, phonology, morphology, syntax (word class) and word frequency."}, {"id": "amr-bank-abstract-meaning-representation", "name": "AMR Bank (Abstract Meaning Representation)", "description": "The AMR Bank is a set of English sentences paired with simple, readable semantic representations. Version 3.0 released in 2020 consists of 59,255 sentences."}, {"id": "multinational-structured-address-dataset", "name": "Multinational Structured Address Dataset", "description": "The Multinational Structured Address Dataset is a collection of addresses of 61 different countries. The addresses can either be \"complete\" (all the usual address components) or \"incomplete\" (missing some usual address components)."}, {"id": "light-snowfall-dense", "name": "Light Snowfall (DENSE)", "description": "We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are: - We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions. - The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects. - In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods. - Please check out our paper for more information."}, {"id": "dbpedia", "name": "DBpedia", "description": "DBpedia (from \"DB\" for \"database\") is a project aiming to extract structured content from the information created in the Wikipedia project. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets."}, {"id": "quac-question-answering-in-context", "name": "QuAC (Question Answering in Context)", "description": "Question Answering in Context is a large-scale dataset that consists of around 14K crowdsourced Question Answering dialogs with 98K question-answer pairs in total. Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text."}, {"id": "seasondepth", "name": "SeasonDepth", "description": "Aa new cross-season scaleless monocular depth prediction dataset from CMU Visual Localization dataset through structure from motion."}, {"id": "casia-fasd", "name": "CASIA-FASD", "description": "CASIA-FASD is a small face anti-spoofing dataset  containing 50 subjects."}, {"id": "hasy", "name": "HASY", "description": "HASY is a dataset of single symbols similar to MNIST. It contains 168,233 instances of 369 classes. HASY contains two challenges: A classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge."}, {"id": "reside", "name": "RESIDE", "description": "A new large-scale benchmark consisting of both synthetic and real-world hazy images, called REalistic Single Image DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents, and is divided into five subsets, each serving different training or evaluation purposes. "}, {"id": "lakh-pianoroll-dataset", "name": "Lakh Pianoroll Dataset", "description": "The Lakh Pianoroll Dataset (LPD) is a collection of 174,154 multitrack pianorolls derived from the Lakh MIDI Dataset (LMD)."}, {"id": "isarcasm", "name": "iSarcasm", "description": "iSarcasm is a dataset of tweets, each labelled as either sarcastic or non_sarcastic. Each sarcastic tweet is further labelled for one of the following types of ironic speech:"}, {"id": "examplestack", "name": "ExampleStack", "description": "This is a dataset of code snippets in StackOverflow that have been used in Github repositories by extending and adapting them. The dataset links SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. "}, {"id": "multi-task-crowd", "name": "Multi Task Crowd", "description": "Multi Task Crowd is a new 100 image dataset fully annotated for crowd counting, violent behaviour detection and density level classification."}, {"id": "cinic-10", "name": "CINIC-10", "description": "CINIC-10 is a dataset for image classification. It has a total of 270,000 images, 4.5 times that of CIFAR-10. It is constructed from two different sources: ImageNet and CIFAR-10. Specifically, it was compiled as a bridge between CIFAR-10 and ImageNet. It is split into three equal subsets - train, validation, and test - each of which contain 90,000 images."}, {"id": "au-dataset-for-visuo-haptic-object-recognition-for-robots", "name": "AU Dataset for Visuo-Haptic Object Recognition for Robots", "description": "Multimodal object recognition is still an emerging field. Thus, publicly available datasets are still rare and of small size. This dataset was developed to help fill this void and presents multimodal data for 63 objects with some visual and haptic ambiguity. The dataset contains visual, kinesthetic and tactile (audio/vibrations) data. To completely solve sensory ambiguity, sensory integration/fusion would be required. This report describes the creation and structure of the dataset. The first section explains the underlying approach used to capture the visual and haptic properties of the objects. The second section describes the technical aspects (experimental setup) needed for the collection of the data. The third section introduces the objects, while the final section describes the structure and content of the dataset."}, {"id": "vent", "name": "Vent", "description": "The Vent dataset is a large annotated dataset of text, emotions, and social connections. It comprises more than 33 millions of posts by nearly a million of users together with their social connections. Each post has an associated emotion. There are 705 different emotions, organized in 63 \"emotion categories\", forming a two-level taxonomy of affects."}, {"id": "fusedchat", "name": "FusedChat", "description": "FusedChat is an inter-mode dialogue dataset. It contains dialogue sessions fusing task-oriented dialogues (TOD) and open-domain dialogues (ODD). Based on MultiWOZ, FusedChat appends or prepends an ODD to every existing TOD. See more details in the paper."}, {"id": "ut-kinect-utkinect-action3d-dataset", "name": "UT-Kinect (UTKinect-Action3D Dataset)", "description": "The UT-Kinect dataset is a dataset for action recognition from depth sequences. The videos were captured using a single stationary Kinect. There are 10 action types: walk, sit down, stand up, pick up, carry, throw, push, pull, wave hands, clap hands. There are 10 subjects, Each subject performs each actions twice. Three channels were recorded: RGB, depth and skeleton joint locations. The three channel are synchronized. The framerate is 30f/s."}, {"id": "xai-bench", "name": "XAI-Bench", "description": "XAI-Bench is a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets released offer a wide variety of parameters that can be configured to simulate real-world data."}, {"id": "finegym", "name": "FineGym", "description": "FineGym is an action recognition dataset build on top of gymnasium videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a \"balance beam\" event will be annotated as a sequence of elementary sub-actions derived from five sets: \"leap-jumphop\", \"beam-turns\", \"flight-salto\", \"flight-handspring\", and \"dismount\", where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes."}, {"id": "citeworth", "name": "CiteWorth", "description": "CiteWorth is a a large, contextualized, rigorously cleaned labelled dataset for cite-worthiness detection built from a massive corpus of extracted plain-text scientific documents."}, {"id": "cvrptw", "name": "CVRPTW", "description": "Random sampled instances of the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) for 20, 50 and 100 customer nodes."}, {"id": "hyperlex", "name": "HyperLex", "description": "A dataset and evaluation resource that quantifies the extent of of the semantic category membership, that is, type-of relation also known as hyponymy-hypernymy or lexical entailment (LE) relation between 2,616 concept pairs. "}, {"id": "qaconv", "name": "QAConv", "description": "QAConv is a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations including business emails, panel discussions, and work channels. Unlike opendomain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge. In total, we collect 34,204 QA pairs, including span-based, free-form, and unanswerable questions, from 10,259 selected conversations with both human-written and machine-generated questions. We segment long conversations into chunks, and use a question generator and dialogue summarizer as auxiliary tools to collect multi-hop questions. The dataset has two testing scenarios, chunk mode and full mode, depending on whether the grounded chunk is provided or retrieved from a large conversational pool."}, {"id": "obj-mda", "name": "OBJ-MDA", "description": "The dataset contains images of 16 artworks included in the cultural site \u201cGalleria Regionale di Palazzo Bellomo2\u201d. The collection covers different types of artworks, as well as books, sculptures and paintings. The dataset three domains: i) synthetic images generated from a 3D model of the cultural site and automatically labeled during the generation process;  ii) real images collected by 10 visitors with a HoloLens device and manually labeled; iii) realimages collected by the same visitors with a GoPro and manually labeled."}, {"id": "axonem", "name": "AxonEM", "description": "The AxonEM dataset consists of two 30x30x30 um^3 EM image volumes from the human and mouse cortex, respectively. It is used for 3D axon instance segmentation of brain cortical regions. The authors proofread over 18,000 axon instances to provide dense 3D axon instance segmentation, enabling large-scale evaluation of axon reconstruction methods. In addition, the authors also densely annotate nine ground truth subvolumes for training, per each data volume."}, {"id": "mapillary-vistas-dataset", "name": "Mapillary Vistas Dataset", "description": "Mapillary Vistas Dataset is a diverse street-level imagery dataset with pixel\u2011accurate and instance\u2011specific human annotations for understanding street scenes around the world."}, {"id": "grep-biasir-gender-representation-bias-for-information-retrieval", "name": "Grep-BiasIR (Gender Representation-Bias for Information Retrieval)", "description": "Grep-BiasIR is a novel thoroughly-audited dataset which aim to facilitate the studies of gender bias in the retrieved results of IR systems."}, {"id": "sdcnl-suicide-vs-depression-classification", "name": "SDCNL (Suicide vs Depression Classification)", "description": "We develop a primary dataset based on our task of suicide or depression classification. This dataset is web-scraped from Reddit. We collect our data from subreddits using the Python Reddit API. We specifically scrape from two subreddits, r/SuicideWatch3 and r/Depression. The dataset contains 1,895 total posts. We utilize two fields from the scraped data: the original text of the post as our inputs, and the subreddit it belongs to as labels. Posts from r/SuicideWatch are labeled as suicidal, and posts from r/Depression are labeled as depressed. We make this dataset and the web-scraping script available in our code."}, {"id": "sts-benchmark", "name": "STS Benchmark", "description": "STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017. The selection of datasets include text from image captions, news headlines and user forums."}, {"id": "ton-iot", "name": "ToN_IoT", "description": "The TON_IoT datasets are new generations of Internet of Things (IoT) and Industrial IoT (IIoT) datasets for evaluating the fidelity and efficiency of different cybersecurity applications based on Artificial Intelligence (AI). The datasets have been called \u2018ToN_IoT\u2019 as they include heterogeneous data sources collected from Telemetry datasets of IoT and IIoT sensors, Operating systems datasets of Windows 7 and 10 as well as Ubuntu 14 and 18 TLS and Network traffic datasets. The datasets were collected from a realistic and large-scale network designed at the IoT Lab of the UNSW Canberra Cyber, the School of Engineering and Information technology (SEIT), UNSW Canberra @ the Australian Defence Force Academy (ADFA). "}, {"id": "fb15k-freebase-15k", "name": "FB15k (Freebase 15K)", "description": "The FB15k dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. It has a total of  592,213 triplets with 14,951 entities and 1,345 relationships. FB15K-237 is a variant of the original dataset where inverse relations are removed, since it was found that a large number of test triplets could be obtained by inverting triplets in the training set."}, {"id": "collate", "name": "CollATe", "description": "The CollATe dataset is large dataset consisting of two types of collusive entities on YouTube \u2013 videos submitted to gain collusive likes and comment requests, and channels submitted to gain collusive subscriptions."}, {"id": "codesearchnet", "name": "CodeSearchNet", "description": "The CodeSearchNet Corpus is a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. The CodeSearchNet Corpus includes: * Six million methods overall * Two million of which have associated documentation (docstrings, JavaDoc, and more) * Metadata that indicates the original location (repository or line number, for example) where the data was found"}, {"id": "www-crowd", "name": "WWW Crowd", "description": "WWW Crowd provides 10,000 videos with over 8 million frames from 8,257 diverse scenes, therefore offering a comprehensive dataset for the area of crowd understanding."}, {"id": "tour20", "name": "Tour20", "description": "Contains 140 videos with multiple human created summaries, which were acquired in a controlled experiment. "}, {"id": "bafmd-bias-aware-face-mask-detection-dataset", "name": "BAFMD (Bias-Aware Face Mask Detection Dataset)", "description": "BAFMD contains images posted on Twitter during the pandemic from around the world with more images from underrepresented race and age groups to mitigate the problem for the face mask detection task."}, {"id": "talkdown", "name": "TalkDown", "description": "TalkDown is a labelled dataset for condescension detection in context. The dataset is derived from Reddit, a set of online communities that is diverse in content and tone. The dataset is built from COMMENT and REPLY pairs in which the REPLY targets a specific quoted span (QUOTED) in the COMMENT as being condescending. The dataset contains 3,255 positive (condescend) samples and 3,255 negative ones."}, {"id": "convfinqa-conversational-finance-question-answering", "name": "ConvFinQA (Conversational Finance Question Answering)", "description": "ConvFinQA is a dataset designed to study the chain of numerical reasoning in conversational question answering. The dataset contains 3892 conversations containing 14115 questions where 2715 of the conversations are simple conversations, and the rest 1,177 are hybrid conversations."}, {"id": "ipre", "name": "IPRE", "description": "A dataset for inter-personal relationship extraction which aims to facilitate information extraction and knowledge graph construction research. In total, IPRE has over 41,000 labeled sentences for 34 types of relations, including about 9,000 sentences annotated by workers."}, {"id": "meta-dataset", "name": "Meta-Dataset", "description": "The Meta-Dataset benchmark is a large few-shot learning benchmark and consists of multiple datasets of different data distributions. It does not restrict few-shot tasks to have fixed ways and shots, thus representing a more realistic scenario. It consists of 10 datasets from diverse domains: "}, {"id": "trek-150", "name": "TREK-150", "description": "TREK-150 is a benchmark dataset for object tracking in First Person Vision (FPV) videos composed of 150 densely annotated video sequences."}, {"id": "scirex", "name": "SciREX", "description": "SCIREX is a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. The dataset is annotated by integrating automatic and human annotations, leveraging existing scientific knowledge resources."}, {"id": "refmatte-referring-image-matting", "name": "RefMatte (Referring Image Matting)", "description": "RefMatte is the first large-scale challenging dataset under the task referring image matting, generated by a comprehensive image composition and expression generation engine on top of current public high-quality matting foregrounds with flexible logics and re-labelled diverse attributes.  RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions, which can be further extended easily in the future."}, {"id": "oxford-affine", "name": "Oxford-Affine", "description": "The Oxford-Affine dataset is a small dataset containing 8 scenes with sequence of 6 images per scene. The images in a sequence are related by homographies."}, {"id": "paws-x", "name": "PAWS-X", "description": "PAWS-X contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples in PAWS-Wiki."}, {"id": "cite", "name": "CITE", "description": "CITE is a crowd-sourced resource for multimodal discourse: this resource characterises inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations."}, {"id": "iwslt2015", "name": "IWSLT2015", "description": "The IWSLT 2015 Evaluation Campaign featured three tracks: automatic speech recognition (ASR), spoken language translation (SLT), and machine translation (MT). For ASR we offered two tasks, on English and German, while for SLT and MT a number of tasks were proposed, involving English, German, French, Chinese, Czech, Thai, and Vietnamese. All tracks involved the transcription or translation of TED talks, either made available by the official TED website or by other TEDx events. A notable change with respect to previous evaluations was the use of unsegmented speech in the SLT track in order to better fit a real application scenario."}, {"id": "acdc-adverse-conditions-dataset-with-correspondences-adverse-conditions-dataset-with-correspondences", "name": "ACDC (Adverse Conditions Dataset with Correspondences) (Adverse Conditions Dataset with Correspondences)", "description": "We introduce ACDC, the Adverse Conditions Dataset with Correspondences for training and testing semantic segmentation methods on adverse visual conditions. It comprises a large set of 4006 images which are evenly distributed between fog, nighttime, rain, and snow. Each adverse-condition image comes with a high-quality fine pixel-level semantic annotation, a corresponding image of the same scene taken under normal conditions and a binary mask that distinguishes between intra-image regions of clear and uncertain semantic content."}, {"id": "vietnamese-intent-detection-and-slot-filling", "name": "Vietnamese intent detection and slot filling", "description": "This is a dataset for intent detection and slot filling for the Vietnamese language. The dataset consists of 5,871 gold annotated utterances with 28 intent labels and 82 slot types."}, {"id": "ama-articulated-mesh-animation", "name": "AMA (Articulated Mesh Animation)", "description": "Articulated Mesh Animation (AMA) is a real-world dataset containing 10 mesh sequences depicting 3 different humans performing various actions"}, {"id": "fieldsafe", "name": "FieldSAFE", "description": "The FieldSAFE dataset is a multi-modal dataset for obstacle detection in agriculture. It comprises 2 hours of raw sensor data from a tractor-mounted sensor system in a grass mowing scenario in Denmark, October 2016."}, {"id": "bonn-rgb-d-dynamic", "name": "Bonn RGB-D Dynamic", "description": "Bonn RGB-D Dynamic is a dataset for RGB-D SLAM, containing highly dynamic sequences. We provide 24 dynamic sequences, where people perform different tasks, such as manipulating boxes or playing with balloons, plus 2 static sequences. For each scene we provide the ground truth pose of the sensor, recorded with an Optitrack Prime 13 motion capture system. The sequences are in the same format as the TUM RGB-D Dataset, so that the same evaluation tools can be used. Furthermore, we provide a ground truth 3D point cloud of the static environment recorded using a Leica BLK360 terrestrial laser scanner."}, {"id": "clevr-compositional-language-and-elementary-visual-reasoning", "name": "CLEVR (Compositional Language and Elementary Visual Reasoning)", "description": "CLEVR (Compositional Language and Elementary Visual Reasoning) is a synthetic Visual Question Answering dataset. It contains images of 3D-rendered objects; each image comes with a number of highly compositional questions that fall into different categories. Those categories fall into 5 classes of tasks: Exist, Count, Compare Integer, Query Attribute and Compare Attribute. The CLEVR dataset consists of: a training set of 70k images and 700k questions, a validation set of 15k images and 150k questions, A test set of 15k images and 150k questions about objects, answers, scene graphs and functional programs for all train and validation images and questions. Each object present in the scene, aside of position, is characterized by a set of four attributes: 2 sizes: large, small, 3 shapes: square, cylinder, sphere, 2 material types: rubber, metal, 8 color types: gray, blue, brown, yellow, red, green, purple, cyan, resulting in 96 unique combinations."}, {"id": "vocalset-vocalset-a-singing-voice-dataset", "name": "VocalSet (VocalSet: A Singing Voice Dataset)", "description": "VocalSet is a a singing voice dataset consisting of 10.1 hours of monophonic recorded audio of professional singers demonstrating both standard and extended vocal techniques on all 5 vowels. Existing singing voice datasets aim to capture a focused subset of singing voice characteristics, and generally consist of just a few singers. VocalSet contains recordings from 20 different singers (9 male, 11 female) and a range of voice types.  VocalSet aims to improve the state of existing singing voice datasets and singing voice research by capturing not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts."}, {"id": "vqa-hat-vqa-human-attention", "name": "VQA-HAT (VQA Human Attention)", "description": "VQA-HAT (Human ATtention) is a dataset to evaluate the informative regions of an image depending on the question being asked about it. The dataset consists of human visual attention maps over the images in the original VQA dataset. It contains more than 60k attention maps."}, {"id": "vizwiz-priv-visual-privacy-dataset", "name": "VizWiz-Priv (Visual Privacy dataset)", "description": "VizWiz-Priv includes 8,862 regions showing private content across 5,537 images taken by blind people. Of these, 1,403 are paired with questions and 62% of those directly ask about the private content."}, {"id": "a-fb15k237", "name": "A-FB15k237", "description": "This dataset is based on FB15k237 and a pre-trained language-model-based KGE. The main task is to add the new knowledge that the pre-trained model didn't see in the previous training stage. The model can be downloaded from here."}, {"id": "cholect50-cholecystectomy-action-triplet", "name": "CholecT50 (Cholecystectomy Action Triplet)", "description": "CholecT50 is a dataset of endoscopic videos of laparoscopic cholecystectomy surgery introduced to enable research on fine-grained action recognition in laparoscopic surgery. It is annotated with triplet information in the form of <instrument, verb, target>. The dataset is a collection of 50 videos consisting of 45 videos from the Cholec80 dataset  and 5 videos from an in-house dataset of the same surgical procedure."}, {"id": "caltech-256", "name": "Caltech-256", "description": "Caltech-256 is an object recognition dataset containing 30,607 real-world images, of different sizes, spanning 257 classes (256 object classes and an additional clutter class). Each class is represented by at least 80 images. The dataset is a superset of the Caltech-101 dataset."}, {"id": "gqa", "name": "GQA", "description": "The GQA dataset is a large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs. Each training and validation image is also associated with scene graph annotations describing the classes and attributes of those objects in the scene, and their pairwise relations. Along with the images and question-answer pairs, the GQA dataset provides two types of pre-extracted visual features for each image \u2013 convolutional grid features of size 7\u00d77\u00d72048 extracted from a ResNet-101 network trained on ImageNet, and object detection features of size Ndet\u00d72048 (where Ndet is the number of detected objects in each image with a maximum of 100 per image) from a Faster R-CNN detector."}, {"id": "university-1652", "name": "University-1652", "description": "Contains data from three platforms, i.e., synthetic drones, satellites and ground cameras of 1,652 university buildings around the world. University-1652 is a drone-based geo-localization dataset and enables two new tasks, i.e., drone-view target localization and drone navigation. "}, {"id": "diva-hisdb", "name": "DIVA-HisDB", "description": "The database consists of 150 annotated pages of three different medieval manuscripts with challenging layouts. Furthermore, we provide a layout analysis ground-truth which has been iterated on, reviewed, and refined by an expert in medieval studies."}, {"id": "nico", "name": "NICO++", "description": "The goal of NICO Challenge is to facilitate the OOD (Out-of-Distribution) generalization in visual recognition through promoting the research on the intrinsic learning mechanisms with native invariance and generalization ability. The training data is a mixture of several observed contexts while the test data is composed of unseen contexts. Participants are tasked with developing reliable algorithms across different contexts (domains) to improve the generalization ability of models."}, {"id": "darpa", "name": "DARPA", "description": "Darpa is a dataset consisting of communications between source IPs and destination IPs. This dataset contains different attacks between IPs."}, {"id": "10000-people-human-pose-recognition-data", "name": "10,000 People - Human Pose Recognition Data", "description": "Description: 10,000 People - Human Pose Recognition Data. This dataset includes indoor and outdoor scenes.This dataset covers males and females. Age distribution ranges from teenager to the elderly, the middle-aged and young people are the majorities. The data diversity includes different shooting heights, different ages, different light conditions, different collecting environment, clothes in different seasons, multiple human poses. For each subject, the labels of gender, race, age, collecting environment and clothes were annotated. The data can be used for human pose recognition and other tasks."}, {"id": "cas-vsr-w1k-lrw-1000", "name": "CAS-VSR-W1k (LRW-1000)", "description": "LRW-1000 has been renamed as CAS-VSR-W1k.* It is a naturally-distributed large-scale benchmark for word-level lipreading in the wild, including 1000 classes with about 718,018 video samples from more than 2000 individual speakers. There are more than 1,000,000 Chinese character instances in total. Each class corresponds to the syllables of a Mandarin word which is composed by one or several Chinese characters. This dataset aims to cover a natural variability over different speech modes and imaging conditions to incorporate challenges encountered in practical applications."}, {"id": "office-home", "name": "Office-Home", "description": "Office-Home is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art \u2013 artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart \u2013 collection of clipart images; Product \u2013 images of objects without a background and Real-World \u2013 images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class."}, {"id": "torcs-the-open-racing-car-simulator", "name": "TORCS (The Open Racing Car Simulator)", "description": "TORCS (The Open Racing Car Simulator) is a driving simulator. It is capable of simulating the essential elements of vehicular dynamics such as mass, rotational inertia, collision, mechanics of suspensions, links and differentials, friction and aerodynamics. Physics simulation is simplified and is carried out through Euler integration of differential equations at a temporal discretization level of 0.002 seconds. The rendering pipeline is lightweight and based on OpenGL that can be turned off for faster training. TORCS offers a large variety of tracks and cars as free assets. It also provides a number of programmed robot cars with different levels of performance that can be used to benchmark the performance of human players and software driving agents. TORCS was built with the goal of developing Artificial Intelligence for vehicular control and has been used extensively by the machine learning community ever since its inception."}, {"id": "ccmixter", "name": "CCMixter", "description": "CCMixter is a singing voice separation dataset consisting of 50 full-length stereo tracks from ccMixter featuring many different musical genres. For each song there are three WAV files available: the background music, the voice signal, and their sum."}, {"id": "tieredimagenet", "name": "tieredImageNet", "description": "The tieredImageNet dataset is a larger subset of ILSVRC-12 with 608 classes (779,165 images) grouped into 34 higher-level nodes in the ImageNet human-curated hierarchy. This set of nodes is partitioned into 20, 6, and 8 disjoint sets of training, validation, and testing nodes, and the corresponding classes form the respective meta-sets. As argued in Ren et al. (2018), this split near the root of the ImageNet hierarchy results in a more challenging, yet realistic regime with test classes that are less similar to training classes."}, {"id": "yt-ugc-youtube-ugc", "name": "YT-UGC (YouTube UGC)", "description": "YT-UGC is a large scale UGC (User Generated Content) dataset (1,500 20 sec video clips) sampled from millions of YouTube videos. The dataset covers popular categories like Gaming, Sports, and new features like High Dynamic Range (HDR). This dataset can be used to study video compression and quality assessment."}, {"id": "deeperforensics-1-0", "name": "DeeperForensics-1.0", "description": "DeeperForensics-1.0 represents the largest face forgery detection dataset by far, with 60,000 videos constituted by a total of 17.6 million frames, 10 times larger than existing datasets of the same kind. The full dataset includes 48,475 source videos and 11,000 manipulated videos. The source videos are collected on 100 paid and consented actors from 26 countries, and the manipulated videos are generated by a newly proposed many-to-many end-to-end face swapping method, DF-VAE. 7 types of real-world perturbations at 5 intensity levels are employed to ensure a larger scale and higher diversity."}, {"id": "pixelhelp", "name": "PixelHelp", "description": "PixelHelp includes 187 multi-step instructions of 4 task categories deined in https://support.google.com/pixelphone and annotated by human. This dataset includes 88 general tasks, such as configuring accounts, 38 Gmail tasks, 31 Chrome tasks, and 30 Photos related tasks. This dataset is an updated opensource version of the original PixelHelp dataset, which was used for testing the end-to-end grounding quality of the model in paper \"Mapping Natural Language Instructions to Mobile UI Action Sequences\". The similar accuracy is acquired on this version of the dataset."}, {"id": "pemsd4", "name": "PeMSD4", "description": "The dataset refers to the traffic speed data in San Francisco Bay Area, containing 307 sensors on 29 roads. The time span of the dataset is January-February in 2018. It is a popular benchmark for traffic forecasting."}, {"id": "epic-kitchens-55", "name": "EPIC-KITCHENS-55", "description": "The EPIC-KITCHENS-55 dataset comprises a set of 432 egocentric videos recorded by 32 participants in their kitchens at 60fps with a head mounted camera. There is no guiding script for the participants who freely perform activities in kitchens related to cooking, food preparation or washing up among others. Each video is split into short action segments (mean duration is 3.7s) with specific start and end times and a verb and noun annotation describing the action (e.g. \u2018open fridge\u2018). The verb classes are 125 and the noun classes 331. The dataset is divided into one train and two test splits."}, {"id": "mlpf-simulated-particle-level-dataset-of-ttbar-with-pu200-using-pythia8-delphes3-for-machine-learned-particle-flow-mlpf", "name": "MLPF (Simulated particle-level dataset of ttbar with PU200 using Pythia8+Delphes3 for machine learned particle flow (MLPF))", "description": "Dataset of 50,000 top quark-antiquark (ttbar) events produced in proton-proton collisions at 14 TeV, overlaid with minimum bias events corresponding to a pileup of 200 on average. The dataset consists of detector hits as the input, generator particles as the ground truth and reconstructed particles from DELPHES for additional validation. The DELPHES model corresponds to a CMS-like detector with a multi-layered charged particle tracker, an electromagnetic and hadron calorimeter. Pythia8 and Delphes3 were used for the simulation."}, {"id": "arcov19-rumors", "name": "ArCOV19-Rumors", "description": "ArCOV19-Rumors is an Arabic COVID-19 Twitter dataset for misinformation detection composed of tweets containing claims from 27th January till the end of April 2020. "}, {"id": "argoverse-2-lidar", "name": "Argoverse 2 Lidar", "description": "The Argoverse 2 Lidar Dataset is a collection of 20,000 scenarios with lidar sensor data, HD maps, and ego-vehicle pose. It does not include imagery or 3D annotations. The dataset is designed to support research into self-supervised learning in the lidar domain, as well as point cloud forecasting."}, {"id": "aesthetic-visual-analysis", "name": "Aesthetic Visual Analysis", "description": "Aesthetic Visual Analysis is a dataset for aesthetic image assessment that contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style."}, {"id": "coqa-conversational-question-answering-challenge", "name": "CoQA (Conversational Question Answering Challenge)", "description": "CoQA is a large-scale dataset for building Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation."}, {"id": "deepfashion", "name": "DeepFashion", "description": "DeepFashion is a dataset containing around 800K diverse fashion images with their rich annotations (46 categories, 1,000 descriptive attributes, bounding boxes and landmark information) ranging from well-posed product images to real-world-like consumer photos."}, {"id": "cos-e-commonsense-explanations-dataset", "name": "CoS-E (Commonsense Explanations Dataset)", "description": "CoS-E consists of human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations"}, {"id": "semclinbr", "name": "SemClinBr", "description": "A semantically annotated corpus using clinical texts from multiple medical specialties, document types, and institutions."}, {"id": "apnea-ecg-physionet-apnea-ecg-database", "name": "Apnea-ECG (PhysioNet Apnea-ECG Database)", "description": "The data consist of 70 records, divided into a learning set of 35 records (a01 through a20, b01 through b05, and c01 through c10), and a test set of 35 records (x01 through x35), all of which may be downloaded from this page. Recordings vary in length from slightly less than 7 hours to nearly 10 hours each. Each recording includes a continuous digitized ECG signal, a set of apnea annotations (derived by human experts on the basis of simultaneously recorded respiration and related signals), and a set of machine-generated QRS annotations (in which all beats regardless of type have been labeled normal). In addition, eight recordings (a01 through a04, b01, and c01 through c03) are accompanied by four additional signals (Resp C and Resp A, chest and abdominal respiratory effort signals obtained using inductance plethysmography; Resp N, oronasal airflow measured using nasal thermistors; and SpO2, oxygen saturation)."}, {"id": "glint360k", "name": "Glint360K", "description": "The largest and cleanest face recognition dataset Glint360K,  which contains 17,091,657 images of 360,232 individuals, baseline models trained on Glint360K can easily achieve state-of-the-art performance."}, {"id": "cata7", "name": "Cata7", "description": "Cata7 is the first cataract surgical instrument dataset for semantic segmentation. The dataset consists of seven videos while each video records a complete cataract surgery. All videos are from Beijing Tongren Hospital. Each video is split into a sequence of images, where resolution is 1920\u00d71080 pixels. To reduce redundancy, the videos are downsampled from 30 fps to 1 fps. Also, images without surgical instruments are manually removed. Each image is labeled with precise edges and types of surgical instruments. This dataset contains 2,500 images, which are divided into training and test sets. The training set consists of five video sequences and test set consists of two video sequence."}, {"id": "banking77", "name": "BANKING77", "description": "Dataset composed of online banking queries annotated with their corresponding intents."}, {"id": "x-fact", "name": "X-Fact", "description": "X-FACT is a large publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models."}, {"id": "textseg", "name": "TextSeg", "description": "TextSeg is a large-scale fine-annotated and multi-purpose text detection and segmentation dataset, collecting scene and design text with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions."}, {"id": "friedman1", "name": "Friedman1", "description": "The friedman1 data set is commonly used to test semi-supervised regression methods."}, {"id": "gtsrb-german-traffic-sign-recognition-benchmark", "name": "GTSRB (German Traffic Sign Recognition Benchmark)", "description": "The German Traffic Sign Recognition Benchmark (GTSRB) contains 43 classes of traffic signs, split into 39,209 training images and 12,630 test images. The images have varying light conditions and rich backgrounds."}, {"id": "benchie", "name": "BenchIE", "description": "BenchIE: a benchmark and evaluation framework for comprehensive evaluation of OIE systems for English, Chinese and German. In contrast to existing OIE benchmarks, BenchIE takes into account informational equivalence of extractions: our gold standard consists of fact synsets, clusters in which we exhaustively list all surface forms of the same fact."}, {"id": "kitti-depth", "name": "KITTI-Depth", "description": "The KITTI-Depth dataset includes depth maps from projected LiDAR point clouds that were matched against the depth estimation from the stereo cameras. The depth images are highly sparse with only 5% of the pixels available and the rest is missing. The dataset has 86k training images, 7k validation images, and 1k test set images on the benchmark server with no access to the ground truth."}, {"id": "kolektorsdd2-kolektor-surface-defect-dataset-2", "name": "KolektorSDD2 (Kolektor Surface-Defect Dataset 2)", "description": "KolektorSDD2 is a surface-defect detection dataset with over 3000 images containing several types of defects, obtained while addressing a real-world industrial problem."}, {"id": "msu-mfsd", "name": "MSU-MFSD", "description": "The MSU-MFSD dataset contains 280 video recordings of genuine and attack faces. 35 individuals have participated in the development of this database with a total of 280 videos. Two kinds of cameras with different resolutions (720\u00d7480 and 640\u00d7480) were used to record the videos from the 35 individuals. For the real accesses, each individual has two video recordings captured with the Laptop cameras and Android, respectively. For the video attacks, two types of cameras, the iPhone and Canon cameras were used to capture high definition videos on each of the subject. The videos taken with Canon camera were then replayed on iPad Air screen to generate the HD replay attacks while the videos recorded by the iPhone mobile were replayed itself to generate the mobile replay attacks. Photo attacks were produced by printing the 35 subjects\u2019 photos on A3 papers using HP colour printer. The recording videos with respect to the 35 individuals were divided into training (15 subjects with 120 videos) and testing (40 subjects with 160 videos) datasets, respectively."}, {"id": "lra-long-range-arena", "name": "LRA (Long-Range Arena)", "description": "Long-range arena (LRA) is an effort toward systematic evaluation of efficient transformer models. The project aims at establishing benchmark tasks/datasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc. Long-Range Arena is specifically focused on evaluating model quality under long-context scenarios. The benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning."}, {"id": "flyingchairs", "name": "FlyingChairs", "description": "The \"Flying Chairs\" are a synthetic dataset with optical flow ground truth. It consists of 22872 image pairs and corresponding flow fields. Images show renderings of 3D chair models moving in front of random backgrounds from Flickr. Motions of both the chairs and the background are purely planar."}, {"id": "genericskb", "name": "GenericsKB", "description": "The GenericsKB contains 3.4M+ generic sentences about the world, i.e., sentences expressing general truths such as \"Dogs bark,\" and \"Trees remove carbon dioxide from the atmosphere.\" Generics are potentially useful as a knowledge source for AI systems requiring general world knowledge. The GenericsKB is the first large-scale resource containing naturally occurring generic sentences (as opposed to extracted or crowdsourced triples), and is rich in high-quality, general, semantically complete statements. Generics were primarily extracted from three large text sources, namely the Waterloo Corpus, selected parts of Simple Wikipedia, and the ARC Corpus. A filtered, high-quality subset is also available in GenericsKB-Best, containing 1,020,868 sentences."}, {"id": "govreport", "name": "GovReport", "description": "GovReport is a dataset for long document summarization, with significantly longer documents and summaries. It consists of reports written by government research agencies including Congressional Research Service and U.S. Government Accountability Office."}, {"id": "unimorph-4-0-universal-morphology", "name": "UniMorph 4.0 (Universal Morphology)", "description": "The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology in the world\u2019s languages. The goal of UniMorph is to annotate morphological data in a universal schema that allows an inflected word from any language to be defined by its lexical meaning, typically carried by the lemma, and by a rendering of its inflectional form in terms of a bundle of morphological features from our schema. The specification of the schema is described here and in Sylak-Glassman (2016)."}, {"id": "covidqa", "name": "CovidQA", "description": "The beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. "}, {"id": "udiva", "name": "UDIVA", "description": "UDIVA is a new non-acted dataset of face-to-face dyadic interactions, where interlocutors perform competitive and collaborative tasks with different behavior elicitation and cognitive workload. The dataset consists of 90.5 hours of dyadic interactions among 147 participants distributed in 188 sessions, recorded using multiple audiovisual and physiological sensors. Currently, it includes sociodemographic, self and peer-reported personality, internal state, and relationship profiling from participants. "}, {"id": "howto100m", "name": "HowTo100M", "description": "HowTo100M is a large-scale dataset of narrated videos with an emphasis on instructional videos where content creators teach complex tasks with an explicit intention of explaining the visual content on screen. HowTo100M features a total of:"}, {"id": "lc25000-lung-and-colon-histopathological-image-dataset", "name": "LC25000 (Lung And Colon Histopathological Image Dataset)", "description": "The LC25000 dataset contains 25,000 color images with 5 classes of 5,000 images each. All images are 768 x 768 pixels in size and are in jpeg file format. The 5 classes are: colon adenocarcinomas, benign colonic tissues, lung adenocarcinomas, lung squamous cell carcinomas and bening lung tissues."}, {"id": "inquisitive", "name": "INQUISITIVE", "description": "A dataset of ~19K questions that are elicited while a person is reading through a document."}, {"id": "antilles-antilles-an-open-french-linguistically-enriched-part-of-speech-corpus", "name": "ANTILLES (ANTILLES: An Open French Linguistically Enriched Part-of-Speech Corpus)", "description": "ANTILLES is a part-of-speech tagging corpus based on UD_French-GSD which was originally created in 2015 and is based on the universal dependency treebank v2.0."}, {"id": "spot-the-diff", "name": "Spot-the-diff", "description": "Spot-the-diff is a dataset consisting of 13,192 image pairs along with corresponding human provided text annotations stating the differences between the two images."}, {"id": "icdar-2015", "name": "ICDAR 2015", "description": "ICDAR 2015 was a scene text detection used for the ICDAR 2015 conference."}, {"id": "open-entity", "name": "Open Entity", "description": "The Open Entity dataset is a collection of about 6,000 sentences with fine-grained entity types annotations. The entity types are free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Sentences were sampled from Gigaword, OntoNotes and web articles. On average each sentence has 5 labels."}, {"id": "shifts-weather", "name": "Shifts-Weather", "description": "A dataset of real distributional shift across multiple large-scale tasks."}, {"id": "m5product", "name": "M5Product", "description": "The M5Product dataset is a large-scale multi-modal pre-training dataset with coarse and fine-grained annotations for E-products."}, {"id": "mad", "name": "MAD", "description": "MAD (Movie Audio Descriptions) is an automatically curated large-scale dataset for the task of natural language grounding in videos or natural language moment retrieval. MAD exploits available audio descriptions of mainstream movies. Such audio descriptions are redacted for visually impaired audiences and are therefore highly descriptive of the visual content being displayed.  MAD contains over 384,000 natural language sentences grounded in over 1,200 hours of video, and provides a unique setup for video grounding as the visual stream is truly untrimmed with an average video duration of 110 minutes. 2 orders of magnitude longer than legacy datasets. "}, {"id": "sydney-urban-objects", "name": "Sydney Urban Objects", "description": "This dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR, collected in the CBD of Sydney, Australia. There are 631 individual scans of objects across classes of vehicles, pedestrians, signs and trees."}, {"id": "railsem19-railsem19-a-dataset-for-semantic-rail-scene-understanding", "name": "RailSem19 (RailSem19: A Dataset for Semantic Rail Scene Understanding)", "description": "RailSem19 offers 8500 unique images taken from a the ego-perspective of a rail vehicle (trains and trams). Extensive semantic annotations are provided, both geometry-based (rail-relevant polygons, all rails as polylines) and dense label maps with many Cityscapes-compatible road labels. Many frames show areas of intersection between road and rail vehicles (railway crossings, trams driving on city streets). RailSem19 is usefull for rail applications and road applications alike."}, {"id": "crc", "name": "CRC", "description": "Request access: cadpath.ai@impdiagnostics.com"}, {"id": "edub-seg-egocentric-dataset-of-the-university-of-barcelona-segmentation", "name": "EDUB-Seg (Egocentric Dataset of the University of Barcelona \u2013 Segmentation)", "description": "Egocentric Dataset of the University of Barcelona \u2013 Segmentation (EDUB-Seg) is a dataset for egocentric event segmentation acquired by the Narrative Clip, which takes a picture every 30 seconds. The dataset contains a total of 18,735 images captured by 7 different users during overall 20 days. To ensure diversity, all users were wearing the camera in different contexts: while attending a conference, on holiday, during the weekend, and during the week."}, {"id": "c3", "name": "C3", "description": "C3 is a free-form multiple-Choice Chinese machine reading Comprehension dataset."}, {"id": "ionosphere", "name": "ionosphere", "description": "The original ionosphere dataset from UCI machine learning repository is a binary classification dataset with dimensionality 34. There is one attribute having values all zeros, which is discarded. So the total number of dimensions are 33. The \u2018bad\u2019 class is considered as outliers class and the \u2018good\u2019 class as inliers."}, {"id": "recon-recon-outdoor-navigation-dataset", "name": "RECON (RECON Outdoor Navigation Dataset)", "description": "https://sites.google.com/view/recon-robot/dataset"}, {"id": "anthroprotect", "name": "AnthroProtect", "description": "For a detailed description, we refer to Section 3 in our research article."}, {"id": "phy-q", "name": "Phy-Q", "description": "Phy-Q is a benchmark that requires an agent to reason about physical scenarios and take an action accordingly. Inspired by the physical knowledge acquired in infancy and the capabilities required for robots to operate in real-world environments, the authors identify 15 essential physical scenarios. For each scenario, a wide variety of distinct task templates are created, and all the task templates within the same scenario can be solved by using one specific physical rule. "}, {"id": "sen12ms-cr-ts", "name": "SEN12MS-CR-TS", "description": "SEN12MS-CR-TS is a multi-modal and multi-temporal data set for cloud removal. It contains time-series of paired and co-registered Sentinel-1 and cloudy as well as cloud-free Sentinel-2 data from European Space Agency's Copernicus mission. Each time series contains 30 cloudy and clear observations regularly sampled throughout the year 2018. Our multi-temporal data set is readily pre-processed and backward-compatible with SEN12MS-CR."}, {"id": "icfg-pedes-identity-centric-and-fine-grained-person-description-dataset", "name": "ICFG-PEDES (Identity-Centric and Fine-Grained Person Description Dataset)", "description": "One large-scale database for Text-to-Image Person Re-identification, i.e., Text-based Person Retrieval. "}, {"id": "wdc-products", "name": "WDC Products", "description": "WDC Products is an entity matching benchmark which provides for the systematic evaluation of matching systems along combinations of three dimensions while relying on real-word data. The three dimensions are "}, {"id": "mcmaster", "name": "McMaster", "description": "The McMaster dataset is a dataset for color demosaicing, which contains 18 cropped images of size 500\u00d7500."}, {"id": "jw300", "name": "JW300", "description": "A parallel corpus of over 300 languages with around 100 thousand parallel sentences per language pair on average."}, {"id": "askparents", "name": "AskParents", "description": "AskParents is a dataset for advice classification extracted from Reddit. In this dataset, posts are annotated for whether they contain advice or not. It contains 8,701 samples for training, 802 for validation and 1,091 for testing."}, {"id": "robust04", "name": "Robust04", "description": "The goal of the Robust track is to improve the consistency of retrieval technology by focusing on poorly performing topics. In addition, the track brings back a classic, ad hoc retrieval task in TREC that provides a natural home for new participants. An ad hoc task in TREC investigates the performance of systems that search a static set of documents using previously-unseen topics. For each topic, participants create a query and submit a ranking of the top 1000 documents for that topic."}, {"id": "adima", "name": "ADIMA", "description": "ADIMA is a novel, linguistically diverse, ethically sourced, expert annotated and well-balanced multilingual profanity detection audio dataset comprising of 11,775 audio samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique users."}, {"id": "justice-justice-a-dataset-for-supreme-courts-judgment-prediction", "name": "JUSTICE (JUSTICE: A Dataset for Supreme Court\u2019s Judgment Prediction)", "description": "The dataset contains 3304 cases from the Supreme Court of the United States from 1955 to 2021. Each case has the case's identifiers as well as the facts of the case and the decision outcome. Other related datasets rarely included the facts of the case which could prove to be helpful in natural language processing applications. One potential use case of this dataset is determining the outcome of a case using its facts."}, {"id": "adobevfr-real-adobe-visual-font-recognition-real-world-images-dataset", "name": "AdobeVFR real (Adobe Visual Font Recognition real-world images dataset)", "description": "Subset of AdobeVFR. The dataset contains \"real-world text images\"."}, {"id": "actorshift", "name": "ActorShift", "description": "ActorShift is a dataset where the domain shift comes from the change in actor species: we use humans in the source domain and animals in the target domain. This causes large variances in the appearance and motion of activities. For the corresponding dataset we select 1,305 videos of 7 human activity classes from Kinetics-700 as the source domain: sleeping, watching tv, eating, drinking, swimming, running and opening a door. For the target domain we collect 200 videos from YouTube of animals performing the same activities. We divide them into 35 videos for training (5 per class) and 165 for evaluation. The target domain data is scarce, meaning there is the additional challenge of adapting to the target domain with few unlabeled examples."}, {"id": "bp4d", "name": "BP4D", "description": "The BP4D-Spontaneous dataset is a 3D video database of spontaneous facial expressions in a diverse group of young adults. Well-validated emotion inductions were used to elicit expressions of emotion and paralinguistic communication. Frame-level ground-truth for facial actions was obtained using the Facial Action Coding System. Facial features were tracked in both 2D and 3D domains using both person-specific and generic approaches. The database includes forty-one participants (23 women, 18 men). They were 18 \u2013 29 years of age; 11 were Asian, 6 were African-American, 4 were Hispanic, and 20 were Euro-American.  An emotion elicitation protocol was designed to elicit emotions of participants effectively. Eight tasks were covered with an interview process and a series of activities to elicit eight emotions. The database is structured by participants. Each participant is associated with 8 tasks. For each task, there are both 3D and 2D videos. As well, the Metadata include manually annotated action units (FACS AU), automatically tracked head pose, and 2D/3D facial landmarks.  The database is in the size of about 2.6TB (without compression)."}, {"id": "3dident", "name": "3DIdent", "description": "Novel benchmark which features aspects of natural scenes, e.g. a complex 3D object and different lighting conditions, while still providing access to the continuous ground-truth factors."}, {"id": "k-lane-kaist-lane", "name": "K-Lane (KAIST-Lane)", "description": "KAIST-Lane (K-Lane) is the world\u2019s first and the largest public urban road and highway lane dataset for Lidar. K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions, e.g., occluded roads of multiple occlusion levels, roads at day and night times, merging (converging and diverging) and curved lanes."}, {"id": "gun-detection-dataset", "name": "Gun Detection Dataset", "description": "This is a gun detection dataset with 51K annotated gun images for gun detection and other 51K cropped gun chip images for gun classification collected from a few different sources."}, {"id": "cityscapes-vps", "name": "Cityscapes-VPS", "description": "Cityscapes-VPS is a video extension of the Cityscapes validation split. It provides 2500-frame panoptic labels that temporally extend the 500 Cityscapes image-panoptic labels. There are total 3000-frame panoptic labels which correspond to 5, 10, 15, 20, 25, and 30th frames of each 500 videos, where all instance ids are associated over time. It not only supports video panoptic segmentation (VPS) task, but also provides super-set annotations for video semantic segmentation (VSS) and video instance segmentation (VIS) tasks."}, {"id": "phspd-polarization-human-shape-and-pose-dataset", "name": "PHSPD (Polarization Human Shape and Pose Dataset)", "description": "PHSPD is a home-grown polarization image dataset of various human shapes and poses."}, {"id": "aishell-2", "name": "AISHELL-2", "description": "AISHELL-2 contains 1000 hours of clean read-speech data from iOS is free for academic usage. "}, {"id": "eedi-dataset", "name": "Eedi Dataset", "description": "The Eedi dataset contains from two school years (September 2018 to May 2020) of students\u2019 answers to mathematics questions from Eedi, a leading educational platform which millions of students interact with daily around the globe. Eedi offers diagnostic questions to students from primary to high school (roughly between 7 and 18 years old). Each diagnostic question is a multiple-choice question with 4 possible answer choices, exactly one of which is correct. Currently, the platform mainly focuses on mathematics questions."}, {"id": "animal-kingdom", "name": "Animal Kingdom", "description": "Animal Kingdom is a large and diverse dataset that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors. The wild animal footage used in the dataset records different times of the day in an extensive range of environments containing variations in backgrounds, viewpoints, illumination and weather conditions. More specifically, the dataset contains 50 hours of annotated videos to localize relevant animal behavior segments in long videos for the video grounding task, 30K video sequences for the fine-grained multi-label action recognition task, and 33K frames for the pose estimation task, which correspond to a diverse range of animals with 850 species across 6 major animal classes."}, {"id": "timehetnet-meta-dataset-for-time-series-with-heterogeneous-networks", "name": "TimeHetNet (Meta Dataset for Time Series with heterogeneous networks)", "description": "This meta-dataset is composed of previously known datasets. "}, {"id": "wsj0-2mix", "name": "WSJ0-2mix", "description": "WSJ0-2mix is a speech recognition corpus of speech mixtures using utterances from the Wall Street Journal (WSJ0) corpus."}, {"id": "uit-viic", "name": "UIT-ViIC", "description": "UIT-ViIC contains manually written captions for images from Microsoft COCO dataset relating to sports played with ball. UIT-ViIC consists of 19,250 Vietnamese captions for 3,850 images."}, {"id": "talking-with-hands-16-2m", "name": "Talking With Hands 16.2M", "description": "This is a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. This dataset features synchronized body and finger motion as well as audio data. It represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis."}, {"id": "covidgr", "name": "COVIDGR", "description": "Under a close collaboration with an expert radiologist team of the Hospital Universitario San Cecilio, the COVIDGR-1.0 dataset of patients' anonymized X-ray images has been built. 852 images have been collected following a strict labeling protocol. They are categorized into 426 positive cases and 426 negative cases. Positive images correspond to patients who have been tested positive for COVID-19 using RT-PCR within a time span of at most 24h between the X-ray image and the test. Every image has been taken using the same type of equipment and with the same format: only the posterior-anterior view is considered."}, {"id": "slakh2100-synthesized-lakh-dataset", "name": "Slakh2100 (Synthesized Lakh Dataset)", "description": "The Synthesized Lakh (Slakh) Dataset is a dataset for audio source separation that is synthesized from the Lakh MIDI Dataset v0.1 using professional-grade sample-based virtual instruments. This first release of Slakh, called Slakh2100, contains 2100 automatically mixed tracks and accompanying MIDI files synthesized using a professional-grade sampling engine. The tracks in Slakh2100 are split into training (1500 tracks), validation (375 tracks), and test (225 tracks) subsets, totaling 145 hours of mixtures."}, {"id": "aquaint", "name": "AQUAINT", "description": "The AQUAINT Corpus consists of newswire text data in English, drawn from three sources: the Xinhua News Service (People's Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. It was prepared by the LDC for the AQUAINT Project, and will be used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST)."}, {"id": "realnews", "name": "RealNews", "description": "RealNews is a large corpus of news articles from Common Crawl. Data is scraped from Common Crawl, limited to the 5000 news domains indexed by Google News. The authors used the Newspaper Python library to extract the body and metadata from each article. News from Common Crawl dumps from December 2016 through March 2019 were used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, RealNews is 120 gigabytes without compression."}, {"id": "swde-structured-web-data-extraction", "name": "SWDE (Structured Web Data Extraction)", "description": "This dataset is a real-world web page collection used for research on the automatic extraction of structured data (e.g., attribute-value pairs of entities) from the Web. We hope it could serve as a useful benchmark for evaluating and comparing different methods for structured web data extraction."}, {"id": "arzen-corpus-of-egyptian-arabic-english-code-switching", "name": "ArzEn (Corpus of Egyptian Arabic-English Code-switching)", "description": "Corpus of Egyptian Arabic-English Code-switching (ArzEn) is a spontaneous conversational speech corpus, obtained through informal interviews held at the German University in Cairo. The participants discussed broad topics, including education, hobbies, work, and life experiences. The corpus currently contains 12 hours of speech, having 6,216 utterances. The recordings were transcribed and translated into monolingual Egyptian Arabic and monolingual English."}, {"id": "fewrel-few-shot-relation-classification-dataset", "name": "FewRel (Few-Shot Relation Classification Dataset)", "description": "The FewRel (Few-Shot Relation Classification Dataset) contains 100 relations and 70,000 instances from Wikipedia. The dataset is divided into three subsets: training set (64 relations), validation set (16 relations) and test set (20 relations)."}, {"id": "aesthetics-text-corpus", "name": "Aesthetics Text Corpus", "description": "An exhaustive list of stop lemmas created from 12 corpora across multiple domains, consisting of over 13 million words, from which more than 200,000 lemmas were generated, and 11 publicly available stop word lists comprising over 1000 words, from which nearly 400 unique lemmas were generated."}, {"id": "rsicd-remote-sensing-image-captioning-dataset", "name": "RSICD (Remote Sensing Image Captioning Dataset)", "description": "The Remote Sensing Image Captioning Dataset (RSICD) is a dataset for remote sensing image captioning task. It contains more than ten thousands remote sensing images which are collected from Google Earth, Baidu Map, MapABC and Tianditu. The images are fixed to 224X224 pixels with various resolutions. The total number of remote sensing images is 10921, with five sentences descriptions per image."}, {"id": "qa2d-question-to-declarative-sentence-qa2d-dataset", "name": "QA2D (Question to Declarative Sentence (QA2D) Dataset)", "description": "The Question to Declarative Sentence (QA2D) Dataset contains 86k question-answer pairs and their manual transformation into declarative sentences. 95% of question answer pairs come from SQuAD (Rajkupar et al., 2016) and the remaining 5% come from four other question answering datasets."}, {"id": "motionsense", "name": "MotionSense", "description": "This dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket using SensingKit which collects information from Core Motion framework on iOS devices. All data is collected in 50Hz sample rate. A total of 24 participants in a range of gender, age, weight, and height performed 6 activities in 15 trials in the same environment and conditions: downstairs, upstairs, walking, jogging, sitting, and standing."}, {"id": "refer-youtube-vos", "name": "Refer-YouTube-VOS", "description": "There exist previous works [6, 10] that constructed referring segmentation datasets for videos. Gavrilyuk et al. [6] extended the A2D [33] and J-HMDB [9] datasets with natural sentences; the datasets focus on describing the \u2018actors\u2019 and \u2018actions\u2019 appearing in videos, therefore the instance annotations are limited to only a few object categories corresponding to the dominant \u2018actors\u2019 performing a salient \u2018action\u2019. Khoreva et al. [10] built a dataset based on DAVIS [25], but the scales are barely sufficient to learn an end-to-end model from scratch"}, {"id": "coda-the-color-dataset", "name": "CoDa (The Color Dataset)", "description": "The Color Dataset (CoDa) is a probing dataset to evaluate the representation of visual properties in language models. CoDa consists of color distributions for 521 common objects, which are split into 3 groups: Single, Multi, and Any. "}, {"id": "sqa3d-situated-question-answering-in-3d-scenes", "name": "SQA3D (Situated Question Answering in 3D Scenes)", "description": "SQA3D is a dataset for embodied scene understanding, where an agent needs to comprehend the scene it situates  from an first person's perspective and answer questions. The questions are designed to be situated, embodied and knowledge-intensive. We offer three different modalities to represent a 3D scene: 3D scan, egocentric video and BEV picture."}, {"id": "pmlb-penn-machine-learning-benchmarks", "name": "PMLB (Penn Machine Learning Benchmarks)", "description": "The Penn Machine Learning Benchmarks (PMLB) is a large, curated set of benchmark datasets used to evaluate and compare supervised machine learning algorithms. These datasets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features."}, {"id": "youcook", "name": "YouCook", "description": "This data set was prepared from 88 open-source YouTube cooking videos. The YouCook dataset contains videos of people cooking various recipes. The videos were downloaded from YouTube and are all in the third-person viewpoint; they represent a significantly more challenging visual problem than existing cooking and kitchen datasets (the background kitchen/scene is different for many and most videos have dynamic camera changes). In addition, frame-by-frame object and action annotations are provided for training data (as well as a number of precomputed low-level features). Finally, each video has a number of human provided natural language descriptions (on average, there are eight different descriptions per video). This dataset has been created to serve as a benchmark in describing complex real-world videos with natural language descriptions."}, {"id": "wmca-wide-multi-channel-presentation-attack", "name": "WMCA (Wide Multi Channel Presentation Attack)", "description": "The Wide Multi Channel Presentation Attack (WMCA) database consists of 1941 short video recordings of both bonafide and presentation attacks from 72 different identities. The data is recorded from several channels including color, depth, infra-red, and thermal."}, {"id": "dpd-dual-view-image-defocus-deblurring", "name": "DPD (Dual-view) (Image Defocus Deblurring)", "description": "DPD dataset has two versions - single view and dual-view. This branch is for dual view benchmark evaluation."}, {"id": "opiec-open-information-extraction-corpus", "name": "OPIEC (Open Information Extraction Corpus)", "description": "OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia. It containing more than 341M triples. Each triple from the corpus is composed of rich meta-data: each token from the subj / obj / rel along with NLP annotations (POS tag, NER tag, ...), provenance sentence (along with its dependency parse, sentence order relative to the article), original (golden) links contained in the Wikipedia articles, space / time."}, {"id": "mpi-faust-dataset", "name": "MPI FAUST Dataset", "description": "Contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology."}, {"id": "publaynet", "name": "PubLayNet", "description": "PubLayNet is a dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated."}, {"id": "ade20k", "name": "ADE20K", "description": "The ADE20K semantic segmentation dataset contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels. There are totally 150 semantic categories, which include stuffs like sky, road, grass, and discrete objects like person, car, bed."}, {"id": "antibody-watch", "name": "Antibody Watch", "description": "Antibody Watch is a dataset of text snippets extracted from over 2000 PubMed articles with annotations denoting specificity of antibodies."}, {"id": "open-buildings", "name": "Open Buildings", "description": "Building footprints are useful for a range of important applications, from population estimation, urban planning and humanitarian response, to environmental and climate science. This large-scale open dataset contains the outlines of buildings derived from high-resolution satellite imagery in order to support these types of uses. The project being based in Ghana, the current focus is on the continent of Africa."}, {"id": "cdtb-color-and-depth-tracking", "name": "CDTB (Color-and-Depth Tracking)", "description": "dataset is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The sequences were recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. It contains around 100,000 samples.)"}, {"id": "conll-2012", "name": "CoNLL-2012", "description": "The CoNLL-2012 shared task involved predicting coreference in English, Chinese, and Arabic, using the final version, v5.0, of the OntoNotes corpus. It was a follow-on to the English-only task organized in 2011."}, {"id": "photi-lakeice", "name": "Photi-LakeIce", "description": "A new benchmark dataset of webcam images, Photi-LakeIce, from multiple cameras and two different winters, along with pixel-wise ground truth annotations. "}, {"id": "modanet", "name": "ModaNet", "description": "ModaNet is a street fashion images dataset consisting of annotations related to RGB images. ModaNet provides multiple polygon annotations for each image. Each polygon is associated with a label from 13 meta fashion categories. The annotations are based on images in the PaperDoll image set, which has only a few hundred images annotated by the superpixel-based tool."}, {"id": "interspeech-2021-deep-noise-suppression-challenge", "name": "Interspeech 2021 Deep Noise Suppression Challenge", "description": "The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality."}, {"id": "lfsd-light-field-saliency-database", "name": "LFSD (Light Field Saliency Database)", "description": "The Light Field Saliency Database (LFSD) contains 100 light fields with 360\u00d7360 spatial resolution. A rough focal stack and an all-focus image are provided for each light field. The images in this dataset usually have one salient foreground object and a background with good color contrast."}, {"id": "cxc-crisscrossed-captions", "name": "CxC (Crisscrossed Captions)", "description": "Crisscrossed Captions (CxC) contains 247,315 human-labeled annotations including positive and negative associations between image pairs, caption pairs and image-caption pairs."}, {"id": "era-event-recognition-in-aerial-videos", "name": "ERA (Event Recognition in Aerial videos)", "description": "Consists of 2,864 videos each with a label from 25 different classes corresponding to an event unfolding 5 seconds. The ERA dataset is designed to have a significant intra-class variation and inter-class similarity and captures dynamic events in various circumstances and at dramatically various scales."}, {"id": "paws-paraphrase-adversaries-from-word-scrambling", "name": "PAWS (Paraphrase Adversaries from Word Scrambling)", "description": "Paraphrase Adversaries from Word Scrambling (PAWS) is a dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification. The dataset has two subsets, one based on Wikipedia and the other one based on the Quora Question Pairs (QQP) dataset."}, {"id": "simmc-situated-and-interactive-multimodal-conversations", "name": "SIMMC (Situated and Interactive Multimodal Conversations)", "description": "Situated Interactive MultiModal Conversations (SIMMC) is the task of taking multimodal actions grounded in a co-evolving multimodal input content in addition to the dialog history. This dataset contains two SIMMC datasets totalling ~13K human-human dialogs (~169K utterances) using a multimodal Wizard-of-Oz (WoZ) setup, on two shopping domains: (a) furniture (grounded in a shared virtual environment) and (b) fashion (grounded in an evolving set of images)."}, {"id": "tatoeba", "name": "Tatoeba", "description": "The Tatoeba dataset consists of up to 1,000 English-aligned sentence pairs covering 122 languages."}, {"id": "rnadesign", "name": "RNADesign", "description": "An environment for RNA design given structure constraints with structures from different datasets to choose from."}, {"id": "resisc45", "name": "RESISC45", "description": "RESISC45 dataset is a dataset for Remote Sensing Image Scene Classification (RESISC). It contains 31,500 RGB images of size 256\u00d7256 divided into 45 scene classes, each class containing 700 images. Among its notable features, RESISC45 contains varying spatial resolution ranging from 20cm to more than 30m/px."}, {"id": "60k-stack-overflow-questions-60k-stack-overflow-questions-from-2016-2020-classified-into-three-categories-based-on-their-quality", "name": "60k Stack Overflow Questions (60k Stack Overflow Questions from 2016-2020 classified into three categories based on their quality)", "description": "The dataset contains 60,000 Stack Overflow questions from 2016-2020, classified into three categories: "}, {"id": "vindr-ribcxr", "name": "VinDr-RibCXR", "description": "VinDr-RibCXR is a benchmark dataset for automatic segmentation and labeling of individual ribs from chest X-ray (CXR) scans. The VinDr-RibCXR contains 245 CXRs with corresponding ground truth annotations provided by human experts."}, {"id": "ephoie-phtnsantader-gmail-com", "name": "EPHOIE (phtnsantader@gmail.com)", "description": "EPHOIE is a fully-annotated dataset which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. "}, {"id": "dynasent", "name": "DynaSent", "description": "DynaSent is an English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers."}, {"id": "cuhk-avenue", "name": "CUHK Avenue", "description": "Avenue Dataset contains 16 training and 21 testing video clips. The videos are captured in CUHK campus avenue with 30652 (15328 training, 15324 testing) frames in total."}, {"id": "arsarcasm-v2", "name": "ArSarcasm-v2", "description": "ArSarcasm-v2 is an extension of the original ArSarcasm dataset published along with the paper From Arabic Sentiment Analysis to Sarcasm Detection: The ArSarcasm Dataset. ArSarcasm-v2 conisists of ArSarcasm along with portions of DAICT corpus and some new tweets. Each tweet was annotated for sarcasm, sentiment and dialect. The final dataset consists of 15,548 tweets divided into 12,548 training tweets and 3,000 testing tweets. ArSarcasm-v2 was used and released as a part of the shared task on sarcasm detection and sentiment analysis in Arabic."}, {"id": "dibco-2011", "name": "DIBCO 2011", "description": "DIBCO 2011 is the International Document Image Binarization Contest organized in the context of ICDAR 2011 conference. The general objective of the contest is to identify current advances in document image binarization for both machine-printed and handwritten document images using evaluation performance measures that conform to document image analysis and recognition."}, {"id": "libri-adhoc40", "name": "Libri-adhoc40", "description": "Libri-adhoc40 is a synchronized speech corpus which collects the replayed Librispeech data from loudspeakers by ad-hoc microphone arrays of 40 strongly synchronized distributed nodes in a real office environment. Besides, to provide the evaluation target for speech frontend processing and other applications, the authors also recorded the replayed speech in an anechoic chamber."}, {"id": "kaggle-eyepacs-kaggle-eyepacs-diabetic-retinopathy-detection-identify-signs-of-diabetic-retinopathy-in-eye-images", "name": "Kaggle EyePACS (Kaggle EyePACS. Diabetic Retinopathy Detection Identify signs of diabetic retinopathy in eye images)", "description": "Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people."}, {"id": "alchemy", "name": "Alchemy", "description": "The DeepMind Alchemy environment is a meta-reinforcement learning benchmark that presents tasks sampled from a task distribution with deep underlying structure. It was created to test for the ability of agents to reason and plan via latent state inference, as well as useful exploration and experimentation. "}, {"id": "infographicvqa", "name": "InfographicVQA", "description": "InfographicVQA is a dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills."}, {"id": "natural-questions", "name": "Natural Questions", "description": "The Natural Questions corpus is a question answering dataset containing 307,373 training examples, 7,830 development examples, and 7,842 test examples. Each example is comprised of a google.com query and a corresponding Wikipedia page. Each Wikipedia page has a passage (or long answer) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer. The long and the short answer annotations can however be empty. If they are both empty, then there is no answer on the page at all. If the long answer annotation is non-empty, but the short answer annotation is empty, then the annotated passage answers the question but no explicit short answer could be found. Finally 1% of the documents have a passage annotated with a short answer that is \u201cyes\u201d or \u201cno\u201d, instead of a list of short spans."}, {"id": "omniflow", "name": "OmniFlow", "description": "OmniFlow is a synthetic omnidirectional human optical flow dataset. Based on a rendering engine the authors created a naturalistic 3D indoor environment with textured rooms, characters, actions, objects, illumination and motion blur where all components of the environment are shuffled during the data capturing process. The simulation has as output rendered images of household activities and the corresponding forward and backward optical flow. The dataset consists of 23,653 image pairs and corresponding forward and backward optical flow."}, {"id": "iconart", "name": "IconArt", "description": "This dataset contains 5955 painting images (from WikiCommons) : a train set of 2978 images and a test set of 2977 images (for classification task). 1480 of the 2977 images are annotated with bounding boxes for 7 iconographic classes : \u2018angel\u2019,\u2018Child_Jesus\u2019,\u2018crucifixion_of_Jesus\u2019,\u2018Mary\u2019,\u2018nudity\u2019, \u2018ruins\u2019,\u2018Saint_Sebastien\u2019."}, {"id": "mteb-massive-text-embedding-benchmark", "name": "MTEB (Massive Text Embedding Benchmark)", "description": "MTEB is a benchmark which spans 8 embedding tasks covering a total of 56 datasets and 112 languages. The 8 task types are Bitext mining, Classification, Clustering, Pair Classification, Reranking, Retrieval, Semantic Textual Similarity and Summarisation. The 56 dataset contains varying text lengths and they are grouped into three categories: Sentence to sentence, Paragraph to paragraph and Sentence to paragraph."}, {"id": "structured3d", "name": "Structured3D", "description": "Structured3D is a large-scale photo-realistic dataset containing 3.5K house designs (a) created by professional designers with a variety of ground truth 3D structure annotations (b) and generate photo-realistic 2D images (c). The dataset consists of rendering images and corresponding ground truth annotations (e.g., semantic, albedo, depth, surface normal, layout) under different lighting and furniture configurations."}, {"id": "ghera", "name": "Ghera", "description": "Ghera is a repository of Android app vulnerabilities."}, {"id": "dbp-5l-greek", "name": "DBP-5L (Greek)", "description": "DPB-5L is a Multilingual KG dataset containing 5 KGs in English, French, Japanese, Greek, and Spanish.  The dataset is used for the Knowledge Graph Completion and Entity Alignment task. DPB-5L (Greek) is a subset of DPB-5L with Greek KG."}, {"id": "wiki-cs", "name": "Wiki-CS", "description": "Wiki-CS is a Wikipedia-based dataset for benchmarking Graph Neural Networks. The dataset is constructed from Wikipedia categories, specifically 10 classes corresponding to branches of computer science, with very high connectivity. The node features are derived from the text of the corresponding articles.  They were calculated as the average of pretrained GloVe word embeddings (Pennington et al., 2014), resulting in 300-dimensional node features."}, {"id": "isbda", "name": "ISBDA", "description": "Consists of user-generated aerial videos from social media with annotations of instance-level building damage masks. This provides the first benchmark for quantitative evaluation of models to assess building damage using aerial videos."}, {"id": "semeval-2022-task-12-symlink-linking-mathematical-symbols-to-their-descriptions", "name": "SemEval 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions", "description": "Symlink is a SemEval shared task of extracting mathematical symbols and their descriptions from LaTeX source of scientific documents. This is a new task in SemEval 2022, which attracted 180 individual registrations and 59 final submissions from 7 participant teams."}, {"id": "dibco-2013", "name": "DIBCO 2013", "description": "DIBCO 2013 is the international Document Image Binarization Contest organized in the context of ICDAR 2013 conference. The general objective of the contest is to identify current advances in document image binarization for both machine-printed and handwritten document images using evaluation performance measures that conform to document image analysis and recognition."}, {"id": "bmeld", "name": "BMELD", "description": "BMELD is a bilingual (English-Chinese) dialogue corpus for Neural chat translation."}, {"id": "human-optical-flow-human-optical-flow-dataset", "name": "Human Optical Flow (Human Optical Flow dataset)", "description": "A synthetic data of videos of human action sequences and the corresponding optical flow."}, {"id": "arctic-articulated-objects-in-free-form-hand-interaction", "name": "ARCTIC (Articulated Objects in Free-form Hand Interaction)", "description": "ARCTIC is a dataset of free-form interactions of hands and articulated objects. ARCTIC has 1.2M images paired with accurate 3D meshes for both hands and for objects that move and deform over time. The dataset also provides hand-object contact information."}, {"id": "nycbike2", "name": "NYCBike2", "description": "Bike flow data of New York City."}, {"id": "docnli", "name": "DocNLI", "description": "DocNLI is a large-scale dataset for document-level NLI. DocNLI is transformed from a broad range of NLP problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, DocNLI has pretty limited artifacts which unfortunately widely exist in some popular sentence-level NLI datasets."}, {"id": "afew-va-afew-va-database-for-valence-and-arousal-estimation-in-the-wild", "name": "AFEW-VA (AFEW-VA Database for Valence and Arousal Estimation In-The-Wild)", "description": "The AFEW-VA databaset is a collection of highly accurate per-frame annotations levels of valence and arousal, along with per-frame annotations of 68 facial landmarks for 600 challenging video clips. These clips are extracted from feature films and were also annotated in terms of discrete emotion categories in the form of the AFEW database (that can be obtained there)."}, {"id": "trajair-a-general-aviation-trajectory-dataset", "name": "TrajAir: A General Aviation Trajectory Dataset", "description": "This dataset contains aircraft trajectories in an untowered terminal airspace collected over 8 months surrounding the Pittsburgh-Butler Regional Airport [ICAO:KBTP], a single runway GA airport, 10 miles North of the city of Pittsburgh, Pennsylvania. The trajectory data is recorded using an on-site setup that includes an ADS-B receiver. The trajectory data provided spans days from 18 Sept 2020 till 23 Apr 2021 and includes a total of 111 days of data discounting downtime, repairs, and bad weather days with no traffic. Data is collected starting at 1:00 AM local time to 11:00 PM local time. The dataset uses an Automatic Dependent Surveillance-Broadcast (ADS-B) receiver placed within the airport premises to capture the trajectory data. The receiver uses both the 1090 MHz and 978 MHz frequencies to listen to these broadcasts. The ADS-B uses satellite navigation to produce accurate location and timestamp for the targets which is recorded on-site using our custom setup. Weather data during the data collection time period is also included for environmental context. The weather data is obtained post-hoc using the METeorological Aerodrome Reports (METAR) strings generated by the Automated Weather Observing System (AWOS) system at KBTP. The raw METAR string is then appended to the raw trajectory data by matching the closest UTC timestamps."}, {"id": "jft-300m", "name": "JFT-300M", "description": "JFT-300M is an internal Google dataset used for training image classification models. Images are labeled using an algorithm that uses complex mixture of raw web signals, connections between web-pages and user feedback. This results in over one billion labels for the 300M images (a single image can have multiple labels). Of the billion image labels, approximately 375M are selected via an algorithm that aims to maximize label precision of selected images."}, {"id": "bl30k", "name": "BL30K", "description": "BL30K is a synthetic dataset rendered using Blender with ShapeNet's data. We break the dataset into six segments, each with approximately 5K videos. The videos are organized in a similar format as DAVIS and YouTubeVOS, so dataloaders for those datasets can be used directly. Each video is 160 frames long, and each frame has a resolution of 768*512. There are 3-5 objects per video, and each object has a random smooth trajectory -- we tried to optimize the trajectories in a greedy fashion to minimize object intersection (not guaranteed), with occlusions still possible (happen a lot in reality). See MiVOS for details."}, {"id": "jsb-chorales", "name": "JSB Chorales", "description": "The JSB chorales are a set of short, four-voice pieces of music well-noted for their stylistic homogeneity. The chorales were originally composed by Johann Sebastian Bach in the 18th century. He wrote them by first taking pre-existing melodies from contemporary Lutheran hymns and then harmonising them to create the parts for the remaining three voices. The version of the dataset used canonically in representation learning contexts consists of 382 such chorales, with a train/validation/test split of 229, 76 and 77 samples respectively."}, {"id": "ssd-sub-slot-dialogue-dataset", "name": "SSD (Sub-Slot Dialogue dataset)", "description": "SSD (Sub-slot Dialog) dataset: This is the dataset for the ACL 2022 paper \"A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots\". arxiv"}, {"id": "textcaps", "name": "TextCaps", "description": "Contains 145k captions for 28k images. The dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. "}, {"id": "rl-unplugged", "name": "RL Unplugged", "description": "RL Unplugged is suite of benchmarks for offline reinforcement learning. The RL Unplugged is designed around the following considerations: to facilitate ease of use, the datasets are provided with a unified API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. This is a dataset accompanying the paper RL Unplugged: Benchmarks for Offline Reinforcement Learning."}, {"id": "openlane-v2-val", "name": "OpenLane-V2 val", "description": "OpenLane-V2 is the world's first perception and reasoning benchmark for scene structure in autonomous driving. The primary task of the dataset is scene structure perception and reasoning, which requires the model to recognize the dynamic drivable states of lanes in the surrounding environment. The challenge of this dataset includes not only detecting lane centerlines and traffic elements but also recognizing the attribute of traffic elements and topology relationships on detected objects."}, {"id": "fgvc-aircraft", "name": "FGVC-Aircraft", "description": "FGVC-Aircraft contains 10,200 images of aircraft, with 100 images for each of 102 different aircraft model variants, most of which are airplanes. The (main) aircraft in each image is annotated with a tight bounding box and a hierarchical airplane model label. Aircraft models are organized in a four-levels hierarchy. The four levels, from finer to coarser, are:"}, {"id": "eth-eth-pedestrian", "name": "ETH (ETH Pedestrian)", "description": "ETH is a dataset for pedestrian detection. The testing set contains 1,804 images in three video clips. The dataset is captured from a stereo rig mounted on car, with a resolution of 640 x 480 (bayered), and a framerate of 13--14 FPS."}, {"id": "devil-diagnostic-evaluation-of-video-inpainting-on-landscapes", "name": "DEVIL (Diagnostic Evaluation of Video Inpainting on Landscapes)", "description": "Diagnostic Evaluation of Video Inpainting on Landscapes (DEVIL) benchmark is composed of a curated video/occlusion mask dataset and a comprehensive evaluation scheme"}, {"id": "scribblekitti", "name": "ScribbleKITTI", "description": "ScribbleKITTI is a scribble-annotated dataset for LiDAR semantic segmentation."}, {"id": "mfrc-moral-foundations-reddit-corpus", "name": "MFRC (Moral Foundations Reddit Corpus)", "description": "Moral Foundations Reddit Corpus (MFRC) is a collection of 16,123 Reddit comments that have been curated from 12 distinct subreddits, hand-annotated by at least three trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality, Implicit/Explicit Morality) based on the updated Moral Foundations Theory (MFT) framework."}, {"id": "shellcode-ia32", "name": "Shellcode_IA32", "description": "Shellcode_IA32 is a dataset containing 20 years of shellcodes from a variety of sources is the largest collection of shellcodes in assembly available to date."}, {"id": "chase-db1", "name": "CHASE_DB1", "description": "CHASE_DB1 is a dataset for retinal vessel segmentation which contains 28 color retina images with the size of 999\u00d7960 pixels which are collected from both left and right eyes of 14 school children. Each image is annotated by two independent human experts."}, {"id": "montgomery-county-x-ray-set", "name": "Montgomery County X-ray Set", "description": "X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health andHuman Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays areabnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities,including effusions and miliary patterns. The data set includes radiology readings available as a text files and summary of its content"}, {"id": "niloc-neural-inertial-localizatio", "name": "NILoc (Neural Inertial Localizatio)", "description": "IMU, WiFi data along with aligned Visual SLAM groundtruth locations from a smartphone carried during natural human motion"}, {"id": "red-real-embodied-dataset", "name": "RED (Real Embodied Dataset)", "description": "The Real Embodied Dataset (RED) is a computer vision large-scale dataset for grasping in cluttered scenes. It contains complete segmentation masks for partially occluded objects, with their order of occlusion."}, {"id": "physionet-challenge-2020", "name": "PhysioNet Challenge 2020", "description": "CPSC Database and CPSC-Extra Database INCART Database PTB and PTB-XL Database The Georgia 12-lead ECG Challenge (G12EC) Database Undisclosed Database The first source is the public (CPSC Database) and unused data (CPSC-Extra Database) from the China Physiological Signal Challenge in 2018 (CPSC2018), held during the 7th International Conference on Biomedical Engineering and Biotechnology in Nanjing, China. The unused data from the CPSC2018 is NOT the test data from the CPSC2018. The test data of the CPSC2018 is included in the final private database that has been sequestered. This training set consists of two sets of 6,877 (male: 3,699; female: 3,178) and 3,453 (male: 1,843; female: 1,610) of 12-ECG recordings lasting from 6 seconds to 60 seconds. Each recording was sampled at 500 Hz."}, {"id": "squid-stereo-quantitative-underwater-image-dataset", "name": "SQUID (Stereo Quantitative Underwater Image Dataset)", "description": "A dataset of images taken in different locations with varying water properties, showing color charts in the scenes. Moreover, to obtain ground truth, the 3D structure of the scene was calculated based on stereo imaging. This dataset enables a quantitative evaluation of restoration algorithms on natural images."}, {"id": "tum-kitchen", "name": "TUM Kitchen", "description": "The TUM Kitchen dataset is an action recognition dataset that contains 20 video sequences captured by 4 cameras with overlapping views. The camera network captures the scene from four viewpoints with 25 fps, and every RGB frame is of the resolution 384\u00d7288 by pixels. The action labels are frame-wise, and provided for the left arm, the right arm and the torso separately."}, {"id": "toxcast-scaffold-scaffold-split-of-toxcast-dataset", "name": "ToxCast(scaffold) (Scaffold split of ToxCast  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "mmd-multimodal-dialogs", "name": "MMD (Multimodal Dialogs)", "description": "The MMD (MultiModal Dialogs) dataset is a dataset for multimodal domain-aware conversations. It consists of over 150K conversation sessions between shoppers and sales agents, annotated by a group of in-house annotators using a semi-automated manually intense iterative process. "}, {"id": "iemocap-the-interactive-emotional-dyadic-motion-capture-iemocap-database", "name": "IEMOCAP (The Interactive Emotional Dyadic Motion Capture\u00a0(IEMOCAP) Database)", "description": "Multimodal Emotion Recognition IEMOCAP The IEMOCAP dataset consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset. Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance. The dataset is recorded across 5 sessions with 5 pairs of speakers."}, {"id": "sportsmot", "name": "SportsMOT", "description": "Multi-object tracking (MOT) is a fundamental task in computer vision, aiming to estimate objects (e.g., pedestrians and vehicles) bounding boxes and identities in video sequences."}, {"id": "charades-sta", "name": "Charades-STA", "description": "Charades-STA is a new dataset built on top of Charades by adding sentence temporal annotations."}, {"id": "mimii-due", "name": "MIMII DUE", "description": "This dataset is a sound dataset for malfunctioning industrial machine investigation and inspection with domain shifts due to changes in operational and environmental conditions (MIMII DUE). The dataset consists of normal and abnormal operating sounds of five different types of industrial machines, i.e., fans, gearboxes, pumps, slide rails, and valves. The data for each machine type includes six subsets called \"sections'', and each section roughly corresponds to a single product. Each section consists of data from two domains, called the source domain and the target domain, with different conditions such as operating speed and environmental noise. This dataset is a subset of the dataset for DCASE 2021 Challenge Task 2, so the dataset is entirely the same as data included in the development dataset and additional training dataset."}, {"id": "snli-stanford-natural-language-inference", "name": "SNLI (Stanford Natural Language Inference)", "description": "The SNLI dataset (Stanford Natural Language Inference) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Flickr30k, while hypotheses were generated by crowd-sourced annotators who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Annotators were instructed to judge the relation between sentences given that they describe the same event. Each pair is labeled as \u201centailment\u201d, \u201cneutral\u201d, \u201ccontradiction\u201d or \u201c-\u201d, where \u201c-\u201d indicates that an agreement could not be reached."}, {"id": "suim-segmentation-of-underwater-imagery", "name": "SUIM (Segmentation of Underwater IMagery)", "description": "The Segmentation of Underwater IMagery (SUIM) dataset contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants."}, {"id": "manytypes4py", "name": "ManyTypes4Py", "description": "ManyTypes4Py is a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a lightweight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files."}, {"id": "conala-cmu-conala-the-code-natural-language-challenge", "name": "CoNaLa (CMU CoNaLa, the Code/Natural Language Challenge)", "description": "The CMU CoNaLa, the Code/Natural Language Challenge dataset is a joint project from the Carnegie Mellon University NeuLab and Strudel labs. Its purpose is for testing the generation of code snippets from natural language. The data comes from StackOverflow questions. There are 2379 training and 500 test examples that were manually annotated. Every example has a natural language intent and its corresponding python snippet.  In addition to the manually annotated dataset, there are also 598,237 mined intent-snippet pairs. These examples are similar to the hand-annotated ones except that they contain a probability if the pair is valid."}, {"id": "polyvore-polyvore-outfits", "name": "Polyvore (Polyvore Outfits)", "description": "This dataset contains 21,889 outfits from polyvore.com, in which 17,316 are for training, 1,497 for validation and 3,076 for testing."}, {"id": "fdh-flickr-diverse-humans", "name": "FDH (Flickr Diverse Humans)", "description": "The Flickr Diverse Humans (FDH) dataset consists of 1.53M images of human figures from the YFCC100M dataset. Each image is annotated with keypoints, pixel-to-vertex correspondences (from CSE ) and a segmentation mask."}, {"id": "refresd-rationalized-english-french-semantic-divergences", "name": "REFreSD (Rationalized English-French Semantic Divergences)", "description": "Consists of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales."}, {"id": "mfqe-v2", "name": "MFQE v2", "description": "A dataset for compressed video quality enhancement."}, {"id": "biomrc", "name": "BIOMRC", "description": "A large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et al. (2018)."}, {"id": "hotelrec", "name": "HotelRec", "description": "Publicly available dataset in the hotel domain (50M versus 0.9M) and additionally, the largest recommendation dataset in a single domain and with textual reviews (50M versus 22M)."}, {"id": "argoverse-2-motion-forecasting", "name": "Argoverse 2 Motion Forecasting", "description": "The Argoverse 2 Motion Forecasting Dataset is a curated collection of 250,000 scenarios for training and validation. Each scenario is 11 seconds long and contains the 2D, birds-eye-view centroid and heading of each tracked object sampled at 10 Hz."}, {"id": "casia-hwdb", "name": "CASIA-HWDB", "description": "CASIA-HWDB is a dataset for handwritten Chinese character recognition. It contains 300 files (240 in HWDB1.1 training set and 60 in HWDB1.1 test set). Each file contains about 3000 isolated gray-scale Chinese character images written by one writer, as well as their corresponding labels."}, {"id": "jigsaws-jhu-isi-gesture-and-skill-assessment-working-set", "name": "JIGSAWS (JHU-ISI Gesture and Skill Assessment Working Set)", "description": "The JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS) is a surgical activity dataset for human motion modeling. The data was collected through a collaboration between The Johns Hopkins University (JHU) and Intuitive Surgical, Inc. (Sunnyvale, CA. ISI) within an IRB-approved study. The release of this dataset has been approved by the Johns Hopkins University IRB.   The dataset was captured using the da Vinci Surgical System from eight surgeons with different levels of skill performing five repetitions of three elementary surgical tasks on a bench-top model: suturing, knot-tying and needle-passing, which are standard components of most surgical skills training curricula. The JIGSAWS dataset consists of three components:"}, {"id": "activitynet-qa", "name": "ActivityNet-QA", "description": "The ActivityNet-QA dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benchmark for testing the performance of VideoQA models on long-term spatio-temporal reasoning."}, {"id": "phyre-physical-reasoning", "name": "PHYRE (PHYsical REasoning)", "description": "Benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. "}, {"id": "kumc", "name": "KUMC", "description": "The KUMC dataset for polyp detection and classification was collected from the University of Kansas Medical Center. It contains 80 colonoscopy video sequences which are manually labeled with bounding boxes as well as the polyp classes for the entire dataset."}, {"id": "aracovid19-ssd", "name": "AraCovid19-SSD", "description": "AraCovid19-SSD is a manually annotated Arabic COVID-19 sarcasm and sentiment detection dataset containing 5,162 tweets."}, {"id": "sounddescs", "name": "SoundDescs", "description": "We introduce a new audio dataset called SoundDescs that can be used for tasks such as text to audio retrieval, audio captioning etc. This dataset contains 32,979 pairs of audio files and text descriptions. There are 23 categories found in SoundDescs including but not limited to nature, clocks, fire etc."}, {"id": "relic", "name": "RELiC", "description": "RELiC is a large-scale dataset of 79k excerpts of literary scholarship, each containing a quotation from a primary source and the surrounding critical analysis. 79 public domain primary sources and over 8,836 secondary sources are represented in RELiC."}, {"id": "cityscapes-panoptic-parts", "name": "Cityscapes Panoptic Parts", "description": "The Cityscapes Panoptic Parts dataset introduces part-aware panoptic segmentation annotations for the Cityscapes dataset. It extends the original panoptic annotations for the Cityscapes dataset with part-level annotations for selected scene-level classes. "}, {"id": "emotic-emotions-in-context", "name": "EMOTIC (EMOTIons in Context)", "description": "The EMOTIC dataset, named after EMOTions In Context, is a database of images with people in real environments, annotated with their apparent emotions. The images are annotated with an extended list of 26 emotion categories combined with the three common continuous dimensions Valence, Arousal and Dominance."}, {"id": "cifar-100", "name": "CIFAR-100", "description": "The CIFAR-100 dataset (Canadian Institute for Advanced Research, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 color images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 images per class. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). There are 500 training images and 100 testing images per class."}, {"id": "imagewoof", "name": "Imagewoof", "description": "Imagewoof is a subset of 10 dog breed classes from Imagenet. The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog."}, {"id": "dtu-dtu-mvs-dataset-2014", "name": "DTU (DTU MVS dataset - 2014)", "description": "DTU MVS 2014 is a multi-view stereo dataset, which is an order of magnitude larger in number of scenes and with a significant increase in diversity. Specifically, it contains 80 scenes of large variability. Each scene consists of 49 or 64 accurate camera positions and reference structured light scans, all acquired by a 6-axis industrial robot."}, {"id": "skm-tea-stanford-knee-mri-with-multi-task-evaluation", "name": "SKM-TEA (Stanford Knee MRI with Multi-Task Evaluation)", "description": "The SKM-TEA dataset pairs raw quantitative knee MRI (qMRI) data, image data, and dense labels of tissues and pathology for end-to-end exploration and evaluation of the MR imaging pipeline.  This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient knee MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies."}, {"id": "pars-absa", "name": "Pars-ABSA", "description": "Pars-ABSA is a manually annotated Persian dataset, Pars-ABSA, which is verified by 3 native Persian speakers. The dataset consists of 5,114 positive, 3,061 negative and 1,827 neutral data samples from 5,602 unique reviews."}, {"id": "trackml-challenge-throughput-phase-dataset-tracking-machine-learning-challenge", "name": "TrackML challenge Throughput phase dataset (Tracking Machine Learning Challenge)", "description": "The dataset comprises multiple independent events, where each event contains simulated measurements (essentially 3D points) of particles generated in a collision between proton bunches at the Large Hadron Collider at CERN. The goal of the tracking machine learning challenge is to group the recorded measurements or hit for each event into tracks, sets of hits that belong to the same initial particle. A solution must uniquely associate each hit to one track. The training dataset contains the recorded hit, their ground truth counterpart and their association to particles, and the initial parameters of those particles. The test dataset contains only the recorded hits."}, {"id": "klue-korean-language-understanding-evaluation", "name": "KLUE (Korean Language Understanding Evaluation)", "description": "Korean Language Understanding Evaluation (KLUE) benchmark is a series of datasets to evaluate natural language understanding capability of Korean language models. KLUE consists of 8 diverse and representative tasks, which are accessible to anyone without any restrictions. With ethical considerations in mind, we deliberately design annotation guidelines to obtain unambiguous annotations for all datasets. Furthermore, we build an evaluation system and carefully choose evaluations metrics for every task, thus establishing fair comparison across Korean language models."}, {"id": "icvl-hand-posture-icvl-hand-posture-dataset", "name": "ICVL Hand Posture (ICVL Hand Posture Dataset)", "description": "The ICVL dataset is a hand pose estimation dataset that consists of 330K training frames and 2 testing sequences with each 800 frames. The dataset is collected from 10 different subjects with 16 hand joint annotations for each frame."}, {"id": "chameleon-48-32-20-fixed-splits", "name": "Chameleon (48%/32%/20% fixed splits)", "description": "Node classification on Chameleon with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "partial-ilids", "name": "Partial-iLIDS", "description": "Partial iLIDS is a dataset for occluded person person re-identification. It contains a total of 476 images of 119 people captured by 4 non-overlapping cameras. Some images contain people occluded by other individuals or luggage."}, {"id": "moviefib-movie-fill-in-the-blank", "name": "MovieFIB (Movie Fill-in-the-Blank)", "description": "A quantitative benchmark for developing and understanding video of fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. "}, {"id": "3doh50k", "name": "3DOH50K", "description": "3DOH50K is the first real 3D human dataset for the problem of human reconstruction and pose estimation in occlusion scenarios. It contains 51600 images with accurate 2D pose and 3D pose, SMPL parameters, and binary mask."}, {"id": "svd-short-video-dataset", "name": "SVD (Short Video Dataset)", "description": "SVD is a large-scale short video dataset, which contains over 500,000 short videos collected from http://www.douyin.com and over 30,000 labeled pairs of near-duplicate videos."}, {"id": "ncls-neural-cross-lingual-summarization-corpora", "name": "NCLS (Neural Cross-Lingual Summarization Corpora)", "description": "Presents two high-quality large-scale CLS datasets based on existing monolingual summarization datasets."}, {"id": "dan", "name": "DaN+", "description": "DaN+ is a new multi-domain corpus and annotation guidelines for Danish nested named entities (NEs) and lexical normalization to support research on cross-lingual cross-domain learning for a less-resourced language."}, {"id": "casia-mfsd", "name": "CASIA-MFSD", "description": "CASIA-MFSD is a dataset for face anti-spoofing. It contains 50 subjects, and 12 videos for each subject under different resolutions and light conditions. Three different spoof attacks are designed: replay, warp print and cut print attacks. The database contains 600 video recordings, in which 240 videos of 20 subjects are used for training and 360 videos of 30 subjects for testing."}, {"id": "cola-corpus-of-linguistic-acceptability", "name": "CoLA (Corpus of Linguistic Acceptability)", "description": "The Corpus of Linguistic Acceptability (CoLA) consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors. The public version contains 9594 sentences belonging to training and development sets, and excludes 1063 sentences belonging to a held out test set."}, {"id": "vispro", "name": "VisPro", "description": "VisPro dataset contains coreference annotation of 29,722 pronouns from 5,000 dialogues."}, {"id": "voices-voices-obscured-in-complex-environmental-settings", "name": "VOICES (Voices Obscured In Complex Environmental Settings)", "description": "The VOICES corpus is a dataset to promote speech and signal processing research of speech recorded by far-field microphones in noisy room conditions. "}, {"id": "ycb-video", "name": "YCB-Video", "description": "The YCB-Video dataset is a large-scale video dataset for 6D object pose estimation. provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames."}, {"id": "densepass", "name": "DensePASS", "description": "DensePASS - a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the Pinhole-to-Panoramic transfer and accompanied with pinhole camera training examples obtained from Cityscapes. DensePASS covers both, labelled- and unlabelled 360-degree images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source domain (i.e. pinhole) data."}, {"id": "qulac", "name": "Qulac", "description": "A dataset on asking Questions for Lack of Clarity in open-domain information-seeking conversations. Qulac presents the first dataset and offline evaluation framework for studying clarifying questions in open-domain information-seeking conversational search systems."}, {"id": "pannuke", "name": "PanNuke", "description": "PanNuke is a semi automatically generated nuclei instance segmentation and classification dataset with exhaustive nuclei labels across 19 different tissue types. The dataset consists of 481 visual fields, of which 312 are randomly sampled from more than 20K whole slide images at different magnifications, from multiple data sources. In total the dataset contains 205,343 labeled nuclei, each with an instance segmentation mask. "}, {"id": "eicu-crd-eicu-collaborative-research-database", "name": "eICU-CRD (eICU Collaborative Research Database)", "description": "The eICU Collaborative Research Database is a large multi-center critical care database made available by Philips Healthcare in partnership with the MIT Laboratory for Computational Physiology."}, {"id": "hatexplain", "name": "HateXplain", "description": "Covers multiple aspects of the issue. Each post in the dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based."}, {"id": "lit-pcba-esr1-ant-esr1-ant-target-of-lit-pcba-dataset", "name": "LIT-PCBA(ESR1_ant) (ESR1_ant target of LIT-PCBA Dataset)", "description": "Comparative evaluation of virtual screening methods requires a rigorous benchmarking procedure on diverse, realistic, and unbiased data sets. Recent investigations from numerous research groups unambiguously demonstrate that artificially constructed ligand sets classically used by the community (e.g., DUD, DUD-E, MUV) are unfortunately biased by both obvious and hidden chemical biases, therefore overestimating the true accuracy of virtual screening methods. We herewith present a novel data set (LIT-PCBA) specifically designed for virtual screening and machine learning. LIT-PCBA relies on 149 dose\u2013response PubChem bioassays that were additionally processed to remove false positives and assay artifacts and keep active and inactive compounds within similar molecular property ranges. To ascertain that the data set is suited to both ligand-based and structure-based virtual screening, target sets were restricted to single protein targets for which at least one X-ray structure is available in complex with ligands of the same phenotype (e.g., inhibitor, inverse agonist) as that of the PubChem active compounds. Preliminary virtual screening on the 21 remaining target sets with state-of-the-art orthogonal methods (2D fingerprint similarity, 3D shape similarity, molecular docking) enabled us to select 15 target sets for which at least one of the three screening methods is able to enrich the top 1%-ranked compounds in true actives by at least a factor of 2. The corresponding ligand sets (training, validation) were finally unbiased by the recently described asymmetric validation embedding (AVE) procedure to afford the LIT-PCBA data set, consisting of 15 targets and 7844 confirmed active and 407,381 confirmed inactive compounds. The data set mimics experimental screening decks in terms of hit rate (ratio of active to inactive compounds) and potency distribution. It is available online at http://drugdesign.unistra.fr/LIT-PCBA for download and for benchmarking novel virtual screening methods, notably those relying on machine learning."}, {"id": "ethos-multi-label-hate-speech-detection-dataset", "name": "ETHOS (multi-labEl haTe speecH detectiOn dataSet)", "description": "ETHOS is a hate speech detection dataset. It is built from YouTube and Reddit comments validated through a crowdsourcing platform. It has two subsets, one for binary classification and the other for multi-label classification. The former contains 998 comments, while the latter contains fine-grained hate-speech annotations for 433 comments."}, {"id": "styleptb", "name": "StylePTB", "description": "StylePTB is a fine-grained text style transfer benchmark. It consists of paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as compositions of multiple transfers which allow modelling of fine-grained stylistic changes as building blocks for more complex, high-level transfers."}, {"id": "first-person-hand-action-benchmark", "name": "First-Person Hand Action Benchmark", "description": "First-Person Hand Action Benchmark is a collection of RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. "}, {"id": "dogc", "name": "DOGC", "description": "Intended to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies."}, {"id": "metr-la-point-missing", "name": "METR-LA Point Missing", "description": "The original dataset from Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting contains traffic readings collected from 207 loop detectors on highways in Los Angeles County, aggregated in 5 minutes intervals over four months between March 2012 and June 2012."}, {"id": "mmptrack-multi-camera-multiple-people-tracking-dataset", "name": "MMPTRACK (Multi-camera Multiple People Tracking Dataset)", "description": "Multi-camera Multiple People Tracking (MMPTRACK) dataset has about 9.6 hours of videos, with over half a million frame-wise annotations. The dataset is densely annotated, e.g., per-frame bounding boxes and person identities are available, as well as camera calibration parameters. Our dataset is recorded with 15 frames per second (FPS) in five diverse and challenging environment settings., e.g., retail, lobby, industry, cafe, and office. This is by far the largest publicly available multi-camera multiple people tracking dataset. "}, {"id": "chameleon-60-20-20-random-splits", "name": "Chameleon(60%/20%/20% random splits)", "description": "Node classification on Chameleon with 60%/20%/20% random splits for training/validation/test."}, {"id": "marida-marine-debris-archive", "name": "MARIDA (Marine Debris Archive)", "description": "MARIDA (Marine Debris Archive) is the first dataset based on the multispectral Sentinel-2 (S2) satellite data, which distinguishes Marine Debris from various marine features that co-exist, including Sargassum macroalgae, Ships, Natural Organic Material, Waves, Wakes, Foam, dissimilar water types (i.e., Clear, Turbid Water, Sediment-Laden Water, Shallow Water), and Clouds. MARIDA is an open-access dataset which enables the research community to explore the spectral behaviour of certain floating materials, sea state features and water types, to develop and evaluate Marine Debris detection solutions based on artificial intelligence and deep learning architectures, as well as satellite pre-processing pipelines.  Although it is designed to be beneficial for several machine learning tasks, it primarily aims to benchmark weakly supervised pixel-level semantic segmentation learning methods. "}, {"id": "pascal-voc-2011", "name": "PASCAL VOC 2011", "description": "PASCAL VOC 2011 is an image segmentation dataset. It contains around 2,223 images for training, consisting of 5,034 objects. Testing consists of 1,111 images with 2,028 objects. In total there are over 5,000 precisely segmented objects for training."}, {"id": "poseprior", "name": "PosePrior", "description": "Accurate modeling of priors over 3D human pose is fundamental to many problems in computer vision."}, {"id": "deepglobe", "name": "DeepGlobe", "description": "We observe that satellite imagery is a powerful source of information as it contains more structured and uniform data, compared to traditional images. Although computer vision community has been accomplishing hard tasks on everyday image datasets using deep learning, satellite images are only recently gaining attention for maps and population analysis. This workshop aims at bringing together a diverse set of researchers to advance the state-of-the-art in satellite image analysis."}, {"id": "lit-pcba-mapk1-mapk1-target-of-lit-pcba-dataset", "name": "LIT-PCBA(MAPK1) (MAPK1 target of LIT-PCBA Dataset)", "description": "Comparative evaluation of virtual screening methods requires a rigorous benchmarking procedure on diverse, realistic, and unbiased data sets. Recent investigations from numerous research groups unambiguously demonstrate that artificially constructed ligand sets classically used by the community (e.g., DUD, DUD-E, MUV) are unfortunately biased by both obvious and hidden chemical biases, therefore overestimating the true accuracy of virtual screening methods. We herewith present a novel data set (LIT-PCBA) specifically designed for virtual screening and machine learning. LIT-PCBA relies on 149 dose\u2013response PubChem bioassays that were additionally processed to remove false positives and assay artifacts and keep active and inactive compounds within similar molecular property ranges. To ascertain that the data set is suited to both ligand-based and structure-based virtual screening, target sets were restricted to single protein targets for which at least one X-ray structure is available in complex with ligands of the same phenotype (e.g., inhibitor, inverse agonist) as that of the PubChem active compounds. Preliminary virtual screening on the 21 remaining target sets with state-of-the-art orthogonal methods (2D fingerprint similarity, 3D shape similarity, molecular docking) enabled us to select 15 target sets for which at least one of the three screening methods is able to enrich the top 1%-ranked compounds in true actives by at least a factor of 2. The corresponding ligand sets (training, validation) were finally unbiased by the recently described asymmetric validation embedding (AVE) procedure to afford the LIT-PCBA data set, consisting of 15 targets and 7844 confirmed active and 407,381 confirmed inactive compounds. The data set mimics experimental screening decks in terms of hit rate (ratio of active to inactive compounds) and potency distribution. It is available online at http://drugdesign.unistra.fr/LIT-PCBA for download and for benchmarking novel virtual screening methods, notably those relying on machine learning."}, {"id": "colored-mnist", "name": "Colored MNIST", "description": "Colored MNIST is a synthetic binary classification task derived from MNIST."}, {"id": "seq-funcat", "name": "Seq Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "spirs", "name": "SPIRS", "description": "A  first-of-its-kind large dataset of sarcastic/non-sarcastic tweets with high-quality labels and extra features: (1) sarcasm perspective labels (2) new contextual features. The dataset is expected to advance sarcasm detection research. "}, {"id": "adhd-200", "name": "ADHD-200", "description": "Attention Deficit Hyperactivity Disorder (ADHD) affects at least 5-10% of school-age children and is associated with substantial lifelong impairment, with annual direct costs exceeding $36 billion/year in the US. Despite a voluminous empirical literature, the scientific community remains without a comprehensive model of the pathophysiology of ADHD. Further, the clinical community remains without objective biological tools capable of informing the diagnosis of ADHD for an individual or guiding clinicians in their decision-making regarding treatment."}, {"id": "hm3d-habitat-matterport-3d", "name": "HM3D (Habitat-Matterport 3D)", "description": "Habitat-Matterport 3D (HM3D) is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences, stores, and other private indoor spaces."}, {"id": "cowc-cars-overhead-with-context", "name": "COWC (Cars Overhead With Context)", "description": "The Cars Overhead With Context (COWC) data set is a large set of annotated cars from overhead. It is useful for training a device such as a deep neural network to learn to detect and/or count cars."}, {"id": "intra", "name": "IntrA", "description": "IntrA is an open-access 3D intracranial aneurysm dataset that makes the application of points-based and mesh-based classification and segmentation models available. This dataset can be used to diagnose intracranial aneurysms and to extract the neck for a clipping operation in medicine and other areas of deep learning, such as normal estimation and surface reconstruction."}, {"id": "market-1501-c", "name": "Market-1501-C", "description": "Market-1501-C is an evaluation set that consists of algorithmically generated corruptions applied to the Market-1501 test-set.  These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."}, {"id": "hyperspectral-city", "name": "Hyperspectral City", "description": "Propose a dataset which adopts multi-channel visual input."}, {"id": "manytypes4typescript", "name": "ManyTypes4TypeScript", "description": " Type Inference dataset for TypeScript. Click on DOI tag for dataset files."}, {"id": "middlebury-2003", "name": "Middlebury 2003", "description": "Middlebury 2003 is a stereo dataset for indoor scenes."}, {"id": "conll-2009", "name": "CoNLL-2009", "description": "The task builds on the CoNLL-2008 task and extends it to multiple languages. The core of the task is to predict syntactic and semantic dependencies and their labeling. Data is provided for both statistical training and evaluation, which extract these labeled dependencies from manually annotated treebanks such as the Penn Treebank for English, the Prague Dependency Treebank for Czech and similar treebanks for Catalan, Chinese, German, Japanese and Spanish languages, enriched with semantic relations (such as those captured in the Prop/Nombank and similar resources). Great effort has been devoted to provide the participants with a common and relatively simple data representation for all the languages, similar to the last year's English data."}, {"id": "facebook-msc-facebook-multi-session-chat", "name": "Facebook MSC (Facebook MULTI-SESSION CHAT)", "description": "Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a humanhuman dataset consisting of multiple chat sessions whereby the speaking partners learn about each other\u2019s interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art."}, {"id": "blimp-benchmark-of-linguistic-minimal-pairs", "name": "BLiMP (Benchmark of Linguistic Minimal Pairs)", "description": "BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English. BLiMP consists of 67 sub-datasets, each containing 1000 minimal pairs isolating specific contrasts in syntax, morphology, or semantics. The data is automatically generated according to expert-crafted grammars. Aggregate human agreement with the labels is 96.4%."}, {"id": "everybody-dance-now", "name": "Everybody Dance Now", "description": "Everybody Dance Now is a dataset of videos that can be used for training and motion transfer. It contains long single-dancer videos that can be used to train and evaluate the model. All subjects have consented to allowing the data to be used for research purposes."}, {"id": "cwru-bearing-dataset", "name": "CWRU Bearing Dataset", "description": "Data was collected for normal bearings, single-point drive end and fan end defects.  Data was collected at 12,000 samples/second and at 48,000 samples/second for drive end bearing experiments.  All fan end bearing data was collected at 12,000 samples/second."}, {"id": "mgtab-multi-relational-graph-based-twitter-account-detection-benchmark", "name": "MGTAB (Multi-Relational Graph-Based Twitter Account Detection Benchmark)", "description": "MGTAB is the first standardized graph-based benchmark for stance and bot detection. MGTAB contains 10,199 expert-annotated users and 7 types of relationships, ensuring high-quality annotation and diversified relations. For more details, please refer to the MGTAB paper."}, {"id": "foodseg103", "name": "FoodSeg103", "description": "FoodSeg103 is a new food image dataset containing 7,118 images. Images are annotated with 104 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. It's provided as a large-scale benchmark for food image segmentation."}, {"id": "dir-lab-copdgene-the-deformable-image-registration-laboratory", "name": "DIR-LAB COPDgene (The Deformable Image Registration Laboratory)", "description": "Inspiratory and exipratory breath-hold CT image pairs acquired from the National Heart Lung Blood Institute COPDgene study archive."}, {"id": "hico-det", "name": "HICO-DET", "description": "HICO-DET is a dataset for detecting human-object interactions (HOI) in images. It contains 47,776 images (38,118 in train set and 9,658 in test set), 600 HOI categories constructed by 80 object categories and 117 verb classes. HICO-DET provides more than 150k annotated human-object pairs. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects."}, {"id": "fsc147", "name": "FSC147", "description": "We introduce a dataset of 147 object categories containing over 6000 images that are suitable for the few-shot counting task. We collected and annotated images ourselves. Our dataset consists of 6135 images across a di- verse set of 147 object categories, from kitchen utensils and office stationery to vehicles and animals. The object count in our dataset varies widely, from 7 to 3731 objects, with an average count of 56 objects per image. In each image, each object instance is annotated with a dot at its approxi- mate center. In addition, three object instances are selected randomly as exemplar instances; these exemplars are also annotated with axis-aligned bounding boxes."}, {"id": "data-collected-with-package-delivery-quadcopter-drone", "name": "Data Collected with Package Delivery Quadcopter Drone", "description": "This experiment was performed in order to empirically measure the energy use of small, electric Unmanned Aerial Vehicles (UAVs). We autonomously direct a DJI \u00ae Matrice 100 (M100) drone to take off, carry a range of payload weights on a triangular flight pattern, and land. Between flights, we varied specified parameters through a set of discrete options, payload of 0 , 250 g and 500 g; altitude during cruise of 25 m, 50 m, 75 m and 100 m; and speed during cruise of 4 m/s, 6 m/s, 8 m/s, 10 m/s and 12 m/s."}, {"id": "cityscapes-vipriors-subset", "name": "Cityscapes VIPriors subset", "description": "The training and validation data are subsets of the training split of the Cityscapes dataset. The test set is taken from the validation split of the Cityscapes dataset."}, {"id": "circor-digiscope", "name": "CirCor DigiScope", "description": "CirCor DigiScope is currently the largest pediatric heart sound dataset. A total of 5282 recordings have been collected from the four main auscultation locations of 1568 patients, in the process 215780 heart sounds have been manually annotated. Each cardiac murmur has been manually annotated by an expert annotator according to its timing, shape, pitch, grading and quality."}, {"id": "places365", "name": "Places365", "description": "The Places365 dataset is a scene recognition dataset. It is composed of 10 million images comprising 434 scene classes. There are two versions of the dataset: Places365-Standard with 1.8 million train and 36000 validation images from K=365 scene classes, and Places365-Challenge-2016, in which the size of the training set is increased up to 6.2 million extra images, including 69 new scene classes (leading to a total of 8 million train images from 434 scene classes)."}, {"id": "wmt-2018", "name": "WMT 2018", "description": "WMT 2018 is a collection of datasets used in shared tasks of the Third Conference on Machine Translation. The conference builds on a series of twelve previous annual workshops and conferences on Statistical Machine Translation."}, {"id": "dair-v2x", "name": "DAIR-V2X", "description": "DAIR-V2X is a  large-scale, multi-modality, multi-view dataset from real scenarios for VICAD. DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames are captured from real scenes with 3D annotations."}, {"id": "aachen-day-night", "name": "Aachen Day-Night", "description": "Aachen Day-Night is a dataset designed for benchmarking 6DOF outdoor visual localization in changing conditions. It focuses on localizing high-quality night-time images against a day-time 3D model. There are 14,607 images with changing conditions of weather, season and day-night cycles."}, {"id": "refit-an-electrical-load-measurements-dataset-of-united-kingdom-households-from-a-two-year-longitudinal-study", "name": "REFIT (An electrical load measurements dataset of United Kingdom households from a two-year longitudinal study)", "description": "Smart meter roll-outs provide easy access to granular meter measurements, enabling advanced energy services, ranging from demand response measures, tailored energy feedback and smart home/building automation. To design such services, train and validate models, access to data that resembles what is expected of smart meters, collected in a real-world setting, is necessary. The REFIT electrical load measurements dataset described in this paper includes whole house aggregate loads and nine individual appliance measurements at 8-second intervals per house, collected continuously over a period of two years from 20 houses. During monitoring, the occupants were conducting their usual routines. At the time of publishing, the dataset has the largest number of houses monitored in the United Kingdom at less than 1-minute intervals over a period greater than one year. The dataset comprises 1,194,958,790 readings, that represent over 250,000 monitored appliance uses. The data is accessible in an easy-to-use comma-separated format, is time-stamped and cleaned to remove invalid measurements, correctly label appliance data and fill in small gaps of missing data."}, {"id": "commonsenseqa", "name": "CommonsenseQA", "description": "The CommonsenseQA is a dataset for commonsense question answering task. The dataset consists of 12,247 questions with 5 choices each. The dataset was generated by Amazon Mechanical Turk workers in the following process (an example is provided in parentheses):"}, {"id": "silhouettes-caltech-101-silhouettes", "name": "Silhouettes (CalTech 101 Silhouettes)", "description": "The Caltech 101 Silhouettes dataset consists of 4,100 training samples, 2,264 validation samples and 2,307 test samples. The datast is based on CalTech 101 image annotations. Each image in the CalTech 101 data set includes a high-quality polygon outline of the primary object in the scene. To create the CalTech 101 Silhouettes data set, the authors center and scale each outline and render it on a DxD pixel image-plane. The outline is rendered as a filled, black polygon on a white background. Many object classes exhibit silhouettes that have distinctive class-specific features. A relatively small number of classes like soccer ball, pizza, stop sign, and yin-yang are indistinguishable based on shape, but have been left-in in the data."}, {"id": "remasc", "name": "ReMASC", "description": "We introduce a new database of voice recordings with the goal of supporting research on vulnerabilities and protection of voice-controlled systems. In contrast to prior efforts, the proposed database contains genuine and replayed recordings of voice commands obtained in realistic usage scenarios and using state-of-the-art voice assistant development kits. Specifically, the database contains recordings from four systems (each with a different microphone array) in a variety of environmental conditions with different forms of background noise and relative positions between speaker and device. To the best of our knowledge, this is the first database that has been specifically designed for the protection of voice controlled systems (VCS) against various forms of replay attacks."}, {"id": "image-paragraph-captioning", "name": "Image Paragraph Captioning", "description": "The Image Paragraph Captioning dataset allows researchers to benchmark their progress in generating paragraphs that tell a story about an image. The dataset contains 19,561 images from the Visual Genome dataset. Each image contains one paragraph. The training/val/test sets contains 14,575/2,487/2,489 images."}, {"id": "wikiart", "name": "WikiArt", "description": "WikiArt contains painting from 195 different artists. The dataset has 42129 images for training and 10628 images for testing."}, {"id": "fair-play", "name": "FAIR-Play", "description": "FAIR-Play is a video-audio dataset consisting of 1,871 video clips and their corresponding binaural audio clips recording in a music room. The video clip and binaural clip of the same index are roughly aligned."}, {"id": "raven-fair", "name": "RAVEN-FAIR", "description": "RAVEN-FAIR is a modified version of the RAVEN dataset."}, {"id": "vidit-virtual-image-dataset-for-illumination-transfer", "name": "VIDIT (Virtual Image Dataset for Illumination Transfer)", "description": "VIDIT  is a reference evaluation benchmark and to push forward the development of illumination manipulation methods. VIDIT includes 390 different Unreal Engine scenes, each captured with 40 illumination settings, resulting in 15,600 images. The illumination settings are all the combinations of 5 color temperatures (2500K, 3500K, 4500K, 5500K and 6500K) and 8 light directions (N, NE, E, SE, S, SW, W, NW). Original image resolution is 1024x1024."}, {"id": "visual-beliefs", "name": "Visual Beliefs", "description": "Visual Beliefs is a dataset of abstract scenes to study visual beliefs. The dataset consists of 8-frame scenes, and in each scene a person has a mistaken belief. The dataset can be used for two tasks: predicting who is mistaken and predicting when are they mistaken. "}, {"id": "openmic-2018", "name": "OpenMIC-2018", "description": "OpenMIC-2018 is an instrument recognition dataset containing 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform."}, {"id": "shopsign", "name": "ShopSign", "description": "A newly developed natural scene text dataset of Chinese shop signs in street views. "}, {"id": "extreme-classification-extreme-multi-label-classification", "name": "EXTREME CLASSIFICATION (Extreme Multi-label Classification)", "description": "The objective in extreme multi-label classification is to learn feature architectures and classifiers that can automatically tag a data point with the most relevant subset of labels from an extremely large label set. This repository provides resources that can be used for evaluating the performance of extreme multi-label algorithms including datasets, code, and metrics."}, {"id": "cluener2020", "name": "CLUENER2020", "description": "CLUENER2020 is a well-defined fine-grained dataset for named entity recognition in Chinese. CLUENER2020 contains 10 categories. "}, {"id": "methods2test", "name": "methods2test", "description": "methods2test is a supervised dataset consisting of Test Cases and their corresponding Focal Methods from a set of Java software repositories. Methods2test was constructed by parsing the Java projects to obtain classes and methods with their associated metadata. Next each Test Class was matched to its corresponding Focal Class. Finally, each Test Case within a Test Class was mapped to the related Focal Method to obtain a set of Mapped Test Cases."}, {"id": "activitynet", "name": "ActivityNet", "description": "The ActivityNet dataset contains 200 different types of activities and a total of 849 hours of videos collected from YouTube. ActivityNet is the largest benchmark for temporal activity detection to date in terms of both the number of activity categories and number of videos, making the task particularly challenging. Version 1.3 of the dataset contains 19994 untrimmed videos in total and is divided into three disjoint subsets, training, validation, and testing by a ratio of 2:1:1. On average, each activity category has 137 untrimmed videos. Each video on average has 1.41 activities which are annotated with temporal boundaries. The ground-truth annotations of test videos are not public."}, {"id": "sensaturban", "name": "SensatUrban", "description": "The SensatUrbat dataset is an urban-scale photogrammetric point cloud dataset with nearly three billion richly annotated points, which is five times the number of labeled points than the existing largest point cloud dataset. The dataset consists of large areas from two UK cities, covering about 6 km^2 of the city landscape. In the dataset, each 3D point is labeled as one of 13 semantic classes, such as ground, vegetation, car, etc.."}, {"id": "violin-video-and-language-inference", "name": "Violin (VIdeO-and-Language INference)", "description": "Video-and-Language Inference is the task of joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. The Violin dataset is a dataset for this task which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels."}, {"id": "camvid-cambridge-driving-labeled-video-database", "name": "CamVid (Cambridge-driving Labeled Video Database)", "description": "CamVid (Cambridge-driving Labeled Video Database) is a road/driving scene understanding database which was originally captured as five video sequences with a 960\u00d7720 resolution camera mounted on the dashboard of a car. Those sequences were sampled (four of them at 1 fps and one at 15 fps) adding up to 701 frames. Those stills were manually annotated with 32 classes: void, building, wall, tree, vegetation, fence, sidewalk, parking block, column/pole, traffic cone, bridge, sign, miscellaneous text, traffic light, sky, tunnel, archway, road, road shoulder, lane markings (driving), lane markings (non-driving), animal, pedestrian, child, cart luggage, bicyclist, motorcycle, car, SUV/pickup/truck, truck/bus, train, and other moving object"}, {"id": "social-iq", "name": "Social-IQ", "description": "Social-IQ is an unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies. By providing a rich source of open-ended questions and answers, Social-IQ opens the door to explainable social intelligence. The dataset contains rigorously annotated and validated videos, questions and answers, as well as annotations for the complexity level of each question and answer. Social-IQ contains 1,250 natural in-the-wild social situations, 7,500 questions and 52,500 correct and incorrect answers. "}, {"id": "acted-facial-expressions-in-the-wild-afew", "name": "Acted Facial Expressions In The Wild (AFEW)", "description": "Acted Facial Expressions In The Wild (AFEW) is a dynamic temporal facial expressions data corpus consisting of close to real world environment extracted from movie"}, {"id": "ci-avsr", "name": "CI-AVSR", "description": "Cantonese In-car Audio-Visual Speech Recognition (CI-AVSR) is a dataset for in-car command recognition in the Cantonese language with both video and audio data. It consists of 4,984 samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese speakers. Furthermore, the dataset is augmented using common in-car background noises to simulate real environments, producing a dataset 10 times larger than the collected one."}, {"id": "tal-corpus-the-tongue-and-lips-corpus", "name": "TaL Corpus (The Tongue and Lips Corpus)", "description": "The Tongue and Lips (TaL) corpus is a multi-speaker corpus of ultrasound images of the tongue and video images of lips. This corpus contains synchronised imaging data of extraoral (lips) and intraoral (tongue) articulators from 82 native speakers of English."}, {"id": "sherlock", "name": "SHERLOCK", "description": "SHERLOCK is a corpus of 363K commonsense inferences grounded in 103K images. Annotators highlight localized clues (color bubbles) and draw plausible abductive inferences about them (speech bubbles). It can be used for testing machine capacity for abductive reasoning beyond literal image contents."}, {"id": "har-human-activity-recognition-using-smartphones", "name": "HAR (Human Activity Recognition Using Smartphones)", "description": "The Human Activity Recognition Dataset has been collected from 30 subjects performing six different activities (Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, Laying). It consists of inertial sensor data that was collected using a smartphone carried by the subjects."}, {"id": "eegeyenet", "name": "EEGEyeNet", "description": "EEEyeNet is a dataset and benchmark with the goal of advancing research in the intersection of brain activities and eye movements. It consists of simultaneous Electroencephalography (EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms."}, {"id": "mtg-jamendo", "name": "MTG-Jamendo", "description": "The MTG-Jamendo dataset is an open dataset for music auto-tagging. The dataset contains over 55,000 full audio tracks with 195 tags categories (87 genre tags, 40 instrument tags, and 56 mood/theme tags). It is built using music available at Jamendo under Creative Commons licenses and tags provided by content uploaders. All audio is distributed in 320kbps MP3 format."}, {"id": "trecvid-avs18-iacc-3", "name": "TRECVID-AVS18 (IACC.3)", "description": "Internet Archive videos (IACC.3) under Creative Commons licenses. The test video collection for TRECVID-AVS2016-TRECVID-AVS2018 contains 335,944 web video clips (600hr)."}, {"id": "microsoft-research-social-media-conversation-corpus", "name": "Microsoft Research Social Media Conversation Corpus", "description": "Microsoft Research Social Media Conversation Corpus consists of 127M context-message-response triples from the Twitter FireHose, covering the 3-month period June 2012 through August 2012. Only those triples where context and response were generated by the same user were extracted. To minimize noise, only triples that contained at least one frequent bigram that appeared more than 3 times in the corpus was selected. This produced a corpus of 29M Twitter triples."}, {"id": "articulation-gan-unsupervised-modeling-of-articulatory-learning", "name": "Articulation GAN: Unsupervised modeling of articulatory learning", "description": "Checkpoints, generated EMA representations, audio outputs, and annotations for paper titled \"Articulation GAN: Unsupervised modeling of articulatory learning\""}, {"id": "got-10k-generic-object-tracking-benchmark", "name": "GOT-10k (Generic Object Tracking Benchmark)", "description": "The GOT-10k dataset contains more than 10,000 video segments of real-world moving objects and over 1.5 million manually labelled bounding boxes. The dataset contains more than 560 classes of real-world moving objects and 80+ classes of motion patterns."}, {"id": "inat2021-inaturalist-2021", "name": "iNat2021 (iNaturalist 2021)", "description": "iNat2021 is a large-scale image dataset collected and annotated by community scientists that contains over 2.7M images from 10k different species."}, {"id": "50-salads", "name": "50 Salads", "description": "Activity recognition research has shifted focus from distinguishing full-body motion patterns to recognizing complex interactions of multiple entities. Manipulative gestures \u2013 characterized by interactions between hands, tools, and manipulable objects \u2013 frequently occur in food preparation, manufacturing, and assembly tasks, and have a variety of applications including situational support, automated supervision, and skill assessment. With the aim to stimulate research on recognizing manipulative gestures we introduce the 50 Salads dataset. It captures 25 people preparing 2 mixed salads each and contains over 4h of annotated accelerometer and RGB-D video data. Including detailed annotations, multiple sensor types, and two sequences per participant, the 50 Salads dataset may be used for research in areas such as activity recognition, activity spotting, sequence analysis, progress tracking, sensor fusion, transfer learning, and user-adaptation."}, {"id": "lorenz-dataset", "name": "Lorenz Dataset", "description": "The Lorenz dataset contains 100000 time-series with length 24. The data has 5 modes and it is obtained using the Lorenz equation with 5 different seed values."}, {"id": "lsui-large-scale-underwater-image-dataset", "name": "LSUI (Large Scale  Underwater Image Dataset)", "description": "We released a large-scale underwater image (LSUI) dataset including 5004 image pairs, which involve richer underwater scenes (lighting conditions, water types and target categories) and better visual quality reference images than the existing ones."}, {"id": "cvefixes", "name": "CVEfixes", "description": "CVEfixes is a comprehensive vulnerability dataset that is automatically collected and curated from Common Vulnerabilities and Exposures (CVE) records in the public U.S. National Vulnerability Database (NVD). The goal is to support data-driven security research based on source code and source code metrics related to fixes for CVEs in the NVD by providing detailed information at different interlinked levels of abstraction, such as the commit-, file-, and method level, as well as the repository- and CVE level."}, {"id": "hutter-prize", "name": "Hutter Prize", "description": "The Hutter Prize Wikipedia dataset, also known as enwiki8, is a byte-level dataset consisting of the first 100 million bytes of a Wikipedia XML dump. For simplicity we shall refer to it as a character-level dataset. Within these 100 million bytes are 205 unique tokens."}, {"id": "mcvqa-multilingual-and-code-mixed-visual-question-answering", "name": "MCVQA (Multilingual and Code-mixed Visual Question Answering)", "description": "The MCVQA dataset consists of 248, 349 training questions and 121, 512 validation questions for real images in Hindi and Code-mixed. For each Hindi question, we also provide its 10 corresponding answers in Hindi."}, {"id": "mupots-3d-multiperson-pose-test-set-in-3dmulti-person-pose-estimation-test-set-in-3d", "name": "MuPoTS-3D (Multiperson Pose Test Set in 3DMulti-person Pose estimation Test Set in 3D)", "description": "MuPoTs-3D (Multi-person Pose estimation Test Set in 3D) is a dataset for pose estimation composed of more than 8,000 frames from 20 real-world scenes with up to three subjects. The poses are annotated with a 14-point skeleton model."}, {"id": "conala-ext-conala-extended-with-question-text", "name": "CoNaLa-Ext (CoNaLa Extended With Question Text)", "description": "The CoNaLa Extended With Question Text is an extension to the original CoNaLa Dataset (Papers With Code Link) proposed in the NLP4Prog workshop paper \"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation\". The key additions are that every example now has the full question body from its respective StackOverflow Question."}, {"id": "hda-facial-tattoo-and-painting-database", "name": "HDA Facial Tattoo and Painting Database", "description": "The Hochschule Darmstadt (HDA) facial tattoo and paintings database contains 500 pairs of facial images of individuals with and without facial tattoos or paintings. The database was collected from multiple online sources."}, {"id": "shifts", "name": "Shifts", "description": "The Shifts Dataset is a dataset for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car (SDC) vehicle motion prediction. All of these data modalities and tasks are affected by real, `in-the-wild' distributional shifts and pose interesting challenges with respect to uncertainty estimation."}, {"id": "scannet200", "name": "ScanNet200", "description": "The ScanNet200 benchmark studies 200-class 3D semantic segmentation - an order of magnitude more class categories than previous 3D scene understanding benchmarks. The source of scene data is identical to ScanNet, but parses a larger vocabulary for semantic and instance segmentation"}, {"id": "schiller-shiller", "name": "Schiller (Shiller)", "description": "Schiller contains handwritten texts written in modern German. Train sample consists of 244 lines, validation - 21 lines and test - 63 lines."}, {"id": "doccvqa-document-collection-visual-question-answering", "name": "DocCVQA (Document Collection Visual Question Answering)", "description": "DocCVQA is a Document Visual Question Answering dataset, where the questions are posed over a whole collection of 14,362 scanned documents. Therefore, the task can be seen as a retrieval-style evidence seeking task where given a question, the aim is to identify and retrieve all the documents in a large document collection that are relevant to answering this question as well as provide the answer."}, {"id": "iwslt-2019", "name": "IWSLT 2019", "description": "The IWSLT 2019 dataset contains source, Machine Translated, reference and Post-Edited text, which can be used to quantify and evaluate Post-editing effort after automatic MT."}, {"id": "live-laboratory-for-image-video-engineering", "name": "LIVE (Laboratory for Image & Video Engineering)", "description": "Briefly describe the dataset. Provide:"}, {"id": "agrr-2019", "name": "AGRR-2019", "description": "Consists of 7.5k sentences with gapping (as well as 15k relevant negative sentences) and comprises data from various genres: news, fiction, social media and technical texts. The dataset was prepared for the Automatic Gapping Resolution Shared Task for Russian (AGRR-2019) - a competition aimed at stimulating the development of NLP tools and methods for processing of ellipsis. "}, {"id": "qm9-quantum-machines-9", "name": "QM9 (Quantum Machines 9)", "description": "QM9 provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships."}, {"id": "middlebury-middlebury-stereo", "name": "Middlebury (Middlebury Stereo)", "description": "The Middlebury Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a novel technique that employs structured lighting and does not require the calibration of the light projectors."}, {"id": "tox21-scaffold-scaffold-split-of-tox21-dataset", "name": "Tox21(scaffold) (Scaffold split of Tox21  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "math", "name": "MATH", "description": "MATH is a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations."}, {"id": "dogwhistle", "name": "DogWhistle", "description": "Cant (also known as doublespeak, cryptolect, argot, anti-language or secret language) is important for understanding advertising, comedies and dog-whistle politics. DogWhistle is a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective."}, {"id": "wilddash", "name": "WildDash", "description": "WildDash is a benchmark evaluation method is presented that uses the meta-information to calculate the robustness of a given algorithm with respect to the individual hazards."}, {"id": "tuda", "name": "TUDA", "description": "Overall duration per microphone: about 36 hours (31 hrs train / 2.5 hrs dev / 2.5 hrs test) Count of microphones: 3 (Microsoft Kinect, Yamaha, Samson) Count of wave-files per microphone: about 14500 Overall count of participations: 180 (130 male / 50 female)"}, {"id": "wikisrs", "name": "WikiSRS", "description": "WikiSRS is a novel dataset of similarity and relatedness judgments of paired Wikipedia entities (people, places, and organizations), as assigned by Amazon Mechanical Turk workers."}, {"id": "cat2000", "name": "CAT2000", "description": "Includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings."}, {"id": "off-complicated-sequential", "name": "Off_Complicated_sequential", "description": "SMAC+ offensive complicated scenario with sequential episodic buffer"}, {"id": "xsid-cross-lingual-slot-and-intent-detection", "name": "xSID (Cross-lingual Slot and Intent Detection)", "description": "xSID, a new evaluation benchmark for cross-lingual (X) Slot and Intent Detection in 13 languages from 6 language families,  including a very low-resource dialect, covering Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), German (de), Indonesian (id), Italian (it), Japanese (ja), Kazakh (kk), Serbian (sr), Turkish (tr) and an Austro-Bavarian German dialect, South Tyrolean (de-st)."}, {"id": "vgmidi", "name": "VGMIDI", "description": "VGMIDI is a dataset of piano arrangements of video game soundtracks. It contains 200 MIDI pieces labeled according to emotion and 3,850 unlabeled pieces. Each labeled piece was annotated by 30 human subjects according to the Circumplex (valence-arousal) model of emotion using a custom web tool."}, {"id": "gsl-greek-sign-language", "name": "GSL (Greek Sign Language)", "description": "The Greek Sign Language (GSL) is a large-scale RGB+D dataset, suitable for Sign Language Recognition (SLR) and Sign Language Translation (SLT). The video captures are conducted using an Intel RealSense D435 RGB+D camera at a rate of 30 fps. Both the RGB and the depth streams are acquired in the same spatial resolution of 848\u00d7480 pixels. To increase variability in the videos, the camera position and orientation is slightly altered within subsequent recordings. Seven different signers are employed to perform 5 individual and commonly met scenarios in different public services. The average length of each scenario is twenty sentences."}, {"id": "docvqa", "name": "DocVQA", "description": "DocVQA consists of 50,000 questions defined on 12,000+ document images."}, {"id": "anubis-skeleton-based-action-recognition-dataset", "name": "ANUBIS (Skeleton-Based Action Recognition Dataset)", "description": "ANUBIS is a large-scale human skeleton dataset containing 80 actions. Compared with previously collected datasets, ANUBIS is advantageous in the following four aspects: (1) employing more recently released sensors; (2) containing novel back view; (3) encouraging high enthusiasm of subjects; (4) including actions of the COVID pandemic era."}, {"id": "oposum", "name": "OpoSum", "description": "OPOSUM is a dataset for the training and evaluation of Opinion Summarization models which contains Amazon reviews from six product domains: Laptop Bags, Bluetooth Headsets, Boots, Keyboards, Televisions, and Vacuums. The six training collections were created by downsampling from the Amazon Product Dataset introduced in McAuley et al. (2015) and contain reviews and their respective ratings. "}, {"id": "burstsr", "name": "BurstSR", "description": "BurstSR is a dataset consisting of smartphone bursts and high-resolution DSLR ground-truth"}, {"id": "hyper-kvasir-dataset", "name": "Hyper-Kvasir Dataset", "description": "HyperKvasir dataset contains 110,079 images and 374 videos where it captures anatomical landmarks and pathological and normal findings. A total of around 1 million images and video frames altogether."}, {"id": "fcdb-fashion-culture-database", "name": "FCDB (Fashion Culture DataBase)", "description": "Consists of 76 million geo-tagged images in 16 cosmopolitan cities."}, {"id": "arnli", "name": "ArNLI", "description": "Natural Language Inference processes pairs of sentences to extract their semantic relations. Pair sentences are annotated with three classes (Contradictions, Entailment, Neutral). "}, {"id": "enzymes", "name": "ENZYMES", "description": "ENZYMES is a dataset of 600 protein tertiary structures obtained from the BRENDA enzyme database. The ENZYMES dataset contains 6 enzymes."}, {"id": "soba-shadow-object-association", "name": "SOBA (Shadow-OBject Association)", "description": "A new dataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of shadow and object instances in 1,000 photos, each with individual labeled masks."}, {"id": "nyu-vpr", "name": "NYU-VPR", "description": "NYU-VPR is a dataset for Visual place recognition (VPR) that contains more than 200,000 images over a 2km\u00d72km area near the New York University campus, taken within the whole year of 2016."}, {"id": "medleydb", "name": "MedleyDB", "description": "MedleyDB, is a dataset of annotated, royalty-free multitrack recordings. It was curated primarily to support research on melody extraction. For each song melody f\u2080 annotations are provided as well as instrument activations for evaluating automatic instrument recognition. The original dataset consists of 122 multitrack songs out of which 108 include melody annotations."}, {"id": "toyota-smarthome-dataset", "name": "Toyota Smarthome Dataset", "description": "A large scale dataset with daily-living activities performed in a natural manner."}, {"id": "pa-100k-pa-100k-dataset", "name": "PA-100K (PA-100K Dataset)", "description": "PA-100K is a recent-proposed large pedestrian attribute dataset, with 100,000 images in total collected from outdoor surveillance cameras. It is split into 80,000 images for the training set, and 10,000 for the validation set and 10,000 for the test set. This dataset is labeled by 26 binary attributes. The common features existing in both selected dataset is that the images are blurry due to the relatively low resolution and the positive ratio of each binary attribute is low."}, {"id": "compare", "name": "COMPARE", "description": "COMPARE is a taxonomy and a dataset of comparison discussions in peer reviews of research papers in the domain of experimental deep learning."}, {"id": "medical-question-pairs-medical-question-pairs-mqp-dataset", "name": "Medical Question Pairs (Medical Question Pairs (MQP) Dataset)", "description": "This repository contains a dataset of 3048 similar and dissimilar medical question pairs hand-generated and labeled by Curai's doctors. The dataset is described in detail in our paper."}, {"id": "minos", "name": "MINOS", "description": "MINOS is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites."}, {"id": "300-vw-300-videos-in-the-wild", "name": "300-VW (300 Videos in the Wild)", "description": "300 Videos in the Wild (300-VW) is a dataset for evaluating facial landmark tracking algorithms in the wild. The dataset authors collected a large number of long facial videos recorded in the wild. Each video has duration of ~1 minute (at 25-30 fps). All frames have been annotated with regards to the same mark-up (i.e. set of facial landmarks) used in the 300 W competition as well (a total of 68 landmarks). The dataset includes 114 videos (circa 1 min each)."}, {"id": "mrqa", "name": "MRQA", "description": "The MRQA (Machine Reading for Question Answering) dataset is a dataset for evaluating the generalization capabilities of reading comprehension systems."}, {"id": "deepfish", "name": "DeepFish", "description": "DeepFish as a benchmark suite with a large-scale dataset to train and test methods for several computer vision tasks. The dataset consists of approximately 40 thousand images collected underwater from 20 habitats in the marine environments of tropical Australia. It contains classification labels as well as point-level and segmentation labels to have a more comprehensive fish analysis benchmark. These labels enable models to learn to automatically monitor fish count, identify their locations, and estimate their sizes."}, {"id": "hate-speech", "name": "Hate Speech", "description": "Dataset of hate speech annotated on Internet forum posts in English at sentence-level. The source forum in Stormfront, a large online community of white nacionalists. A total of 10,568 sentence have been been extracted from Stormfront and classified as conveying hate speech or not."}, {"id": "3d60", "name": "3D60", "description": "Collects high quality 360 datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to 360 via rendering. "}, {"id": "imagenet-c", "name": "ImageNet-C", "description": "ImageNet-C is an open source data set that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set."}, {"id": "rpc-retail-product-checkout", "name": "RPC (Retail Product Checkout)", "description": "RPC is a large-scale retail product checkout dataset and collects 200 retail SKUs. The collected SKUs can be divided into 17 meta categories, i.e., puffed food, dried fruit, dried food, instant drink, instant noodles, dessert, drink, alcohol, milk, canned food, chocolate, gum, candy, seasoner, personal hygiene, tissue, stationery."}, {"id": "horae", "name": "HORAE", "description": "A new dataset of annotated pages from books of hours, a type of handwritten prayer books owned and used by rich lay people in the late middle ages. The dataset was created for conducting historical research on the evolution of the religious mindset in Europe at this period since the book of hours represent one of the major sources of information thanks both to their rich illustrations and the different types of religious sources they contain. "}, {"id": "lcsts", "name": "LCSTS", "description": "LCSTS is a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. The authors also manually tagged the relevance of 10,666 short summaries with their corresponding short texts 10,666 short summaries with their corresponding short texts."}, {"id": "cuhk03-chinese-university-of-hong-kong-re-identification", "name": "CUHK03 (Chinese University of Hong Kong Re-identification)", "description": "The CUHK03 consists of 14,097 images of 1,467 different identities, where 6 campus cameras were deployed for image collection and each identity is captured by 2 campus cameras. This dataset provides two types of annotations, one by manually labelled bounding boxes and the other by bounding boxes produced by an automatic detector. The dataset also provides 20 random train/test splits in which 100 identities are selected for testing and the rest for training"}, {"id": "uco-laeo", "name": "UCO-LAEO", "description": "A dataset for building models that detect people Looking At Each Other (LAEO) in video sequences."}, {"id": "taskonomy", "name": "Taskonomy", "description": "Taskonomy provides a large and high-quality dataset of varied indoor scenes."}, {"id": "followup", "name": "FollowUp", "description": "1000 query triples on 120 tables."}, {"id": "medqa-usmle", "name": "MedQA-USMLE", "description": "Multiple choice question answering based on the United States Medical License Exams (USMLE). The dataset is collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively."}, {"id": "frgc-face-recognition-grand-challenge", "name": "FRGC (Face Recognition Grand Challenge)", "description": "The data for FRGC consists of 50,000 recordings divided into training and validation partitions. The training partition is designed for training algorithms and the validation partition is for assessing performance of an approach in a laboratory setting. The validation partition consists of data from 4,003 subject sessions. A subject session is the set of all images of a person taken each time a person's biometric data is collected and consists of four controlled still images, two uncontrolled still images, and one three-dimensional image. The controlled images were taken in a studio setting, are full frontal facial images taken under two lighting conditions and with two facial expressions (smiling and neutral). The uncontrolled images were taken in varying illumination conditions; e.g., hallways, atriums, or outside. Each set of uncontrolled images contains two expressions, smiling and neutral. The 3D image was taken under controlled illumination conditions. The 3D images consist of both a range and a texture image. The 3D images were acquired by a Minolta Vivid 900/910 series sensor."}, {"id": "icdar-2017", "name": "ICDAR 2017", "description": "ICDAR2017 is a dataset for scene text detection."}, {"id": "reclor", "name": "ReClor", "description": "Logical reasoning is an important ability to examine, analyze, and critically evaluate arguments as they occur in ordinary language as the definition from Law School Admission Council. ReClor is a dataset extracted from logical reasoning questions of standardized graduate admission examinations."}, {"id": "haa500-human-centric-atomic-action-dataset", "name": "HAA500 (Human-Centric Atomic Action Dataset)", "description": "HAA500 is a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. Unlike existing atomic action datasets, where coarse-grained atomic actions were labeled with action-verbs, e.g., \"Throw\", HAA500 contains fine-grained atomic actions where only consistent actions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in Basketball\", to minimize ambiguities in action classification. HAA500 has been carefully curated to capture the movement of human figures with less spatio-temporal label noises to greatly enhance the training of deep neural networks. "}, {"id": "sel", "name": "SEL", "description": "The semantic line (SEL) dataset contains 1,750 outdoor images in total, which are split into 1,575 training and 175 testing images. Each semantic line is annotated by the coordinates of the two end-points on an image boundary. If an image has a single dominant line, it is set as the ground truth primary semantic line. If an image has multiple semantic lines, the line with the best rank by human annotators is set as the ground-truth primary line, and the others as additional ground-truth semantic lines. In SEL, 61% of the images contain multiple semantic lines."}, {"id": "roomr-room-rearrangement", "name": "RoomR (Room Rearrangement)", "description": "The task of Room Rearrangement consists on an agent exploring a room and recording objects' initial configurations. The agent is removed and the poses and states (e.g., open/closed) of some objects in the room are changed. The agent must restore the initial configurations of all objects in the room. "}, {"id": "sun3d", "name": "SUN3D", "description": "SUN3D contains a large-scale RGB-D video database, with 8 annotated sequences. Each frame has a semantic segmentation of the objects in the scene and information about the camera pose. It is composed by 415 sequences captured in 254 different spaces, in 41 different buildings. Moreover, some places have been captured multiple times at different moments of the day."}, {"id": "eqa-embodied-question-answering", "name": "EQA (Embodied Question Answering)", "description": "The EQA (Embodied Question Answering) dataset is a dataset of visual questions and answers grounded in House3D. For this dataset an agent is spawned at a random location in a 3D environment and asked a question (for e.g. \"What color is the car?\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person vision, and then answer the question (\"orange\")."}, {"id": "paris6k", "name": "Paris6k", "description": "Click to add a brief description of the dataset (Markdown and LaTeX enabled)."}, {"id": "x-stance", "name": "x-stance", "description": "A large-scale stance detection dataset from comments written by candidates of elections in Switzerland. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection. It contains 67 000 comments on more than 150 political issues (targets)."}, {"id": "fashion-gen", "name": "Fashion-Gen", "description": "Fashion-Gen consists of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles."}, {"id": "middlebury-2014", "name": "Middlebury 2014", "description": "The Middlebury 2014 dataset contains a set of 23 high resolution stereo pairs for which known camera calibration parameters and ground truth disparity maps obtained with a structured light scanner are available. The images in the Middlebury dataset all show static indoor scenes with varying difficulties including repetitive structures, occlusions, wiry objects as well as untextured areas."}, {"id": "wine-wine-data-set", "name": "Wine (Wine Data Set)", "description": "These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines."}, {"id": "lipophilicity-scaffold-scaffold-split-of-lipophilicity-dataset", "name": "Lipophilicity(scaffold) (Scaffold split of Lipophilicity  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "ags-corpus-ag-s-corpus-of-news-articlesnews", "name": "AG\u2019s Corpus (AG's corpus of news articlesNews)", "description": "Antonio Gulli\u2019s corpus of news articles is a collection of more than 1 million news articles. The articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non - commercial activity."}, {"id": "cic-catalonia-independence-corpus", "name": "CIC (Catalonia Independence Corpus)", "description": "The dataset is annotated with stance towards one topic, namely, the independence of Catalonia."}, {"id": "wikidata-14m", "name": "Wikidata-14M", "description": "Wikidata-14M is a recommender system dataset for recommending items to Wikidata editors. It consists of 220,000 editors responsible for 14 million interactions with 4 million items."}, {"id": "dpm-dont-patronize-me", "name": "DPM (Don\u2019t Patronize Me!)", "description": "Don\u2019t Patronize Me! (DPM) is an annotated dataset with Patronizing and Condescending Language towards vulnerable communities."}, {"id": "cuge", "name": "CUGE", "description": "CUGE is a Chinese Language Understanding and Generation Evaluation benchmark with the following features: (1) Hierarchical benchmark framework, where datasets are principally selected and organized with a language capability-task-dataset hierarchy. (2) Multi-level scoring strategy, where different levels of model performance are provided based on the hierarchical framework."}, {"id": "dblp-temporal", "name": "DBLP Temporal", "description": "DBLP Temporal is a dataset for temporal entity resolution, based on author profiles extracted from the Digital Bibliography and Library Project (DBLP)."}, {"id": "dataset-for-methane-combustion", "name": "Dataset for methane combustion", "description": "The dataset contains 578,731 structures for methane combustion and their energies and forces under MN15/6-31G** level."}, {"id": "peopleart", "name": "PeopleArt", "description": "People-Art is an object detection dataset which consists of people in 43 different styles. People contained in this dataset are quite different from those in common photographs. There are 42 categories of art styles and movements including Naturalism, Cubism, Socialist Realism, Impressionism, and Suprematism"}, {"id": "hixray", "name": "HiXray", "description": "HiXray is a High-quality X-ray security inspection image dataset, which contains 102,928 common prohibited items of 8 categories. It has been gathered from the real-world airport security inspection and annotated by professional security inspectors"}, {"id": "delicious", "name": "Delicious", "description": "Delicious : This data set contains tagged web pages retrieved from the website delicious.com."}, {"id": "msu-hdr-video-reconstruction-benchmark", "name": "MSU HDR Video Reconstruction Benchmark", "description": "This is a dataset for a video inverse-tone-mapping task. The dataset contains various contents for the task of restoring HDR video: fireworks, flowers, football, night city, scenes with reflections. Videos have different brightness ranges and contain different types of lighting. The camera for shooting the dataset captures 14 stops of the dynamic range."}, {"id": "imagenet-o", "name": "ImageNet-O", "description": "ImageNet-O consists of images from classes that are not found in the ImageNet-1k dataset. It is used to test the robustness of vision models to out-of-distribution samples. It's reported using the AUPR metric."}, {"id": "inria-person", "name": "INRIA Person", "description": "The INRIA Person dataset is a dataset of images of persons used for pedestrian detection. It consists of 614 person detections for training and 288 for testing."}, {"id": "chinesefoodnet", "name": "ChineseFoodNet", "description": "ChineseFoodNet aims to automatically recognizing pictured Chinese dishes. Most of the existing food image datasets collected food images either from recipe pictures or selfie. In the dataset, images of each food category of the dataset consists of not only web recipe and menu pictures but photos taken from real dishes, recipe and menu as well. ChineseFoodNet contains over 180,000 food photos of 208 categories, with each category covering a large variations in presentations of same Chinese food. "}, {"id": "large-age-gap", "name": "Large Age-Gap", "description": "Large Age-Gap (LAG) is a dataset for face verification, The dataset contains 3,828 images of 1,010 celebrities. For each identity at least one child/young image and one adult/old image are present."}, {"id": "cuhk03-c", "name": "CUHK03-C", "description": "CUHK03-C is an evaluation set that consists of algorithmically generated corruptions applied to the CUHK03 test-set.  These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."}, {"id": "raincityscapes", "name": "RainCityscapes", "description": "A  dataset for rain removal with scene depth information. Compared with previous datasets, this dataset are all outdoor photos, each with a depth map, and the rain images exhibit different degrees of rain and fog."}, {"id": "kdconv-knowledge-driven-conversation", "name": "KdConv (Knowledge-driven Conversation)", "description": "KdConv is a Chinese multi-domain Knowledge-driven Conversation dataset, grounding the topics in multi-turn conversations to knowledge graphs. KdConv contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics, while the corpus can also used for exploration of transfer learning and domain adaptation."}, {"id": "coco-microsoft-common-objects-in-context", "name": "COCO (Microsoft Common Objects in Context)", "description": "The MS COCO (Microsoft Common Objects in Context) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images."}, {"id": "redcaps", "name": "RedCaps", "description": "RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. The data is collected from a manually curated set of subreddits (350 total), which give coarse image labels and allow steering of the dataset composition without labeling individual instances."}, {"id": "basque-timebank", "name": "Basque TimeBank", "description": "A set of basque documents annotated with EusTimeML - a mark-up language for temporal information in Basque."}, {"id": "lreid", "name": "LReID", "description": "LReID is a benchmark for lifelong person reidentification. It has been built using existing datasets, and it consists of two subsets: LReID-Seen and LReID-Unseen."}, {"id": "webvid", "name": "WebVid", "description": "WebVid contains 10 million video clips with captions, sourced from the web. The videos are diverse and rich in their content."}, {"id": "atlas", "name": "Atlas", "description": "Atlas is a dataset for e-commerce clothing product categorization. The Atlas dataset consists of a high-quality product taxonomy dataset focusing on clothing products which contain 186,150 images under clothing category with 3 levels and 52 leaf nodes in the taxonomy."}, {"id": "ddpm-deception-detection-and-physiological-monitoring", "name": "DDPM (Deception Detection and Physiological Monitoring)", "description": "The Deception Detection and Physiological Monitoring (DDPM) dataset captures an interview scenario in which the interviewee attempts to deceive the interviewer on selected responses. The interviewee is recorded in RGB, near-infrared, and long-wave infrared, along with cardiac pulse, blood oxygenation, and audio. After collection, data were annotated for interviewer/interviewee, curated, ground-truthed, and organized into train/test parts for a set of canonical deception detection experiments. The dataset contains almost 13 hours of recordings of 70 subjects, and over 8 million visible-light, near-infrared, and thermal video frames, along with appropriate meta, audio, and pulse oximeter data."}, {"id": "aflw-19-the-19-landmark-variant-of-aflw", "name": "AFLW-19 (The 19 landmark variant of AFLW.)", "description": "The original AFLW provides at most 21 points for each face, but excluding coordinates for invisible landmarks, causing difficulties for training most of the existing baseline approaches. To make fair comparisons, the authors manually annotate the coordinates of these invisible landmarks to enable training of those baseline approaches. The new annotation does not include two ear points because it is very difficult to decide the location of invisible ears. This causes the point number of AFLW-19 to be 19."}, {"id": "parade", "name": "PARADE", "description": "PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge."}, {"id": "phoenix14t-rwth-phoenix-weather-2014t", "name": "PHOENIX14T (RWTH-PHOENIX-Weather-2014T)", "description": "Over a period of three years (2009 - 2011) the daily news and weather forecast airings of the German public tv-station PHOENIX featuring sign language interpretation have been recorded and the weather forecasts of a subset of 386 editions have been transcribed using gloss notation. Furthermore, we used automatic speech recognition with manual cleaning to transcribe the original German speech. As such, this corpus allows to train end-to-end sign language translation systems from sign language video input to spoken language."}, {"id": "few-nerd", "name": "Few-NERD", "description": "Few-NERD is a large-scale, fine-grained manually annotated named entity recognition dataset, which contains 8 coarse-grained types, 66 fine-grained types, 188,200 sentences, 491,711 entities, and 4,601,223 tokens. Three benchmark tasks are built, one is supervised (Few-NERD (SUP)) and the other two are few-shot (Few-NERD (INTRA) and Few-NERD (INTER))."}, {"id": "vivid-vision-for-visibility-dataset", "name": "ViViD++ (Vision for Visibility Dataset)", "description": "A dataset capturing diverse visual data formats that target varying luminance conditions, and was recorded from alternative vision sensors, by handheld or mounted on a car, repeatedly in the same space but in different conditions."}, {"id": "urmp-university-of-rochester-multi-modal-musical-performance", "name": "URMP (University of Rochester Multi-Modal Musical Performance)", "description": "URMP (University of Rochester Multi-Modal Musical Performance) is a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises 44 simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece the dataset provided the musical score in MIDI format, the high-quality individual instrument audio recordings and the videos of the assembled pieces."}, {"id": "webqa", "name": "WebQA", "description": "WebQA, is a new benchmark for multimodal multihop reasoning in which systems are presented with the same style of data as humans when searching the web: Snippets and Images. The system must then identify which information is relevant across modalities and combine it with reasoning to answer the query. Systems will be evaluated on both the correctness of their answers and their sources."}, {"id": "legalnero-romanian-named-entity-recognition-in-the-legal-domain", "name": "LegalNERo (Romanian Named Entity Recognition in the Legal domain)", "description": "LegalNERo is a manually annotated corpus for named entity recognition in the Romanian legal domain.  It provides gold annotations for organizations, locations, persons, time and legal resources mentioned in legal documents. Additionally it offers GEONAMES codes for the named entities annotated as location (where a link could be established). "}, {"id": "fusion-360-gallery", "name": "Fusion 360 Gallery", "description": "The Fusion 360 Gallery Dataset contains rich 2D and 3D geometry data derived from parametric CAD models. The dataset is produced from designs submitted by users of the CAD package Autodesk Fusion 360 to the Autodesk Online Gallery. The dataset provides valuable data for learning how people design, including sequential CAD design data, designs segmented by modelling operation, and design hierarchy and connectivity data."}, {"id": "mex", "name": "MEx", "description": "A multi-sensor, multi-modal dataset, implemented to benchmark Human Activity Recognition(HAR) and Multi-modal Fusion algorithms. Collection of this dataset was inspired by the need for recognising and evaluating quality of exercise performance to support patients with Musculoskeletal Disorders(MSD)."}, {"id": "ifakefacedb", "name": "iFakeFaceDB", "description": "iFakeFaceDB is a face image dataset for the study of synthetic face manipulation detection, comprising about 87,000 synthetic face images generated by the Style-GAN model and transformed with the GANprintR approach. All images were aligned and resized to the size of 224 x 224."}, {"id": "hra-human-rights-archive-database", "name": "HRA (Human Rights Archive Database)", "description": "A verified-by-experts repository of 3050 human rights violations photographs, labelled with human rights semantic categories, comprising a list of the types of human rights abuses encountered at present. "}, {"id": "whu", "name": "WHU", "description": "Created for MVS tasks and is a large-scale multi-view aerial dataset generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters."}, {"id": "emopars", "name": "EmoPars", "description": "EmoPars is a dataset of 30,000 Persian Tweets labeled with Ekman\u2019s six basic emotions (Anger, Fear, Happiness, Sadness, Hatred, and Wonder). This is the first publicly available emotion dataset in the Persian language."}, {"id": "mitoem", "name": "MitoEM", "description": "Contains mitochondria instances."}, {"id": "artie-bias-corpus", "name": "Artie Bias Corpus", "description": "Artie Bias Corpus is an open dataset for detecting demographic bias in speech applications."}, {"id": "avecl-umons", "name": "AVECL-UMons", "description": "A dataset for audio-visual event classification and localization in the context of office environments. The audio-visual dataset is composed of 11 event classes recorded at several realistic positions in two different rooms. Two types of sequences are recorded according to the number of events in the sequence. The dataset comprises 2662 unilabel sequences and 2724 multilabel sequences corresponding to a total of 5.24 hours. "}, {"id": "the-tourism-forecasting-competition", "name": "The tourism forecasting competition", "description": "from the paper"}, {"id": "coco-stuff-common-objects-in-context-stuff", "name": "COCO-Stuff (Common Objects in COntext-stuff)", "description": "The Common Objects in COntext-stuff (COCO-stuff) dataset is a dataset for scene understanding tasks like semantic segmentation, object detection and image captioning. It is constructed by annotating the original COCO dataset, which originally annotated things while neglecting stuff annotations. There are 164k images in COCO-stuff dataset that span over 172 categories including 80 things, 91 stuff, and 1 unlabeled class."}, {"id": "multinli-multi-genre-natural-language-inference", "name": "MultiNLI (Multi-Genre Natural Language Inference)", "description": "The Multi-Genre Natural Language Inference (MultiNLI) dataset has 433K sentence pairs. Its size and mode of collection are modeled closely like SNLI. MultiNLI offers ten distinct genres (Face-to-face, Telephone, 9/11, Travel, Letters, Oxford University Press, Slate, Verbatim, Goverment and Fiction) of written and spoken English data. There are matched dev/test sets which are derived from the same sources as those in the training set, and mismatched sets which do not closely resemble any seen at training time."}, {"id": "skeleton-mimetics", "name": "Skeleton-Mimetics", "description": "A dataset derived from the recently introduced Mimetics dataset."}, {"id": "camels-multifield-dataset", "name": "CAMELS Multifield Dataset", "description": "CMD is a publicly available collection of hundreds of thousands 2D maps and 3D grids containing different properties of the gas, dark matter, and stars from more than 2,000 different universes. The data has been generated from thousands of state-of-the-art (magneto-)hydrodynamic and gravity-only N-body simulations from the CAMELS project."}, {"id": "fluocells-fluorescent-neuronal-cells", "name": "fluocells (Fluorescent Neuronal Cells)", "description": "By releasing this dataset, we aim at providing a new testbed for computer vision techniques using Deep Learning. The main peculiarity is the shift from the domain of \"natural images\" proper of common benchmark dataset to biological imaging. We anticipate that the advantages of doing so could be two-fold: i) fostering research in biomedical-related fields - for which popular pre-trained models perform typically poorly - and ii) promoting methodological research in deep learning by addressing peculiar requirements of these images. Possible applications include but are not limited to semantic segmentation, object detection and object counting. The data consist of 283 high-resolution pictures (1600x1200 pixels) of mice brain slices acquired through a fluorescence microscope. The final goal is to individuate and count neurons highlighted in the pictures by means of a marker, so to assess the result of a biological experiment. The corresponding ground-truth labels were generated through a hybrid approach involving semi-automatic and manual semantic segmentation. The result consists of black (0) and white (255) images having pixel-level annotations of where the stained neurons are located. For more information, please refer to Morelli, R. et al., 2021. Automating cell counting in fluorescent microscopy through deep learning with c-ResUnet. Scientific reports. https://doi.org/10.1038/s41598-021-01929-5. The collection of original images was supported by funding from the University of Bologna (RFO 2018) and the European Space Agency (Research agreement collaboration 4000123556)."}, {"id": "ms-2-multi-document-summarization-of-medical-studies", "name": "MS^2 (Multi-Document Summarization of Medical Studies)", "description": "MS^2 (Multi-Document Summarization of Medical Studies) is a dataset of over 470k documents and 20k summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is one of the first large-scale, publicly available multi-document summarization dataset in the biomedical domain."}, {"id": "inter4k", "name": "Inter4K", "description": "A video dataset for benchmarking upsampling methods. Inter4K contains 1,000 ultra-high resolution videos with 60 frames per second (fps) from online resources. The dataset provides standardized video resolutions at ultra-high definition (UHD/4K), quad-high definition (QHD/2K), full-high definition (FHD/1080p), (standard) high definition (HD/720p), one quarter of full HD (qHD/520p) and one ninth of a full HD (nHD/360p). We use frame rates of 60, 50, 30, 24 and 15 fps for each resolution. Based on this standardization, both super-resolution and frame interpolation tests can be performed for different scaling sizes ($\\times 2$, $\\times 3$ and $\\times 4$). In this paper, we use Inter4K to address frame upsampling and interpolation. Inter4K provides both standardized UHD resolution and 60 fps for all of videos by also containing a diverse set of 1,000 5-second videos. Differences between scenes originate from the equipment (e.g., professional 4K cameras or phones), lighting conditions, variations in movements, actions or objects. The dataset is divided into 800 videos for training, 100 videos for validation and 100 videos for testing."}, {"id": "xcopa", "name": "XCOPA", "description": "The Cross-lingual Choice of Plausible Alternatives (XCOPA) dataset is a benchmark to evaluate the ability of machine learning models to transfer commonsense reasoning across languages. The dataset is the translation and reannotation of the English COPA (Roemmele et al. 2011) and covers 11 languages from 11 families and several areas around the globe. The dataset is challenging as it requires both the command of world knowledge and the ability to generalise to new languages."}, {"id": "human4d", "name": "HUMAN4D", "description": "HUMAN4D is a large and multimodal 4D dataset that contains a variety of human activities simultaneously captured by a professional marker-based MoCap, a volumetric capture and an audio recording system. By capturing 2 female and $2$ male professional actors performing various full-body movements and expressions, HUMAN4D provides a diverse set of motions and poses encountered as part of single- and multi-person daily, physical and social activities (jumping, dancing, etc. ), along with multi-RGBD (mRGBD), volumetric and audio data."}, {"id": "pubmed", "name": "Pubmed", "description": "The Pubmed dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words."}, {"id": "fs-mol", "name": "FS-Mol", "description": "A Few-Shot Learning Dataset of Molecules."}, {"id": "toxigen", "name": "ToxiGen", "description": "A large-scale and machine-generated dataset of 274,186 toxic and benign statements about 13 minority groups. "}, {"id": "tarc", "name": "TArC", "description": "A morpho-syntactically annotated Tunisian Arabish Corpus (TArC)."}, {"id": "msra-td500-msra-text-detection-500-database", "name": "MSRA-TD500 (MSRA Text Detection 500 Database)", "description": "The MSRA-TD500 dataset is a text detection dataset that contains 300 training images and 200 test images. Text regions are arbitrarily orientated and annotated at sentence level. Different from the other datasets, it contains both English and Chinese text."}, {"id": "synthcity", "name": "SynthCity", "description": "SynthCity is a 367.9M point synthetic full colour Mobile Laser Scanning point cloud. Every point is assigned a label from one of nine categories."}, {"id": "parabank", "name": "ParaBank", "description": "A large-scale English paraphrase dataset that surpasses prior work in both quantity and quality."}, {"id": "juice-juice-dataset", "name": "JuICe (JuICe Dataset)", "description": "JuICe is a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data."}, {"id": "fig-qa", "name": "Fig-QA", "description": "Fig-QA consists of 10256 examples of human-written creative metaphors that are paired as a Winograd schema. It can be used to evaluate the commonsense reasoning of models. The metaphors themselves can also be used as training data for other tasks, such as metaphor detection or generation."}, {"id": "fakenewsnet", "name": "FakeNewsNet", "description": "FakeNewsNet is collected from two fact-checking websites: GossipCop and PolitiFact containing news contents with labels annotated by professional journalists and experts, along with social context information."}, {"id": "air-act2act", "name": "AIR-Act2Act", "description": "AIR-Act2Act is a human-human interaction dataset for teaching non-verbal social behaviors to robots. It is different from other datasets because elderly people have participated in as performers. The authors recruited 100 elderly people and two college students to perform 10 interactions in an indoor environment. The entire dataset has 5,000 interaction samples, each of which contains depth maps, body indexes and 3D skeletal data that are captured with three Microsoft Kinect v2 cameras. In addition, the dataset also contains the joint angles of a humanoid NAO robot which are converted from the human behavior that robots need to learn."}, {"id": "pedx", "name": "PedX", "description": "PedX is a large-scale multi-modal collection of pedestrians at complex urban intersections. The dataset provides high-resolution stereo images and LiDAR data with manual 2D and automatic 3D annotations. The data was captured using two pairs of stereo cameras and four Velodyne LiDAR sensors."}, {"id": "habitat-platform", "name": "Habitat Platform", "description": "A platform for research in embodied artificial intelligence (AI)."}, {"id": "smm4h-social-media-mining-for-health-shared-task", "name": "SMM4H (Social Media Mining for Health Shared Task)", "description": "Social Media Mining for Health (SMM4H) Shared Task is a massive data source for biomedical and public health applications."}, {"id": "easycom", "name": "EasyCom", "description": "The Easy Communications (EasyCom) dataset is a world-first dataset designed to help mitigate the cocktail party effect from an augmented-reality (AR) -motivated multi-sensor egocentric world view. The dataset contains AR glasses egocentric multi-channel microphone array audio, wide field-of-view RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head and face bounding boxes and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem."}, {"id": "luna16", "name": "LUNA16", "description": "The LUNA16 (LUng Nodule Analysis) dataset is a dataset for lung segmentation. It consists of 1,186 lung nodules annotated in 888 CT scans."}, {"id": "kinetics-400", "name": "Kinetics 400", "description": "The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands."}, {"id": "dcase-2014", "name": "DCASE 2014", "description": "DCASE2014 is an audio classification benchmark."}, {"id": "headqa", "name": "HeadQA", "description": "HeadQA is a multi-choice question answering testbed to encourage research on complex reasoning. The questions come from exams to access a specialized position in the Spanish healthcare system, and are challenging even for highly specialized humans. "}, {"id": "mc-taco", "name": "MC-TACO", "description": "MC-TACO is a dataset of 13k question-answer pairs that require temporal commonsense comprehension. The dataset contains five temporal properties, (1) duration (how long an event takes), (2) temporal ordering (typical order of events), (3) typical time (when an event occurs), (4) frequency (how often an event occurs), and (5) stationarity (whether a state is maintained for a very long time or indefinitely). "}, {"id": "montreal-archive-of-sleep-studies", "name": "Montreal Archive of Sleep Studies", "description": "The Montreal Archive of Sleep Studies (MASS) is an open-access and collaborative database of laboratory-based polysomnography (PSG) recordings O\u2019Reilly, C., et al. (2014) J Seep Res, 23(6):628-635. Its goal is to provide a standard and easily accessible source of data for benchmarking the various systems developed to help the automation of sleep analysis. It also provides a readily available source of data for fast validation of experimental results and for exploratory analyses. Finally, it is a shared resource that can be used to foster large-scale collaborations in sleep studies."}, {"id": "wits-why-is-this-sarcastic", "name": "WITS (Why Is This Sarcastic?)", "description": "This dataset is an extension of MASAC, a multimodal, multi-party, Hindi-English code-mixed dialogue dataset compiled from the popular Indian TV show, \u2018Sarabhai v/s Sarabhai\u2019.  WITS was created by augmenting MASAC with natural language explanations for each sarcastic dialogue. The dataset consists of the transcribed sarcastic dialogues from 55 episodes of the TV show, along with audio and video multimodal signals. It was designed to facilitate Sarcasm Explanation in Dialogue (SED), a novel task aimed at generating a natural language explanation for a given sarcastic dialogue, that spells out the intended irony. Each data instance in WITS is associated with a corresponding video, audio, and textual transcript where the last utterance is sarcastic in nature. All the final selected explanations contain the following attributes:"}, {"id": "msls-mapillary-street-level-sequences-dataset", "name": "MSLS (Mapillary Street-level Sequences Dataset)", "description": "The largest and most diverse dataset for lifelong place recognition from image sequences in urban and suburban settings."}, {"id": "imagenet-ctest10k", "name": "ImageNet ctest10k", "description": "Colorization validation set for unconditional/conditional colorization tasks.  Subset of the ImageNet validation images and excludes andy grayscale single-channel images."}, {"id": "emotional-dialogue-acts", "name": "Emotional Dialogue Acts", "description": "Emotional Dialogue Acts data contains dialogue act labels for existing emotion multi-modal conversational datasets. We chose two popular multimodal emotion datasets: Multimodal EmotionLines Dataset (MELD) and Interactive Emotional dyadic MOtion CAPture database (IEMOCAP).  EDAs reveal associations between dialogue acts and emotional states in a natural-conversational language such as Accept/Agree dialogue acts often occur with the Joy emotion, Apology with Sadness, and Thanking with Joy."}, {"id": "synthhands", "name": "SynthHands", "description": "The SynthHands dataset is a dataset for hand pose estimation which consists of real captured hand motion retargeted to a virtual hand with natural backgrounds and interactions with different objects. The dataset contains data for male and female hands, both with and without interaction with objects. While the hand and foreground object are synthtically generated using Unity, the motion was obtained from real performances as described in the accompanying paper. In addition, real object textures and background images (depth and color) were used. Ground truth 3D positions are provided for 21 keypoints of the hand."}, {"id": "honest-hurtful-sentence-completion-in-english-language-models", "name": "HONEST (Hurtful Sentence Completion in English Language Models)", "description": "The HONEST dataset is a template-based corpus for testing the hurtfulness of sentence completions in language models (e.g., BERT) in six different languages (English, Italian, French, Portuguese, Romanian, and Spanish).  HONEST is composed of 420 instances for each language, which are generated from 28 identity terms (14 male and 14 female) and 15 templates. It uses a set of identifier terms in singular and plural (i.e., woman, women, girl, boys) and a series of predicates (i.e., \u201cworks as [MASK]\u201d, \u201cis known for [MASK]\u201d). The objective is to use language models to fill the sentence, then the hurtfulness of the completion is evaluated."}, {"id": "loed", "name": "LoED", "description": "LoED (LoRaWAN at the Edge Dataset) is a dataset from nine LoRaWAN gateways collected in an urban environment. The dataset contains raw payload information, along with other metadata from the gateway. The dataset contains packet header information and all physical layer properties reported by gateways such as the CRC, RSSI, SNR and spreading factor. Files are provided to analyse the data and get aggregated statistics"}, {"id": "stackex", "name": "STACKEX", "description": "STACKEX expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks. "}, {"id": "sep-28k-stuttering-events-in-podcasts", "name": "SEP-28k (Stuttering Events in Podcasts)", "description": "Stuttering Events in Podcasts (SEP-28k) is a dataset containing over 28k clips labeled with five event types including blocks, prolongations, sound repetitions, word repetitions, and interjections. Audio comes from public podcasts largely consisting of people who stutter interviewing other people who stutter. "}, {"id": "groningen-meaning-bank", "name": "Groningen Meaning Bank", "description": "Groningen Meaning Bank is a semantic resource that anyone can edit and that integrates various semantic phenomena, including predicate-argument structure, scope, tense, thematic roles, animacy, pronouns, and rhetorical relations."}, {"id": "slashdot", "name": "Slashdot", "description": "The Slashdot dataset is a relational dataset obtained from Slashdot. Slashdot is a technology-related news website know for its specific user community. The website features user-submitted and editor-evaluated current primarily technology oriented news. In 2002 Slashdot introduced the Slashdot Zoo feature which allows users to tag each other as friends or foes. The network cotains friend/foe links between the users of Slashdot. The network was obtained in February 2009."}, {"id": "icl-nuim", "name": "ICL-NUIM", "description": "The ICL-NUIM dataset aims at benchmarking RGB-D, Visual Odometry and SLAM algorithms. Two different scenes (the living room and the office room scene) are provided with ground truth. Living room has 3D surface ground truth together with the depth-maps as well as camera poses and as a result perfectly suits not just for benchmarking camera trajectory but also reconstruction. Office room scene comes with only trajectory data and does not have any explicit 3D model with it."}, {"id": "vizwiz-captions", "name": "VizWiz-Captions", "description": "Consists of over 39,000 images originating from people who are blind that are each paired with five captions."}, {"id": "ipn-hand", "name": "IPN Hand", "description": "The IPN Hand dataset is a benchmark video dataset with sufficient size, variation, and real-world elements able to train and evaluate deep neural networks for continuous Hand Gesture Recognition (HGR)."}, {"id": "indian-pines", "name": "Indian Pines", "description": "Indian Pines is a Hyperspectral image segmentation dataset. The input data consists of hyperspectral bands over a single landscape in Indiana, US, (Indian Pines data set) with 145\u00d7145 pixels. For each pixel, the data set contains 220 spectral reflectance bands which represent different portions of the electromagnetic spectrum in the wavelength range 0.4\u22122.5\u22c510\u22126."}, {"id": "pcd-poem-comprehensive-dataset", "name": "PCD (Poem Comprehensive Dataset)", "description": "The Arabic dataset is scraped mainly from \u0627\u0644\u0645\u0648\u0633\u0648\u0639\u0629 \u0627\u0644\u0634\u0639\u0631\u064a\u0629 and \u0627\u0644\u062f\u064a\u0648\u0627\u0646. After merging both, the total number of verses is 1,831,770 poetic verses. Each verse is labeled by its meter, the poet who wrote it, and the age which it was written in. There are 22 meters, 3701 poets and 11 ages: Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman, Andalusian, era between Umayyad and Abbasid, Fatimid, and finally the modern age. We are only interested in the 16 classic meters which are attributed to Al-Farahidi, and they comprise the majority of the dataset with a total number around 1.7M verses. It is important to note that the verses diacritic states are not consistent. This means that a verse can carry full, semi diacritics, or it can carry nothing."}, {"id": "mit-bih-afdb-mit-bih-atrial-fibrilation-database", "name": "MIT-BIH AFDB (MIT-BIH Atrial Fibrilation Database)", "description": "This database includes 25 long-term ECG recordings of human subjects with atrial fibrillation (mostly paroxysmal)."}, {"id": "tut-sed-synthetic-2016", "name": "TUT-SED Synthetic 2016", "description": "TUT-SED Synthetic 2016 contains of mixture signals artificially generated from isolated sound events samples. This approach is used to get more accurate onset and offset annotations than in dataset using recordings from real acoustic environments where the annotations are always subjective. Mixture signals in the dataset are created by randomly selecting and mixing isolated sound events from 16 sound event classes together. The resulting mixtures contains sound events with varying polyphony. All together 994 sound event samples were purchased from Sound Ideas. From the 100 mixtures created, 60% were assigned for training, 20% for testing and 20% for validation. The total amount of audio material in the dataset is 566 minutes. Different instances of the sound events are used to synthesize the training, validation and test partitions. Mixtures were created by randomly selecting event instance and from it, randomly, a segment of length 3-15 seconds. Between events, random length silent region was introduced. Such tracks were created for four to nine event classes, and were then mixed together to form the mixture signal. As sound events are not consistently active during the samples (e.g. footsteps), automatic signal energy based annotation was applied to obtain accurate event activity within the sample. Annotation of the mixture signal was created by pooling together event activity annotation of used samples."}, {"id": "chinahate", "name": "#chinahate", "description": "#chinahate dataset contains a total of 2,172,333 tweets hashtagged #china posted during the time it was collected. It is designed for the task of hate speech detection."}, {"id": "nlu-nllu-a-multi-label-slot-rich-generalisable-dataset-for-natural-language-understanding-in-task-oriented-dialogue", "name": "NLU++ (NLLU++ : A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue)", "description": "nlu++ is a dataset for natural language understanding (NLU) in task-oriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. nlu++ is divided into two domains (banking and hotels) and brings several crucial improvements over current commonly used NLU datasets. 1) Nlu++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences, introducing and validating the idea of intent modules that can be combined into complex intents that convey complex user goals, combined with finer-grained and thus more challenging slot sets. 2) The ontology is divided into domain-specific and generic (i.e., domain-universal) intent modules that overlap across domains, promoting cross-domain reusability of annotated examples. 3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data."}, {"id": "surveillance-camera-fight-dataset", "name": "Surveillance Camera Fight Dataset", "description": "The dataset is collected from the Youtube videos that contains fight instances in it. Also, some non-fight sequences from regular surveillance camera videos are included. * There are 300 videos in total as 150 fight + 150 non-fight * Videos are 2-second long * Only the fight related parts are included in the samples"}, {"id": "shhs-sleep-heart-health-study", "name": "SHHS (Sleep Heart Health Study)", "description": "The Sleep Heart Health Study (SHHS) is a multi-center cohort study implemented by the National Heart Lung & Blood Institute to determine the cardiovascular and other consequences of sleep-disordered breathing. It tests whether sleep-related breathing is associated with an increased risk of coronary heart disease, stroke, all cause mortality, and hypertension.  In all, 6,441 men and women aged 40 years and older were enrolled between November 1, 1995 and January 31, 1998 to take part in SHHS Visit 1. During exam cycle 3 (January 2001- June 2003), a second polysomnogram (SHHS Visit 2) was obtained in 3,295 of the participants. CVD Outcomes data were monitored and adjudicated by parent cohorts between baseline and 2011. More than 130 manuscripts have been published investigating predictors and outcomes of sleep disorders."}, {"id": "adverse-drug-events-ade-corpus", "name": "Adverse Drug Events (ADE) Corpus", "description": "Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports."}, {"id": "cops-ref", "name": "Cops-Ref", "description": "Cops-Ref is a dataset for visual reasoning in context of referring expression comprehension with two main features."}, {"id": "lsmdc-large-scale-movie-description-challenge", "name": "LSMDC (Large Scale Movie Description Challenge)", "description": "This dataset contains 118,081 short video clips extracted from 202 movies. Each video has a caption, either extracted from the movie script or from transcribed DVS (descriptive video services) for the visually impaired. The validation set contains 7408 clips and evaluation is performed on a test set of 1000 videos from movies disjoint from the training and val sets."}, {"id": "med-monotonicity-entailment-dataset", "name": "MED (Monotonicity Entailment Dataset)", "description": "MED is a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications. The dataset was constructed by collecting naturally-occurring examples by crowdsourcing and well-designed ones from linguistics publications. It consists of 5,382 examples."}, {"id": "tau-spatial-sound-events-2019", "name": "TAU Spatial Sound Events 2019", "description": "TAU Spatial Sound Events 2019 consists of 2 datasets: Ambisonic (FOA) and Microphone Array (MIC), of identical sound scenes with the only difference in the format of the audio. The FOA dataset provides four-channel First-Order Ambisonic recordings while the MIC dataset provides four-channel directional microphone recordings from a tetrahedral array configuration. Both formats are extracted from the same microphone array."}, {"id": "covaxlies-v1", "name": "CoVaxLies v1", "description": "CoVaxLies v1 includes 17 known Misinformation Targets (MisTs) found on Twitter about the covid-19 vaccines. Language experts annotated tweets as Relevant or Not Relevant, and then further annotated Relevant tweets with Stance towards each MisT. This collection is a first step in providing large-scale resources for misinformation detection and misinformation stance identification."}, {"id": "math23k-math23k-for-math-word-problem-solving", "name": "Math23K (Math23K for Math Word Problem Solving)", "description": "Math23K is a dataset created for math word problem solving, contains 23, 162 Chinese problems crawled from the Internet. Refer to our paper for more details: The dataset is originally introduced in the paper Deep Neural Solver for Math Word Problems.  The original files are originally split into train/test split, while other research efforts (https://github.com/2003pro/Graph2Tree) perform the train/dev/test split."}, {"id": "elba-elba-element-based-textures-dataset", "name": "ElBa (ElBa: Element Based Textures Dataset)", "description": "ElBa is composed of procedurally-generated realistic renderings, where we vary in a continuous way element shapes and colors and their distribution, to generate 30K texture images with different local symmetry, stationarity, and density of (3M) localized texels, whose attributes are thus known by construction."}, {"id": "silg-symbolic-interactive-language-grounding", "name": "SILG (Symbolic Interactive Language Grounding)", "description": "Symbolic Interactive Language Grounding (SILG) is a multi-environment benchmark which unifies a collection of diverse grounded language learning environments under a common interface. SILG consists of grid-world environments that require generalization to new dynamics, entities, and partially observed worlds (RTFM, Messenger, NetHack), as well as symbolic counterparts of visual worlds that require interpreting rich natural language with respect to complex scenes (ALFWorld, Touchdown). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language specification, and plan complexity."}, {"id": "vidhoi", "name": "VidHOI", "description": "VidHOI is a video-based human-object interaction detection benchmark. VidHOI is based on VidOR which is densely annotated with all humans and predefined objects showing up in each frame. VidOR is also more challenging as the videos are non-volunteering user-generated and thus jittery at times."}, {"id": "lince-linguistic-code-switching-evaluation-dataset", "name": "LinCE (Linguistic Code-switching Evaluation Dataset)", "description": "A centralized benchmark for Linguistic Code-switching Evaluation (LinCE) that combines ten corpora covering four different code-switched language pairs (i.e., Spanish-English, Nepali-English, Hindi-English, and Modern Standard Arabic-Egyptian Arabic) and four tasks (i.e., language identification, named entity recognition, part-of-speech tagging, and sentiment analysis). "}, {"id": "phantom-physical-anomalous-trajectory-or-motion-phantom", "name": "PHANTOM (Physical Anomalous Trajectory or Motion (PHANTOM))", "description": "To evaluate the presented approaches, we created the Physical Anomalous Trajectory or Motion (PHANTOM) dataset consisting of six classes featuring everyday objects or physical setups, and showing nine different kinds of anomalies. We designed our classes to evaluate detection of various modes of video abnormalities that are generally excluded in video AD settings."}, {"id": "xfund-a-multilingual-form-understanding-benchmark", "name": "XFUND (A Multilingual Form Understanding Benchmark)", "description": "XFUND is a multilingual form understanding benchmark dataset that includes human-labeled forms with key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese)."}, {"id": "oulu-npu", "name": "OULU-NPU", "description": "The Oulu-NPU face presentation attack detection database consists of 4950 real access and attack videos. These videos were recorded using the front cameras of six mobile devices (Samsung Galaxy S6 edge, HTC Desire EYE, MEIZU X5, ASUS Zenfone Selfie, Sony XPERIA C5 Ultra Dual and OPPO N3) in three sessions with different illumination conditions and background scenes. The presentation attack types considered in the OULU-NPU database are print and video-replay. The 2D face artefacts were created using two printers and two display devices. "}, {"id": "chime-home", "name": "CHiME-Home", "description": "CHiME-Home is a dataset for sound source recognition in a domestic environment. It uses around 6.8 hours of domestic environment audio recordings. The recordings were obtained from the CHiME projects \u2013 computational hearing in multisource environments \u2013 where recording equipment was positioned inside an English Victorian semi-detached house. The recordings were selected from 22 sessions totalling 19.5 hours, with each session made between 7:30 in the morning and 20:00 in the evening. In the considered recordings, the equipment was placed in the lounge (sitting room) near the door opening onto a hallway, with the hallway opening onto a kitchen with no door. With the lounge door typically open, prominent sounds thus may originate from sources both in the lounge and kitchen."}, {"id": "mr-tydi", "name": "Mr. TYDI", "description": "Mr. TyDi is a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data."}, {"id": "sid-see-in-the-dark", "name": "SID (See-in-the-Dark)", "description": "The See-in-the-Dark (SID) dataset contains 5094 raw short-exposure images, each with a corresponding long-exposure reference image. Images were captured using two cameras: Sony \u03b17SII and Fujifilm X-T2."}, {"id": "esd-emotional-speech-database", "name": "ESD (Emotional Speech Database)", "description": "ESD is an Emotional Speech Database for voice conversion research. The ESD database consists of 350 parallel utterances spoken by 10 native English and 10 native Chinese speakers and covers 5 emotion categories (neutral, happy, angry, sad and surprise). More than 29 hours of speech data were recorded in a controlled acoustic environment. The database is suitable for multi-speaker and cross-lingual emotional voice conversion studies."}, {"id": "r2r-room-to-room", "name": "R2R (Room-to-Room)", "description": "R2R is a dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings, as illustrated in the demo above. For training, each instruction is associated with a Matterport3D Simulator trajectory. 22k instructions are available, with an average length of 29 words. There is a test evaluation server for this dataset available at EvalAI."}, {"id": "fg-net", "name": "FG-NET", "description": "FGNet is a dataset for age estimation and face recognition across ages. It is composed of a total of 1,002 images of 82 people with age range from 0 to 69 and an age gap up to 45 years"}, {"id": "lccc-large-scale-cleaned-chinese-conversation-corpus", "name": "LCCC (Large-scale Cleaned Chinese Conversation corpus)", "description": "Contains a base version (6.8million dialogues) and a large version (12.0 million dialogues). "}, {"id": "msaw-multi-sensor-all-weather-mapping", "name": "MSAW (Multi-Sensor All Weather Mapping)", "description": "Multi-Sensor All Weather Mapping (MSAW) is a dataset and challenge, which features two collection modalities (both SAR and optical). The dataset and challenge focus on mapping and building footprint extraction using a combination of these data sources. MSAW covers 120 km^2 over multiple overlapping collects and is annotated with over 48,000 unique building footprints labels, enabling the creation and evaluation of mapping algorithms for multi-modal data. "}, {"id": "ovis-occluded-video-instance-segmentation", "name": "OVIS (Occluded Video Instance Segmentation)", "description": "OVIS is a new large scale benchmark dataset for video instance segmentation task. It is designed with the philosophy of perceiving object occlusions in videos, which could reveal the complexity and the diversity of real-world scenes. OVIS consists of:"}, {"id": "onestopqa", "name": "OneStopQA", "description": "OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance."}, {"id": "r2vq-recipe-to-video-questions", "name": "R2VQ (Recipe-to-Video Questions)", "description": "R2VQ is a dataset designed for testing competence-based comprehension of machines over a multimodal recipe collection, which contains text-video aligned recipes."}, {"id": "dreaddit", "name": "Dreaddit", "description": "Consists of 190K posts from five different categories of Reddit communities."}, {"id": "deepstab", "name": "DeepStab", "description": "DeepStab is a dataset for online video stabilization consisting of synchronized steady/unsteady video pairs collected via a well designed hand-held hardware."}, {"id": "asirra-animal-species-image-recognition-for-restricting-access", "name": "ASIRRA ((Animal Species Image Recognition for Restricting Access)", "description": "Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords."}, {"id": "public-git-archive", "name": "Public Git Archive", "description": "The Public Git Archive is a dataset of 182,014 top-bookmarked Git repositories from GitHub totalling 6 TB. The dataset provides the source code of the projects, the related metadata, and development history."}, {"id": "film-60-20-20-random-splits", "name": "Film (60%/20%/20% random splits)", "description": "Node classification on Film with 60%/20%/20% random splits for training/validation/test."}, {"id": "hutics-human-deictic-gestures-dataset", "name": "HuTics (Human Deictic Gestures Dataset)", "description": "HuTics contains 2040 images showing how humans use deictic gestures to interact with various daily-life objects. The images are annotated by segmentation masks of the object(s) of interest. The original purpose of the data collection is for gesture-aware object-agnostic segmentation tasks."}, {"id": "emnist-extended-mnist", "name": "EMNIST (Extended MNIST)", "description": "EMNIST (extended MNIST) has 4 times more data than MNIST. It is a set of handwritten digits with a 28 x 28 format."}, {"id": "frll-morphs-face-research-london-lab-morphs", "name": "FRLL-Morphs (Face Research London Lab Morphs)", "description": "FRLL-Morphs is a dataset of morphed faces based on images selected from the publicly available Face Research London Lab dataset [1]."}, {"id": "semantic-question-similarity-in-arabic-nsurl-2019-shared-task-8-semantic-question-similarity-in-arabic", "name": "Semantic Question Similarity in Arabic (NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic)", "description": "NSURL-2019 Shared Task 8: Semantic Question Similarity in Arabic"}, {"id": "aip-environment", "name": "AIP Environment", "description": "AI Playground (AIP) is an open-source, Unreal Engine-based tool for generating and labeling virtual image data. With AIP, it is trivial to capture the same image under different conditions (e.g., fidelity, lighting, etc.) and with different ground truths (e.g., depth or surface normal values). AIP is easily extendable and can be used with or without code."}, {"id": "evican", "name": "EVICAN", "description": "Deep learning use for quantitative image analysis is exponentially increasing. However, training accurate, widely deployable deep learning algorithms requires a plethora of annotated (ground truth) data. Image collections must contain not only thousands of images to provide sufficient example objects (i.e. cells), but also contain an adequate degree of image heterogeneity. We present a new dataset, EVICAN-Expert visual cell annotation, comprising partially annotated grayscale images of 30 different cell lines from multiple microscopes, contrast mechanisms and magnifications that is readily usable as training data for computer vision applications. With 4600 images and \u223c26 000 segmented cells, our collection offers an unparalleled heterogeneous training dataset for cell biology deep learning application development. The dataset is freely available (https://edmond.mpdl.mpg.de/imeji/collection/l45s16atmi6Aa4sI?q=)."}, {"id": "cnn-filter-db", "name": "CNN Filter DB", "description": "A database of over 1.4 billion 3x3 convolution filters extracted from hundreds of diverse CNN models with relevant meta information."}, {"id": "chineselp", "name": "ChineseLP", "description": "The ChineseLP dataset contains 411 vehicle images (mostly of passenger cars) with Chinese license plates (LPs). It consists of 252 images captured by the authors and 159 images downloaded from the internet. The images present great variations in resolution (from 143 \u00d7 107 to 2048 \u00d7 1536 pixels), illumination and background."}, {"id": "swimseg-singapore-whole-sky-imaging-segmentation-database", "name": "SWIMSEG (Singapore Whole sky IMaging SEGmentation Database)", "description": "The SWIMSEG dataset contains 1013 images of sky/cloud patches, along with their corresponding binary segmentation maps. The ground truth annotation was done in consultation with experts from Singapore Meteorological Services.  All images were captured in Singapore using WAHRSIS, a calibrated ground-based whole sky imager, over a period of 22 months from October 2013 to July 2015. Each patch covers about 60-70 degrees of the sky with a resolution of 600x600 pixels."}, {"id": "uava-uav-assistant", "name": "UAVA (UAV Assistant)", "description": "The UAVA,UAV-Assistant, dataset is specifically designed for fostering applications which consider UAVs and humans as cooperative agents. We employ a real-world 3D scanned dataset (Matterport3D), physically-based rendering, a gami\ufb01ed simulator for realistic drone navigation trajectory collection, to generate realistic multimodal data both from the user\u2019s exocentric view of the drone, as well as the drone\u2019s egocentric view."}, {"id": "tekgen", "name": "TekGen", "description": "The Dataset is part of the KELM corpus"}, {"id": "propara", "name": "ProPara", "description": "The ProPara dataset is designed to train and test comprehension of simple paragraphs describing processes (e.g., photosynthesis), designed for the task of predicting, tracking, and answering questions about how entities change during the process."}, {"id": "sosd-searching-on-sorted-data", "name": "SOSD (Searching on Sorted Data)", "description": "SOSD is a collection of dataset to benchmark the lookup performance of learned indexes."}, {"id": "openeds2020", "name": "OpenEDS2020", "description": "OpenEDS2020 is a dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display mounted with two synchronized eye-facing cameras. The dataset, which is anonymized to remove any personally identifiable information on participants, consists of 80 participants of varied appearance performing several gaze-elicited tasks, and is divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560 sequences containing 550,400 eye-images and respective gaze vectors, created to foster research in spatio-temporal gaze estimation and prediction approaches; and 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz, with up to 29,500 images, of which 5% contain a semantic segmentation label, devised to encourage the use of temporal information to propagate labels to contiguous frames. "}, {"id": "vimeo90k", "name": "Vimeo90K", "description": "The Vimeo-90K is a large-scale high-quality video dataset for lower-level video processing. It proposes three different video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution."}, {"id": "chaoyang", "name": "Chaoyang", "description": "Chaoyang dataset contains 1111 normal, 842 serrated, 1404 adenocarcinoma, 664 adenoma, and 705 normal, 321 serrated, 840 adenocarcinoma, 273 adenoma samples for training and testing, respectively. This noisy dataset is constructed in the real scenario.  "}, {"id": "imagenette", "name": "Imagenette", "description": "Imagenette is a subset of 10 easily classified classes from Imagenet (bench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute)."}, {"id": "re-docred-revisiting-document-level-relation-extraction", "name": "Re-DocRED (Revisiting Document Level Relation Extraction)", "description": "The Re-DocRED Dataset resolved the following problems of DocRED:"}, {"id": "webface260m", "name": "WebFace260M", "description": "WebFace260M is a million-scale face benchmark, which is constructed for the research community towards closing the data gap behind the industry."}, {"id": "places205", "name": "Places205", "description": "The Places205 dataset is a large-scale scene-centric dataset with 205 common scene categories. The training dataset contains around 2,500,000 images from these categories. In the training set, each scene category has the minimum 5,000 and maximum 15,000 images. The validation set contains 100 images per category (a total of 20,500 images), and the testing set includes 200 images per category (a total of 41,000 images)."}, {"id": "freiburg-forest", "name": "Freiburg Forest", "description": "The Freiburg Forest dataset was collected using a Viona autonomous mobile robot platform equipped with cameras for capturing multi-spectral and multi-modal images. The dataset may be used for evaluation of different perception algorithms for segmentation, detection, classification, etc. All scenes were recorded at 20 Hz with a camera resolution of 1024x768 pixels. The data was collected on three different days to have enough variability in lighting conditions as shadows and sun angles play a crucial role in the quality of acquired images. The robot traversed about 4.7 km each day. The dataset creators provide manually annotated pixel-wise ground truth segmentation masks for 6 classes: Obstacle, Trail, Sky, Grass, Vegetation, and Void."}, {"id": "cad-120", "name": "CAD-120", "description": "The CAD-60 and CAD-120 data sets comprise of RGB-D video sequences of humans performing activities which are recording using the Microsoft Kinect sensor. Being able to detect human activities is important for making personal assistant robots useful in performing assistive tasks. The CAD dataset comprises twelve different activities (composed of several sub-activities) performed by four people in different environments, such as a kitchen, a living room, and office, etc."}, {"id": "binarycorp", "name": "BinaryCorp", "description": "BinaryCorp is built for binary similarity detection based on the ArchLinux official repositories and Arch User Repository. BinaryCorp contains tens of thousands of software, including editors, instant messenger, HTTP server, web browser, compiler, graphics library, cryptographic library, etc.  The binary code similarity task requires a large number of labeled data, thus we use the infrastructures provided by ArchLinux to construct our dataset with different optimization levels (e.g O0, O1, O2, O3, Os)."}, {"id": "spgispeech", "name": "SPGISpeech", "description": "SPGISpeech (pronounced \u201cspeegie-speech\u201d) is a large-scale transcription dataset, freely available for academic research. SPGISpeech is a collection of 5,000 hours of professionally-transcribed financial audio. Contrary to previous transcription datasets, SPGISpeech contains global english accents, strongly varying audio quality as well as both spontaneous and presentation style speech. The transcripts have each been cross-checked by multiple professional editors for high accuracy and are fully formatted including sentence structure and capitalization."}, {"id": "jesc-japanese-english-subtitle-corpus", "name": "JESC (Japanese-English Subtitle Corpus)", "description": "Japanese-English Subtitle Corpus is a large Japanese-English parallel corpus covering the underrepresented domain of conversational dialogue. It consists of more than 3.2 million examples, making it the largest freely available dataset of its kind. The corpus was assembled by crawling and aligning subtitles found on the web. "}, {"id": "cofw-caltech-occluded-faces-in-the-wild", "name": "COFW (Caltech Occluded Faces in the Wild)", "description": "The Caltech Occluded Faces in the Wild (COFW) dataset is designed to present faces in real-world conditions. Faces show large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food, hands, microphones,\u2028etc.). All images were hand annotated using the same 29 landmarks as in LFPW. Both the landmark positions as well as their occluded/unoccluded state were annotated. The faces are occluded to different degrees, with large variations in the type of occlusions encountered. COFW has an average occlusion of over 23."}, {"id": "molecule3d", "name": "Molecule3D", "description": "Molecule3D is a new benchmark that includes a dataset with precise ground-state geometries of approximately 4 million molecules derived from density functional theory (DFT). It also provides a set of software tools for data processing, splitting, training, and evaluation, etc."}, {"id": "viznet-sato", "name": "VizNet-Sato", "description": "VizNet-Sato is a dataset from the authors of Sato and is based on the VizNet dataset. The authors choose from VizNet only relational web tables with headers matching their selected 78 DBpedia semantic types.  The selected tables are divided into two categories: Full tables and Multi-column only tables. The first category corresponds to 78,733 selected tables from VizNet, while the second category includes 32,265 tables which have more than one column.  The tables of both categories are divided into 5 subsets to be able to conduct 5-fold cross validation: 4 subsets are used for training and the last for evaluation."}, {"id": "breizhcrops", "name": "BreizhCrops", "description": "BreizhCrops is a satellite image time series dataset for crop type classification. It consists on aggregated label data and Sentinel-2 top-of-atmosphere as well as bottom-of-atmosphere time series in the region of Brittany (Breizh in local language), north-east France."}, {"id": "jerichoworld", "name": "JerichoWorld", "description": "JerichoWorld  is a dataset that enables the creation of learning agents that can build knowledge graph-based world models of interactive narratives. Interactive narratives -- or text-adventure games -- are partially observable environments structured as long puzzles or quests in which an agent perceives and interacts with the world purely through textual natural language. Each individual game typically contains hundreds of locations, characters, and objects -- each with their own unique descriptions -- providing an opportunity to study the problem of giving language-based agents the structured memory necessary to operate in such worlds. "}, {"id": "wikitables-turl", "name": "WikiTables-TURL", "description": "The WikiTables-TURL dataset was constructed by the authors of TURL and is based on the WikiTable corpus, which is a large collection of Wikipedia tables. The dataset consists of 580,171 tables divided into fixed training, validation and testing splits. Additionally, the dataset contains metadata about each table, such as the table name, table caption  and column headers. "}, {"id": "azure-functions-trace-2019", "name": "Azure Functions Trace 2019", "description": "This is a set of files representing part of the workload of Microsoft's Azure Functions offering, collected in July of 2019. This dataset is a subset of the data described in, and analyzed, in the USENIX ATC 2020 paper 'Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider'."}, {"id": "glue-general-language-understanding-evaluation-benchmark", "name": "GLUE (General Language Understanding Evaluation benchmark)", "description": "General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI."}, {"id": "groove-groove-midi-dataset", "name": "Groove (Groove MIDI Dataset)", "description": "The Groove MIDI Dataset (GMD) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming."}, {"id": "cityscapes-dvps", "name": "Cityscapes-DVPS", "description": "Cityscapes-DVPS is derived from Cityscapes-VPS by adding re-computed depth maps from Cityscapes dataset. Cityscapes-DVPS is distributed under Creative Commons Attribution-NonCommercial-ShareAlike license."}, {"id": "speechocean762", "name": "speechocean762", "description": "speechocean762 is an open-source speech corpus designed for pronunciation assessment use, consisting of 5000 English utterances from 250 non-native speakers, where half of the speakers are children. Five experts annotated each of the utterances at sentence-level, word-level and phoneme-level. This corpus is allowed to be used freely for commercial and non-commercial purposes. To avoid subjective bias, each expert scores independently under the same metric"}, {"id": "overruling", "name": "Overruling", "description": "The Overruling dataset is a law dataset corresponding to the task of determining when a sentence is overruling a prior decision. This is a binary classification task, where positive examples are overruling sentences and negative examples are non-overruling sentences extracted from legal opinions. In law, an overruling sentence is a statement that nullifies a previous case decision as a precedent, by a constitutionally valid statute or a decision by the same or higher ranking court which establishes a different rule on the point of law involved. The Overruling dataset consists of 2,400 sentences."}, {"id": "hide", "name": "HIDE", "description": "Consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes. "}, {"id": "qasc-question-answering-via-sentence-composition", "name": "QASC (Question Answering via Sentence Composition)", "description": "QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences."}, {"id": "xr-egopose", "name": "xR-EgoPose", "description": "xR-EgoPose is an egocentric synthetic dataset for egocentric 3D human pose estimation. It consists of ~380 thousand photo-realistic egocentric camera images in a variety of indoor and outdoor spaces."}, {"id": "voxpopuli", "name": "VoxPopuli", "description": "VoxPopuli is a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours."}, {"id": "pascal-5i", "name": "PASCAL-5i", "description": "PASCAL-5i is a dataset used to evaluate few-shot segmentation. The dataset is sub-divided into 4 folds each containing 5 classes. A fold contains labelled samples from 5 classes that are used for evaluating the few-shot learning method. The rest 15 classes are used for training."}, {"id": "riedones3d", "name": "Riedones3D", "description": "Riedones3D is a dataset of 2,070 scans of coins. With this dataset, the authors propose two benchmarks, one for point cloud registration, essential for coin die recognition, and a benchmark of coin die clustering"}, {"id": "scdb-simple-concept-database", "name": "SCDB (Simple Concept DataBase)", "description": "Includes annotations for 10 distinguishable concepts."}, {"id": "tum-monovo", "name": "TUM monoVO", "description": "TUM monoVO is a dataset for evaluating the tracking accuracy of monocular Visual Odometry (VO) and SLAM methods. It contains 50 real-world sequences comprising over 100 minutes of video, recorded across different environments \u2013 ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position: this allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground-truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated: the dataset creators provide the exposure times for each frame as reported by the sensor, the camera response function and the lens attenuation factors (vignetting)."}, {"id": "citysim-dataset-a-drone-based-vehicle-trajectory-dataset-for-safety-oriented-research-and-digital-twins", "name": "CitySim Dataset (A Drone-Based Vehicle Trajectory Dataset for Safety Oriented Research and Digital Twins)", "description": "The development of safety-oriented research ideas and applications requires fine-grained vehicle trajectory data that not only has high accuracy but also captures a substantial number of critical safety events. This paper introduces the CitySim Dataset, which was devised with a core objective of facilitating safety-based research and applications. CitySim has vehicle trajectories extracted from 1140-minutes of drone videos recorded at 12 different locations. It covers a variety of road geometries including freeway basic segments, weaving segments, expressway merge/diverge segments, signalized intersections, stop-controlled intersections, and intersections without sign/signal control. CitySim trajectories were generated through a five-step procedure which ensured the trajectory accuracy. Furthermore, the dataset provides vehicle rotated bounding box information which is demonstrated to improve safety evaluation. Compared to other video-based trajectory datasets, the CitySim Dataset has significantly more critical safety events with higher severity including cut-in, merge, and diverge events. In addition, CitySim facilitates research towards digital twin applications by providing relevant assets like the recording locations'3D base maps and signal timings. These features enable more comprehensive conditions for safety research and applications such as autonomous vehicle safety and location-based safety analysis. The dataset is available online at https://github.com/ozheng1993/UCF-SST-CitySim-Dataset."}, {"id": "is-a", "name": "IS-A", "description": "The IS-A dataset is a dataset of relations extracted from a medical ontology. The different entities in the ontology are related by the \u201cis a\u201d relation. For example, \u2018acute leukemia\u2019 is a \u2018leukemia\u2019. The dataset has 294,693 nodes with 356,541 edges between them."}, {"id": "ikea-object-state-dataset", "name": "IKEA Object State Dataset", "description": "IKEA Object State Dataset is a new dataset that contains IKEA furniture 3D models, RGBD video of the assembly process, the 6DoF pose of furniture parts and their bounding box."}, {"id": "hypersim", "name": "Hypersim", "description": "For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. Hypersim is a photorealistic synthetic dataset for holistic indoor scene understanding. It contains 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry."}, {"id": "3d-cars", "name": "3D Cars", "description": "Car CAD models from \"3d object detection and viewpoint estimation with a deformable 3d cuboid model\" were used to generate the dataset. For each of the 199 car models, the authors generated $64\\times64$ color renderings from 24 rotation angles each offset by 15 degrees, as well as from 4 different camera elevations."}, {"id": "fakenewsamt-celebrity", "name": "FakeNewsAMT & Celebrity", "description": "FakeNewsAMT & Celebrity include two novel datasets for the task of fake news detection, covering seven different news domains."}, {"id": "jhmdb-joint-annotated-human-motion-data-base", "name": "JHMDB (Joint-annotated Human Motion Data Base)", "description": "JHMDB is an action recognition dataset that consists of 960 video sequences belonging to 21 actions. It is a subset of the larger HMDB51 dataset collected from digitized movies and YouTube videos. The dataset contains video and annotation for puppet flow per frame (approximated optimal flow on the person), puppet mask per frame, joint positions per frame, action label per clip and meta label per clip (camera motion, visible body parts, camera viewpoint, number of people, video quality)."}, {"id": "android-common-libraries", "name": "Android Common Libraries", "description": "This dataset was constructed from an analysis of about 1.5 million apps from Google Play to identify a set of common libraries, to facilitate Android app analysis. It contains 1,113 libraries supporting common functionalities and 240 libraries for advertisement."}, {"id": "kitti-360", "name": "KITTI-360", "description": "KITTI-360 is a large-scale dataset that contains rich sensory information and full annotations. It is the successor of the popular KITTI dataset,  providing more comprehensive semantic/instance labels in 2D and 3D, richer 360 degree sensory information (fisheye images and pushbroom laser scans), very accurate and geo-localized vehicle and camera poses, and a series of new challenging benchmarks."}, {"id": "presil-precise-synthetic-image-and-lidar", "name": "PreSIL (Precise Synthetic Image and LiDAR)", "description": "Consists of over 50,000 frames and includes high-definition images with full resolution depth information, semantic segmentation (images), point-wise segmentation (point clouds), and detailed annotations for all vehicles and people. "}, {"id": "litbank", "name": "LitBank", "description": "LitBank is an annotated dataset of 100 works of English-language fiction to support tasks in natural language processing and the computational humanities, described in more detail in the following publications:"}, {"id": "cropandweed-dataset", "name": "CropAndWeed Dataset", "description": "The CropAndWeed dataset is focused on the fine-grained identification of 74 relevant crop and weed species with a strong emphasis on data variability. Annotations of labeled bounding boxes, semantic masks and stem positions are provided for about 112k instances in more than 8k high-resolution images of both real-world agricultural sites and specifically cultivated outdoor plots of rare weed types. Additionally, each sample is enriched with meta-annotations regarding environmental conditions."}, {"id": "esports-sensors-dataset", "name": "eSports Sensors Dataset", "description": "The eSports Sensors dataset contains sensor data collected from 10 players in 22 matches in League of Legends. The sensor data collected includes:"}, {"id": "pubmed-rct-pubmed-200k-rct", "name": "PubMed RCT (PubMed 200k RCT)", "description": "PubMed 200k RCT is new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: the authors hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field."}, {"id": "acl-fig", "name": "ACL-Fig", "description": "ACL-Fig is a large-scale automatically annotated corpus consisting of 112,052 scientific figures extracted from 56K research papers in the ACL Anthology. The ACL-Fig-pilot dataset contains 1,671 manually labeled scientific figures belonging to 19 categories."}, {"id": "pixraw10p", "name": "pixraw10P", "description": "face image datasets"}, {"id": "hake", "name": "HAKE", "description": "HAKE is built upon existing activity datasets and provides human body part level atomic action labels (Part States)."}, {"id": "simplequestions", "name": "SimpleQuestions", "description": "SimpleQuestions is a large-scale factoid question answering dataset. It consists of 108,442 natural language questions, each paired with a corresponding fact from Freebase knowledge base. Each fact is a triple (subject, relation, object) and the answer to the question is always the object. The dataset is divided into training, validation, and test  sets with 75,910, 10,845 and 21,687 questions respectively."}, {"id": "bnb", "name": "BnB", "description": "BnB is a large-scale and diverse in-domain VLN (Vision and Language Navigation) dataset."}, {"id": "2000-hub5-english", "name": "2000 HUB5 English", "description": "2000 HUB5 English Evaluation Transcripts was developed by the Linguistic Data Consortium (LDC)  and consists of transcripts of 40 English telephone conversations used in the 2000 HUB5 evaluation sponsored by NIST (National Institute of Standards and Technology). "}, {"id": "ycb-slide-ycb-slide-a-tactile-interaction-dataset", "name": "YCB-Slide (YCB-Slide: A tactile interaction dataset)", "description": "The YCB-Slide dataset comprises of DIGIT sliding interactions on YCB objects. We envision this can contribute towards efforts in tactile localization, mapping, object understanding, and learning dynamics models. We provide access to DIGIT images, sensor poses, RGB video feed, ground-truth mesh models, and ground-truth heightmaps + contact masks (simulation only). This dataset is supplementary to the MidasTouch paper, a CoRL 2022 submission."}, {"id": "aw-oie-all-words-openie", "name": "AW-OIE (All Words OpenIE)", "description": "All Words Open IE (AW-OIE) is an open information extraction dataset derived from Question-Answer Meaning Representation (QAMR) dataset."}, {"id": "ccgbank", "name": "CCGbank", "description": "CCGbank is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar derivations. It pairs syntactic derivations with sets of word-word dependencies which approximate the underlying predicate-argument structure. The dataset contains 99.44% of the sentences in the Penn Treebank, for which it corrects a number of inconsistencies and errors in the original annotation."}, {"id": "medvidqa-medical-video-question-answering", "name": "MedVidQA (Medical Video Question Answering)", "description": "The MedVidQA dataset contains the collection of 3, 010 manually created health-related questions and timestamps as visual answers to those questions from trusted video sources, such as accredited medical schools with an established reputation, health institutes, health education, and medical practitioners."}, {"id": "m2cai16-tool-locations", "name": "m2cai16-tool-locations", "description": "The m2cai16-tool-locations dataset contains spatial tool annotations for 2,532 frames across the first 10 videos in the m2cai16-tool dataset, which includes 15 videos in total. The dataset consists of 3,141 annotations of 7 surgical instrument classes, with an average of 1.2 labels per frame and 7 instrument classes per video."}, {"id": "gigaspeech", "name": "GigaSpeech", "description": "GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training."}, {"id": "qasper", "name": "QASPER", "description": "QASPER is a dataset for question answering on scientific research papers. It consists of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers."}, {"id": "mimic-iv-ed", "name": "MIMIC-IV-ED", "description": "MIMIC-IV-ED is a large, freely available database of emergency department (ED) admissions at the Beth Israel Deaconess Medical Center between 2011 and 2019. As of MIMIC-ED v1.0, the database contains 448,972 ED stays. Vital signs, triage information, medication reconciliation, medication administration, and discharge diagnoses are available. All data are deidentified to comply with the Health Information Portability and Accountability Act (HIPAA) Safe Harbor provision. MIMIC-ED is intended to support a diverse range of education initiatives and research studies."}, {"id": "mlma-hate-speech", "name": "MLMA Hate Speech", "description": "A new multilingual multi-aspect hate speech analysis dataset and use it to test the current state-of-the-art multilingual multitask learning approaches."}, {"id": "freiburg-groceries", "name": "Freiburg Groceries", "description": "Freiburg Groceries is a groceries classification dataset consisting of 5000 images of size 256x256, divided into 25 categories. It has imbalanced class sizes ranging from 97 to 370 images per class. Images were taken in various aspect ratios and padded to squares."}, {"id": "food-101", "name": "Food-101", "description": "The  Food-101 dataset consists of 101 food categories with 750 training and 250 test images per category, making a total of 101k images. The labels for the test images have been manually cleaned, while the training set contains some noise."}, {"id": "meir-multimodal-entity-image-repurposing", "name": "MEIR (Multimodal Entity Image Repurposing)", "description": "MEIR is a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr."}, {"id": "indiapoliceevents", "name": "IndiaPoliceEvents", "description": "IndiaPoliceEvents is a corpus of 21,391 sentences from 1,257 English-language Times of India articles about events in the state of Gujarat during March 2002. This dataset is used for automated event extraction."}, {"id": "toronto-3d", "name": "Toronto-3D", "description": "Toronto-3D is a large-scale urban outdoor point cloud dataset acquired by an MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of road and consists of about 78.3 million points. Point clouds has 10 attributes and classified in 8 labelled object classes."}, {"id": "dexycb", "name": "DexYCB", "description": "DexYCB is a dataset for capturing hand grasping of objects. It can be used three relevant tasks: 2D object and keypoint detection, 6D object pose estimation, and 3D hand pose estimation. "}, {"id": "epic-hotspot", "name": "EPIC-Hotspot", "description": "From Grounded Human-Object Interaction Hotspots from Video (ICCV'19): We collect annotations for interaction keypoints on EPIC Kitchens in order to quantitatively evaluate our method in parallel to the OPRA dataset (where annotations are available). We note that these annotations are collected purely for evaluation, and are not used for training our model. We select the 20 most frequent verbs, and select 31 nouns that afford these interactions."}, {"id": "mindcraft", "name": "MindCraft", "description": "MindCraft is a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners' beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication."}, {"id": "cvss", "name": "CVSS", "description": "CVSS is a massively multilingual-to-English speech to speech translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21 languages into English. CVSS is derived from the Common Voice  speech corpus and the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the translation text from CoVoST 2 into speech using state-of-the-art TTS systems"}, {"id": "argoverse-hd", "name": "Argoverse-HD", "description": "Argoverse-HD is a dataset built for streaming object detection, which encompasses real-time object detection, video object detection, tracking, and short-term forecasting. It contains the video data from Argoverse 1.1 with our own MS COCO-style bounding box annotations with track IDs. The annotations are backward-compatible with COCO as one can directly evaluate COCO pre-trained models on this dataset to estimate the efficiency or the cross-dataset generalization capability of the models. The dataset contains high-quality and temporally-dense annotations for high-resolution videos (1920 x 1200 @ 30 FPS). Overall, there are 70,000 image frames and 1.3 million bounding boxes."}, {"id": "emotyda-emotion-aware-dialogue-act", "name": "EMOTyDA (Emotion aware Dialogue Act)", "description": "EMOTyDA is a multimodal Emotion aware Dialogue Act dataset collected from open-sourced dialogue datasets."}, {"id": "exeq-300k", "name": "EXEQ-300k", "description": "The EXEQ-300k dataset contains 290,479 detailed questions with corresponding math headlines from Mathematics Stack Exchange. The dataset can be used to generate concise math headlines from detailed math questions."}, {"id": "salsa", "name": "SALSA", "description": "A novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis."}, {"id": "cos960", "name": "COS960", "description": "A benchmark dataset with 960 pairs of Chinese wOrd Similarity, where all the words have two morphemes in three Part of Speech (POS) tags with their human annotated similarity rather than relatedness. "}, {"id": "aadb2021-cation-coordinated-conformers-of-20-proteinogenic-amino-acids-with-different-protonation-states", "name": "AADB2021 (Cation-coordinated conformers of 20 proteinogenic amino acids with different protonation states)", "description": "We present a data set from a first-principles study of amino-methylated and acetylated (capped) dipeptides of the 20 proteinogenic amino acids \u2013 including alternative possible side chain protonation states and their interactions with selected divalent cations (Ca$^{2+}$, Mg$^{2+}$ and Ba$^{2+}$. The data covers 21,909 stationary points on the respective potential-energy surfaces in a wide relative energy range of up to 4 eV (390 kJ/mol). Relevant properties of interest, like partial charges, were derived for the conformers."}, {"id": "nerf-neural-radiance-fields", "name": "NeRF (Neural Radiance Fields)", "description": "Neural Radiance Fields (NeRF) is a method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. The dataset contains three parts with the first 2 being synthetic renderings of objects called Diffuse Synthetic 360\u25e6 and Realistic Synthetic 360\u25e6 while the third is real images of complex scenes. Diffuse Synthetic 360\u25e6 consists of four Lambertian objects with simple geometry. Each object is rendered at 512x512 pixels from viewpoints sampled on the upper hemisphere. Realistic Synthetic 360\u25e6 consists of eight objects of complicated geometry and realistic non-Lambertian materials. Six of them are rendered from viewpoints sampled on the upper hemisphere and the two left are from viewpoints sampled on a full sphere with all of them at 800x800 pixels. The real images of complex scenes consist of 8 forward-facing scenes captured with a cellphone at a size of 1008x756 pixels."}, {"id": "realestate10k", "name": "RealEstate10K", "description": "RealEstate10K is a large dataset of camera poses corresponding to 10 million frames derived from about 80,000 video clips, gathered from about 10,000 YouTube videos. For each clip, the poses form a trajectory where each pose specifies the camera position and orientation along the trajectory. These poses are derived by running SLAM and bundle adjustment algorithms on a large set of videos."}, {"id": "tac-2010", "name": "TAC 2010", "description": "TAC 2010 is a dataset for summarization that consists of 44 topics, each of which is associated with a set of 10 documents. The test dataset is composed of approximately 44 topics, divided into five categories: Accidents and Natural Disasters, Attacks, Health and Safety, Endangered Resources, Investigations and Trials."}, {"id": "dsiod-driving-scenario-input-output-dataset", "name": "DSIOD (Driving Scenario Input Output Dataset)", "description": "This dataset contains data which enables the evaluation of metamodels and approches for targeted test case selection without setting up test environments or performing test runs. The dataset is split into different scenarios: Each scenario comes with one or more tabular datasets containing the inputs and outputs of different test cases (concrete scenarios). A configuration file describes which of the columns are inputs and outputs and explains the different parameters. The config also contains verbal descriptions of the scenarios. Additionally, animations of the scenarios are available."}, {"id": "pascal-voc-2007", "name": "PASCAL VOC 2007", "description": "PASCAL VOC 2007 is a dataset for image recognition. The twenty object classes that have been selected are:"}, {"id": "facetsum", "name": "FacetSum", "description": "FacetSum is a faceted summarization dataset for scientific documents. FacetSum has been built on Emerald journal articles, covering a diverse range of domains. Different from traditional document-summary pairs, FacetSum provides multiple summaries, each targeted at specific sections of a long document, including the purpose, method, findings, and value."}, {"id": "continual-world", "name": "Continual World", "description": "Continual World is a benchmark consisting of realistic and meaningfully diverse robotic tasks built on top of Meta-World as a testbed."}, {"id": "carb-crowdsourced-automatic-open-relation-extraction-benchmark", "name": "CaRB (Crowdsourced automatic open Relation extraction Benchmark)", "description": "CaRB [Bhardwaj et al., 2019] is developed by re-annotating the dev and test splits of OIE2016 via crowd-sourcing. Besides improving annotation quality, CaRB also provides a new matching scorer. CaRB scorer uses token level match and it matches relation with relation, arguments with arguments."}, {"id": "broad-twitter-corpus", "name": "Broad Twitter Corpus", "description": "This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire."}, {"id": "lesion-boundary-segmentation-dataset", "name": "Lesion Boundary Segmentation Dataset", "description": "Lesion Boundary Segmentation Dataset is a dataset for lesion segmentation from the ISIC2018 challenge. The dataset contains skin lesions and their corresponding annotations."}, {"id": "brwac", "name": "BRWAC", "description": "Composed by 2.7 billion tokens, and has been annotated with tagging and parsing information. "}, {"id": "cedar-signature", "name": "CEDAR Signature", "description": "CEDAR Signature is a database of off-line signatures for signature verification. Each of 55 individuals contributed 24 signatures thereby creating 1,320 genuine signatures. Some were asked to forge three other writers\u2019 signatures, eight times per subject, thus creating 1,320 forgeries. Each signature was scanned at 300 dpi gray-scale and binarized using a gray-scale histogram. Salt pepper noise removal and slant normalization were two steps involved in image preprocessing. The database has 24 genuines and 24 forgeries available for each writer."}, {"id": "fbms-59-freiburg-berkeley-motion-segmentation", "name": "FBMS-59 (Freiburg-Berkeley Motion Segmentation)", "description": "The Freiburg-Berkeley Motion Segmentation Dataset (FBMS-59) is a dataset for motion segmentation, which extends the BMS-26 dataset with 33 additional video sequences. A total of 720 frames is annotated. FBMS-59 comes with a split into a training set and a test set. Typical challenges appear in both sets."}, {"id": "anli-adversarial-nli", "name": "ANLI (Adversarial NLI)", "description": "The Adversarial Natural Language Inference (ANLI, Nie et al.) is a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. Particular, the data is selected to be difficult to the state-of-the-art models, including BERT and RoBERTa."}, {"id": "aquatrash", "name": "AquaTrash", "description": "This dataset contains 369 images of Trash used for deep learning. Each image is manually labelled by our team for accurate detections making a total of 470 bounding boxes. There are total 4 classes {(0: glass), (1:paper), (2:metal), (3:plastic)}"}, {"id": "msr-vtt", "name": "MSR-VTT", "description": "MSR-VTT (Microsoft Research Video to Text) is a large-scale dataset for the open domain video captioning, which consists of 10,000 video clips from 20 categories, and each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. There are about 29,000 unique words in all captions. The standard splits uses 6,513 clips for training, 497 clips for validation, and 2,990 clips for testing."}, {"id": "blurb-biomedical-language-understanding-and-reasoning-benchmark", "name": "BLURB (Biomedical Language Understanding and Reasoning Benchmark)", "description": "BLURB is a collection of resources for biomedical natural language processing. In general domains such as newswire and the Web, comprehensive benchmarks and leaderboards such as GLUE have greatly accelerated progress in open-domain NLP. In biomedicine, however, such resources are ostensibly scarce. In the past, there have been a plethora of shared tasks in biomedical NLP, such as BioCreative, BioNLP Shared Tasks, SemEval, and BioASQ, to name just a few. These efforts have played a significant role in fueling interest and progress by the research community, but they typically focus on individual tasks. The advent of neural language models such as BERTs provides a unifying foundation to leverage transfer learning from unlabeled text to support a wide range of NLP applications. To accelerate progress in biomedical pretraining strategies and task-specific methods, it is thus imperative to create a broad-coverage benchmark encompassing diverse biomedical tasks."}, {"id": "turl-twitter-news-url-corpus", "name": "TURL (Twitter News URL Corpus)", "description": "Twitter News URL Corpus is a human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification."}, {"id": "matbench", "name": "Matbench", "description": "The Matbench test suite v0.1 contains 13 supervised ML tasks from 10 datasets. Matbench\u2019s data are sourced from various subdisciplines of materials science, such as experimental mechanical properties (alloy strength), computed elastic properties, computed and experimental electronic properties, optical and phonon properties, and thermodynamic stabilities for crystals, 2D materials, and disordered metals. The number of samples in each task ranges from 312 to 132,752, representing both relatively scarce experimental materials properties and comparatively abundant properties such as DFT-GGA formation energies. Each task is a self-contained dataset containing a single material primitive as input (either composition or composition plus crystal structure) and target property as output for each sample."}, {"id": "biolama", "name": "BioLAMA", "description": "BioLAMA is a benchmark comprised of 49K biomedical factual knowledge triples for probing biomedical Language Models. It is used to assess the capabilities of Language Models for being valid biomedical knowledge bases."}, {"id": "awa-pose", "name": "AwA Pose", "description": "AwA Pose is a large scale animal keypoint dataset with ground truth annotations for keypoint detection of quadruped animals from images."}, {"id": "rsdd-time", "name": "RSDD-Time", "description": "RSDD-Time is a dataset of 598 manually annotated self-reported depression diagnosis posts from Reddit that include temporal information about the diagnosis. Annotations include whether a mental health condition is present and how recently the diagnosis happened. Additionally, the dataset includes exact temporal spans that relate to the date of diagnosis. "}, {"id": "dark-zurich", "name": "Dark Zurich", "description": "Dark Zurich is an image dataset containing a total of 8779 images captured at nighttime, twilight, and daytime, along with the respective GPS coordinates of the camera for each image. These GPS annotations are used to construct cross-time-of-day correspondences, i.e., to match each nighttime or twilight image to its daytime counterpart."}, {"id": "multirc-multi-sentence-reading-comprehension", "name": "MultiRC (Multi-Sentence Reading Comprehension)", "description": "MultiRC (Multi-Sentence Reading Comprehension) is a dataset of short paragraphs and multi-sentence questions, i.e., questions that can be answered by combining information from multiple sentences of the paragraph. The dataset was designed with three key challenges in mind: * The number of correct answer-options for each question is not pre-specified. This removes the over-reliance on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, the task is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually. * The correct answer(s) is not required to be a span in the text. * The paragraphs in the dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets. The entire corpus consists of around 10K questions (including about 6K multiple-sentence questions). The 60% of the data is released as training and development data. The rest of the data is saved for evaluation and every few months a new unseen additional data is included for evaluation to prevent unintentional overfitting over time."}, {"id": "k-radar-kaist-radar", "name": "K-Radar (KAIST-Radar)", "description": "KAIST-Radar (K-Radar) is a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS."}, {"id": "checked", "name": "CHECKED", "description": "Chinese dataset on COVID-19 misinformation. CHECKED provides ground-truth on credibility, carefully obtained by ensuring the specific sources are used. CHECKED includes microblogs related to COVID-19, identified by using a specific list of keywords, covering a total 2120 microblogs published from December 2019 to August 2020. The dataset contains a rich set of multimedia information for each microblog including ground-truth label, textual, visual, response, and social network information."}, {"id": "vot2019", "name": "VOT2019", "description": "VOT2019 is a Visual Object Tracking benchmark for short-term tracking in RGB."}, {"id": "united-nations-parallel-corpus", "name": "United Nations Parallel Corpus", "description": "The first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish."}, {"id": "t-rex", "name": "T-REx", "description": "A dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences)."}, {"id": "newb", "name": "NewB", "description": "A text corpus of more than 200,000 sentences from eleven news sources regarding Donald Trump."}, {"id": "kadid-10k", "name": "KADID-10k", "description": "Konstanz artificially distorted image quality database (KADID-10k) contains 81 pristine images, each degraded by 25 distortions in 5 levels."}, {"id": "pump-and-dump-dataset", "name": "Pump and dump dataset", "description": "The Pump and dump dataset is an annotated set of messages to detect cryptocurrency market manipulations. It consists of a list of a list of pump and dumps arranged by groups on Telegram. All the pump and dumps in the dataset are on the trading pair SYM/BTC."}, {"id": "ava-laeo", "name": "AVA-LAEO", "description": "Dataset to address the problem of detecting people Looking At Each Other (LAEO) in video sequences."}, {"id": "m2dgr-a-multi-modal-and-multi-scenario-slam-dataset-for-ground-robots", "name": "M2DGR (a Multi-modal and Multi-scenario SLAM Dataset for Ground Robots)", "description": "We collected long-term challenging sequences for ground robots both indoors and outdoors with a complete sensor suite, which includes six surround-view fish-eye cameras, a sky-pointing fish-eye camera, a perspective color camera, an event camera, an infrared camera, a 32-beam LIDAR, two GNSS receivers, and two IMUs. To our knowledge, this is the first SLAM dataset focusing on ground robot navigation with such rich sensory information. We recorded trajectories in a few challenging scenarios like lifts, complete darkness, which can easily fail existing localization solutions. These situations are commonly faced in ground robot applications, while they are seldom discussed in previous datasets. We launched a comprehensive benchmark for ground robot navigation. On this benchmark, we evaluated existing state-of-the-art SLAM algorithms of various designs and analyzed their characteristics and defects individually."}, {"id": "sider-scaffold-scaffold-split-of-sider-dataset", "name": "SIDER(scaffold) (Scaffold split of SIDER  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "pit-paraphrase-and-semantic-similarity-in-twitter", "name": "PIT (Paraphrase and Semantic Similarity in Twitter)", "description": "Paraphrase and Semantic Similarity in Twitter (PIT) presents a constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. "}, {"id": "fashion-iq", "name": "Fashion IQ", "description": "Fashion IQ support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images."}, {"id": "mnist-1d", "name": "MNIST-1D", "description": "A minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94% accuracy respectively (these models obtain 94, 99+, and 99+% on MNIST)."}, {"id": "moon-phases-moon-phases-and-derived", "name": "Moon Phases (moon phases and derived)", "description": "Dates with Moon phases extended days until next phase (1992/1/4 to 2027/12/20)"}, {"id": "msrc-12-msrc-12-kinect-gesture-dataset", "name": "MSRC-12 (MSRC-12 Kinect Gesture Dataset)", "description": "The Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system. The data set includes 594 sequences and 719,359 frames\u2014approximately six hours and 40 minutes\u2014collected from 30 people performing 12 gestures. In total, there are 6,244 gesture instances. The motion files contain tracks of 20 joints estimated using the Kinect Pose Estimation pipeline. The body poses are captured at a sample rate of 30Hz with an accuracy of about two centimeters in joint positions."}, {"id": "inferwiki", "name": "InferWiki", "description": "InferWiki is a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. Third, the dataset includes various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation."}, {"id": "ppr10k-portrait-photo-retouching-dataset", "name": "PPR10K (Portrait Photo Retouching dataset)", "description": "PPR10K is a dataset for portrait photo retouching (PPR), which aims to enhance the visual quality of a collection of flat-looking portrait photos. The Portrait Photo Retouching dataset (PPR10K) is a large-scale and diverse dataset that contains:"}, {"id": "cacd-cross-age-celebrity-dataset", "name": "CACD (Cross-Age Celebrity Dataset)", "description": "The Cross-Age Celebrity Dataset (CACD) contains 163,446 images from 2,000 celebrities collected from the Internet. The images are collected from search engines using celebrity name and year (2004-2013) as keywords. Therefore, it is possible to estimate the ages of the celebrities on the images by simply subtract the birth year from the year of which the photo was taken."}, {"id": "slowflow", "name": "SlowFlow", "description": "SlowFlow is an optical flow dataset collected by applying Slow Flow technique on data from a high-speed camera and analyzing the performance of the state-of-the-art in optical flow under various levels of motion blur."}, {"id": "cord-consolidated-receipt-dataset-for-post-ocr-parsing", "name": "CORD (Consolidated Receipt Dataset for Post-OCR Parsing)", "description": "OCR is inevitably linked to NLP since its final output is in text. Advances in document intelligence are driving the need for a unified technology that integrates OCR with various NLP tasks, especially semantic parsing. Since OCR and semantic parsing have been studied as separate tasks so far, the datasets for each task on their own are rich, while those for the integrated post-OCR parsing tasks are relatively insufficient. In this study, we publish a consolidated dataset for receipt parsing as the first step towards post-OCR parsing tasks. The dataset consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing. The proposed dataset can be used to address various OCR and parsing tasks."}, {"id": "mela-bitchute", "name": "MeLa BitChute", "description": "MeLa BitChute is a near-complete dataset of over 3M videos from 61K channels over 2.5 years (June 2019 to December 2021) from the social video hosting platform BitChute, a commonly used alternative to YouTube. Additionally, the dataset includes a variety of video-level metadata, including comments, channel descriptions, and views for each video."}, {"id": "shapenet-vipc", "name": "ShapeNet-ViPC", "description": "A large-scale dataset for the point cloud completion task on the ShapeNet dataset."}, {"id": "ppmi-parkinsons-progression-markers-initiative", "name": "PPMI (Parkinson\u2019s Progression Markers Initiative)", "description": "The Parkinson\u2019s Progression Markers Initiative (PPMI) dataset originates from an observational clinical and longitudinal study comprising evaluations of people with Parkinson\u2019s disease (PD), those people with high risk, and those who are healthy."}, {"id": "ontonotes-5-0", "name": "OntoNotes 5.0", "description": "OntoNotes 5.0 is a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference)."}, {"id": "h3ds", "name": "H3DS", "description": "H3DS a high-resolution 3D full head textured scans and 360\u00ba images dataset collected with a structured light scanner, consisting of 23 3D full-head scans containing images, masks and camera poses. The 3D geometry has been captured using a structured light scanner, which leads to precise ground truth geometries."}, {"id": "calfw-cross-age-lfw", "name": "CALFW (Cross-Age LFW)", "description": "A renovation of Labeled Faces in the Wild (LFW), the de facto standard testbed for unconstraint face verification. "}, {"id": "newsph-nli", "name": "NewsPH-NLI", "description": "NewsPH-NLI is a sentence entailment benchmark dataset in the low-resource Filipino language. "}, {"id": "cell", "name": "Cell", "description": "The CELL benchmark is made of fluorescence microscopy images of cell. "}, {"id": "araus-affective-responses-to-augmented-urban-soundscapes", "name": "ARAUS (Affective Responses to Augmented Urban Soundscapes)", "description": "Choosing optimal maskers for existing soundscapes to effect a desired perceptual change via soundscape augmentation is non-trivial due to extensive varieties of maskers and a dearth of benchmark datasets with which to compare and develop soundscape augmentation models. To address this problem, we make publicly available the ARAUS (Affective Responses to Augmented Urban Soundscapes) dataset, which comprises a five-fold cross-validation set and independent test set totaling 25,440 unique subjective perceptual responses to augmented soundscapes presented as audio-visual stimuli. Each augmented soundscape is made by digitally adding \"maskers\" (bird, water, wind, traffic, construction, or silence) to urban soundscape recordings at fixed soundscape-to-masker ratios. Responses were then collected by asking participants to rate how pleasant, annoying, eventful, uneventful, vibrant, monotonous, chaotic, calm, and appropriate each augmented soundscape was, in accordance with ISO 12913-2:2018. Participants also provided relevant demographic information and completed standard psychological questionnaires. We perform exploratory and statistical analysis of the responses obtained to verify internal consistency and agreement with known results in the literature. Finally, we demonstrate the benchmarking capability of the dataset by training and comparing four baseline models for urban soundscape pleasantness: a low-parameter regression model, a high-parameter convolutional neural network, and two attention-based networks in the literature."}, {"id": "wdc-dialogue", "name": "WDC-Dialogue", "description": "WDC-Dialogue is a dataset built from the Chinese social media to train EVA. Specifically, conversations from various sources are gathered and a rigorous data cleaning pipeline is designed to enforce the quality of WDC-Dialogue. "}, {"id": "3dpw", "name": "3DPW", "description": "The 3D Poses in the Wild dataset is the first dataset in the wild with accurate 3D poses for evaluation. While other datasets outdoors exist, they are all restricted to a small recording volume. 3DPW is the first one that includes video footage taken from a moving phone camera."}, {"id": "kinetics-100", "name": "Kinetics-100", "description": "Kinetics-100 is a dataset split created from the Kinetics dataset to evaluate the performance of few-shot action recognition models. 100 classes are randomly selected from a total of 400 categories, each composed of 100 examples. The 100 classes are further split into 64, 12, and 24 non-overlapping classes to use as the meta-training set, meta-validation set, and meta-testing set, respectively.  Link to the selected samples can be found here: https://github.com/ffmpbgrnn/CMN/tree/master/kinetics-100"}, {"id": "mujoco", "name": "MuJoCo", "description": "MuJoCo (multi-joint dynamics with contact) is a physics engine used to implement environments to benchmark Reinforcement Learning methods."}, {"id": "physion", "name": "Physion", "description": "Physion is a visual and physical prediction benchmark to measure the performance of machine learning models on making predictions about commonplace real world physical events. In realistically simulating a wide variety of physical phenomena -- rigid and soft-body collisions, stable multi-object configurations, rolling and sliding, projectile motion -- this dataset presents a more comprehensive challenge than existing benchmarks. Moreover, the dataset also contains human responses for the stimuli so that model predictions can be directly compared to human judgments."}, {"id": "entailmentbank", "name": "EntailmentBank", "description": "EntailmentBank is a dataset that contains multistep entailment trees. At each node in the tree (typically) two or more facts compose together to produce a new conclusion. Given a hypothesis (question + answer), three increasingly difficult explanation tasks are defined: generate a valid entailment tree given (a) all relevant sentences (the leaves of the gold entailment tree) (b) all relevant and some irrelevant sentences (c) a corpus."}, {"id": "moviescope", "name": "Moviescope", "description": "Moviescope is a large-scale dataset of 5,000 movies with corresponding video trailers, posters, plots and metadata. Moviescope is based on the IMDB 5000 dataset consisting of 5.043 movie records. It is augmented by crawling video trailers associated with each movie from YouTube and text plots from Wikipedia."}, {"id": "maestro", "name": "MAESTRO", "description": "The MAESTRO dataset contains over 200 hours of paired audio and MIDI recordings from ten years of International Piano-e-Competition. The MIDI data includes key strike velocities and sustain/sostenuto/una corda pedal positions. Audio and MIDI files are aligned with \u223c3 ms accuracy and sliced to individual musical pieces, which are annotated with composer, title, and year of performance. Uncompressed audio is of CD quality or higher (44.1\u201348 kHz 16-bit PCM stereo)."}, {"id": "agent", "name": "AGENT", "description": "Inspired by cognitive development studies on intuitive psychology, we present a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology."}, {"id": "perceptual-similarity", "name": "Perceptual Similarity", "description": "Perceptual Similarity is a dataset of human perceptual similarity judgments."}, {"id": "almawave-slu", "name": "Almawave-SLU", "description": "Almawave-SLU is the first Italian dataset for Spoken Language Understanding (SLU). It is derived through a semi-automatic procedure and is used as a benchmark of various open source and commercial systems."}, {"id": "lama-language-model-analysis", "name": "LAMA (LAnguage Model Analysis)", "description": "LAnguage Model Analysis (LAMA) consists of a set of knowledge sources, each comprised of a set of facts. LAMA is a probe for analyzing the factual and commonsense knowledge contained in pretrained language models."}, {"id": "xed", "name": "XED", "description": "XED is a multilingual fine-grained emotion dataset. The dataset consists of human-annotated Finnish (25k) and English sentences (30k), as well as projected annotations for 30 additional languages, providing new resources for many low-resource languages. "}, {"id": "pascal-face", "name": "PASCAL Face", "description": "The PASCAL FACE dataset is a dataset for face detection and face recognition. It has a total of 851 images which are a subset of the PASCAL VOC and has a total of 1,341 annotations. These datasets contain only a few hundreds of images and have limited variations in face appearance."}, {"id": "cifar100-lt", "name": "CIFAR100-LT", "description": "The Long-tailed Version of CIFAR100"}, {"id": "parashoot", "name": "ParaShoot", "description": "ParaShoot is the first question answering dataset in modern Hebrew. The dataset follows the format and crowdsourcing methodology of SQuAD, and contains approximately 3000 annotated examples, similar to other question-answering datasets in low-resource languages."}, {"id": "ivqa-instructional-video-question-answering", "name": "iVQA (Instructional Video Question Answering)", "description": "An open-ended VideoQA benchmark that aims to: i) provide a well-defined evaluation by including five correct answer annotations per question and ii) avoid questions which can be answered without the video. "}, {"id": "plantdoc", "name": "PlantDoc", "description": "PlantDoc is a dataset for visual plant disease detection. The dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images."}, {"id": "dlr-acd", "name": "DLR-ACD", "description": "The DLR-ACD dataset is a collection of aerial images for crowd counting and density estimation, as well as for person localization at mass events. It contains 33 large aerial images acquired through 16 different flight campaigns at various mass events and over urban scenes involving crowds, such as sport events, city centers, open-air fairs and festivals."}, {"id": "movieshots", "name": "MovieShots", "description": "MovieShots is a dataset to facilitate the shot type analysis in videos. It is a large-scale shot type annotation set that contains 46K shots from 7,858 movies covering a wide variety of movie genres to ensure the inclusion of all scale and movement types of shot. Each shot has two attributes, shot scale and shot movement."}, {"id": "adefan", "name": "ADEFAN", "description": "This data set contains 50 low resolution (640 x 360) short videos containing a variety real life activities."}, {"id": "physionet-challenge-2021-the-physionet-computing-in-cardiology-challenge-2021", "name": "PhysioNet Challenge 2021 (The PhysioNet/Computing in Cardiology Challenge 2021)", "description": "The training data contains twelve-lead ECGs. The validation and test data contains twelve-lead, six-lead, four-lead, three-lead, and two-lead ECGs:"}, {"id": "xqa", "name": "XQA", "description": "XQA is a data which consists of a total amount of 90k question-answer pairs in nine languages for cross-lingual open-domain question answering."}, {"id": "minerl", "name": "MineRL", "description": "MineRLis an imitation learning dataset with over 60 million frames of recorded human player data. The dataset includes a set of tasks which highlights many of the hardest problems in modern-day Reinforcement Learning: sparse rewards and hierarchical policies."}, {"id": "nwpu-crowd", "name": "NWPU-Crowd", "description": "NWPU-Crowd consists of 5,109 images, in a total of 2,133,375 annotated heads with points and boxes. Compared with other real-world datasets, it contains various illumination scenes and has the largest density range (0~20,033). "}, {"id": "stream-51", "name": "Stream-51", "description": "A new dataset for streaming classification consisting of temporally correlated images from 51 distinct object categories and additional evaluation classes outside of the training distribution to test novelty recognition. "}, {"id": "logic2text", "name": "Logic2Text", "description": "Logic2Text is a large-scale dataset with 10,753 descriptions involving common logic types paired with the underlying logical forms. The logical forms show diversified graph structure of free schema, which poses great challenges on the model's ability to understand the semantics."}, {"id": "msc", "name": "MSC", "description": "MSC is a dataset for Macro-Management in StarCraft 2 based on the platfrom SC2LE. It consists of well-designed feature vectors, pre-defined high-level actions and final result of each match. It contains 36,619 high quality replays, which are unbroken and played by relatively professional players."}, {"id": "apricot-mask", "name": "APRICOT-Mask", "description": "We present the APRICOT-Mask dataset, which augments the APRICOT dataset with pixel-level annotations of adversarial patches. We hope APRICOT-Mask along with the APRICOT dataset can facilitate the research in building defenses against physical patch attacks, especially patch detection and removal techniques."}, {"id": "newsqa", "name": "NewsQA", "description": "The NewsQA dataset is a crowd-sourced machine reading comprehension dataset of 120,000 question-answer pairs."}, {"id": "umist", "name": "UMIST", "description": "The Sheffield (previously UMIST) Face Database consists of 564 images of 20 individuals (mixed race/gender/appearance). Each individual is shown in a range of poses from profile to frontal views \u2013 each in a separate directory labelled 1a, 1b, \u2026 1t and images are numbered consecutively as they were taken. The files are all in PGM format, approximately 220 x 220 pixels with 256-bit grey-scale."}, {"id": "deepfix", "name": "DeepFix", "description": "DeepFix consists of a program repair dataset (fix compiler errors in C programs). It enables research around automatically fixing programming errors using deep learning."}, {"id": "dancetrack", "name": "DanceTrack", "description": "A large-scale multi-object tracking dataset for human tracking in occlusion, frequent crossover, uniform appearance and diverse body gestures. It is proposed to emphasize the importance of motion analysis in multi-object tracking instead of mainly appearance-matching-based diagram."}, {"id": "dibco-2017", "name": "DIBCO 2017", "description": "DIBCO 2017 is the international Competition on Document Image Binarization organized in conjunction with the ICDAR 2017 conference. The general objective of the contest is to identify current advances in document image binarization of machine-printed and handwritten document images using performance evaluation measures that are motivated by document image analysis and recognition requirements"}, {"id": "ishape", "name": "iShape", "description": "iShape is an irregular shape dataset for instance segmentation. iShape contains six sub-datasets with one real and five synthetics, each represents a scene of a typical irregular shape."}, {"id": "ufpr-eyeglasses", "name": "UFPR-Eyeglasses", "description": "The UFPR-Eyeglasses dataset has 1,135 images of both eyes (2,270 cropped images of each eye) from 83 subjects (166 classes). The dataset is used to evaluate the effect of the occlusion caused by eyeglasses in periocular recognition."}, {"id": "timebank", "name": "TimeBank", "description": "Enriches the TimeML annotations of TimeBank by adding information about the Topic Time in terms of Klein (1994). The annotations are partly automatic, partly inferential and partly manual. The corpus was converted into the native format of the annotation software GraphAnno and POS-tagged using the Stanford bidirectional dependency network tagger. "}, {"id": "shrec-19-shrec-19-track-matching-humans-with-different-connectivity", "name": "SHREC'19 (SHREC'19 track Matching Humans with Different Connectivity)", "description": "Shape matching plays an important role in geometry processing and shape analysis. In the last decades, much research has been devoted to improve the quality of matching between surfaces. This huge effort is motivated by several applications such as object retrieval, animation and information transfer just to name a few. Shape matching is usually divided into two main categories: rigid and non rigid matching. In both cases, the standard evaluation is usually performed on shapes that share the same connectivity, in other words, shapes represented by the same mesh. This is mainly due to the availability of a \u201cnatural\u201d ground truth that is given for these shapes. Indeed, in most cases the consistent connectivity directly induces a ground truth correspondence between vertices. However, this standard practice obviously does not allow to estimate the robustness of a method with respect to different connectivity. With this track, we propose a benchmark to evaluate the performance of point-to-point matching pipelines when the shapes to be matched have different connectivity (see Figure 1). We consider the concurrent presence of 1) different meshing, 2) rigid transformation in 3D space, 3) non-rigid deformations, 4) different vertex density, ranging from 5K to more than 50K, and 5) topological changes induced by mesh gluing in areas of contact. The correspondence between these shapes is obtained through the recently proposed registration pipeline FARM [1]. This method provides a high-quality registration of the SMPL model [2] to a large set of human meshes coming from different datasets from which we obtain a well-defined correspondence for all the meshes registered and SMPL itself."}, {"id": "bankex", "name": "BANKEX", "description": "Contains stock market closing prices of ten financial institutions. Closing Price in Indian Rupee (INR). Daily samples retrieved between 12 July 2005 and 3 November 2017. All time series with 3 032 samples."}, {"id": "cqasumm", "name": "CQASUMM", "description": "CQASUMM is a dataset for CQA (Community Question Answering) summarization, constructed from the 4.4 million Yahoo! Answers L6 dataset. The dataset contains ~300k annotated samples."}, {"id": "tripclick", "name": "TripClick", "description": "TripClick is a large-scale dataset of click logs in the health domain, obtained from user interactions of the Trip Database health web search engine. "}, {"id": "common-voice", "name": "Common Voice", "description": "Common Voice is an audio dataset that consists of a unique MP3 and corresponding text file. There are 9,283 recorded hours in the dataset. The dataset also includes demographic metadata like age, sex, and accent. The dataset consists of 7,335 validated hours in 60 languages."}, {"id": "lexglue", "name": "LexGLUE", "description": "Legal General Language Understanding Evaluation (LexGLUE) benchmark is a collection of datasets for evaluating model performance across a diverse set of legal NLU tasks in a standardized way."}, {"id": "dqn-replay-dataset", "name": "DQN Replay Dataset", "description": "The DQN Replay Dataset was collected as follows: We first train a DQN agent, on all 60 Atari 2600 games with sticky actions enabled for 200 million frames (standard protocol) and save all of the experience tuples of (observation, action, reward, next observation) (approximately 50 million) encountered during training."}, {"id": "pace-2022-exact-pace-2022-directed-feedback-vertex-set-exact-track", "name": "PACE 2022 Exact (PACE 2022 Directed Feedback Vertex Set, Exact Track)", "description": "This is the set of graphs used in the PACE 2022 challenge for computing the Directed Feedback Vertex Set, from the Exact track. It consists of 200 labelled directed graphs. The graphs range in size up to from N=512 up to N=131072 vertices, and up to 1315170 edges. The graphs are mostly not symmetric (an edge form u->v does not imply an edge from v->u), although some are symmetric. The graph labels are integers ranging from 1 to N."}, {"id": "101-hours-scene-noise-data-by-voice-recorder-datatng", "name": "101 Hours \u2013 Scene Noise Data by Voice Recorder (Datatng)", "description": "Description The data is multi-scene noise data, covering subway, supermarket, restaurant, road, airport, exhibition hall, high-speed rail, highway, city road, cinema and other daily life scenes.The data is recorded by the professional recorder Sony ICD-UX560F, which is collected in a high sampling rate and two-channel format, and the recording is clear and natural. The valid data is 101 hours."}, {"id": "galaxy-zoo-decals", "name": "Galaxy Zoo DECaLS", "description": "Approx. 300,000 images of galaxies labelled by shape."}, {"id": "asr-ramc-bigccsc-a-chinese-conversational-speech-corpus", "name": "ASR-RAMC-BIGCCSC: A CHINESE CONVERSATIONAL SPEECH CORPUS", "description": "A Rich Annotated Mandarin Conversational (RAMC) Speech Dataset, including 180 hours of Mandarin Chinese dialogue, 150, 10 and 20 hours for the training set, development set and test set respectively. It contains 351 multi-turn dialogues, each of which is a coherent and compact conversation centered around one theme."}, {"id": "memotion-analysis-semeval-2020-task-8-memotion-analysis-the-visuo-lingual-metaphor", "name": "Memotion Analysis (SemEval-2020 Task 8: Memotion Analysis -- The Visuo-Lingual Metaphor!)", "description": "A multimodal dataset for sentiment analysis on internet memes."}, {"id": "event2mind", "name": "Event2Mind", "description": "Event2Mind is a corpus of 25,000 event phrases covering a diverse range of everyday events and situations."}, {"id": "mmi-mmi-facial-expression-database", "name": "MMI (MMI Facial Expression Database)", "description": "The MMI Facial Expression Database consists of over 2900 videos and high-resolution still images of 75 subjects. It is fully annotated for the presence of AUs in videos (event coding), and partially coded on frame-level, indicating for each frame whether an AU is in either the neutral, onset, apex or offset phase. A small part was annotated for audio-visual laughters."}, {"id": "celeb-df", "name": "Celeb-DF", "description": "Celeb-DF is a large-scale challenging dataset for deepfake forensics. It includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding DeepFake videos."}, {"id": "blendedmvs", "name": "BlendedMVS", "description": "BlendedMVS is a novel large-scale dataset, to provide sufficient training ground truth for learning-based MVS. The dataset was created by applying a 3D reconstruction pipeline to recover high-quality textured meshes from images of well-selected scenes. Then, these mesh models were rendered to color images and depth maps. "}, {"id": "nighttime-driving", "name": "Nighttime Driving", "description": "Nighttime Driving is a dataset of road scenes consisting of 35,000 images ranging from daytime to twilight time and to nighttime. "}, {"id": "nct-crc-he-100k", "name": "NCT-CRC-HE-100K", "description": "The NCT-CRC-HE-100K dataset is a set of 100,000 non-overlapping image patches extracted from 86 H$\\&$E stained human cancer tissue slides and normal tissue from the NCT biobank (National Center for Tumor Diseases) and the UMM pathology archive (University Medical Center Mannheim). While the dataset Colorectal Cacner-Validation-Histology-7K (CRC-VAL-HE-7K) consist of 7180 images extracted from 50 patients with colorectal adenocarcinoma and were used to create a dataset that does not overlap with patients in the NCT-CRC-HE-100K dataset. It was created by pathologists by manually delineating tissue regions in whole slide images into the following nine tissue classes: Adipose (ADI), background (BACK), debris (DEB), lymphocytes (LYM), mucus (MUC), smooth muscle (MUS), normal colon mucosa (NORM), cancer-associated stroma (STR), colorectal adenocarcinoma epithelium (TUM)."}, {"id": "dialogsum", "name": "DialogSum", "description": "DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics."}, {"id": "staqc", "name": "StaQC", "description": "StaQC (Stack Overflow Question-Code pairs) is a large dataset of around 148K Python and 120K SQL domain question-code pairs, which are automatically mined from StackOverflow."}, {"id": "faceforensics-2", "name": "FaceForensics++", "description": "FaceForensics++ is a forensics dataset consisting of 1000 original video sequences that have been manipulated with four automated face manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The data has been sourced from 977 youtube videos and all videos contain a trackable mostly frontal face without occlusions which enables automated tampering methods to generate realistic forgeries."}, {"id": "thchs-30", "name": "THCHS-30", "description": "THCHS-30 is a free Chinese speech database THCHS-30 that can be used to build a full-fledged Chinese speech recognition system."}, {"id": "nju2k", "name": "NJU2K", "description": "NJU2K is a large RGB-D dataset containing 1,985 image pairs. The stereo images were collected from the Internet and 3D movies, while photographs were taken by a Fuji W3 camera."}, {"id": "stl-10-self-taught-learning-10", "name": "STL-10 (Self-Taught Learning 10)", "description": "The STL-10 is an image dataset derived from ImageNet and popularly used to evaluate algorithms of unsupervised feature learning or self-taught learning. Besides 100,000 unlabeled images, it contains 13,000 labeled images from 10 object classes (such as birds, cats, trucks), among which 5,000 images are partitioned for training while the remaining 8,000 images for testing. All the images are color images with 96\u00d796 pixels in size."}, {"id": "signal-1m-related-tweets", "name": "Signal-1M Related Tweets", "description": "Signal-1M Related Tweets consists of A TREC-like data collection to evaluate approaches for the task of related-tweet retrieval for news articles."}, {"id": "ocd-out-of-context-dataset", "name": "OCD (Out-of-Context Dataset)", "description": "OCD (Out-of-Context Dataset) is a synthetic dataset with fine-grained control over scene context. The images are generated using a 3D simulation engine in the VirtualHome environment, which allows to control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment."}, {"id": "relative-human", "name": "Relative Human", "description": "Relative Human (RH) contains multi-person in-the-wild RGB images with rich human annotations, including:"}, {"id": "ecssd-extended-complex-scene-saliency-dataset", "name": "ECSSD (Extended Complex Scene Saliency Dataset)", "description": "The Extended Complex Scene Saliency Dataset (ECSSD) is comprised of complex scenes, presenting textures and structures common to real-world images. ECSSD contains 1,000 intricate images and respective ground-truth saliency maps, created as an average of the labeling of five human participants."}, {"id": "ecoli", "name": "Ecoli", "description": "The Ecoli dataset is a dataset for protein localization. It contains 336 E.coli proteins split into 8 different classes."}, {"id": "sind-a-drone-dataset-at-signalized-intersection-in-china", "name": "SinD (A Drone Dataset at Signalized Intersection in China)", "description": "The SIND dataset is based on 4K video captured by drones, providing information including traffic participant trajectories, traffic light status, and high-definition maps"}, {"id": "convai2-conversational-intelligence-challenge-2", "name": "ConvAI2 (Conversational Intelligence Challenge 2)", "description": "The ConvAI2 NeurIPS competition aimed at finding approaches to creating high-quality dialogue agents capable of meaningful open domain conversation. The ConvAI2 dataset for training models is based on the PERSONA-CHAT dataset. The speaker pairs each have assigned profiles coming from a set of 1155 possible personas (at training time), each consisting of at least 5 profile sentences, setting aside 100 never seen before personas for validation. As the original PERSONA-CHAT test set was released, a new hidden test set consisted of 100 new personas and over 1,015 dialogs was created by crowdsourced workers."}, {"id": "rucola", "name": "RuCoLA", "description": "The Russian Corpus of Linguistic Acceptability (RuCoLA) is built from the ground up under the well-established binary LA approach. RuCoLA consists of 9.8k in-domain sentences from linguistic publications and 3.6k out-of-domain sentence produced by generative models."}, {"id": "3d-bsls-6d-3d-scans-of-bins-by-structured-light-scanner-for-6d-pose-estimation", "name": "3D-BSLS-6D (3D scans of Bins by Structured-Light Scanner for 6D pose estimation)", "description": "Dataset consist of both real captures from Photoneo PhoXi structured light scanner devices annotated by hand and synthetic samples produced by custom generator. In comparison with existing datasets for 6D pose estimation, some notable differences include:"}, {"id": "4seasons", "name": "4Seasons", "description": "4Seasons is adataset covering seasonal and challenging perceptual conditions for autonomous driving."}, {"id": "codex-small", "name": "CoDEx Small", "description": "CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. "}, {"id": "banglalekha-isolated", "name": "BanglaLekha-Isolated", "description": "This dataset contains Bangla handwritten numerals, basic characters and compound characters. This dataset was collected from multiple geographical location within Bangladesh and includes sample collected from a variety of aged groups. This dataset can also be used for other classification problems i.e: gender, age, district. "}, {"id": "detection-of-traffic-anomaly-dota", "name": "Detection of Traffic Anomaly (DoTA)", "description": "Contains 4,677 videos with temporal, spatial, and categorical annotations."}, {"id": "kth-tips2", "name": "KTH-TIPS2", "description": "The KTH-TIPS (Textures under varying Illumination, Pose and Scale) image database was created to extend the CUReT database in two directions, by providing variations in scale as well as pose and illumination, and by imaging other samples of a subset of its materials in different settings."}, {"id": "hopkins155", "name": "Hopkins155", "description": "The Hopkins 155 dataset consists of 156 video sequences of two or three motions. Each video sequence motion corresponds to a low-dimensional subspace. There are 39\u2212550 data vectors drawn from two or three motions for each video sequence."}, {"id": "scribble", "name": "Scribble", "description": "Scribble is a new outline dataset consisting of 200 images (150 train, 50 test) for each of 10 classes \u2013 basketball, chicken, cookie, cupcake, moon, orange, soccer, strawberry, watermelon and pineapple. All the images have a white background and were collected using search keywords on popular search engines. In each image, we obtain rough outlines for the image. We find the largest blob in the image after thresholding it into a black and white image. We fill the interior holes of the largest blob and obtain a smooth outline using the SavitzkyGolay filter."}, {"id": "market-1501", "name": "Market-1501", "description": "Market-1501 is a large-scale public benchmark dataset for person re-identification. It contains 1501 identities which are captured by six different cameras, and 32,668 pedestrian image bounding-boxes obtained using the Deformable Part Models pedestrian detector. Each person has 3.6 images on average at each viewpoint. The dataset is split into two parts: 750 identities are utilized for training and the remaining 751 identities are used for testing. In the official testing protocol 3,368 query images are selected as probe set to find the correct match across 19,732 reference gallery images."}, {"id": "ctdar-icdar-2019-competition-on-table-detection-and-recognition", "name": "cTDaR (ICDAR 2019 Competition on Table Detection and Recognition)", "description": "Table is a compact and efficient form for summarizing and presenting correlative information in handwritten and printed archival documents, scientific journals, reports, financial statements and so on. Table recognition is fundamental for the extraction of information from structured documents. The ICDAR 2019 cTDaR evaluates two aspects of table analysis: table detection and recognition. The participating methods will be evaluated on a modern dataset and archival documents with printed and handwritten tables present."}, {"id": "malnet", "name": "MalNet", "description": "MalNet is a large public graph database, representing a large-scale ontology of software function call graphs. MalNet contains over 1.2 million graphs, averaging over 17k nodes and 39k edges per graph, across a hierarchy of 47 types and 696 families."}, {"id": "tusimple-lane", "name": "TuSimple Lane", "description": "TuSimple Lane is an extension of the TuSimple dataset with 14,336 lane boundaries annotations. Each lane boundary in the dataset is annotated using 7 different classes such as \u201cSingle Dashed\u201d, \u201cDouble Dashed\u201d or \u201cSingle White Continuous\u201d."}, {"id": "radarscenes", "name": "RadarScenes", "description": "RadarScenes is a real-world radar point cloud dataset for automotive applications."}, {"id": "quda", "name": "Quda", "description": "Aims to help V-NLIs recognize analytic tasks from free-form natural language by training and evaluating cutting-edge multi-label classification models. The dataset contains  diverse user queries, and each is annotated with one or multiple analytic tasks. "}, {"id": "jester-jokes", "name": "Jester (Jokes)", "description": "6.5 million anonymous ratings of jokes by users of the Jester Joke Recommender System."}, {"id": "ohsumed", "name": "Ohsumed", "description": "Ohsumed includes medical abstracts from the MeSH categories of the year 1991. In [Joachims, 1997] were used the first 20,000 documents divided in 10,000 for training and 10,000 for testing. The specific task was to categorize the 23 cardiovascular diseases categories. After selecting the such category subset, the unique abstract number becomes 13,929 (6,286 for training and 7,643 for testing). As current computers can easily manage larger number of documents we make available all 34,389 cardiovascular diseases abstracts out of 50,216 medical abstracts contained in the year 1991."}, {"id": "gicoref-gender-inclusive-coreference", "name": "GICoref (Gender Inclusive Coreference)", "description": "GICoref is a fully annotated coreference resolution dataset written by and about trans people."}, {"id": "jetclass-a-large-scale-dataset-for-deep-learning-in-jet-physics", "name": "JetClass (A Large-Scale Dataset for Deep Learning in Jet Physics)", "description": "JetClass is a new large-scale dataset to facilitate deep learning research in particle physics. It consists of 100M particle jets for training, 5M for validation and 20M for testing. The dataset contains 10 classes of jets, simulated with MadGraph + Pythia + Delphes. A detailed description of the JetClass dataset is presented in the paper Particle Transformer for Jet Tagging. An interface to use the dataset is provided here."}, {"id": "pgr-phenotype-gene-relations", "name": "PGR (Phenotype-Gene Relations)", "description": "Phenotype-Gene Relations (PGR) is a corpus that consists of 1712 abstracts, 5676 human phenotype annotations, 13835 gene annotations, and 4283 relations. "}, {"id": "botswana", "name": "Botswana", "description": "Botswana is a hyperspectral image classification dataset. The NASA EO-1 satellite acquired a sequence of data over the Okavango Delta, Botswana in 2001-2004. The Hyperion sensor on EO-1 acquires data at 30 m pixel resolution over a 7.7 km strip in 242 bands covering the 400-2500 nm portion of the spectrum in 10 nm windows. Preprocessing of the data was performed by the UT Center for Space Research to mitigate the effects of bad detectors, inter-detector miscalibration, and intermittent anomalies. Uncalibrated and noisy bands that cover water absorption features were removed, and the remaining 145 bands were included as candidate features: [10-55, 82-97, 102-119, 134-164, 187-220]. The data analyzed in this study, acquired May 31, 2001, consist of observations from 14 identified classes representing the land cover types in seasonal swamps, occasional swamps, and drier woodlands located in the distal portion of the Delta."}, {"id": "vitt-video-timeline-tags", "name": "ViTT (Video Timeline Tags)", "description": "The ViTT dataset consists of human produced segment-level annotations for 8,169 videos. Of these, 5,840 videos have been annotated once, and the rest of the videos have been annotated twice or more. A total of 12,461 sets of annotations are released. The videos in the dataset are from the Youtube-8M dataset."}, {"id": "synscapes", "name": "Synscapes", "description": "Synscapes is a synthetic dataset for street scene parsing created using photorealistic rendering techniques, and show state-of-the-art results for training and validation as well as new types of analysis. "}, {"id": "auxad", "name": "AuxAD", "description": "AuxAD is a a distantly supervised dataset for acronym disambiguation."}, {"id": "ba-2motifs", "name": "BA-2motifs", "description": "It's a synthetic dataset, which contains 1000 graphs divided into two classes according to the motif they contain: either a \u201chouse\u201d or a five-node cycle."}, {"id": "cicero-contextualized-commonsense-inference-in-dialogues", "name": "CICERO (Contextualized Commonsense Inference in Dialogues)", "description": "CICERO contains 53,000 inferences for five commonsense dimensions -- cause, subsequent event, prerequisite, motivation, and emotional reaction -- collected from 5600 dialogues. It involves two challenging generative and multi-choice alternative selection tasks for the state-of-the-art NLP models to solve. Download the dataset using this link."}, {"id": "spatialvoc2k", "name": "SpatialVOC2K", "description": "A multilingual image dataset with spatial relation annotations and object features for image-to-text generation, built using 2,026 images from the PASCAL VOC2008 dataset. "}, {"id": "argscichat", "name": "ArgSciChat", "description": "ArgSciChat is an argumentative dialogue dataset. It consists of 498 messages collected from 41 dialogues on 20 scientific papers. It can be used to evaluate conversational agents and further encourage research on argumentative scientific agents."}, {"id": "titan", "name": "TITAN", "description": "TITAN consists of 700 labeled video-clips (with odometry) captured from a moving vehicle on highly interactive urban traffic scenes in Tokyo. The dataset includes 50 labels including vehicle states and actions, pedestrian age groups, and targeted pedestrian action attributes that are organized hierarchically corresponding to atomic, simple/complex-contextual, transportive, and communicative actions. "}, {"id": "s-coco-synthetic-coco", "name": "S-COCO (Synthetic COCO)", "description": "Synthetic COCO (S-COCO) is a synthetically created dataset for homography estimation learning. It was introduced by DeTone et al., where the source and target images are generated by duplicating the same COCO image. The source patch $I_S$ is generated by randomly cropping a source candidate at position $p$ with a size of 128 \u00d7128 pixels. Then the patch\u2019s corners are randomly perturbed vertically and horizontally by values within the range [\u2212$\\rho$,$\\rho$] and the four correspondences define a homography $H_{ST}$ . The inverse of this homography $H_{TS} = (H_{ST} )^{-1}$ is applied to the target candidate and from the resulted warped image a target patch $I_T$ is cropped at the same location p. Both $I_S$ and $I_T$ are the input data with the homography $H_{ST}$ as ground truth."}, {"id": "fsd50k-freesound-database-50k", "name": "FSD50K (Freesound Database 50K)", "description": "Freesound Dataset 50k (or FSD50K for short) is an open dataset of human-labeled sound events containing 51,197 Freesound clips unequally distributed in 200 classes drawn from the AudioSet Ontology. FSD50K has been created at the Music Technology Group of Universitat Pompeu Fabra. It consists mainly of sound events produced by physical sound sources and production mechanisms, including human sounds, sounds of things, animals, natural sounds, musical instruments and more."}, {"id": "seed-sjtu-emotion-eeg-dataset", "name": "SEED (SJTU Emotion EEG Dataset)", "description": "The SEED dataset contains subjects' EEG signals when they were watching films clips. The film clips are carefully selected so as to induce different types of emotion, which are positive, negative, and neutral ones."}, {"id": "h3d-honda-research-institute-3d", "name": "H3D (Honda Research Institute 3D)", "description": "The H3D is a large scale full-surround 3D multi-object detection and tracking dataset. It is gathered from HDD dataset, a large scale naturalistic driving dataset collected in San Francisco Bay Area. H3D consists of following features:"}, {"id": "melinda", "name": "Melinda", "description": "Introduces a new dataset, MELINDA, for Multimodal biomEdicaL experImeNt methoD clAssification. The dataset is collected in a fully automated distant supervision manner, where the labels are obtained from an existing curated database, and the actual contents are extracted from papers associated with each of the records in the database. "}, {"id": "24-hours-chinese-mandarin-synthesis-corpus-female-general", "name": "24 Hours - Chinese Mandarin Synthesis Corpus-Female, General", "description": "Description: Chinese Mandarin Synthesis Corpus-Female, General. It is recorded by Chinese native speaker. It covers oral sentences, audio books, news, advertising, customer service and movie commentary, and the phonemes and tones are balanced. Professional phonetician participates in the annotation. It precisely matches with the research and development needs of the speech synthesis."}, {"id": "rap-richly-annotated-pedestrian", "name": "RAP (Richly Annotated Pedestrian)", "description": "The Richly Annotated Pedestrian (RAP) dataset is a dataset for pedestrian attribute recognition. It contains 41,585 images collected from indoor surveillance cameras. Each image is annotated with 72 attributes, while only 51 binary attributes with the positive ratio above 1% are selected for evaluation. There are 33,268 images for the training set and 8,317 for testing."}, {"id": "radqa-a-question-answering-dataset-to-improve-comprehension-of-radiology-reports", "name": "RadQA (A Question Answering Dataset to Improve Comprehension of Radiology Reports)", "description": "RadQA is a radiology question answering dataset with 3074 questions posed against radiology reports and annotated with their corresponding answer spans (resulting in a total of 6148 question-answer evidence pairs) by physicians. The questions are manually created using the clinical referral section of the reports that take into account the actual information needs of ordering physicians and eliminate bias from seeing the answer context (and, further, organically create unanswerable questions). The answer spans are marked within the Findings and Impressions sections of a report. The dataset aims to satisfy the complex clinical requirements by including complete (yet concise) answer phrases (which are not just entities) that can span multiple lines."}, {"id": "webvision", "name": "WebVision", "description": "The WebVision dataset is designed to facilitate the research on learning visual representation from noisy web data. It is a large scale web images dataset that contains more than 2.4 million of images crawled from the Flickr website and Google Images search. "}, {"id": "eqasc", "name": "eQASC", "description": "This dataset contains 98k 2-hop explanations for questions in the QASC dataset, with annotations indicating if they are valid (~25k) or invalid (~73k) explanations."}, {"id": "cwl-eeg-fmri-dataset", "name": "CWL EEG/fMRI Dataset", "description": "EEG/fMRI Data from 8 subject doing a simple eyes open/eyes closed task is provided on this webpage."}, {"id": "renoir", "name": "RENOIR", "description": "A dataset of color images corrupted by natural noise due to low-light conditions, together with spatially and intensity-aligned low noise images of the same scenes."}, {"id": "fairytaleqa", "name": "FairytaleQA", "description": "FairytaleQA is a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Annotated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly story narratives, covering seven types of narrative elements or relations. It can support narrative Question Generation (QG) and Narrative Question Answering (QA) tasks."}, {"id": "xformal", "name": "XFORMAL", "description": " XFORMAL is a multilingual formal style transfer benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian."}, {"id": "plant-seedlings-dataset", "name": "Plant Seedlings Dataset", "description": "A database of images of approximately 960 unique plants belonging to 12 species at several growth stages is made publicly available. It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm."}, {"id": "massachusetts-roads-dataset-road-and-building-detection-datasets-massachusetts-roads-dataset", "name": "Massachusetts Roads Dataset (Road and Building Detection Datasets - Massachusetts Roads Dataset)", "description": "The datasets introduced in Chapter 6 of my PhD thesis are below. See the thesis for more details. If you use any of these datasets for research purposes you should use the following citation in any resulting publications:"}, {"id": "conll-2002", "name": "CoNLL 2002", "description": "The shared task of CoNLL-2002 concerns language-independent named entity recognition. The types of named entities include: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. The participants of the shared task were offered training and test data for at least two languages. Information sources other than the training data might have been used in this shared task."}, {"id": "citr-dut-citr-dataset-and-dut-dataset", "name": "CITR & DUT (CITR dataset and DUT dataset)", "description": "Consists of two pedestrian trajectory datasets, CITR dataset and DUT dataset, so that the pedestrian motion models can be further calibrated and verified, especially when vehicle influence on pedestrians plays an important role. "}, {"id": "earnings-21", "name": "Earnings-21", "description": "Earnings-21, a 39-hour corpus of earnings calls containing entity-dense speech from nine different financial sectors. This corpus is intended to benchmark ASR (Automatic Speech Recognition) systems in the wild with special attention towards named entity recognition."}, {"id": "bsd-berkeley-segmentation-dataset", "name": "BSD (Berkeley Segmentation Dataset)", "description": "BSD is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300."}, {"id": "wireframe", "name": "Wireframe", "description": "The Wireframe dataset consists of 5,462 images (5,000 for training, 462 for test) of indoor and outdoor man-made scenes."}, {"id": "k-emocon", "name": "K-EmoCon", "description": "A multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. "}, {"id": "vfp290k", "name": "VFP290K", "description": "Vision-based Fallen Person (VFP290K) is a novel, large-scale dataset for the detection of fallen persons composed of fallen person images collected in various real-world scenarios. VFP290K consists of 294,714 frames of fallen persons extracted from 178 videos, including 131 scenes in 49 locations."}, {"id": "apiza-corpus", "name": "Apiza Corpus", "description": "The Apiza Corpus is a WoZ-like (Wizard of Oz) set of dialogues between 30 programmers and a simulated virtual assistant. This corpus can be used to study or train a virtual assistant for software engineering."}, {"id": "promise12", "name": "PROMISE12", "description": "The PROMISE12 dataset was made available for the MICCAI 2012 prostate segmentation challenge. Magnetic Resonance (MR) images (T2-weighted) of 50 patients with various diseases were acquired at different locations with several MRI vendors and scanning protocols."}, {"id": "ebm-nlp", "name": "EBM-NLP", "description": "EBM-NLP annotates PICO (Participants, Interventions, Comparisons and Outcomes) spans in clinical trial abstracts.  The corresponding PICO Extraction task aims to identify the spans in clinical trial abstracts that describe the respective PICO elements."}, {"id": "discofuse", "name": "DiscoFuse", "description": "DiscoFuse was created by applying a rule-based splitting method on two corpora - sports articles crawled from the Web, and Wikipedia. See the paper for a detailed description of the dataset generation process and evaluation."}, {"id": "dayton", "name": "Dayton", "description": "The Dayton dataset is a dataset for ground-to-aerial (or aerial-to-ground) image translation, or cross-view image synthesis. It contains images of road views and aerial views of roads. There are 76,048 images in total and the train/test split is 55,000/21,048. The images in the original dataset have 354\u00d7354 resolution."}, {"id": "nucmm", "name": "NucMM", "description": "NucMM is a dataset for segmenting 3D cell nuclei from microscopy image volumes that pushes the task forward to the sub-cubic millimeter scale. It consists of two fully annotated volumes: one electron microscopy (EM) volume containing nearly the entire zebrafish brain with around 170,000 nuclei; and one micro-CT (uCT) volume containing part of a mouse visual cortex with about 7,000 nuclei."}, {"id": "aquamuse", "name": "aquamuse", "description": "5,519 query-based summaries, each associated with an average of 6 input documents selected from an index of 355M documents from Common Crawl."}, {"id": "nyu-hand", "name": "NYU Hand", "description": "The NYU Hand pose dataset contains 8252 test-set and 72757 training-set frames of captured RGBD data with ground-truth hand-pose information. For each frame, the RGBD data from 3 Kinects is provided: a frontal view and 2 side views. The training set contains samples from a single user only (Jonathan Tompson), while the test set contains samples from two users (Murphy Stein and Jonathan Tompson). A synthetic re-creation (rendering) of the hand pose is also provided for each view."}, {"id": "miap-more-inclusive-annotations-for-people", "name": "MIAP (More Inclusive Annotations for People)", "description": "MIAP is a dataset created by obtaining a new set of annotations on a subset of the Open Images dataset, containing bounding boxes and attributes for all of the people visible in those images, as the original Open Images dataset annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image."}, {"id": "etymdb-2-0", "name": "EtymDB 2.0", "description": "A multilingual etymological database extracted from the Wiktionary (described in Methodological Aspects of Developing and Managing an Etymological Lexical Resource: Introducing EtymDB-2.0)"}, {"id": "hilti-slam-challenge", "name": "Hilti SLAM Challenge", "description": "Hilti SLAM Challenge is a dataset for Simultaneous Localization and Mapping (SLAM) algorithms due to sparsity, varying illumination conditions, and dynamic objects. The sensor platform used to collect this dataset contains a number of visual, lidar and inertial sensors which have all been rigorously calibrated. All data is temporally aligned to support precise multi-sensor fusion. Each dataset includes accurate ground truth to allow direct testing of SLAM results. Raw data as well as intrinsic and extrinsic sensor calibration data from twelve datasets in various environments is provided. Each environment represents common scenarios found in building construction sites in various stages of completion."}, {"id": "mpii-human-pose", "name": "MPII Human Pose", "description": "MPII Human Pose Dataset is a dataset for human pose estimation. It consists of around 25k images extracted from online videos. Each image contains one or more people, with over 40k people annotated in total. Among the 40k samples, \u223c28k samples are for training and the remainder are for testing. Overall the dataset covers 410 human activities and each image is provided with an activity label. Images were extracted from a YouTube video and provided with preceding and following un-annotated frames."}, {"id": "qdax", "name": "QDax", "description": "QDax is a benchmark suite designed for for Deep Neuroevolution in Reinforcement Learning domains for robot control. The suite includes the definition of tasks, environments, behavioral descriptors, and fitness. It specify different benchmarks based on the complexity of both the task and the agent controlled by a deep neural network. The benchmark uses standard Quality-Diversity metrics, including coverage, QD-score, maximum fitness, and an archive profile metric to quantify the relation between coverage and fitness."}, {"id": "kaokore", "name": "KaoKore", "description": "Consists of faces extracted from pre-modern Japanese artwork."}, {"id": "3d-vehicle-tracking-simulation-dataset", "name": "3D Vehicle Tracking Simulation Dataset", "description": "To collect the 3D Vehicle Tracking Simulation Dataset, a driving simulation is used to obtain accurate 3D bounding box annotations at no cost of human efforts. The data collection and annotation pipeline extend the previous works like VIPER and FSV, especially in terms of linking identities across frames. The simulation is based on Grand Theft Auto V, a modern game that simulates a functioning city and its surroundings in a photo-realistic three-dimensional world. Note that the pipeline is real-time, providing the potential of largescale data collection, while VIPER requires expensive offline processings."}, {"id": "winoground", "name": "Winoground", "description": "Winoground is a dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning. Given two images and two captions, the goal is to match them correctly -- but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance."}, {"id": "camo", "name": "CAMO++", "description": "CAMO++ is a dataset for camouflaged object segmentation. This dataset increases the number of images with hierarchical pixel-wise ground-truths. The authors also provide a benchmark suite for the task of camouflaged instance segmentation."}, {"id": "news-articles-dataset-with-summary", "name": "News Articles Dataset with Summary", "description": "This dataset is the news articles scraped from New York Times, CNN, Business Insider and Breitbart. The original dataset published in Kaggle did not provide any human summaries, it only offered the title of the article, while this could be used as the summary, it is not ideal as the headline title was too short. We generated the label manually by adding the human summary for the available articles. We also added another column called theme to the dataset, this column would state the genre of the news articles."}, {"id": "hpd-head-pose-detection", "name": "HPD (Head-Pose Detection)", "description": "These images were generated using Blender and IEE-Simulator with different head-poses, where the images are labelled according to nine classes (straight, turned bottom-left, turned left, turned top-left, turned bottom-right, turned right, turned top-right, reclined, looking up). The dataset contains 16,013 training images and 2,825 testing images, in addition to 4,700 images for improvements."}, {"id": "aware-aware-aspect-based-sentiment-analysis-dataset-of-apps-reviews-for-requirements-elicitation", "name": "AWARE (AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation)", "description": "The peer-reviewed paper of AWARE dataset is published in ASEW 2021, and can be accessed through: http://doi.org/10.1109/ASEW52652.2021.00049. Kindly cite this paper when using AWARE dataset."}, {"id": "tbbr-thermal-bridges-on-building-rooftops", "name": "TBBR (Thermal Bridges on Building Rooftops)", "description": "The dataset of Thermal Bridges on Building Rooftops (TBBR dataset) consists of annotated combined RGB and thermal drone images with a height map. All images were converted to a uniform format of 3000$\\times$4000 pixels, aligned, and cropped to 2400$\\times$3400 to remove empty borders."}, {"id": "adressa-smartmedia-adressa-news-dataset", "name": "Adressa (SmartMedia Adressa News Dataset)", "description": "The Adressa Dataset is a news dataset that includes news articles (in Norwegian) in connection with anonymized users. We hope that this dataset will be helpful to achieve a better understanding of the news articles in conjunction with their readers. This dataset is published with the collaboration of Norwegian University of Science and Technology (NTNU) and Adressavisen (local newspaper in Trondheim, Norway) as a part of RecTech project on recommendation technology. For further details of the project and the dataset please refer to the paper mentioned below for citations."}, {"id": "wisconsin-60-20-20-random-splits", "name": "Wisconsin(60%/20%/20% random splits)", "description": "Node classification on Wisconsin with 60%/20%/20% random splits for training/validation/test."}, {"id": "wikiconvert", "name": "WikiConvert", "description": "Wiki-Convert is a 900,000+ sentences dataset of precise number annotations from English Wikipedia. It relies on Wiki contributors' annotations in the form of a {{Convert}} template."}, {"id": "gbcu-gallbladder-cancer-ultrasound-dataset", "name": "GBCU (Gallbladder Cancer Ultrasound Dataset)", "description": "GBCU is the first public dataset for Gallbladder Cancer identification from Ultrasound images. GBCU contains a total of 1255 (432 normal, 558 benign, and 265 malignant) annotated abdominal Ultrasound images collected from 218 patients. Of the 218 patients, 71, 100, and 47 were from the normal, benign, and malignant classes, respectively. The sizes of the training and testing sets are 1133 and 122, respectively. To ensure generalization to unseen patients, all images of any particular patient were either in the train or the test split. We acquired data samples from patients referred to PGIMER, Chandigarh (a referral hospital in Northern India) for abdominal ultrasound examinations of suspected Gallbladder pathologies. The study was approved by the Ethics Committee of PGIMER, Chandigarh. We obtained informed written consent from the patients at the time of recruitment, and protect their privacy by fully anonymizing the data. Grayscale B-mode static images, including both sagittal and axial sections, were recorded by radiologists for each patient using a Logiq S8 machine. "}, {"id": "activitynet-entities", "name": "ActivityNet Entities", "description": "ActivityNet-Entities, augments the challenging ActivityNet Captions dataset with 158k bounding box annotations, each grounding a noun phrase. This allows training video description models with this data, and importantly, evaluate how grounded or \"true\" such model are to the video they describe."}, {"id": "k2hpd", "name": "K2HPD", "description": "Includes 100K depth images under challenging scenarios."}, {"id": "mizan", "name": "MIZAN", "description": "Persian-English parallel corpus with more than one million sentence pairs collected from masterpieces of literature."}, {"id": "mvtecad-mvtec-anomaly-detection-dataset", "name": "MVTecAD (MVTEC ANOMALY DETECTION DATASET)", "description": "MVTec AD is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over 5000 high-resolution images divided into fifteen different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects."}, {"id": "mhist-minimalist-histopathology-image-analysis-dataset", "name": "MHIST (Minimalist Histopathology image analysis dataset)", "description": "The minimalist histopathology image analysis dataset (MHIST) is a binary classification dataset of 3,152 fixed-size images of colorectal polyps, each with a gold-standard label determined by the majority vote of seven board-certified gastrointestinal pathologists. MHIST also includes each image\u2019s annotator agreement level. As a minimalist dataset, MHIST occupies less than 400 MB of disk space, and a ResNet-18 baseline can be trained to convergence on MHIST in just 6 minutes using approximately 3.5 GB of memory on a NVIDIA RTX 3090. As example use cases, the authors use MHIST to study natural questions that arise in histopathology image classification such as how dataset size, network depth, transfer learning, and high-disagreement examples affect model performance."}, {"id": "codraw", "name": "CoDraw", "description": "The Collaborative Drawing game (CoDraw) dataset contains ~10K dialogs consisting of ~138K messages exchanged between human players in the CoDraw game. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language."}, {"id": "indictts", "name": "IndicTTS", "description": "A special corpus of Indian languages covering 13 major languages of India. It comprises of 10000+ spoken sentences/utterances each of mono and English recorded by both Male and Female native speakers. Speech waveform files are available in .wav format along with the corresponding text. We hope that these recordings will be useful for researchers and speech technologists working on synthesis and recognition. You can request zip archives of the entire database here."}, {"id": "sum", "name": "SUM", "description": "SUM is a new benchmark dataset of semantic urban meshes which covers about 4 km2 in Helsinki (Finland), with six classes: Ground, Vegetation, Building, Water, Vehicle, and Boat."}, {"id": "ucf-cc-50", "name": "UCF-CC-50", "description": "UCF-CC-50 is a dataset for crowd counting and consists of images of extremely dense crowds. It has 50 images with 63,974 head center annotations in total. The head counts range between 94 and 4,543 per image. The small dataset size and large variance make this a very challenging counting dataset."}, {"id": "indonlg", "name": "IndoNLG", "description": "IndoNLG is a benchmark to measure natural language generation (NLG) progress in three low-resource\u2014yet widely spoken\u2014languages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks."}, {"id": "videolq", "name": "VideoLQ", "description": "VideoLQ consists of videos downloaded from various video hosting sites such as Flickr and YouTube, with a Creative Common license."}, {"id": "mmarco", "name": "mMARCO", "description": "mMARCO is a multilingual version of the MS MARCO passage ranking dataset comprising 8 languages that was created using machine translation."}, {"id": "hmdb51", "name": "HMDB51", "description": "The HMDB51 dataset is a large collection of realistic videos from various sources, including movies and web videos. The dataset is composed of 6,766 video clips from 51 action categories (such as \u201cjump\u201d, \u201ckiss\u201d and \u201claugh\u201d), with each category containing at least 101 clips. The original evaluation scheme uses three different training/testing splits. In each split, each action class has 70 clips for training and 30 clips for testing. The average accuracy over these three splits is used to measure the final performance."}, {"id": "ecqa-explanations-for-commonsenseqa", "name": "ECQA (Explanations for CommonsenseQA)", "description": "This repository contains the publicly released dataset, code, and models for the Explanations for CommonsenseQA paper presented at ACL-IJCNLP 2021. Directories data and  code inside the root folder contain dataset and code, respectively. The same data and code are also made available through our AIHN collaboration partner institute IIT Delhi. You can download the full paper from here."}, {"id": "2017-robotic-instrument-segmentation-challenge", "name": "2017 Robotic Instrument Segmentation Challenge", "description": "Segmentation of robotic instruments is an important problem for robotic assisted minimially invasive surgery. It can be used for simple 2D applications such as overlay masking or 2D tracking but also for more complex 3D tasks such as pose estimation. In this challenge we invite applicants to participate in 3 different tasks: binary segmentation, multi-label segmentation and instrument recognition. Binary segmentation involves just separating the image into instruments and background, whereas multi-label segmentation requires the user to also recognize which parts of the instrument body correspond to the different articulated parts of a da Vinci robotic instrument. The final recogition task tests whether the user can recognize which segmentation corresponds to which da Vinci instrument type. "}, {"id": "paralex", "name": "Paralex", "description": "Paralex learns from a collection of 18 million question-paraphrase pairs scraped from WikiAnswers."}, {"id": "mag-scholar-c", "name": "MAG-Scholar-C", "description": "MAG-Scholar-C is constructed by Bojchevski et al. based on Microsoft Academic Graph (MAG), in which nodes refer to papers, edges represent citation relations among papers and features are bag-of-words  of paper abstracts."}, {"id": "objectfolder", "name": "ObjectFolder", "description": "ObjectFolder is a dataset for multisensory object-centric perception, reasoning, and interaction. It consists of 100 virtualized objects. ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks."}, {"id": "musedata", "name": "MuseData", "description": "MuseData is an electronic library of orchestral and piano classical music from CCARH. It consists of around 3MB of 783 files."}, {"id": "deap", "name": "DEAP", "description": "The DEAP dataset consists of two parts:"}, {"id": "imigue", "name": "iMiGUE", "description": "iMiGUE is a dataset for emotional artificial intelligence research: identity-free video dataset for Micro-Gesture Understanding and Emotion analysis (iMiGUE). Different from existing public datasets, iMiGUE focuses on nonverbal body gestures without using any identity information, while the predominant researches of emotion analysis concern sensitive biometric data, like face and speech. Most importantly, iMiGUE focuses on micro-gestures, i.e., unintentional behaviors driven by inner feelings, which are different from ordinary scope of gestures from other gesture datasets which are mostly intentionally performed for illustrative purposes. Furthermore, iMiGUE is designed to evaluate the ability of models to analyze the emotional states by integrating information of recognized micro-gesture, rather than just recognizing prototypes in the sequences separately (or isolatedly)."}, {"id": "v-hico", "name": "V-HICO", "description": "V-HICO is a dataset for human-object interaction in videos. There are 6,594 videos, including 5,297 training videos, 635 validation videos, 608 test videos, and 54 unseen test videos, of human-object interaction. To test the performance of models on common human-object interaction classes and generalization to new human-object interaction classes, we provide two test splits, the first one has the same human-object interaction classes in the training split while the second one consists of unseen novel classes."}, {"id": "timit-timit-acoustic-phonetic-continuous-speech-corpus", "name": "TIMIT (TIMIT Acoustic-Phonetic Continuous Speech Corpus)", "description": "The TIMIT Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used for evaluation of automatic speech recognition systems. It consists of recordings of 630 speakers of 8 dialects of American English each reading 10 phonetically-rich sentences. It also comes with the word and phone-level transcriptions of the speech."}, {"id": "3dssg", "name": "3DSSG", "description": "3DSSG provides 3D semantic scene graphs for 3RScan. A semantic scene graph is defined by a set of tuples between nodes and edges where nodes represent specific 3D object instances in a 3D scan. Nodes are defined by its semantics, a hierarchy of classes as well as a set of attributes that describe the visual and physical appearance of the object instance and their affordances. The edges in our graphs are the semantic relationships (predicates) between the nodes such as standing on, hanging on, more comfortable than or same material."}, {"id": "medmnist", "name": "MedMNIST", "description": "A collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28x28 images, which requires no background knowledge. "}, {"id": "faithdial", "name": "FaithDial", "description": "FaithDial is a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark."}, {"id": "emailsum-email-thread-summarization", "name": "EmailSum (Email Thread Summarization)", "description": "Email Thread Summarization (EmailSum) is a dataset which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549 email threads (each containing 3 to 10 emails) over a wide variety of topics. It was developed to spur research in thread summarization."}, {"id": "newspaper-navigator", "name": "Newspaper Navigator", "description": "The largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model."}, {"id": "maven-ere", "name": "MAVEN-ERE", "description": "MAVEN-ERE is a dataset designed for event relation extraction tasks containing 103,193 event coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations."}, {"id": "yt-bb-youtube-boundingboxes", "name": "YT-BB (YouTube-BoundingBoxes)", "description": "YouTube-BoundingBoxes (YT-BB) is a large-scale data set of video URLs with densely-sampled object bounding box annotations. The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. "}, {"id": "swinyseg-singapore-whole-sky-nychthemeron-image-segmentation-database", "name": "SWINySEG (Singapore Whole sky Nychthemeron Image SEGmentation Database)", "description": "The SWINySEG dataset contains 6768 daytime- and nighttime-images of sky/cloud patches along with their corresponding binary ground truth maps. The images in the SWINySeg dataset are taken from two of our earlier sky/cloud image segmentation datasets -- SWIMSEG and SWINSEG.  All images were captured in Singapore using WAHRSIS, a calibrated ground-based whole sky imager, over a period of 12 months from January to December 2016. The ground truth annotation was done in consultation with experts from Singapore Meteorological Services."}, {"id": "fixatons", "name": "FixaTons", "description": "FixaTons is a large collection of datasets human scanpaths (temporally ordered sequences of fixations) and saliency maps. "}, {"id": "mvtec-3d-ad-the-mvtec-3d-anomaly-detection-dataset", "name": "MVTEC 3D-AD (THE MVTEC 3D ANOMALY DETECTION DATASET)", "description": "MVTec 3D Anomaly Detection Dataset (MVTec 3D-AD) is a comprehensive 3D dataset for the task of unsupervised anomaly detection and localization. It contains over 4000 high-resolution scans acquired by an industrial 3D sensor. Each of the 10 different object categories comprises a set of defect-free training and validation samples and a test set of samples with various kinds of defects. Precise ground-truth annotations are provided for each anomalous test sample."}, {"id": "patternnet", "name": "PatternNet", "description": "PatternNet is a large-scale high-resolution remote sensing dataset collected for remote sensing image retrieval. There are 38 classes and each class has 800 images of size 256\u00d7256 pixels. The images in PatternNet are collected from Google Earth imagery or via the Google Map API for some US cities. The following table shows the classes and the corresponding spatial resolutions. The figure shows some example images from each class."}, {"id": "squirrel-48-32-20-fixed-splits", "name": "Squirrel (48%/32%/20% fixed splits)", "description": "Node classification on Squirrel with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "js-fake-chorales", "name": "JS Fake Chorales", "description": "A MIDI dataset of 500 4-part chorales generated by the KS_Chorus algorithm, annotated with results from hundreds of listening test participants, with 300 further unannotated chorales."}, {"id": "ok-vqa-outside-knowledge-visual-question-answering", "name": "OK-VQA (Outside Knowledge Visual Question Answering)", "description": "Outside Knowledge Visual Question Answering (OK-VQA) includes more than 14,000 questions that require external knowledge to answer. "}, {"id": "fdf-flickr-diverse-faces", "name": "FDF (Flickr Diverse Faces)", "description": "A diverse dataset of human faces, including unconventional poses, occluded faces, and a vast variability in backgrounds. "}, {"id": "sroie", "name": "SROIE", "description": "Consists of a dataset with 1000 whole scanned receipt images and annotations for the competition on scanned receipts OCR and key information extraction (SROIE)."}, {"id": "arcade-learning-environment", "name": "Arcade Learning Environment", "description": "The Arcade Learning Environment (ALE) is an object-oriented framework that allows researchers to develop AI agents for Atari 2600 games. It is built on top of the Atari 2600 emulator Stella and separates the details of emulation from agent design."}, {"id": "h-dibco-2018", "name": "H-DIBCO 2018", "description": "H-DIBCO 2018 is the international Handwritten Document Image Binarization Contest organized in the context of ICFHR 2018 conference. The general objective of the contest is to record recent advances in document image binarization using established evaluation performance measures."}, {"id": "fairface", "name": "FairFace", "description": "FairFace is a face image dataset which is race balanced. It contains 108,501 images from 7 different race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups."}, {"id": "crowdhuman", "name": "CrowdHuman", "description": "CrowdHuman is a large and rich-annotated human detection dataset, which contains 15,000, 4,370 and 5,000 images collected from the Internet for training, validation and testing respectively. The number is more than 10\u00d7 boosted compared with previous challenging pedestrian detection dataset like CityPersons. The total number of persons is also noticeably larger than the others with \u223c340k person and \u223c99k ignore region annotations in the CrowdHuman training subset."}, {"id": "freesolv-scaffold-scaffold-split-of-freesolv-dataset", "name": "FreeSolv(scaffold) (Scaffold split of FreeSolv  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "cluster", "name": "CLUSTER", "description": "CLUSTER is a node classification tasks generated with Stochastic Block Models, which is widely used to model communities in social networks by modulating the intra- and extra-communities connections, thereby controlling the difficulty of the task. CLUSTER aims at identifying community clusters in a semi-supervised setting."}, {"id": "iam-iam-handwriting", "name": "IAM (IAM Handwriting)", "description": "The IAM database contains 13,353 images of handwritten lines of text created by 657 writers. The texts those writers transcribed are from the Lancaster-Oslo/Bergen Corpus of British English. It includes contributions from 657 writers making a total of 1,539 handwritten pages comprising of 115,320 words and is categorized as part of modern collection. The database is labeled at the sentence, line, and word levels."}, {"id": "ms-cxr-making-the-most-of-text-semantics-to-improve-biomedical-vision-language-processing", "name": "MS-CXR (Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing)", "description": "The MS-CXR dataset provides 1162 image\u2013sentence pairs of bounding boxes and corresponding phrases, collected across eight different cardiopulmonary radiological findings, with an approximately equal number of pairs for each finding. This dataset complements the existing MIMIC-CXR v.2 dataset and comprises: 1. Reviewed and edited bounding boxes and phrases (1026 pairs of bounding box/sentence); and 2. Manual bounding box labels from scratch (136 pairs of bounding box/sentence).e"}, {"id": "flickrstyle10k", "name": "FlickrStyle10K", "description": "FlickrStyle10K is collected and built on Flickr30K image caption dataset. The original FlickrStyle10K dataset has 10,000 pairs of images and stylized captions including humorous and romantic styles. However, only 7,000 pairs from the of\ufb01cial training set are now publicly accessible. The dataset can be downloaded via https://zhegan27.github.io/Papers/FlickrStyle_v0.9.zip"}, {"id": "lyft-level-5-prediction", "name": "Lyft Level 5 Prediction", "description": "A self-driving dataset for motion prediction, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. "}, {"id": "mosmeddata", "name": "MosMedData", "description": "MosMedData contains anonymised human lung computed tomography (CT) scans with COVID-19 related findings, as well as without such findings. A small subset of studies has been annotated with binary pixel masks depicting regions of interests (ground-glass opacifications and consolidations). CT scans were obtained between 1st of March, 2020 and 25th of April, 2020, and provided by municipal hospitals in Moscow, Russia."}, {"id": "vctk-cstr-vctk-corpus", "name": "VCTK (CSTR VCTK Corpus)", "description": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald & Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856. The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf. All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed."}, {"id": "kvasircapsule-seg", "name": "KvasirCapsule-SEG", "description": "The dataset contains a Video capsule endoscopy dataset for polyp segmentation. "}, {"id": "off-near-parallel-smac-off-near-parallel-20", "name": "Off_Near_parallel (SMAC+_Off_Near_parallel_20)", "description": "smac+ offensive near scenario with 20 parallel episodic buffer"}, {"id": "shanghaitech", "name": "ShanghaiTech", "description": "The Shanghaitech dataset is a large-scale crowd counting dataset. It consists of 1198 annotated crowd images. The dataset is divided into two parts, Part-A containing 482 images and Part-B containing 716 images. Part-A is split into train and test subsets consisting of 300 and 182 images, respectively. Part-B is split into train and test subsets consisting of 400 and 316 images. Each person in a crowd image is annotated with one point close to the center of the head. In total, the dataset consists of 330,165 annotated people. Images from Part-A were collected from the Internet, while images from Part-B were collected on the busy streets of Shanghai."}, {"id": "youtube-ugc-youtube-ugc-dataset", "name": "YouTube-UGC (YouTube UGC dataset)", "description": "This YouTube dataset is a sampling from thousands of User Generated Content (UGC) as uploaded to YouTube distributed under the Creative Commons license. This dataset was created in order to assist in the advancement of video compression and quality assessment research of UGC videos."}, {"id": "replay-attack", "name": "Replay-Attack", "description": "The Replay-Attack Database for face spoofing consists of 1300 video clips of photo and video attack attempts to 50 clients, under different lighting conditions. All videos are generated by either having a (real) client trying to access a laptop through a built-in webcam or by displaying a photo or a video recording of the same client for at least 9 seconds."}, {"id": "olpbench", "name": "OLPBENCH", "description": "OLPBENCH is a large Open Link Prediction benchmark, which was derived from the state-of-the-art Open Information Extraction corpus OPIEC (Gashteovski et al., 2019). OLPBENCH contains 30M open triples, 1M distinct open relations and 2.5M distinct mentions of approximately 800K entities. "}, {"id": "spartqa-spatial-reasoning-on-textual-question-answering", "name": "SPARTQA (SPAtial Reasoning on Textual Question Answering)", "description": "SpartQA is a textual question answering benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state-of-the-art language models (LM)."}, {"id": "analytic-provenance", "name": "Analytic Provenance", "description": "Analytic provenance is a data repository that can be used to study human analysis activity, thought processes, and software interaction with visual analysis tools during exploratory data analysis. It was collected during a series of user studies involving exploratory data analysis scenario with textual and cyber security data. Interactions logs, think-alouds, videos and all coded data in this study are available online for research purposes. "}, {"id": "pushshift-reddit", "name": "Pushshift Reddit", "description": "Pushshift makes available all the submissions and comments posted on Reddit between June 2005 and April 2019. The dataset consists of 651,778,198 submissions and 5,601,331,385 comments posted on 2,888,885 subreddits."}, {"id": "endoscapes-endoscapes-semantic-segmentation", "name": "Endoscapes (Endoscapes - Semantic Segmentation)", "description": "Cholecystectomy is a very common abdominal surgical procedure almost ubiquitously performed with a laparoscopic approach, hence guided by an endoscopic video. Deep learning models for LC video analysis have been developed with the aim of assisting surgeons during interventions, improving staff awareness and readiness, and facilitating postoperative documentation and research. . However, datasets and models for video semantic segmentation of LC are lacking. Recognizing fine-grained hepatocystic anatomy through semantic segmentation could help surgeons better assess the critical view of safety (CVS), a universally recommended technique consisting in well exposing anatomical landmarks to prevent bile duct injuries. Additionally, segmentation masks of hepatocystic structures could be leveraged by deep learning models for automatic assessment of CVS  and surgical action recognition to improve their performance. We believe that generating a dataset for video semantic segmentation of hepatocystic anatomy will promote surgical data science research and accelerate the development of applications for surgical safety. To generate a representative dataset, consecutive endoscopic videos of LC performed at Nouvel Hopital Civil (Strasbourg, France) were collected.  Non-endoscopic, i.e., out-of-body, video frames were blackedout to comply with European data protection regulations. A frame every 30 seconds was sampled from the portion of the endoscopic video showing the hepatocystic anatomy being dissected, the most critical phase of the surgical procedure, and when surgeons should achieve the CVS. Such unselected and regularly spaced video frames were manually annotated with pixel-wise semantic annotations of anatomical and surgical instances, such as the cystic artery and the dissection. Overall, 1933 regularly spaced video frames from 201 LC videos were annotated with segmentation mask for 29 classes of the hepatocystic triangle, respectively. performed in double by specifically trained computer scientists and surgeons."}, {"id": "recipe1m", "name": "Recipe1M+", "description": "Recipe1M+ is a dataset which contains one million structured cooking recipes with 13M associated images."}, {"id": "fishyscapes", "name": "Fishyscapes", "description": "Fishyscapes is a public benchmark for uncertainty estimation in a real-world task of semantic segmentation for urban driving. It evaluates pixel-wise uncertainty estimates towards the detection of anomalous objects in front of the vehicle."}, {"id": "florence-florence-3d-faces", "name": "Florence (Florence 3D Faces)", "description": "The Florence 3D faces dataset consists of:"}, {"id": "creak", "name": "CREAK", "description": "A testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities with commonsense inferences."}, {"id": "dfw-disguised-faces-in-the-wild", "name": "DFW (Disguised Faces in the Wild)", "description": "Contains over 11000 images of 1000 identities with different types of disguise accessories. The dataset is collected from the Internet, resulting in unconstrained face images similar to real world settings. "}, {"id": "conll", "name": "CoNLL++", "description": "CoNLL++ is a corrected version of the CoNLL03 NER dataset where 5.38% of the test sentences have been fixed."}, {"id": "xtd10", "name": "XTD10", "description": "XTD10 is a dataset for cross-lingual image retrieval and tagging consisting of the MSCOCO2014 caption test dataset annotated in 7 languages that were collected using a crowdsourcing platform."}, {"id": "verse", "name": "Verse", "description": "Verse is a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. "}, {"id": "scan2cad", "name": "Scan2CAD", "description": "Scan2CAD is an alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoints pairs between 14225 (3049 unique) CAD models from ShapeNet and their counterpart objects in the scans. The top 3 annotated model classes are chairs, tables and cabinets which arises due to the nature of indoor scenes in ScanNet. The number of objects aligned per scene ranges from 1 to 40 with an average of 9.3."}, {"id": "completion3d", "name": "Completion3D", "description": "The Completion3D benchmark is a dataset for evaluating state-of-the-art 3D Object Point Cloud Completion methods. Ggiven a partial 3D object point cloud the goal is to infer a complete 3D point cloud for the object."}, {"id": "snow100k", "name": "Snow100K", "description": "The Snow100K dataset consists of "}, {"id": "ntu-rgb-d-120", "name": "NTU RGB+D 120", "description": "NTU RGB+D 120 is a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. "}, {"id": "anim-animals-in-motion", "name": "ANIM (ANimals in Motion)", "description": "It comprises synthetic mesh sequences from Deformation Transfer for Triangle Meshes."}, {"id": "rucos-russian-reading-comprehension-with-commonsense-reasoning", "name": "RuCoS (Russian Reading Comprehension with Commonsense Reasoning)", "description": "Russian reading comprehension with Commonsense reasoning (RuCoS) is a large-scale reading comprehension dataset that requires commonsense reasoning. RuCoS consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of RuCoS is to evaluate a machine`s ability of commonsense reasoning in reading comprehension."}, {"id": "semeval-2013-task-2", "name": "SemEval-2013 Task 2", "description": "The SemEval-2013 Task 2 dataset contains data for two subtasks: A, an expression-level subtask, and B, a message-level subtask. Crowdsourcing was used to label a large Twitter training dataset along with additional test sets of Twitter and SMS messages for both subtasks."}, {"id": "video-storytelling", "name": "Video Storytelling", "description": "A new dataset describing textual stories for events. "}, {"id": "uit-viwikiqa", "name": "UIT-ViWikiQA", "description": "The UIT-ViWikiQA is a dataset for evaluating sentence extraction-based machine reading comprehension in the Vietnamese language. The UIT-ViWikiQA dataset is converted from the UIT-ViQuAD dataset, consisting of 23,074 question-answers based on 5,109 passages of 174 Vietnamese articles from Wikipedia."}, {"id": "egtea-egtea-gaze", "name": "EGTEA (EGTEA Gaze+)", "description": "Extended GTEA Gaze+ EGTEA Gaze+ is a large-scale dataset for FPV actions and gaze. It subsumes GTEA Gaze+ and comes with HD videos (1280x960), audios, gaze tracking data, frame-level action annotations, and pixel-level hand masks at sampled frames. Specifically, EGTEA Gaze+ contains 28 hours (de-identified) of cooking activities from 86 unique sessions of 32 subjects. These videos come with audios and gaze tracking (30Hz). We have further provided human annotations of actions (human-object interactions) and hand masks."}, {"id": "semeval-2018-task-9-hypernym-discovery", "name": "SemEval-2018 Task 9: Hypernym Discovery", "description": "The SemEval-2018 hypernym discovery evaluation benchmark (Camacho-Collados et al. 2018) contains three domains (general, medical and music) and is also available in Italian and Spanish (not in this repository). For each domain a target corpus and vocabulary (i.e. hypernym search space) are provided. The dataset contains both concepts (e.g. dog) and entities (e.g. Manchester United) up to trigrams."}, {"id": "hearthstone", "name": "Hearthstone", "description": "This dataset contains card descriptions of the card game Hearthstone and the code that implements them. These are obtained from the open-source implementation Hearthbreaker (https://github.com/danielyule/hearthbreaker)."}, {"id": "texas-60-20-20-random-splits", "name": "Texas(60%/20%/20% random splits)", "description": "Node classification on Texas with 60%/20%/20% random splits for training/validation/test."}, {"id": "bb-bacteria-biotope", "name": "BB (Bacteria Biotope)", "description": "The Bacteria Biotope (BB) Task is part of the BioNLP Open Shared Tasks and meets the BioNLP-OST standards of quality, originality and data formats. Manually annotated data is provided for training, development and evaluation of information extraction methods. Tools for the detailed evaluation of system outputs are available. Support in performing linguistic processing are provided in the form of analyses created by various state-of-the art tools on the dataset texts."}, {"id": "muses-multi-shot-events", "name": "MUSES (MUlti-Shot EventS)", "description": "MUSES is a large-scale dataset for temporal event (action) localization. It focuses on the temporal localization of multi-shot events, which are captured with multiple shots. Such events often appear in edited videos, such as TV shows and movies. "}, {"id": "dynamic-faust", "name": "Dynamic FAUST", "description": "Dynamic FAUST extends the FAUST dataset to dynamic 4D data. It consists of high-resolution 4D scans of human subjects in motion, captured at 60 fps."}, {"id": "yelp2018", "name": "Yelp2018", "description": "The Yelp2018 dataset is adopted from the 2018 edition of the yelp challenge. Wherein local businesses like restaurants and bars are viewed as items. We use the same 10-core setting in order to ensure data quality."}, {"id": "mot17-multiple-object-tracking-17", "name": "MOT17 (Multiple Object Tracking 17)", "description": "The Multiple Object Tracking 17 (MOT17) dataset is a dataset for multiple object tracking. Similar to its previous version MOT16, this challenge contains seven different indoor and outdoor scenes of public places with pedestrians as the objects of interest. A video for each scene is divided into two clips, one for training and the other for testing. The dataset provides detections of objects in the video frames with three detectors, namely SDP, Faster-RCNN and DPM. The challenge accepts both on-line and off-line tracking approaches, where the latter are allowed to use the future video frames to predict tracks."}, {"id": "meshrir", "name": "MeshRIR", "description": "MeshRIR is a dataset of acoustic room impulse responses (RIRs) at finely meshed grid points. Two subdatasets are currently available: one consists of IRs in a 3D cuboidal region from a single source, and the other consists of IRs in a 2D square region from an array of 32 sources. This dataset is suitable for evaluating sound field analysis and synthesis methods."}, {"id": "siqa-social-interaction-qa", "name": "SIQA (Social Interaction QA)", "description": "Social Interaction QA (SIQA) is a question-answering benchmark for testing social commonsense intelligence. Contrary to many prior benchmarks that focus on physical or taxonomic knowledge, Social IQa focuses on reasoning about people\u2019s actions and their social implications. For example, given an action like \"Jesse saw a concert\" and a question like \"Why did Jesse do this?\", humans can easily infer that Jesse wanted \"to see their favorite performer\" or \"to enjoy the music\", and not \"to see what's happening inside\" or \"to see if it works\". The actions in Social IQa span a wide variety of social situations, and answer candidates contain both human-curated answers and adversarially-filtered machine-generated candidates. Social IQa contains over 37,000 QA pairs for evaluating models\u2019 abilities to reason about the social implications of everyday events and situations."}, {"id": "kg20c-a-scholarly-knowledge-graph-benchmark-dataset", "name": "KG20C (A scholarly knowledge graph benchmark dataset)", "description": "KG20C is a Knowledge Graph about high quality papers from 20 top computer science Conferences. It can serve as a standard benchmark dataset in scholarly data analysis for several tasks, including knowledge graph embedding, link prediction, recommendation systems, and question answering . "}, {"id": "autoweaks", "name": "AutoWeakS", "description": "Collects all the courses from XuetangX5, one of the largest MOOCs in China, and this results in 1951 courses. The collected courses involve seven areas: computer science, economics, engineering, foreign language, math, physics, and social science. Each course contains 131 words in its descriptions on average. Contains 706 job postings from the recruiting website operated by JD.com (JD) and 2,456 job postings from the website owned by Tencent corporation (Tencent). The collected job postings involve six areas: technical post, financial post, product post, design post, market post, supply chain and engineering post. "}, {"id": "syn2real", "name": "Syn2Real", "description": "Syn2Real, a synthetic-to-real visual domain adaptation benchmark meant to encourage further development of robust domain transfer methods. The goal is to train a model on a synthetic \"source\" domain and then update it so that its performance improves on a real \"target\" domain, without using any target annotations. It includes three tasks, illustrated in figures above: the more traditional closed-set classification task with a known set of categories; the less studied open-set classification task with unknown object categories in the target domain; and the object detection task, which involves localizing instances of objects by predicting their bounding boxes and corresponding class labels."}, {"id": "atis-airline-travel-information-systems", "name": "ATIS (Airline Travel Information Systems)", "description": "The ATIS (Airline Travel Information Systems) is a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories. The original split contains 4478, 500 and 893 intent-labeled reference utterances in train, development and test set respectively."}, {"id": "kaggledbqa-kaggledbqa-realistic-text-to-sql-dataset", "name": "KaggleDBQA (KaggleDBQA: Realistic Text-to-SQL dataset)", "description": "KaggleDBQA is a challenging cross-domain and complex evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. "}, {"id": "mathematics-dataset", "name": "Mathematics Dataset", "description": "This dataset code generates mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models."}, {"id": "sara-statutory-reasoning-assessment", "name": "SARA (StAtutory Reasoning Assessment)", "description": "A dataset for statutory reasoning in tax law entailment and question answering."}, {"id": "arxiv-academic-paper-dataset", "name": "Arxiv Academic Paper Dataset", "description": "A dataset to enable automatic academic paper rating."}, {"id": "summarizing-source-code-using-a-neural-attention-model", "name": "Summarizing Source Code using a Neural Attention Model", "description": "Presents a new dataset of code snippets with short descriptions, created using data gathered from Stackoverflow, a popular programming help website. Since access is open and unrestricted, the content is inherently noisy (ungrammatical, non-parsable, lacking content)."}, {"id": "spoken-squad", "name": "Spoken-SQuAD", "description": "In SpokenSQuAD, the document is in spoken form, the input question is in the form of text and the answer to each question is always a span in the document. The following procedures were used to generate spoken documents from the original SQuAD dataset. First, the Google text-to-speech system was used to generate the spoken version of the articles in SQuAD. Then CMU Sphinx was sued to generate the corresponding ASR transcriptions. The SQuAD training set was used to generate the training set of Spoken SQuAD, and SQuAD development set was used to generate the testing set for Spoken SQuAD. If the answer of a question did not exist in the ASR transcriptions of the associated article, the question-answer pair was removed from the dataset because these examples are too difficult for listening comprehension machine at this stage."}, {"id": "ade-affordance", "name": "ADE-Affordance", "description": "ADE-Affordance is a new dataset that builds upon ADE20k, which contains annotations enabling such rich visual reasoning. "}, {"id": "road-scene-graph", "name": "Road Scene Graph", "description": "A special scene-graph for intelligent vehicles. Different to classical data representation, this graph provides not only object proposals but also their pair-wise relationships. By organizing them in a topological graph, these data are explainable, fully-connected, and could be easily processed by GCNs (Graph Convolutional Networks)."}, {"id": "imagenet-x", "name": "ImageNet-X", "description": "ImageNet-X is a set of human annotations pinpointing failure types for the popular ImageNet dataset. ImageNet-X labels distinguishing object factors such as pose, size, color, lighting, occlusions, co-occurences, etc. for each image in the validation set and a random subset of 12,000 training samples. It is designed to study the types of mistakes as a function of model's architecture, learning paradigm, and training procedures."}, {"id": "artificial-signal-data-for-signal-alignment-testing", "name": "Artificial signal data for signal alignment testing", "description": "This is a set of signals-pairs, univariate and multivariate, that can be used to test alignment algorithms. Signals are morphologically different."}, {"id": "daquar", "name": "DAQUAR", "description": "DAQUAR (DAtaset for QUestion Answering on Real-world images) is a dataset of human question answer pairs about images."}, {"id": "emrqa", "name": "emrQA", "description": "emrQA has 1 million question-logical form and 400,000+ questionanswer evidence pairs."}, {"id": "amazon-product-data", "name": "Amazon Product Data", "description": "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014."}, {"id": "facewarehouse", "name": "FaceWarehouse", "description": "FaceWarehouse is a 3D facial expression database that provides the facial geometry of 150 subjects, covering a wide range of ages and ethnic backgrounds."}, {"id": "cjrc-chinese-judicial-reading-comprehension", "name": "CJRC (Chinese judicial reading comprehension)", "description": "The Chinese judicial reading comprehension (CJRC) dataset contains approximately 10K documents and almost 50K questions with answers. The documents come from judgment documents and the questions are annotated by law experts. "}, {"id": "midv-2019", "name": "MIDV-2019", "description": "Contains video clips shot with modern high-resolution mobile cameras, with strong projective distortions and with low lighting conditions."}, {"id": "rendered-wb-dataset", "name": "Rendered WB dataset", "description": "A dataset of over 65,000 pairs of incorrectly white-balanced images and their corresponding correctly white-balanced images."}, {"id": "procedural-human-action-videos", "name": "Procedural Human Action Videos", "description": "Procedural Human Action Videos contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. "}, {"id": "1qisaa-data-collection-1qisaa-data-collection-binarized-images-feature-files-and-plotting-scripts-for-writer-identification-test", "name": "1QIsaa data collection (1QIsaa data collection (binarized images, feature files, and plotting scripts) for writer identification test)", "description": "This data set is collected for the ERC project: The Hands that Wrote the Bible: Digital Palaeography and Scribal Culture of the Dead Sea Scrolls PI: Mladen Popovi\u0107 Grant agreement ID: 640497"}, {"id": "kmir-knowledge-memorization-identification-and-reasoning", "name": "KMIR (Knowledge Memorization, Identification, and Reasoning)", "description": "KMIR (Knowledge Memorization, Identification, and Reasoning) is a benchmark that covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. KMIR can be used for evaluating knowledge memorization, identification and reasoning abilities of language models."}, {"id": "drive-digital-retinal-images-for-vessel-extraction", "name": "DRIVE (Digital Retinal Images for Vessel Extraction)", "description": "The Digital Retinal Images for Vessel Extraction (DRIVE) dataset is a dataset for retinal vessel segmentation. It consists of a total of JPEG 40 color fundus images; including 7 abnormal pathology cases. The images were obtained from a diabetic retinopathy screening program in the Netherlands. The images were acquired using Canon CR5 non-mydriatic 3CCD camera with FOV equals to 45 degrees. Each image resolution is 584*565 pixels with eight bits per color channel (3 channels). "}, {"id": "stereomsi", "name": "StereoMSI", "description": "StereoMSI comprises of 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the PIRM2018 challenge."}, {"id": "comparative-question-completion", "name": "Comparative Question Completion", "description": "Comparative Question Completion is a dataset to evaluate what do large Language Models learn."}, {"id": "shenzhen-hospital-x-ray-set", "name": "Shenzhen Hospital X-ray Set", "description": "X-ray images in this data set have been collected by Shenzhen No.3 Hospital in Shenzhen, Guangdong providence,China. The x-rays were acquired as part of the routine care at Shenzhen Hospital. The set contains images in JPEG format. There are 326 normal x-raysand 336 abnormal x-rays showing various manifestations of tuberculosis."}, {"id": "grit-general-robust-image-task-benchmark", "name": "GRIT (General Robust Image Task Benchmark)", "description": "The General Robust Image Task (GRIT) Benchmark is an evaluation-only benchmark for evaluating the performance and robustness of vision systems across multiple image prediction tasks, concepts, and data sources. GRIT hopes to encourage our research community to pursue the following research directions:"}, {"id": "and-dataset", "name": "AND Dataset", "description": "The AND Dataset contains 13700 handwritten samples and 15 corresponding expert examined features for each sample. The dataset is released for public use and the methods can be extended to provide explanations on other verification tasks like face verification and bio-medical comparison. This dataset can serve as the basis and benchmark for future research in explanation based handwriting verification."}, {"id": "species-800", "name": "Species-800", "description": "Species-800 is a corpus for species entities, which is based on manually annotated abstracts. It comprises 800 PubMed abstracts that contain identified organism mentions. To increase the corpus taxonomic mention diversity the 800 abstracts were collected by selecting 100 abstracts from the following 8 categories: bacteriology, botany, entomology, medicine, mycology, protistology, virology and zoology. 800 has been annotated with a focus at the species level; however, higher taxa mentions (such as genera, families and orders) have also been considered."}, {"id": "4d-light-field-dataset", "name": "4D Light Field Dataset", "description": "4D Light Field Dataset is a light field benchmark consisting of 24 carefully designed synthetic, densely sampled 4D light fields with highly accurate disparity ground truth."}, {"id": "who-covid19-dataset", "name": "WHO-COVID19 Dataset", "description": "COVID19 Data from the World Health Organization"}, {"id": "hindi-visual-genome", "name": "Hindi Visual Genome", "description": "Hindi Visual Genome is a multimodal dataset consisting of text and images suitable for English-Hindi multimodal machine translation task and multimodal research."}, {"id": "covid-19-ct-cxr", "name": "COVID-19-CT-CXR", "description": "A public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. "}, {"id": "munich-sentinel2-crop-segmentation", "name": "Munich Sentinel2 Crop Segmentation", "description": "Contains squared blocks of 48\u00d748 pixels including 13 Sentinel-2 bands.  Each 480-m block was mined from a large geographical area of interest (102 km \u00d7 42 km) located north of Munich, Germany."}, {"id": "stanford-light-field", "name": "Stanford Light Field", "description": "The Stanford Light Field Archive is a collection of several light fields for research in computer graphics and vision."}, {"id": "cmrc-2017-chinese-machine-reading-comprehension-2017", "name": "CMRC 2017 (Chinese Machine Reading Comprehension 2017)", "description": "Contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set."}, {"id": "oasis-open-annotations-of-single-image-surfaces", "name": "OASIS (Open Annotations of Single Image Surfaces)", "description": "A dataset for single-image 3D in the wild consisting of annotations of detailed 3D geometry for 140,000 images."}, {"id": "smacv2", "name": "SMACv2", "description": "SMACv2 (StarCraft Multi-Agent Challenge v2) is a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation."}, {"id": "convosumm", "name": "ConvoSumm", "description": "ConvoSumm is a suite of four datasets to evaluate a model\u2019s performance on a broad spectrum of conversation data."}, {"id": "paraqa", "name": "ParaQA", "description": "ParaQA is a question answering (QA) dataset with multiple paraphrased responses for single-turn conversation over knowledge graphs (KG). The dataset was created using a semi-automated framework for generating diverse paraphrasing of the answers using techniques such as back-translation. The existing datasets for conversational question answering over KGs (single-turn/multi-turn) focus on question paraphrasing and provide only up to one answer verbalization. However, ParaQA contains 5000 question-answer pairs with a minimum of two and a maximum of eight unique paraphrased responses for each question."}, {"id": "situatedqa", "name": "SituatedQA", "description": "SituatedQA is an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. Answers to the same question may change depending on the extralinguistic contexts (when and where the question was asked)."}, {"id": "giantsteps", "name": "Giantsteps", "description": "Giantsteps is a dataset that includes songs in major and minor scales for all pitch classes, i.e., a 24-way classification task."}, {"id": "locata", "name": "LOCATA", "description": "The LOCATA dataset is a dataset for acoustic source localization. It consists of real-world ambisonic speech recordings with optically tracked azimuth-elevation labels."}, {"id": "reflacx-reports-and-eye-tracking-data-for-localization-of-abnormalities-in-chest-x-rays", "name": "REFLACX (Reports and eye-tracking data for localization of abnormalities in chest x-rays)", "description": "The REFLACX dataset contains eye-tracking data for 3,032 readings of chest x-rays by five radiologists. The dictated reports were transcribed and have timestamps synchronized with the eye-tracking data. "}, {"id": "hev-i-honda-egocentric-view-intersection-dataset", "name": "HEV-I (Honda Egocentric View-Intersection Dataset)", "description": "Honda Egocentric View-Intersection Dataset (HEV-I) is introduced to enable research on traffic participants interaction modelling, future object localization, as well as learning driver action in challenging driving scenarios. The dataset includes 230 video clips of real human driving in different intersections from the San Francisco Bay Area, collected using an instrumented vehicle equipped with different sensors including cameras, GPS/IMU, and vehicle states signals."}, {"id": "mpi3d-disentanglement", "name": "MPI3D Disentanglement", "description": "A data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position."}, {"id": "100doh-100-days-of-hands-dataset", "name": "100DOH (100 Days Of Hands Dataset)", "description": "The 100 Days Of Hands Dataset (100DOH) is a large-scale video dataset containing hands and hand-object interactions. It consists of 27.3K Youtube videos from 11 categories with nearly 131 days of footage of everyday interaction. The focus of the dataset is hand contact, and it includes both first-person and third-person perspectives. The videos in 100DOH are unconstrained and content-rich, ranging from records of daily life to specific instructional videos. To enforce diversity, the dataset contains no more than 20 videos from each uploader."}, {"id": "afw-annotated-faces-in-the-wild", "name": "AFW (Annotated Faces in the Wild)", "description": "AFW (Annotated Faces in the Wild) is a face detection dataset that contains 205 images with 468 faces. Each face image is labeled with at most 6 landmarks with visibility labels, as well as a bounding box."}, {"id": "webfg-496", "name": "WebFG-496", "description": "WebFG-496 is a dataset for fine-grained recognition that contains 200 subcategories of the \"Bird\" (Web-bird), 100 subcategories of the Aircraft\" (Web-aircraft), and 196 subcategories of the \"Car\" (Web-car). It has a total number of 53339 web training images."}, {"id": "douban-douban-conversation-corpus", "name": "Douban (Douban Conversation Corpus)", "description": "We release Douban Conversation Corpus, comprising a training data set, a development set and a test set for retrieval based chatbot. The statistics of Douban Conversation Corpus are shown in the following table. "}, {"id": "the-quaero-french-medical-corpus", "name": "The QUAERO French Medical Corpus", "description": "A vast amount of information in the biomedical domain is available as natural language free text. An increasing number of documents in the field are written in languages other than English. Therefore, it is essential to develop resources, methods and tools that address Natural Language Processing in the variety of languages used by the biomedical community. In this paper, we report on the development of an extensive corpus of biomedical documents in French annotated at the entity and concept level. Three text genres are covered, comprising a total of 103,056 words. Ten entity categories corresponding to UMLS Semantic Groups were annotated, using automatic pre-annotations validated by trained human annotators. The pre-annotation method was found helful for entities and achieved above 0.83 precision for all text genres. Overall, a total of 26,409 entity annotations were mapped to 5,797 unique UMLS concepts."}, {"id": "selqa", "name": "SelQA", "description": "SelQA is a dataset that consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. "}, {"id": "pace-2018-steiner-tree", "name": "PACE 2018 Steiner Tree", "description": "This is the set of instances use in the PACE 2018 competition, of optimal Steiner Tree computation. The instances are grouped into three tracks of 200 instances each, except for the third track which is only 199 instances. Each instance is an undirected graph."}, {"id": "zju-mocap-zju-mocap-dataset", "name": "ZJU-MoCap (ZJU-MoCap Dataset)", "description": "LightStage is a multi-view dataset, which is proposed in NeuralBody. This dataset captures multiple dynamic human videos using a multi-camera system that has 20+ synchronized cameras. The humans perform complex motions, including twirling, Taichi, arm swings, warmup, punching, and kicking. We provide the SMPL-X parameters recovered with EasyMocap, which contain the motions of body, hand, and face."}, {"id": "u2os", "name": "U2OS", "description": "The archive contains original images from U2OS cells stained with Hoechst 33342 as PNG files. It also contains images (as Photoshop and GIMP files) showing hand-segmentation of the Hoechst images into regions containing single nuclei."}, {"id": "mmed", "name": "MMED", "description": "Contains 25,165 textual news articles collected from hundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and 76,516 image posts shared on Flickr social media, which are annotated according to 412 real-world events. The dataset is collected to explore the problem of organizing heterogeneous data contributed by professionals and amateurs in different data domains, and the problem of transferring event knowledge obtained from one data domain to heterogeneous data domain, thus summarizing the data with different contributors."}, {"id": "liar", "name": "LIAR", "description": "LIAR is a publicly available dataset for fake news detection. A decade-long of 12.8K manually labeled short statements were collected in various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. The LIAR dataset4 includes 12.8K human labeled short statements from POLITIFACT.COM\u2019s API, and each statement is evaluated by a POLITIFACT.COM editor for its truthfulness. "}, {"id": "reqa-retrieval-question-answering", "name": "ReQA (Retrieval Question-Answering)", "description": "Retrieval Question-Answering (ReQA) benchmark tests a model\u2019s ability to retrieve relevant answers efficiently from a large set of documents."}, {"id": "abt-buy", "name": "Abt-Buy", "description": "The Abt-Buy dataset for entity resolution derives from the online retailers Abt.com and Buy.com. The dataset contains 1081 entities from abt.com and 1092 entities from buy.com as well as a gold standard (perfect mapping) with 1097 matching record pairs between the two data sources.  The common attributes between the two data sources are: product name, product description and product price. "}, {"id": "foursquare", "name": "Foursquare", "description": "The Foursquare dataset consists of check-in data for different cities. One subset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227,428 check-ins in New York city and 573,703 check-ins in Tokyo. Each check-in is associated with its time stamp, its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories). Another subset contains long-term (about 18 months from April 2012 to September 2013) global-scale check-in data collected from Foursquare. It contains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415 cities in 77 countries). Those 415 cities are the most checked 415 cities by Foursquare users in the world, each of which contains at least 10K check-ins."}, {"id": "yelp-fraud-multi-relational-graph-dataset-for-yelp-spam-review-detection", "name": "Yelp-Fraud (Multi-relational Graph Dataset for Yelp Spam Review Detection)", "description": "Yelp-Fraud is a multi-relational graph dataset built upon the Yelp spam review dataset, which can be used in evaluating graph-based node classification, fraud detection, and anomaly detection models."}, {"id": "extended-yale-b", "name": "Extended Yale B", "description": "The Extended Yale B database contains 2414 frontal-face images with size 192\u00d7168 over 38 subjects and about 64 images per subject. The images were captured under different lighting conditions and various facial expressions."}, {"id": "agender", "name": "aGender", "description": "The aGender corpus contains audio recordings of predefined utterances and free speech produced by humans of different age and gender. Each utterance is labeled as one of four age groups: Child, Youth, Adult, Senior, and as one of three gender classes: Female, Male and Child."}, {"id": "udacity", "name": "Udacity", "description": "The Udacity dataset is mainly composed of video frames taken from urban roads. It provides a total number of 404,916 video frames for training and 5,614 video frames for testing. This dataset is challenging due to severe lighting changes, sharp road curves and busy traffic."}, {"id": "instantiation-dataset", "name": "Instantiation Dataset", "description": "Instantiation is a dataset for the task of instantiation detection"}, {"id": "emorynlp", "name": "EmoryNLP", "description": "EmoryNLP comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)\u2019s feeling wheel, sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral."}, {"id": "mario-ai", "name": "Mario AI", "description": "Mario AI was a benchmark environment for reinforcement learning. The gameplay in Mario AI, as in the original Nintendo\u2019s version, consists in moving the controlled character, namely Mario, through two-dimensional levels, which are viewed sideways. Mario can walk and run to the right and left, jump, and (depending on which state he is in) shoot fireballs. Gravity acts on Mario, making it necessary to jump over cliffs to get past them. Mario can be in one of three states: Small, Big (can kill enemies by jumping onto them), and Fire (can shoot fireballs)."}, {"id": "hdm05", "name": "HDM05", "description": "HDM05 is a MoCap (motion capture) dataset. It contains more than three hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. HDM05 contains almost 2337 sequences with 130 motion classes performed by 5 different actors."}, {"id": "mlsum-multilingual-summarization", "name": "MLSUM (MultiLingual SUMmarization)", "description": "A large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community."}, {"id": "raven", "name": "RAVEN", "description": "RAVEN consists of 1,120,000 images and 70,000 RPM (Raven's Progressive Matrices) problems, equally distributed in 7 distinct figure configurations."}, {"id": "brain-score", "name": "Brain-Score", "description": "The Brain-Score platform aims to yield strong computational models of the ventral stream. We enable researchers to quickly get a sense of how their model scores against standardized brain benchmarks on multiple dimensions and facilitate comparisons to other state-of-the-art models. At the same time, new brain data can quickly be tested against a wide range of models to determine how well existing models explain the data."}, {"id": "valnov-subtask-a", "name": "ValNov Subtask A", "description": "Binary labels for Validity and Novelty respectively are given for each Conclusion."}, {"id": "youtube-vis-2022-validation", "name": "Youtube-VIS 2022 Validation", "description": "Video object segmentation has been studied extensively in the past decade due to its importance in understanding video spatial-temporal structures as well as its value in industrial applications. Recently, data-driven algorithms (e.g. deep learning) have become the dominant approach to computer vision problems and one of the most important keys to their successes is the availability of large-scale datasets. Previously, we presented the first large-scale video object segmentation dataset named YouTubeVOS and hosted the Large-scale Video Object Segmentation Challenge in conjuction with ECCV 2018, ICCV 2019 and CVPR 2021. This year, we are thrilled to invite you to the 4th Large-scale Video Object Segmentation Challenge in conjunction with CVPR 2022. The benchmark would be an augmented version of the YouTubeVOS dataset with more annotations. Some incorrect annotations are also corrected. For more details, check our website for the workshop and challenge."}, {"id": "e-snli", "name": "e-SNLI", "description": "e-SNLI is used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets."}, {"id": "malf-multi-attribute-labelled-faces", "name": "MALF (Multi-Attribute Labelled Faces)", "description": "The MALF dataset is a large dataset with 5,250 images annotated with multiple facial attributes and it is specifically constructed for fine grained evaluation."}, {"id": "iharmony4", "name": "iHarmony4", "description": "iHarmony4 is a synthesized dataset for Image Harmonization. It contains 4 sub-datasets: HCOCO, HAdobe5k, HFlickr, and Hday2night (based on COCO, Adobe5k, Flickr, day2night datasets respectively), each of which contains synthesized composite images, foreground masks of composite images and corresponding real images. "}, {"id": "swat-a7-secure-water-treatment-swat", "name": "SWAT A7 (Secure Water Treatment (SWaT))", "description": "11 days of continuous operation: 7 under normal operation and 4 days with attack scenarios: + Collected network traffic & all the values obtained from all the 51 sensors and actuators  + Data labelled according to normal and abnormal behaviours + Attack Scenarios: Derived through the attack models developed by our research team. The attack model considers the intent space of a CPS as an attack model. 41 attacks were launched during the 4 days and are described in the PDF."}, {"id": "nas-bench-101", "name": "NAS-Bench-101", "description": "NAS-Bench-101 is the first public architecture dataset for NAS research. To build NASBench-101, the authors carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. The authors trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the precomputed dataset. "}, {"id": "wire57", "name": "WiRe57", "description": "We manually performed the task of Open Information Extraction on 5 short documents, elaborating tentative guidelines for the task, and resulting in a ground truth reference of 347 tuples. [section 1]"}, {"id": "tasty-videos", "name": "Tasty Videos", "description": "A collection of 2511 recipes for zero-shot learning, recognition and anticipation."}, {"id": "rubq-russian-knowledge-base-questions", "name": "RuBQ (Russian Knowledge Base Questions)", "description": "The first Russian knowledge base question answering (KBQA) dataset. The high-quality dataset consists of 1,500 Russian questions of varying complexity, their English machine translations, SPARQL queries to Wikidata, reference answers, as well as a Wikidata sample of triples containing entities with Russian labels. The dataset creation started with a large collection of question-answer pairs from online quizzes. The data underwent automatic filtering, crowd-assisted entity linking, automatic generation of SPARQL queries, and their subsequent in-house verification."}, {"id": "avid", "name": "AViD", "description": "Is a collection of action videos from many different countries. The motivation is to create a public dataset that would benefit training and pretraining of action recognition models for everybody, rather than making it useful for limited countries."}, {"id": "danbooru2020", "name": "Danbooru2020", "description": "A large-scale anime image database with 4.2m+ images annotated with 130m+ text tags describing image contents in detail; it can be useful for machine learning purposes such as image recognition and generation. It has been applied to a wide variety of applications, particularly generative modeling."}, {"id": "movieqa", "name": "MovieQA", "description": "The MovieQA dataset is a dataset for movie question answering. to evaluate automatic story comprehension from both video and text. The data set consists of almost 15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct. The questions can be answered using multiple sources of information: movie clips, plots, subtitles, and for a subset scripts and DVS."}, {"id": "o-2-perm-oxygen-permeability", "name": "$O_2$Perm (Oxygen Permeability)", "description": "The $O_2$Perm dataset is created from the Membrane Society of Australasia portal. It uses monomers as polymer graphs to predict the property of oxygen permeability. It has he limited size (595 polymers), which brings great challenges to the property prediction."}, {"id": "muco-3dhp", "name": "MuCo-3DHP", "description": "MuCo-3DHP is a large scale training data set showing real images of sophisticated multi-person interactions and occlusions. "}, {"id": "quasar-question-answering-by-search-and-reading", "name": "QUASAR (QUestion Answering by Search And Reading)", "description": "The Question Answering by Search And Reading (QUASAR) is a large-scale dataset consisting of QUASAR-S and QUASAR-T. Each of these datasets is built to focus on evaluating systems devised to understand a natural language query, a large corpus of texts and to extract an answer to the question from the corpus. Specifically, QUASAR-S comprises 37,012 fill-in-the-gaps questions that are collected from the popular website Stack Overflow using entity tags. The QUASAR-T dataset contains 43,012 open-domain questions collected from various internet sources. The candidate documents for each question in this dataset are retrieved from an Apache Lucene based search engine built on top of the ClueWeb09 dataset."}, {"id": "desed-domestic-environment-sound-event-detection", "name": "DESED (Domestic environment sound event detection)", "description": "The DESED dataset is a dataset designed to recognize sound event classes in domestic environments. The dataset is designed to be used for sound event detection (SED, recognize events with their time boundaries) but it can also be used for sound event tagging (SET, indicate presence of an event in an audio file). The dataset is composed of 10 event classes to recognize in 10 second audio files. The classes are: Alarm/bell/ringing, Blender, Cat, Dog, Dishes, Electric shaver/toothbrush, Frying, Running water, Speech, Vacuum cleaner."}, {"id": "casia-webface-masks", "name": "CASIA-WebFace+masks", "description": "The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on Spark AR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90%) samples of masks for the CASIA-WebFace data set."}, {"id": "sdf-shader-dataset-a-dataset-and-explorer-for-3d-signed-distance-functions", "name": "SDF Shader Dataset (A Dataset and Explorer for 3D Signed Distance Functions)", "description": "This dataset contains 63 signed distance function shaders collected mostly from Shadertoy."}, {"id": "covidet-emotions-and-their-triggers-during-covid-19", "name": "CovidET (Emotions and their Triggers during Covid-19)", "description": "Crises such as the COVID-19 pandemic continuously threaten our world and emotionally affect billions of people worldwide in distinct ways. Understanding the triggers leading to people's emotions is of crucial importance. Social media posts can be a good source of such analysis, yet these texts tend to be charged with multiple emotions, with triggers scattering across multiple sentences. This paper takes a novel angle, namely, emotion detection and trigger summarization, aiming to both detect perceived emotions in text, and summarize events that trigger each emotion. To support this goal, we introduce CovidET (Emotions and their Triggers during Covid-19), a dataset of ~1,900 English Reddit posts related to COVID-19, which contains manual annotations of perceived emotions and abstractive summaries of their triggers described in the post. We develop strong baselines to jointly detect emotions and summarize emotion triggers. Our analyses show that CovidET presents new challenges in emotion-specific summarization, as well as multi-emotion detection in long social media posts."}, {"id": "multifc", "name": "MultiFC", "description": "Publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. "}, {"id": "focuspath", "name": "FocusPath", "description": "FocusPath is a dataset compiled from diverse Whole Slide Image (WSI) scans in different focus (z-) levels. Images are naturally blurred by out-of-focus lens provided with GT scores of focus levels. The dataset can be used for No-Reference Focus Quality assessment of Digital Pathology/Microscopy images."}, {"id": "canard-a-dataset-for-question-in-context-rewriting", "name": "CANARD (A Dataset for Question-in-Context Rewriting)", "description": "CANARD is a dataset for question-in-context rewriting that consists of questions each given in a dialog context together with a context-independent rewriting of the question. The context of each question is the dialog utterences that precede the question. CANARD can be used to evaluate question rewriting models that handle important linguistic phenomena such as coreference and ellipsis resolution."}, {"id": "mssd-music-streaming-sessions-dataset", "name": "MSSD (Music Streaming Sessions Dataset)", "description": "The Spotify Music Streaming Sessions Dataset (MSSD) consists of 160 million streaming sessions with associated user interactions, audio features and metadata describing the tracks streamed during the sessions, and snapshots of the playlists listened to during the sessions. "}, {"id": "dagm2007", "name": "DAGM2007", "description": "This is a synthetic dataset for defect detection on textured surfaces. It was originally created for a competition at the 2007 symposium of the DAGM (Deutsche Arbeitsgemeinschaft f\u00fcr Mustererkennung e.V., the German chapter of the International Association for Pattern Recognition). The competition was hosted together with the GNSS (German Chapter of the European Neural Network Society)."}, {"id": "v-coco-verbs-in-coco", "name": "V-COCO (Verbs in COCO)", "description": "Verbs in COCO (V-COCO) is a dataset that builds off COCO for human-object interaction detection. V-COCO provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 person instances. Each person has annotations for 29 action categories and there are no interaction labels including objects."}, {"id": "ls3d-w", "name": "LS3D-W", "description": "A 3D facial landmark dataset of around 230,000 images."}, {"id": "doc2dial-doc2dial-document-grounded-dialogue", "name": "Doc2Dial (Doc2Dial: Document-grounded Dialogue)", "description": "For goal-oriented document-grounded dialogs, it often involves complex contexts for identifying the most relevant information, which requires better understanding of the inter-relations between conversations and documents. Meanwhile, many online user-oriented documents use both semi-structured and unstructured contents for guiding users to access information of different contexts. Thus, we create a new goal-oriented document-grounded dialogue dataset that captures more diverse scenarios derived from various document contents from multiple domains such ssa.gov and studentaid.gov. For data collection, we propose a novel pipeline approach for dialogue data construction, which has been adapted and evaluated for several domains."}, {"id": "changesim", "name": "ChangeSim", "description": "ChangeSim is a dataset aimed at online scene change detection (SCD) and more. The data is collected in photo-realistic simulation environments with the presence of environmental non-targeted variations, such as air turbidity and light condition changes, as well as targeted object changes in industrial indoor environments. By collecting data in simulations, multi-modal sensor data and precise ground truth labels are obtainable such as the RGB image, depth image, semantic segmentation, change segmentation, camera poses, and 3D reconstructions. While the previous online SCD datasets evaluate models given well-aligned image pairs, ChangeSim also provides raw unpaired sequences that present an opportunity to develop an online SCD model in an end-to-end manner, considering both pairing and detection. Experiments show that even the latest pair-based SCD models suffer from the bottleneck of the pairing process, and it gets worse when the environment contains the non-targeted variations."}, {"id": "aurora-2", "name": "Aurora-2", "description": "The Aurora-2 data are based on a version of the original TIDigits (as available from LDC) downsampled at 8 kHz. Different noise signals have been artificially added to clean speech data. The software tool  for filtering and noise adding is available in the download area. You can use the tool for creating distorted data at sampling rates of 8 or 16 kHz. The recognition experiments for Aurora-2 are based on the usage of the HTK recognizer as it is available from Cambridge University. Scripts and configuration files are part of the Aurora-2 CDs as they are distributed by ELRA/ELDA. A published paper is available describing some details of the data creation and the recognition experiments."}, {"id": "ard-16-ati-real-world-dataset", "name": "ARD-16 (Ati Real-world Dataset)", "description": "We create ARD-16 (Ati Realworld Dataset), a first of its kind real-world paired correspondence dataset, by applying our dataset generation method on 16-beam VLP-16 Puck LiDAR scans on a slow-moving Unmanned Ground Vehicle. We obtain ground truth poses by using fine resolution brute force scan matching, similar to Google's Cartographer. It was captured in outdoor environment at Robert Bosch centre, IISc with no moving objects during static run and several moving objects (1 car, 1 2-wheeler, few pedestrians) during dynamic run. It consists of 1.5k scans/run and we collected 10 dynamic and 5 static runs. This gives about 14k LiDAR scan pairs for training, validation and testing."}, {"id": "multibooked", "name": "MultiBooked", "description": "MultiBooked is a dataset for supervised aspect-level sentiment analysis in Basque and Catalan, both of which are under-resourced languages. "}, {"id": "sentence-compression", "name": "Sentence Compression", "description": "Sentence Compression is a dataset where the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence where supervised systems which require a structural alignment between the input and output can be successfully trained. "}, {"id": "auditory-detection-of-sound-ads", "name": "Auditory Detection of Sound (ADS)", "description": "Test dataset for unsupervised anomaly detection in sound (ADS)."}, {"id": "spoc-pseudocode-to-code", "name": "SPoC (Pseudocode-to-Code)", "description": "Pseudocode-to-Code (SPoC) is a program synthesis dataset, containing 18,356 programs with human-authored pseudocode and test cases."}, {"id": "atari-head", "name": "Atari-HEAD", "description": "Atari-HEAD is a dataset of human actions and eye movements recorded while playing Atari videos games. For every game frame, its corresponding image frame, the human keystroke action, the reaction time to make that action, the gaze positions, and immediate reward returned by the environment were recorded. The gaze data was recorded using an EyeLink 1000 eye tracker at 1000Hz. The human subjects are amateur players who are familiar with the games. The human subjects were only allowed to play for 15 minutes and were required to rest for at least 15 minutes before the next trial. Data was collected from 4 subjects, 16 games, 175 15-minute trials, and a total of 2.97 million frames/demonstrations."}, {"id": "keypointnet", "name": "KeypointNet", "description": "KeypointNet is a large-scale and diverse 3D keypoint dataset that contains 83,231 keypoints and 8,329 3D models from 16 object categories, by leveraging numerous human annotations, based on ShapeNet models."}, {"id": "artfid-dataset", "name": "ArtFID Dataset", "description": "The ArtFID dataset contains around 250k labeled artworks."}, {"id": "human3-6m", "name": "Human3.6M", "description": "The Human3.6M dataset is one of the largest motion capture datasets, which consists of 3.6 million human poses and corresponding images captured by a high-speed motion capture system. There are 4 high-resolution progressive scan cameras to acquire video data at 50 Hz. The dataset contains activities by 11 professional actors in 17 scenarios: discussion, smoking, taking photo, talking on the phone, etc., as well as provides accurate 3D joint positions and high-resolution videos."}, {"id": "brats-2017", "name": "BraTS 2017", "description": "The BRATS2017 dataset. It contains 285 brain tumor MRI scans, with four MRI modalities as T1, T1ce, T2, and Flair for each scan. The dataset also provides full masks for brain tumors, with labels for ED, ET, NET/NCR. The segmentation evaluation is based on three tasks: WT, TC and ET segmentation."}, {"id": "horse-10", "name": "Horse-10", "description": "Horse-10 is an animal pose estimation dataset. It comprises 30 diverse Thoroughbred horses, for which 22 body parts were labeled by an expert in 8,114 frames (animal pose estimation). Horses have various coat colors and the \u201cin-the-wild\u201d aspect of the collected data at various Thoroughbred yearling sales and farms added additional complexity.  The authors introduce Horse-C to contrast the domain shift inherent in the Horse-10 dataset with domain shift induced by common image corruptions."}, {"id": "accentdb", "name": "AccentDB", "description": "AccentDB is a database that contains samples of 4 Indian-English accents, and a compilation of samples from 4 native-English, and a metropolitan Indian-English accent."}, {"id": "samm-long-videos", "name": "SAMM Long Videos", "description": "The SAMM Long Videos dataset consists of 147 long videos with 343 macro-expressions and 159 micro-expressions. The dataset is FACS-coded with detailed Action Units."}, {"id": "a-large-scale-fish-dataset-a-large-scale-dataset-for-fish-segmentation-and-classification", "name": "A Large Scale Fish Dataset (A Large-Scale Dataset for Fish Segmentation and Classification)", "description": "This dataset contains 9 different seafood types collected from a supermarket in Izmir, Turkey for a university-industry collaboration project at Izmir University of Economics, and this work was published in ASYU 2020. The dataset includes gilt head bream, red sea bream, sea bass, red mullet, horse mackerel, black sea sprat, striped red mullet, trout, shrimp image samples."}, {"id": "reddit", "name": "Reddit", "description": "The Reddit dataset is a graph dataset from Reddit posts made in the month of September, 2014. The node label in this case is the community, or \u201csubreddit\u201d, that a post belongs to. 50 large communities have been sampled to build a post-to-post graph, connecting posts if the same user comments on both. In total this dataset contains 232,965 posts with an average degree of 492. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). For features, off-the-shelf 300-dimensional GloVe CommonCrawl word vectors are used."}, {"id": "semeval-2014-task-4-sub-task-2", "name": "SemEval 2014 Task 4 Sub Task 2", "description": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, regardless of the entities mentioned (e.g., laptops, restaurants) and their aspects (e.g., battery, screen; food, service). By contrast, this task is concerned with aspect based sentiment analysis (ABSA), where the goal is to identify the aspects of given target entities and the sentiment expressed towards each aspect. Datasets consisting of customer reviews with human-authored annotations identifying the mentioned aspects of the target entities and the sentiment polarity of each aspect will be provided."}, {"id": "large-scale-anomaly-detection", "name": "Large-scale Anomaly Detection", "description": "Large-scale Anomaly Detection (LAD) is a database to benchmark anomaly detection in video sequences, which is featured in two aspects. 1) It contains 2000 video sequences including normal and abnormal video clips with 14 anomaly categories including crash, fire, violence, etc. with large scene varieties, making it the largest anomaly analysis database to date. 2) It provides the annotation data, including video-level labels (abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal video frame) to facilitate anomaly detection."}, {"id": "the-peoples-speech", "name": "The People\u2019s Speech", "description": "The People's Speech is a free-to-download 30,000-hour and growing supervised conversational English speech recognition dataset licensed for academic and commercial usage under CC-BY-SA (with a CC-BY subset). The data is collected via searching the Internet for appropriately licensed audio data with existing transcriptions."}, {"id": "h-dibco-2016", "name": "H-DIBCO 2016", "description": "H-DIBCO 2016 is the international Handwritten Document Image Binarization Contest organized in the context of ICFHR 2016 conference"}, {"id": "mpv-multi-pose-virtual-try-on", "name": "MPV (Multi-Pose Virtual try on)", "description": "Consists of 37,723/14,360 person/clothes images, with the resolution of 256x192. Each person has different poses. We split them into the train/test set 52,236/10,544 three-tuples, respectively. You can download the dataset at MPV(Google Drive)"}, {"id": "42street", "name": "42Street", "description": "The 42Street dataset is based on a theater play as an example of such an application. The dataset is created using a public recording of the 42Street theatre play [42street]. The play is 1.5 hours long and was split into 5 equally long parts of 20 minutes each, with various clothes changes between the different parts."}, {"id": "a-curb-dataset", "name": "A Curb Dataset", "description": "This is a dataset with curb annotations by using 3D LiDAR data and we build this dataset based on the SemanticKITTI dataset."}, {"id": "duolingo-slam-shared-task", "name": "Duolingo SLAM Shared Task", "description": "This repository contains gzipped files containing more than 2 million tokens (words) from answers submitted by more than 6,000 students over the course of their first 30 days of using Duolingo. It also contains baseline starter code written in Python. There are three data sets, corresponding to three different language courses. More details on the data set and task are available at: http://sharedtask.duolingo.com. (2018-01-10)"}, {"id": "squad-shifts", "name": "SQuAD-shifts", "description": "Provides four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. "}, {"id": "cc-stories", "name": "CC-Stories", "description": "CC-Stories (or STORIES) is a dataset for common sense reasoning and language modeling. It was constructed by aggregating documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions in commonsense reasoning tasks. The top 1.0% of highest ranked documents is chosen as the new training corpus."}, {"id": "chinese-gigaword", "name": "Chinese Gigaword", "description": "Chinese Gigaword corpus consists of 2.2M of headline-document pairs of news stories covering over 284 months from two Chinese newspapers, namely the Xinhua News Agency of China (XIN) and the Central News Agency of Taiwan (CNA)."}, {"id": "ronec-romanian-named-entity-corpus", "name": "RONEC (Romanian Named Entity Corpus)", "description": "Romanian Named Entity Corpus is a named entity corpus for the Romanian language. The corpus contains over 26000 entities in ~5000 annotated sentences, belonging to 16 distinct classes. The sentences have been extracted from a copy-right free newspaper, covering several styles. This corpus represents the first initiative in the Romanian language space specifically targeted for named entity recognition. "}, {"id": "tiage", "name": "TIAGE", "description": "TIAGE is a topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on TIAGE, three tasks can be conducted to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift triggered response generation and topic-aware dialog generation."}, {"id": "oqmd-v1-2-the-open-quantum-materials-database", "name": "OQMD v1.2 (The Open Quantum Materials Database)", "description": "The OQMD is a database of DFT calculated thermodynamic and structural properties of one million materials, created in Chris Wolverton's group at Northwestern University."}, {"id": "instructional-dt-instr-dt-instructional-discourse-treebank", "name": "Instructional-DT (Instr-DT) (Instructional Discourse Treebank)", "description": "This discourse treebank includes annotated instructional texts originally assembled at the Information Technology Research Institute, University of Brighton. This dataset contains 176 documents with an average of 32.6 EDUs for a total of 5744 EDUs and 53,250 words."}, {"id": "bio-bio-amr-corpus", "name": "Bio (Bio AMR Corpus)", "description": "This corpus includes annotations of cancer-related PubMed articles, covering 3 full papers (PMID:24651010, PMID:11777939, PMID:15630473) as well as the result sections of 46 additional PubMed papers. The corpus also includes about 1000 sentences each from the BEL BioCreative training corpus and the Chicago Corpus."}, {"id": "mose-complex-video-object-segmentation", "name": "MOSE (Complex Video Object Segmentation)", "description": "CoMplex video Object SEgmentation (MOSE) is a dataset to study the tracking and segmenting objects in complex environments. MOSE contains 2,149 video clips and 5,200 objects from 36 categories, with 431,725 high-quality object segmentation masks. The most notable feature of MOSE dataset is complex scenes with crowded and occluded objects."}, {"id": "kinetics-600", "name": "Kinetics-600", "description": "The Kinetics-600 is a large-scale action recognition dataset which consists of around 480K videos from 600 action categories. The 480K videos are divided into 390K, 30K, 60K for training, validation and test sets, respectively. Each video in the dataset is a 10-second clip of action moment annotated from raw YouTube video. It is an extensions of the Kinetics-400 dataset."}, {"id": "youtubean", "name": "Youtubean", "description": "Youtbean is a dataset created from closed captions of YouTube product review videos. It can be used for aspect extraction and sentiment classification."}, {"id": "lsp-leeds-sports-pose", "name": "LSP (Leeds Sports Pose)", "description": "The Leeds Sports Pose (LSP) dataset is widely used as the benchmark for human pose estimation. The original LSP dataset contains 2,000 images of sportspersons gathered from Flickr, 1000 for training and 1000 for testing. Each image is annotated with 14 joint locations, where left and right joints are consistently labelled from a person-centric viewpoint. The extended LSP dataset contains additional 10,000 images labeled for training."}, {"id": "lsvtd", "name": "LSVTD", "description": "LSVTD is a large scale video text dataset for promoting the video text spotting community, which contains 100 text videos from 22 different real-life scenarios. LSVTD covers a wide range of 13 indoor (eg. bookstore, shopping mall) and 9 outdoor scenarios, which is more than 3 times the diversity of IC15."}, {"id": "google-landmarks-dataset-v2", "name": "Google Landmarks Dataset v2", "description": "This is the second version of the Google Landmarks dataset (GLDv2), which contains images annotated with labels representing human-made and natural landmarks. The dataset can be used for landmark recognition and retrieval experiments. This version of the dataset contains approximately 5 million images, split into 3 sets of images: train, index and test"}, {"id": "vistas-np", "name": "Vistas-NP", "description": "The Vistas-NP dataset is an out-of-distribution detection dataset based on the Mapillary Vistas dataset. The original Vistas dataset consists of 18,000 training images and 2,000 validation images with 66 classes. In Vistas-NP the human classes are used as outliers due to their dispersion across scenes and visual diversity from other objects. The dataset is created by excluding all images with class person and the three rider classes to the test subset. Consequently, the dataset has 8,003 train images and 830 validation images. The test set contains 11,167."}, {"id": "volleyball", "name": "Volleyball", "description": "Volleyball is a video action recognition dataset. It has 4830 annotated frames that were handpicked from 55 videos with 9 player action labels and 8 team activity labels. It contains group activity annotations as well as individual activity annotations."}, {"id": "x4k1000fps", "name": "X4K1000FPS", "description": "Dataset of high-resolution (4096\u00d72160), high-fps (1000fps) video frames with extreme motion. X-TEST consists of 15 video clips with 33-length of 4K-1000fps frames.  X-TRAIN consists of 4,408 clips from various types of 110 scenes. The clips are 65-length of 1000fps frames"}, {"id": "geos", "name": "GeoS", "description": "GeoS is a dataset for automatic math problem solving. It is a dataset of SAT plane geometry questions where every question has a textual description in English accompanied by a diagram and multiple choices. Questions and answers are compiled from previous official SAT exams and practice exams offered by the College Board. We annotate ground-truth logical forms for all questions in the dataset."}, {"id": "multidimensional-texture-perception-multidimensional-textural-perception-and-classification-through-whisker", "name": "Multidimensional Texture Perception (Multidimensional Textural Perception and Classification Through Whisker)", "description": "Texture-based studies and designs have been in focus recently. Whisker-based multidimensional surface texture data is missing in the literature. This data is critical for robotics and machine perception algorithms in the classification and regression of textural surfaces. We present a novel sensor design to acquire multidimensional texture information. The surface texture's roughness and hardness were measured experimentally using sweeping and dabbing. The data is made available to the research community for further advancing texture perception studies."}, {"id": "expw-expression-in-the-wild", "name": "ExpW (Expression in-the-Wild)", "description": "The Expression in-the-Wild (ExpW) dataset is for facial expression recognition and contains 91,793 faces manually labeled with expressions. Each of the face images is annotated as one of the seven basic expression categories: \u201cangry\u201d, \u201cdisgust\u201d, \u201cfear\u201d, \u201chappy\u201d, \u201csad\u201d, \u201csurprise\u201d, or \u201cneutral\u201d."}, {"id": "vpcd-video-person-clustering", "name": "VPCD (Video Person-Clustering)", "description": "VPCD contains multi-modal annotations (face, body and voice) for all primary and secondary characters from a range of diverse TV-shows and movies. It is used for evaluating multi-modal person-clustering. It contains body-tracks for each annotated character, face-tracks when visible, and voice-tracks when speaking, with their associated features."}, {"id": "in-shop-in-shop-clothes-retrieval-benchmark", "name": "In-Shop (In-shop Clothes Retrieval Benchmark)", "description": "In-shop Clothes Retrieval Benchmark evaluates the performance of in-shop Clothes Retrieval. This is a large subset of DeepFashion, containing large pose and scale variations. It also has large diversities, large quantities, and rich annotations, including:"}, {"id": "blackbird", "name": "Blackbird", "description": "The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale, aggressive indoor flight dataset collected using a custom-built quadrotor platform for use in evaluation of agile perception. The Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images for each flight were photorealistically rendered using FlightGoggles across a variety of environments to facilitate easy experimentation of high performance perception algorithms. "}, {"id": "live-vqc-live-video-quality-challenge-vqc-database", "name": "LIVE-VQC (LIVE Video Quality Challenge (VQC) Database)", "description": "The great variations of videographic skills in videography, camera designs, compression and processing protocols, communication and bandwidth environments, and displays leads to an enormous variety of video impairments. Current no-reference (NR) video quality models are unable to handle this diversity of distortions. This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions. As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, complex and often commingled distortions that are difficult or impossible to simulate. As a result, NR video quality predictors tested on real-world video data often perform poorly. Towards advancing NR video quality prediction, we have constructed a large-scale video quality assessment database containing 585 videos of unique content , captured using 101 different devices (43 device models) by 80 different users with wide ranges of levels of complex, authentic distortions. We collected a large number of subjective video quality scores via crowdsourcing. A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores , resulting in an average of 240 recorded human opinions per video . This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores."}, {"id": "live-etri-etri-live-space-time-subsampled-video-quality-stsvq-database", "name": "LIVE-ETRI (ETRI-LIVE Space-Time Subsampled Video Quality (STSVQ) Database)", "description": "The video deployed parameter space is continuously increasing to provide more realistic and immersive experiences to global streaming and social media viewers. However, increments in video parameters such as spatial resolution or frame rate are inevitably associated with larger data volumes. Transmitting increasingly voluminous videos through limited bandwidth networks in a perceptually optimal way is a present challenge affecting billions of viewers. One recent practice adopted by the video service providers is space-time resolution adaptation in conjunction with video compression. Consequently, it is important to understand how different levels of space-time subsampling and compression affect the perceptual quality of videos. Towards making progress in this direction, we constructed a large new resource, called the ETRI-LIVE Space-Time Subsampled Video Quality (ETRI-LIVE-STSVQ) database, containing 437 videos generated by applying various levels of combined space-time subsampling and video compression on 15 diverse video contents. We also conducted a large-scale human study on the new dataset, collecting about 15,000 subjective judgments of video quality. The ETRI-LIVE STSVQ database is being made publicly and freely available with the desire to improve future research and development on topics such as video quality modeling and perceptual video coding."}, {"id": "italki-nli", "name": "italki NLI", "description": "A large, crowd-sourced dataset for the Native Language Identification (NLI) task. People learning English as a second language write practice Notebooks which can be used to classify the author's native language using word choice, spelling mistakes and other language features."}, {"id": "xqlfw-cross-quality-labeled-faces-in-the-wild", "name": "XQLFW (Cross-Quality Labeled Faces in the Wild)", "description": "An evaluation protocol for face verification focusing on a large intra-pair image quality difference."}, {"id": "aachen-day-night-v1-1-benchmark", "name": "Aachen Day-Night v1.1 Benchmark", "description": "Aachen Day-Night v1.1 dataset is an extended version of the original Aachen Day-Night dataset. Besides the original query images, the Aachen Day-Night v1.1 dataset contains an additional 93 nighttime queries. In addition, it uses a larger 3D model containing additional images. These additional images were extracted from video sequences captured with different  cameras. Please refer to Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis for more information."}, {"id": "ait-qa-airline-industry-table-qa", "name": "AIT-QA (Airline Industry Table QA)", "description": "AIT-QA is a dataset for Table Question Answering (Table-QA) which is specific to the airline industry. The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings of major airline companies for the fiscal years 2017-2019. It also contains annotations pertaining to the nature of questions, marking those that require hierarchical headers, domain-specific terminology, and paraphrased forms."}, {"id": "multimodal-opinionlevel-sentiment-intensity-mosi", "name": "Multimodal Opinionlevel Sentiment Intensity (MOSI)", "description": "Multimodal Opinionlevel Sentiment Intensity (MOSI) contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2) opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features."}, {"id": "rimes-reconnaissance-indexation-de-donnees-manuscrites-et-de-fac-similes-recognition-indexing-of-handwritten-documents-faxes", "name": "RIMES (Reconnaissance & Indexation de donn\u00e9es Manuscrites et de fac simil\u00c9S / Recognition & Indexing of handwritten documents & faxes)", "description": "The RIMES database (Reconnaissance et Indexation de donn\u00e9es Manuscrites et de fac simil\u00c9S / Recognition and Indexing of handwritten documents and faxes) was created to evaluate automatic systems of recognition and indexing of handwritten letters. Of particular interest are cases such as those sent by postal mail or fax by individuals to companies or administrations."}, {"id": "musdb18", "name": "MUSDB18", "description": "The MUSDB18 is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems."}, {"id": "frenchmedmcqa-frenchmedmcqa-a-french-multiple-choice-question-answering-dataset-for-medical-domain", "name": "FrenchMedMCQA (FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain)", "description": "This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online."}, {"id": "chad-charlotte-anomaly-dataset", "name": "CHAD (Charlotte Anomaly Dataset)", "description": "CHAD is high-resolution, multi-camera dataset for surveillance video anomaly detection. It includes bounding box, Re-ID, and pose annotations, as well as frame-level anomaly labels, dividing all frames into two groups of anomalous or normal. You can find the paper with all the details in the following link: CHAD: Charlotte Anomaly Dataset. Please refer to the page of the dataset for more information."}, {"id": "acticipate", "name": "Acticipate", "description": "Acticipate is a publicly available dataset with recordings of human body-motion and eye-gaze, acquired in an experimental scenario with an actor interacting with three subjects. It contains synchronised and labelled video+gaze and body motion in a dyadic scenario of interaction."}, {"id": "ssig-segplate", "name": "SSIG-SegPlate", "description": "This dataset aims at evaluating the License Plate Character Segmentation (LPCS) problem. The experimental results of the paper Benchmark for License Plate Character Segmentation were obtained using a dataset providing 101 on-track vehicles captured during the day. The video was recorded using a static camera in early 2015."}, {"id": "babel", "name": "BABEL", "description": "BABEL is a large dataset with language labels describing the actions being performed in mocap sequences. BABEL consists of action labels for about 43 hours of mocap sequences from AMASS. Action labels are at two levels of abstraction --  sequence labels describe the overall action in the sequence, and frame labels describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. There are over 28k sequence labels, and 63k frame labels in BABEL, which belong to over 250 unique action categories. Labels from BABEL can be leveraged for tasks like action recognition, temporal action localization, motion synthesis, etc."}, {"id": "deepcom-java", "name": "DeepCom-Java", "description": "The Java dataset introduced in DeepCom (Deep Code Comment Generation), commonly used to evaluate automated code summarization."}, {"id": "astd-arabic-sentiment-tweets-dataset", "name": "ASTD (Arabic Sentiment Tweets Dataset)", "description": "Arabic Sentiment Tweets Dataset (ASTD) is an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, subjective negative, and subjective mixed."}, {"id": "moving-mnist", "name": "Moving MNIST", "description": "The Moving MNIST dataset contains 10,000 video sequences, each consisting of 20 frames. In each video sequence, two digits move independently around the frame, which has a spatial resolution of 64\u00d764 pixels. The digits frequently intersect with each other and bounce off the edges of the frame"}, {"id": "goldfinch-google-image-search-dataset", "name": "Goldfinch (GOogLe image-search Dataset)", "description": "Goldfinch is a dataset for fine-grained recognition challenges. It contains a list of bird, butterfly, aircraft, and dog categories with relevant Google image search and Flickr search URLs. In addition, it also includes a set of active learning annotations on dog categories."}, {"id": "miccai-2015-multi-atlas-abdomen-labeling-challenge", "name": "MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge", "description": "Under Institutional Review Board (IRB) supervision, 50 abdomen CT scans of were randomly selected from a combination of an ongoing colorectal cancer chemotherapy trial, and a retrospective ventral hernia study. The 50 scans were captured during portal venous contrast phase with variable volume sizes (512 x 512 x 85 - 512 x 512 x 198) and field of views (approx. 280 x 280 x 280 mm3 - 500 x 500 x 650 mm3). The in-plane resolution varies from 0.54 x 0.54 mm2 to 0.98 x 0.98 mm2, while the slice thickness ranges from 2.5 mm to 5.0 mm. The standard registration data was generated by NiftyReg."}, {"id": "lasot-large-scale-single-object-tracking", "name": "LaSOT (Large-scale Single Object Tracking)", "description": "LaSOT is a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT one of the largest densely annotated tracking benchmark. The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenges deriving from the wild where target objects may disappear and re-appear again in the view."}, {"id": "proteinkg25", "name": "ProteinKG25", "description": "ProteinKG25 is a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and proteins entities. ProteinKG25 contains 4,990,097 triplets (4,879,951 Protein-GO triplets and 110,146 GO-GO triplets), 612,483 entities (565,254 proteins and 47,229 GO terms) and 31 relations."}, {"id": "kuzushiji-kanji", "name": "Kuzushiji-Kanji", "description": "Kuzushiji-Kanji is an imbalanced dataset of total 3832 Kanji characters (64x64 grayscale, 140,426 images), ranging from 1,766 examples to only a single example per class. Kuzushiji is a Japanese cursive writing style."}, {"id": "wizard-of-oz", "name": "Wizard-of-Oz", "description": "The WoZ 2.0 dataset is a newer dialogue state tracking dataset whose evaluation is detached from the noisy output of speech recognition systems. Similar to DSTC2, it covers the restaurant search domain and has identical evaluation."}, {"id": "newsclippings", "name": "NewsCLIPpings", "description": "NewsCLIPpings is a dataset for detecting mismatched images and captions. Different to previous misinformation datasets, in NewsCLIPpings both the images and captions are unmanipulated, but some of them are mismatched."}, {"id": "prcc", "name": "PRCC", "description": "This dataset consists of 33698 images from 221 identities. Each person in Cameras A and B is wearing the same clothes, but the images are captured in different rooms. For Camera C, the person wears different clothes, and the images are captured in a different day."}, {"id": "360-em", "name": "360 EM", "description": "Data set of 360-degree equirectangular videos, gaze recordings, eye movement (EM) ground-truth and an automatic EM classification algorithm."}, {"id": "videocc3m-video-conceptual-captions", "name": "VideoCC3M (Video-Conceptual-Captions)", "description": "We propose a new, scalable video-mining pipeline which transfers captioning supervision from image datasets to video and audio. We use this pipeline to mine paired video and captions, using the Conceptual Captions3M image dataset as a seed dataset. Our resulting dataset VideoCC3M consists of millions of weakly paired clips with text captions and will be released publicly."}, {"id": "paracrawl", "name": "ParaCrawl", "description": "ParaCrawl v.7.1 is a parallel dataset with 41 language pairs primarily aligned with English (39 out of 41) and mined using the parallel-data-crawling tool Bitextor which includes downloading documents, preprocessing and normalization, aligning documents and segments, and filtering noisy data via Bicleaner. ParaCrawl focuses on European languages, but also includes 9 lower-resource, non-European language pairs in v7.1."}, {"id": "jrdb-act", "name": "JRDB-Act", "description": "JRDB-Act is an extension of the JRDB dataset to create a large-scale multi-modal dataset for spatio-temporal action, social group and activity detection. "}, {"id": "beijing-multi-site-air-quality-dataset", "name": "Beijing Multi-Site Air-Quality Dataset", "description": "This data set includes hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration. The time period is from March 1st, 2013 to February 28th, 2017. Missing data are denoted as NA."}, {"id": "totto", "name": "ToTTo", "description": "ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description."}, {"id": "starcraft-ii-learning-environment", "name": "StarCraft II Learning Environment", "description": "The StarCraft II Learning Environment (S2LE) is a reinforcement learning environment based on the game StarCraft II. The environment consists of three sub-components: a Linux StarCraft II binary, the StarCraft II API and PySC2. The StarCraft II API allows programmatic control of StarCraft II. It can be used to start a game, get observations, take actions, and review replays. PyC2 is a Python environment that wraps the StarCraft II API to ease the interaction between Python reinforcement learning agents and StarCraft II. It defines an action and observation specification, and includes a random agent and a handful of rule-based agents as examples. It also includes some mini-games as challenges and visualization tools to understand what the agent can see and do."}, {"id": "mmdb-multimodal-dyadic-behavior", "name": "MMDB (Multimodal Dyadic Behavior)", "description": "Multimodal Dyadic Behavior (MMDB) dataset is a unique collection of multimodal (video, audio, and physiological) recordings of the social and communicative behavior of toddlers. The MMDB contains 160 sessions of 3-5 minute semi-structured play interaction between a trained adult examiner and a child between the age of 15 and 30 months. The MMDB dataset supports a novel problem domain for activity recognition, which consists of the decoding of dyadic social interactions between adults and children in a developmental context."}, {"id": "a-billion-ways-to-grasp", "name": "A Billion Ways to Grasp", "description": "Robot grasping is often formulated as a learning problem. With the increasing speed and quality of physics simulations, generating large-scale grasping data sets that feed learning algorithms is becoming more and more popular. An often overlooked question is how to generate the grasps that make up these data sets. In this paper, we review, classify, and compare different grasp sampling strategies. Our evaluation is based on a fine-grained discretization of SE(3) and uses physics-based simulation to evaluate the quality and robustness of the corresponding parallel-jaw grasps. Specifically, we consider more than 1 billion grasps for each of the 21 objects from the YCB data set. This dense data set lets us evaluate existing sampling schemes w.r.t. their bias and efficiency. Our experiments show that some popular sampling schemes contain significant bias and do not cover all possible ways an object can be grasped."}, {"id": "201-people-infant-cry-speech-data-by-mobile-phone", "name": "201 People \u2013 Infant Cry Speech Data by Mobile Phone", "description": "Description: Crying sound of 201 infants and young children aged 0~3 years old, a number of paragraphs from each of them; It provides data support for detecting children's crying sound in smart home projects."}, {"id": "maniskill2", "name": "ManiSkill2", "description": "ManiSkill2 is the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. It includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D input data simulated by fully dynamic engines."}, {"id": "ntu-rgb-d-2d", "name": "NTU RGB+D 2D", "description": "NTU RGB+D 2D is a curated version of NTU RGB+D often used for skeleton-based action prediction and synthesis. It contains less number of actions."}, {"id": "kazakhtts", "name": "KazakhTTS", "description": "KazakhTTS is an open-source speech synthesis dataset for Kazakh, a low-resource language spoken by over 13 million people worldwide. The dataset consists of about 91 hours of transcribed audio recordings spoken by two professional speakers (female and male). It is the first publicly available large-scale dataset developed to promote Kazakh text-to-speech (TTS) applications in both academia and industry."}, {"id": "allmusic-mood-subset", "name": "AllMusic Mood Subset", "description": "The AllMusic Mood Subset (AMS) is a dataset for mood classification from songs. It is created by matching a subset of the Million Song Dataset (MSD), totalling 67k tracks, with expert annotations of 188 different moods collected from AllMusic."}, {"id": "sidd-smartphone-image-denoising-dataset", "name": "SIDD (Smartphone Image Denoising Dataset)", "description": "SIDD is an image denoising dataset containing 30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras. Ground truth images are provided along with the noisy images."}, {"id": "11k-hands", "name": "11k Hands", "description": "A large dataset of human hand images (dorsal and palmar sides) with detailed ground-truth information for gender recognition and biometric identification."}, {"id": "orkut", "name": "Orkut", "description": "Orkut is a social network dataset consisting of friendship social network and ground-truth communities from Orkut.com on-line social network where users form friendship each other."}, {"id": "trackml-challenge-accuracy-phase-dataset-tracking-machine-learning-challenge", "name": "TrackML challenge Accuracy phase dataset (Tracking Machine Learning Challenge)", "description": "The dataset comprises multiple independent events, where each event contains simulated measurements (essentially 3D points) of particles generated in a collision between proton bunches at the Large Hadron Collider at CERN. The goal of the tracking machine learning challenge is to group the recorded measurements or hit for each event into tracks, sets of hits that belong to the same initial particle. A solution must uniquely associate each hit to one track. The training dataset contains the recorded hit, their ground truth counterpart and their association to particles, and the initial parameters of those particles. The test dataset contains only the recorded hits."}, {"id": "mmlu-massive-multitask-language-understanding", "name": "MMLU (Massive Multitask Language Understanding)", "description": "MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This makes the benchmark more challenging and more similar to how we evaluate humans. The benchmark covers 57 subjects across STEM, the humanities, the social sciences, and more. It ranges in difficulty from an elementary level to an advanced professional level, and it tests both world knowledge and problem solving ability. Subjects range from traditional areas, such as mathematics and history, to more specialized areas like law and ethics. The granularity and breadth of the subjects makes the benchmark ideal for identifying a model\u2019s blind spots."}, {"id": "roll4real", "name": "Roll4Real", "description": "Consists of real objects rolling on complex terrains (pool table, elliptical bowl, and random height-field). "}, {"id": "mr-mr-movie-reviews", "name": "MR (MR Movie Reviews)", "description": "MR Movie Reviews is a dataset for use in sentiment-analysis experiments. Available are collections of movie-review documents labeled with respect to their overall sentiment polarity (positive or negative) or subjective rating (e.g., \"two and a half stars\") and sentences labeled with respect to their subjectivity status (subjective or objective) or polarity."}, {"id": "coco-10-labeled-data", "name": "COCO 10% labeled data", "description": "Semi-Supervised Object Detection on COCO 10% labeled data"}, {"id": "md17-molecular-dynamics-17", "name": "MD17 (Molecular Dynamics 17)", "description": "Energies and forces for molecular dynamics trajectories of eight organic molecules."}, {"id": "3massiv", "name": "3MASSIV", "description": "A multilingual, multimodal and multi-aspect, expertly-annotated dataset of diverse short videos extracted from short-video social media platform - Moj. 3MASSIV comprises of 50k short videos (~20 seconds average duration) and 100K unlabeled videos in 11 different languages and captures popular short video trends like pranks, fails, romance, comedy expressed via unique audio-visual formats like self-shot videos, reaction videos, lip-synching, self-sung songs, etc."}, {"id": "openbookqa", "name": "OpenBookQA", "description": "OpenBookQA is a new kind of question-answering dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small \u201cbook\u201d of 1,326 core science facts and the application of these facts to novel situations. For training, the dataset includes a mapping from each question to the core science fact it was designed to probe. Answering OpenBookQA questions requires additional broad common knowledge, not contained in the book. The questions, by design, are answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm. Additionally, the dataset includes a collection of 5,167 crowd-sourced common knowledge facts, and an expanded version of the train/dev/test questions where each question is associated with its originating core fact, a human accuracy score, a clarity score, and an anonymized crowd-worker ID."}, {"id": "lpw-labeled-pedestrian-in-the-wild", "name": "LPW (Labeled Pedestrian in the Wild)", "description": "Labeled Pedestrian in the Wild (LPW) is a pedestrian detection dataset that contains 2,731 pedestrians in three different scenes where each annotated identity is captured by from 2 to 4 cameras. The LPW features a notable scale of 7,694 tracklets with over 590,000 images as well as the cleanliness of its tracklets. It distinguishes from existing datasets in three aspects: large scale with cleanliness, automatically detected bounding boxes and far more crowded scenes with greater age span. This dataset provides a more realistic and challenging benchmark, which facilitates the further exploration of more powerful algorithms."}, {"id": "anime-face-dataset-by-character-name", "name": "Anime Face Dataset by Character Name", "description": "This dataset is suitable for the image classification model. Train image classification model to classify anime characters by face image. This dataset includes 130 characters with 75 images each, scrapped from Danbooru. "}, {"id": "mams-multi-aspect-multi-sentiment", "name": "MAMS (Multi Aspect Multi-Sentiment)", "description": "MAMS is a challenge dataset for aspect-based sentiment analysis (ABSA), in which each sentences contain at least two aspects with different sentiment polarities. MAMS dataset contains two versions: one for aspect-term sentiment analysis (ATSA) and one for aspect-category sentiment analysis (ACSA)."}, {"id": "fluo-n2dh-gowt1", "name": "Fluo-N2DH-GOWT1", "description": "GFP-GOWT1 mouse stem cells"}, {"id": "horne-2017-fake-news-data", "name": "Horne 2017 Fake News Data", "description": "The Horne 2017 Fake News Data contains two independed news datasets:"}, {"id": "natural-instructions", "name": "Natural Instructions", "description": "Natural-Instructions is a dataset of 61 distinct tasks, their human-authored instructions and 193k task instances. The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema."}, {"id": "brind-bsds-rind", "name": "BRIND (BSDS-RIND)", "description": "BRIND is a short name of BSDS-RIND is the first public benchmark that dedicated to studying simultaneously the four edge types, namely Reflectance Edge (RE), Illumination Edge (IE), Normal Edge (NE) and Depth Edge (DE)"}, {"id": "torque", "name": "Torque", "description": "Torque is an English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships."}, {"id": "netzschleuder-network-catalogue-repository-and-centrifuge", "name": "Netzschleuder (network catalogue, repository and centrifuge)", "description": "This is a catalogue and repository of network datasets with the aim of aiding scientific research."}, {"id": "isruc-sleep", "name": "ISRUC-Sleep", "description": "ISRUC-Sleep is a polysomnographic (PSG) dataset. The data were obtained from human adults, including healthy subjects, and subjects with sleep disorders under the effect of sleep medication. The dataset, which is structured to support different research objectives, comprises three groups of data: (a) data concerning 100 subjects, with one recording session per subject, (b) data gathered from 8 subjects; two recording sessions were performed per subject, which are useful for studies involving changes in the PSG signals over time, (c) data collected from one recording session related to 10 healthy subjects, which are useful for studies involving comparison of healthy subjects with the patients suffering from sleep disorders."}, {"id": "multiconer", "name": "MultiCoNER", "description": "MultiCoNER is a large multilingual dataset (11 languages) for Named Entity Recognition. It is designed to represent some of the contemporary challenges in NER, including low-context scenarios (short and uncased text), syntactically complex entities such as movie titles, and long-tail entity distributions."}, {"id": "am2ico-adversarial-and-multilingual-meaning-in-context", "name": "AM2iCo (Adversarial and Multilingual Meaning in Context)", "description": "AM2iCo is a wide-coverage and carefully designed cross-lingual and multilingual evaluation set. It aims to assess the ability of state-of-the-art representation models to reason over cross-lingual lexical-level concept alignment in context for 14 language pairs. "}, {"id": "street-scene", "name": "Street Scene", "description": "Street Scene is a dataset for video anomaly detection. Street Scene consists of 46 training and 35 testing high resolution 1280\u00d7720 video sequences taken from a USB camera overlooking a scene of a two-lane street with bike lanes and pedestrian sidewalks during daytime. The dataset is challenging because of the variety of activity taking place such as cars driving, turning, stopping and parking; pedestrians walking, jogging and pushing strollers; and bikers riding in bike lanes. In addition the videos contain changing shadows, moving background such as a flag and trees blowing in the wind, and occlusions caused by trees and large vehicles. There are a total of 56,847 frames for training and 146,410 frames for testing, extracted from the original videos at 15 frames per second. The dataset contains a total of 205 naturally occurring anomalous events ranging from illegal activities such as jaywalking and illegal U-turns to simply those that do not occur in the training set such as pets being walked and a metermaid ticketing a car."}, {"id": "repcount-repetitive-action-counting-dataset", "name": "RepCount (Repetitive Action Counting Dataset)", "description": "Counting repetitive actions are widely seen in human activities such as physical exercise. Existing methods focus on performing repetitive action counting in short videos, which is tough for dealing with longer videos in more realistic scenarios. In the data-driven era, the degradation of such generalization capability is mainly attributed to the lack of long video datasets. To complement this margin, we introduce a new large-scale repetitive action counting dataset called RepCount covering a wide variety of video lengths, along with more realistic situations where action interruption or action inconsistencies occur in the video. Besides, we also provide a fine-grained annotation of the action cycles instead of just counting annotation along with a numerical value. Such a dataset contains 1451  videos with about 20000   annotations, which is more challenging.  Furthermore, the dataset consists of two subsets namely Part-A and Part-B. The videos in Part-A are fetched from YouTube, while the others in Part-B record simulated physical examinations by junior school students and teachers."}, {"id": "oxford-radar-robotcar-dataset", "name": "Oxford Radar RobotCar Dataset", "description": "The Oxford Radar RobotCar Dataset is a radar extension to The Oxford RobotCar Dataset. It has been extended with data from a Navtech CTS350-X Millimetre-Wave FMCW radar and Dual Velodyne HDL-32E LIDARs with optimised ground truth radar odometry for 280 km of driving around Oxford, UK (in addition to all sensors in the original Oxford RobotCar Dataset)."}, {"id": "mawps-math-word-problems", "name": "MAWPS (MAth Word ProblemS)", "description": "MAWPS is an online repository of Math Word Problems, to provide a unified testbed to evaluate different algorithms. MAWPS allows for the automatic construction of datasets with particular characteristics, providing tools for tuning the lexical and template overlap of a dataset as well as for filtering ungrammatical problems from web-sourced corpora. The online nature of this repository facilitates easy community contribution.  Amassed 3,320 problems, including the full datasets used in several previous works."}, {"id": "ember", "name": "EMBER", "description": "A labeled benchmark dataset for training machine learning models to statically detect malicious Windows portable executable files. The dataset includes features extracted from 1.1M binary files: 900K training samples (300K malicious, 300K benign, 300K unlabeled) and 200K test samples (100K malicious, 100K benign)."}, {"id": "olr-2021", "name": "OLR 2021", "description": "The OLR 2021 dataset contains the data for the Oriental Language Recognition (OLR) 2021 Challenge, which intends to improve the performance of language recognition systems and speech recognition systems within multilingual scenarios."}, {"id": "cuhk-pedes", "name": "CUHK-PEDES", "description": "The CUHK-PEDES dataset is a caption-annotated pedestrian dataset. It contains 40,206 images over 13,003 persons. Images are collected from five existing person re-identification datasets, CUHK03, Market-1501, SSM, VIPER, and CUHK01 while each image is annotated with 2 text descriptions by crowd-sourcing workers. Sentences incorporate rich details about person appearances, actions, poses."}, {"id": "tweeteval", "name": "TweetEval", "description": "TweetEval introduces an evaluation framework consisting of seven heterogeneous Twitter-specific classification tasks."}, {"id": "cifar-fs-cifar100-few-shots", "name": "CIFAR-FS (CIFAR100 few-shots)", "description": "CIFAR100 few-shots (CIFAR-FS) is randomly sampled from CIFAR-100 (Krizhevsky & Hinton, 2009) by using the same criteria with which miniImageNet has been generated. The average inter-class similarity is sufficiently high to represent a challenge for the current state of the art. Moreover, the limited original resolution of 32\u00d732 makes the task harder and at the same time allows fast prototyping."}, {"id": "ssn-semantic-scholar-network", "name": "SSN (Semantic Scholar Network)", "description": "SSN (short for Semantic Scholar Network) is a scientific papers summarization dataset which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph."}, {"id": "cvl-database", "name": "CVL-DataBase", "description": "The CVL Database is a public database for writer retrieval, writer identification and word spotting. The database consists of 7 different handwritten texts (1 German and 6 Englisch Texts). In total 310 writers participated in the dataset. 27 of which wrote 7 texts and 283 writers had to write 5 texts. For each text a rgb color image (300 dpi) comprising the handwritten text and the printed text sample is available as well as a cropped version (only handwritten). An unique id identifies the writer, whereas the Bounding Boxes for each single word are stored in an XML file."}, {"id": "wlasl-word-level-american-sign-language", "name": "WLASL (Word-Level American Sign Language)", "description": "WLASL is a large video dataset for Word-Level American Sign Language (ASL) recognition, which features 2,000 common different words in ASL."}, {"id": "astitchinlanguagemodels", "name": "AStitchInLanguageModels", "description": "AStitchInLanguageModels is a dataset for the exploration of idiomaticity in pre-trained language models."}, {"id": "referit3d", "name": "ReferIt3D", "description": "ReferIt3D provides two large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5K template-based utterances leveraging spatial relations among fine-grained object classes to localize a referred object in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying a 2-player object reference game in 3D scenes. This dataset can be used for 3D visual grounding and 3D dense captioning tasks."}, {"id": "kuake-qic-query-intent-classification-dataset", "name": "KUAKE-QIC (Query Intent Classification Dataset)", "description": "KUAKE Query Intent Classification, a dataset for intent classification, is used for the KUAKE-QIC task. Given the queries of search engines, the task requires to classify each of them into one of 11 medical intent categories defined in KUAKE-QIC, including diagnosis, etiology analysis, treatment plan, medical advice, test result analysis, disease description, consequence prediction, precautions, intended effects, treatment fees, and others."}, {"id": "set14", "name": "Set14", "description": "The Set14 dataset is a dataset consisting of 14 images commonly used for testing performance of Image Super-Resolution models."}, {"id": "videomatte240k", "name": "VideoMatte240K", "description": "VideoMatte240K consists of 484 high-resolution green screen videos and generate a total of 240,709 unique frames of alpha mattes and foregrounds with chroma-key software Adobe After Effects. The videos are purchased as stock footage or found as royalty-free materials online. 384 videos are at 4K resolution and 100 are in HD. The videos are split by 479 : 5 to form the train and validation sets. The dataset consists of a vast amount of human subjects, clothing, and poses that are helpful for training robust models."}, {"id": "cryonuseg", "name": "CryoNuSeg", "description": "CryoNuSeg is a fully annotated FS-derived cryosectioned and H&E-stained nuclei instance segmentation dataset. The dataset contains images from 10 human organs that were not exploited in other publicly available datasets, and is provided with three manual mark-ups to allow measuring intra-observer and inter-observer variability."}, {"id": "fr-fs-fall-recognition-in-figure-skating", "name": "FR-FS (Fall Recognition in Figure Skating)", "description": "The FR-FS dataset contains 417 videos collected from FIV dataset and Pingchang 2018 Winter Olympic Games. FR-FS contains the critical movements of the athlete\u2019s take-off, rotation, and landing. Among them, 276 are smooth landing videos, and 141 are fall videos. To test the generalization performance of our proposed model, we randomly select 50% of the videos from the fall and landing videos as the training set and the testing set."}, {"id": "robotic-interestingness-robotic-interestingness-a-dataset-to-push-the-limits-of-online-visual-interesting-scene-prediction", "name": "Robotic Interestingness (Robotic Interestingness: A Dataset to Push the Limits of Online Visual Interesting Scene Prediction)", "description": "Robotic Interestingness dataset was created to promote the development visual interesting scene prediction for such purpose, for robots to better sense the world."}, {"id": "gashissdb", "name": "GasHisSDB", "description": "Four pathologists from Longhua Hospital Shanghai University of Traditional Chinese Medicine provide 600 images of gastric cancer pathology images at size 2048$\\times$2048 pixels. These images were scanned using a NewUsbCamera and digitized at $\\times$20 magnification, tissue-level labels were also given by the four experienced pathologists. Based on that, five biomedical researchers from Northeastern University cropped them to 245,196 sub-sized gastric cancer pathology images, and two experienced pathologists from Liaoning Cancer Hospital and Institute perform the calibration. The 245,196 images were split to three sizes (160$\\times$160, 120$\\times$120, 80$\\times$80) for two categories: abnormal and normal."}, {"id": "tdiuc-task-directed-image-understanding-challenge", "name": "TDIUC (Task Directed Image Understanding Challenge)", "description": "Task Directed Image Understanding Challenge (TDIUC) dataset is a Visual Question Answering dataset which consists of 1.6M questions and 170K images sourced from MS COCO and the Visual Genome Dataset. The image-question pairs are split into 12 categories and 4 additional evaluation matrices which help evaluate models\u2019 robustness against answer imbalance and its ability to answer questions that require higher reasoning capability. The TDIUC dataset divides the VQA paradigm into 12 different task directed question types. These include questions that require a simpler task (e.g., object presence, color attribute) and more complex tasks (e.g., counting, positional reasoning). The dataset includes also an \u201cAbsurd\u201d question category in which questions are irrelevant to the image contents to help balance the dataset."}, {"id": "pisc-people-in-social-context", "name": "PISC (People in Social Context)", "description": "The People in Social Context (PISC) dataset is a dataset that focuses on social relationships. It consists of 22,670 images of 9 types of social relationships. It has annotations for the bounding boxes of all people, as well as the social relationship between all pairs of people in the images. In addition, it also contains occupation annotation. "}, {"id": "physionet-challenge-2018-you-snooze-you-win-the-physionet-computing-in-cardiology-challenge-2018", "name": "PhysioNet Challenge 2018 (You Snooze You Win - The PhysioNet Computing in Cardiology Challenge 2018)", "description": "Data for this challenge were contributed by the Massachusetts General Hospital\u2019s (MGH) Computational Clinical Neurophysiology Laboratory (CCNL), and the Clinical Data Animation Laboratory (CDAC). The dataset includes 1,985 subjects which were monitored at an MGH sleep laboratory for the diagnosis of sleep disorders. The data were partitioned into balanced training (n = 994), and test sets (n = 989)."}, {"id": "covid-hera", "name": "Covid-HeRA", "description": "Covid-HeRA is a dataset for health risk assessment and severity-informed decision making in the presence of COVID19 misinformation. It is a benchmark dataset for risk-aware health misinformation detection, related to the 2019 coronavirus pandemic. Social media posts (Twitter) are annotated based on the perceived likelihood of health behavioural changes and the perceived corresponding risks from following unreliable advice found online."}, {"id": "phinc", "name": "PHINC", "description": "PHINC is a parallel corpus of the 13,738 code-mixed English-Hindi sentences and their corresponding translation in English. The translations of sentences are done manually by the annotators. "}, {"id": "dvsmotion20", "name": "DVSMOTION20", "description": "This dataset is designed to enhance the progress of event-based optical flow algorithms. The data was collected using the IniVation DAViS346 camera, which has a 346 x 260 spatial resolution. The dataset is classified into camera motion data (stationary scene and moving camera) and object motion data (stationary camera and moving objects). The camera motion data contains four real indoor sequences (namely, checkerboard, classroom, conference room, and conference room translation) with ground truth motion inferred from IMU. The movement of the camera in this category was restricted by a gimbal, and the IMU was calibrated before each collection.  The object motion data includes two real sequences (called hands and cars) containing multiple object motions. This category does not have ground-truth motion since the object motion cannot be inferred from IMU."}, {"id": "udd", "name": "UDD", "description": "UDD is an underwater open-sea farm object detection dataset. UDD consists of 3 categories (seacucumber, seaurchin, and scallop) with 2,227 images. It's the first dataset collected in a real open-sea farm for underwater robot picking."}, {"id": "libri-light", "name": "Libri-Light", "description": "Libri-Light is a collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. It contains over 60K hours of audio."}, {"id": "dcase-2019-mobile-tau-urban-acoustic-scenes-2019-mobile", "name": "DCASE 2019 Mobile (TAU Urban Acoustic Scenes 2019 Mobile)", "description": "TAU Urban Acoustic Scenes 2019 Mobile development dataset consists of 10-seconds audio segments from 10 acoustic scenes:"}, {"id": "pavia-university", "name": "Pavia University", "description": "The Pavia University dataset is a hyperspectral image dataset which gathered by a sensor known as the reflective optics system imaging spectrometer (ROSIS-3) over the city of Pavia, Italy. The image consists of 610\u00d7340 pixels with 115 spectral bands. The image is divided into 9 classes with a total of 42,776 labelled samples, including the asphalt, meadows, gravel, trees, metal sheet, bare soil, bitumen, brick, and shadow."}, {"id": "pavia-centre", "name": "Pavia Centre", "description": "Pavia Centre is a hyperspectral dataset acquired by the ROSIS sensor during a flight campaign over Pavia, northern Italy. The number of spectral bands is 102 for Pavia Centre. Pavia Centre is a 1096*1096 pixels image. The geometric resolution is 1.3 meters. Image groundtruths differentiate 9 classes each. Pavia scenes were provided by Prof. Paolo Gamba from the Telecommunications and Remote Sensing Laboratory, Pavia university (Italy)."}, {"id": "iper-impersonator", "name": "iPer (Impersonator)", "description": "iPer is a new dataset, with diverse styles of clothes in videos, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. There are 30 subjects of different conditions of shape, height, and gender. Each subject wears different clothes and performs an A-pose video and a video with random actions. There are 103 clothes in total. The whole dataset contains 206 video sequences with 241,564 frames."}, {"id": "def-infantry-sequential", "name": "Def_Infantry_sequential", "description": "SMAC+ defensive infantry scenario with sequential episodic buffer"}, {"id": "ascent-kb", "name": "Ascent KB", "description": "This dataset contains 8.9M commonsense assertions extracted by the Ascent pipeline developed at the Max Planck Institute for Informatics. The focus of this dataset is on everyday concepts such as elephant, car, laptop, etc. The current version of Ascent KB (v1.0.0) is approximately 19 times larger than ConceptNet (note that, in this comparison, non-commonsense knowledge in ConceptNet such as lexical relations is excluded)."}, {"id": "ocean-drifters-madagascar-ocean-drifters", "name": "Ocean Drifters (Madagascar Ocean Drifters)", "description": "From Schaub, Michael T., et al. \"Random walks on simplicial complexes and the normalized hodge 1-laplacian.\" SIAM Review 62.2 (2020): 353-391."}, {"id": "itg-in-the-groove", "name": "ITG (In The Groove)", "description": "In The Groove (ITG) is an audio dataset where given a raw audio track, the goal is to produce a choreography step chart, similar to those used in the Dance Dance Revolution video game. It contains 133 songs choreographed by a three different authors, with 652 charts for the 133 songs."}, {"id": "fmow-functional-map-of-the-world", "name": "fMoW (Functional Map of the World)", "description": "Functional Map of the World (fMoW) is a dataset that aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. "}, {"id": "dex-net-2-0-dexterity-network-2-0", "name": "Dex-Net 2.0 (Dexterity Network 2.0)", "description": "Dex-Net 2.0 is a dataset associating 6.7 million point clouds and analytic grasp quality metrics with parallel-jaw grasps planned using robust quasi-static GWS analysis on a dataset of 1,500 3D object models."}, {"id": "sdoml", "name": "SDOML", "description": "Machine-learning Data Set Prepared from NASA Solar Dynamics Observatory Mission data."}, {"id": "sword-scenes-with-occluded-regions-dataset", "name": "SWORD ('Scenes with occluded regions' dataset)", "description": "The new dataset contains around 1,500 train videos and 290 test videos, with 50 frames per video on average. The dataset was obtained after processing the manually captured video sequences of static real-life urban scenes. The main property of the dataset is the abundance of close objects and, consequently, the larger prevalence of occlusions. According to the introduced heuristic, the mean area of occluded image parts for SWORD is approximately five times larger than for RealEstate10k data (14% vs 3% respectively). This rationalizes the collection and usage of SWORD and explains that SWORD allows training more powerful models despite being of smaller size."}, {"id": "now-benchmark", "name": "NoW Benchmark", "description": "The goal of this benchmark is to introduce a standard evaluation metric to measure the accuracy and robustness of 3D face reconstruction methods under variations in viewing angle, lighting, and common occlusions. "}, {"id": "kitti-trajectory-prediction", "name": "KITTI-trajectory-prediction", "description": "KITTI is a well established dataset in the computer vision community. It has often been used for trajectory prediction despite not having a well defined split, generating non comparable baselines in different works. This dataset aims at bridging this gap and proposes a well defined split of the KITTI data. Samples are collected as 6 seconds chunks (2seconds for past and 4 for future) in a sliding window fashion from all trajectories in the dataset, including the egovehicle. There are a total of 8613 top-view trajectories for training and 2907 for testing. Since top-view maps are not provided by KITTI, semantic labels of static categories obtained with DeepLab-v3+ from all frames are projected in a common top-view map using the Velodyne 3D point cloud and IMU. The resulting maps have a spatial resolution of 0.5 meters and are provided along with the trajectories."}, {"id": "vluc-video-like-urban-computing", "name": "VLUC (Video-Like Urban Computing)", "description": "VLUC (Video-Like Urban Computing) is a benchmark for video-like computing on citywide traffic density and crowd prediction. It consists of two new datasets BousaiTYO and BousaiOSA and existing datasets TaxiBJ, BikeNYC I-II, and TaxiNYC."}, {"id": "atlas-v2-0-anatomical-tracings-of-lesions-after-stroke-dataset-version-2-0", "name": "ATLAS v2.0 (Anatomical Tracings of Lesions After Stroke Dataset version 2.0)", "description": "Accurate lesion segmentation is critical in stroke rehabilitation research for the quantification of lesion burden and accurate image processing. Current automated lesion segmentation methods for T1-weighted (T1w) MRIs, commonly used in rehabilitation research, lack accuracy and reliability. Manual segmentation remains the gold standard, but it is time-consuming, subjective, and requires significant neuroanatomical expertise. However, many methods developed with ATLAS v1.2 report low accuracy, are not publicly accessible or are improperly validated, limiting their utility to the field. Here we present ATLAS v2.0 (N=1271), a larger dataset of T1w stroke MRIs and manually segmented lesion masks that includes training (public. n=655), test (masks hidden, n=300), and generalizability (completely  hidden, n=316) data. Algorithm development using this larger sample should lead to more robust solutions, and the hidden test and generalizability datasets allow for unbiased performance evaluation via segmentation challenges. We anticipate that ATLAS v2.0 will lead to improved algorithms, facilitating large-scale stroke rehabilitation research."}, {"id": "finer-finnish-news-corpus-for-named-entity-recognition", "name": "Finer (Finnish News Corpus for Named Entity Recognition)", "description": "Finnish News Corpus for Named Entity Recognition (Finer) is a corpus that consists of 953 articles (193,742 word tokens) with six named entity classes (organization, location, person, product, event,and date). The articles are extracted from the archives of Digitoday, a Finnish online technology news source."}, {"id": "aria-automated-retinal-image-analysis-aria-data-set", "name": "ARIA (Automated Retinal Image Analysis (ARIA) Data Set)", "description": "This data set was collected in 2004 to 2006 in the United Kingdom. Subjects were adult males and females, some of whom were healthy (control group), some with age-related macular degeneration (AMD group), and some were diabetic patients (diabetic group). Unfortunately, no other information from this time exists about this subjects."}, {"id": "kornli", "name": "KorNLI", "description": "KorNLI is a Korean Natural Language Inference (NLI) dataset. The dataset is constructed by automatically translating the training sets of the SNLI, XNLI and MNLI datasets. To ensure translation quality, two professional translators with at least seven years of experience who specialize in academic papers/books as well as business contracts post-edited a half of the dataset each and cross-checked each other\u2019s translation afterward. It contains 942,854 training examples translated automatically and 7,500 evaluation (development and test) examples translated manually"}, {"id": "edgar-corpus", "name": "EDGAR-CORPUS", "description": "EDGAR-CORPUS is a novel corpus comprising annual reports from all the publicly traded companies in the US spanning a period of more than 25 years. All the reports are downloaded, split into their corresponding items (sections), and provided in a clean, easy-to-use JSON format. "}, {"id": "icvl-hsi", "name": "ICVL-HSI", "description": "ICVL is a hyperspectral image dataset, collected by \"Sparse Recovery of Hyperspectral Signal from Natural RGB Images\""}, {"id": "p3-psychophysical-patterns-dataset", "name": "P3 (Psychophysical Patterns Dataset)", "description": "A set of patterns used in psychophysical research to evaluate the ability of saliency algorithms to find targets distinct from distractors in orientation, color and size. Each image is a 7x7 grid and contains a single target. All images are 1024x1024px and have corresponding ground truth masks for the target and distractors."}, {"id": "openxai", "name": "OpenXAI", "description": "OpenXAI is the first general-purpose lightweight library that provides a comprehensive list of functions to systematically evaluate the quality of explanations generated by attribute-based explanation methods. OpenXAI supports the development of new datasets (both synthetic and real-world) and explanation methods, with a strong bent towards promoting systematic, reproducible, and transparent evaluation of explanation methods."}, {"id": "lhq-landscapes-high-quality", "name": "LHQ (Landscapes High-Quality)", "description": "A dataset of 90,000 high-resolution nature landscape images, crawled from Unsplash and Flickr and preprocessed with Mask R-CNN and Inception V3."}, {"id": "dane-danish-dependency-treebank", "name": "DaNE (Danish Dependency Treebank)", "description": "Danish Dependency Treebank (DaNE) is a named entity annotation for the Danish Universal Dependencies treebank using the CoNLL-2003 annotation scheme."}, {"id": "nasa-worldview-understanding-clouds-from-satellite-images", "name": "NASA Worldview (Understanding Clouds from Satellite Images)", "description": "In this competition you will be identifying regions in satellite images that contain certain cloud formations, with label names: Fish, Flower, Gravel, Sugar. For each image in the test set, you must segment the regions of each cloud formation label. Each image has at least one cloud formation, and can possibly contain up to all all four."}, {"id": "conll-2014-shared-task-grammatical-error-correction", "name": "CoNLL-2014 Shared Task: Grammatical Error Correction", "description": "CoNLL-2014 will continue the CoNLL tradition of having a high profile shared task in natural language processing. This year's shared task will be grammatical error correction, a continuation of the CoNLL shared task in 2013. A participating system in this shared task is given short English texts written by non-native speakers of English. The system detects the grammatical errors present in the input texts, and returns the corrected essays. The shared task in 2014 will require a participating system to correct all errors present in an essay (i.e., not restricted to just five error types in 2013). Also, the evaluation metric will be changed to F0.5, weighting precision twice as much as recall."}, {"id": "read-2016-htr-dataset-icfhr-2016", "name": "READ 2016 (HTR Dataset ICFHR 2016)", "description": "This dataset arises from the READ project (Horizon 2020)."}, {"id": "crosstask", "name": "CrossTask", "description": "CrossTask dataset contains instructional videos, collected for 83 different tasks. For each task an ordered list of steps with manual descriptions is provided. The dataset is divided in two parts: 18 primary and 65 related tasks. Videos for the primary tasks are collected manually and provided with annotations for temporal step boundaries. Videos for the related tasks are collected automatically and don't have annotations."}, {"id": "coco-text", "name": "COCO-Text", "description": "The COCO-Text dataset is a dataset for text detection and recognition. It is based on the MS COCO dataset, which contains images of complex everyday scenes. The COCO-Text dataset contains non-text images, legible text images and illegible text images. In total there are 22184 training images and 7026 validation images with at least one instance of legible text."}, {"id": "multisense", "name": "MultiSense", "description": "MultiSense is a dataset of 9,504 images annotated with an English verb and its translation in Spanish and German."}, {"id": "cufsf-cuhk-face-sketch-feret-database", "name": "CUFSF (CUHK Face Sketch FERET Database)", "description": "The CUHK Face Sketch FERET (CUFSF) is a dataset for research on face sketch synthesis and face sketch recognition. It contains two types of face images: photo and sketch. Total 1,194 images (one image per subject) were collected with lighting variations from the FERET dataset. For each subject, a sketch is drawn with shape exaggeration."}, {"id": "aqua", "name": "AQUA", "description": "The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers' correctness. The dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. "}, {"id": "ticam-time-of-flight-in-car-cabin-monitoring", "name": "TICaM (Time-of-flight In-car Cabin Monitoring)", "description": "TICaM is a Time-of-flight In-car Cabin Monitoring dataset for vehicle interior monitoring using a single wide-angle depth camera. This dataset addresses the deficiencies of other available in-car cabin datasets in terms of the ambit of labeled classes, recorded scenarios and provided annotations; all at the same time. It consists of an exhaustive list of actions performed while driving and multi-modal labeled images (depth, RGB and IR), with complete annotations for 2D and 3D object detection, instance and semantic segmentation as well as activity annotations for RGB frames. Additional to real recordings, it also contains a synthetic dataset of in-car cabin images with same multi-modality of images and annotations, providing a unique and extremely beneficial combination of synthetic and real data for effectively training cabin monitoring systems and evaluating domain adaptation approaches."}, {"id": "spair-71k", "name": "SPair-71k", "description": "SPair-71k contains 70,958 image pairs with diverse variations in viewpoint and scale. Compared to previous datasets, it is significantly larger in number and contains more accurate and richer annotations. "}, {"id": "emocause", "name": "EmoCause", "description": "EmoCause is a dataset of annotated emotion cause words in emotional situations from the EmpatheticDialogues valid and test set. The goal is to recognize emotion cause words in sentences by training only on sentence-level emotion labels without word-level labels (i.e., weakly-supervised emotion cause recognition). "}, {"id": "bvi-dvc", "name": "BVI-DVC", "description": "Contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools."}, {"id": "summaries-of-genetic-variation", "name": "Summaries of genetic variation", "description": "The dataset represents data generated from a commonly used model in population genetics. It comprises a matrix of 1,000,000 rows and 9 columns, representing parameters and summaries generated by an infinite-sites coalescent model for genetic variation. The first two columns encode the scaled mutation rate (theta) and scaled recombination rate (rho). The subsequent seven columns are data summaries: number of segregating sites (C1), standard uniform random noise acting as a distractor (C2), pairwise mean number of nucleotidic differences (C3), mean $R^2$ across pairs separated by <10% of the simulated genomic regions (C4), number of distinct haplotypes (C5), frequency of the most common haplotype (C6), number of singleton haplotypes (C7)."}, {"id": "fer-face-expression-recognition-plus-dataset", "name": "FER+ (Face Expression Recognition Plus dataset)", "description": "The FER+ dataset is an extension of the original FER dataset, where the images have been re-labelled into one of 8 emotion types: neutral, happiness, surprise, sadness, anger, disgust, fear, and contempt."}, {"id": "flic-frames-labelled-in-cinema", "name": "FLIC (Frames Labelled in Cinema)", "description": "The FLIC dataset contains 5003 images from popular Hollywood movies. The images were obtained by running a state-of-the-art person detector on every tenth frame of 30 movies. People detected with high confidence (roughly 20K candidates) were then sent to the crowdsourcing marketplace Amazon Mechanical Turk to obtain ground truth labelling. Each image was annotated by five Turkers to label 10 upper body joints. The median-of-five labelling was taken in each image to be robust to outlier annotation. Finally, images were rejected manually by if the person was occluded or severely non-frontal."}, {"id": "arsarcasm", "name": "ArSarcasm", "description": "ArSarcasm is a new Arabic sarcasm detection dataset. The dataset was created using previously available Arabic sentiment analysis datasets (SemEval 2017 and ASTD) and adds sarcasm and dialect labels to them. The dataset contains 10,547 tweets, 1,682 (16%) of which are sarcastic."}, {"id": "catalan-timebank-1-0", "name": "Catalan TimeBank 1.0", "description": "Catalan TimeBank 1.0 was developed by researchers at Barcelona Media and consists of Catalan texts in the AnCora corpus annotated with temporal and event information according to the TimeML specification language."}, {"id": "ravdess-ryerson-audio-visual-database-of-emotional-speech-and-song", "name": "RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)", "description": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7,356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18."}, {"id": "rocstories", "name": "ROCStories", "description": "ROCStories is a collection of commonsense short stories. The corpus consists of 100,000 five-sentence stories. Each story logically follows everyday topics created by Amazon Mechanical Turk workers. These stories contain a variety of commonsense causal and temporal relations between everyday events. Writers also develop an additional 3,742 Story Cloze Test stories which contain a four-sentence-long body and two candidate endings. The endings were collected by asking Mechanical Turk workers to write both a right ending and a wrong ending after eliminating original endings of given short stories. Both endings were required to make logical sense and include at least one character from the main story line. The published ROCStories dataset is constructed with ROCStories as a training set that includes 98,162 stories that exclude candidate wrong endings, an evaluation set, and a test set, which have the same structure (1 body + 2 candidate endings) and a size of 1,871."}, {"id": "saint-gall", "name": "Saint Gall", "description": "Saint Gall dataset contains handwritten historical manuscripts written in Latin that date back to the 9th century. It consists of 60 pages, 1 410 text lines and 11 597 words."}, {"id": "bitod", "name": "BiToD", "description": "BiToD is a bilingual multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD contains over 7k multi-domain dialogues (144k utterances) with a large and realistic bilingual knowledge base. It serves as an effective benchmark for evaluating bilingual ToD systems and cross-lingual transfer learning approaches."}, {"id": "odsqa-open-domain-spoken-question-answering", "name": "ODSQA (Open-Domain Spoken Question Answering)", "description": "The ODSQA dataset is a spoken dataset for question answering in Chinese. It contains more than three thousand questions from 20 different speakers."}, {"id": "ebdtheque", "name": "eBDtheque", "description": "The eBDtheque database is a selection of one hundred comic pages from America, Japan (manga) and Europe."}, {"id": "hate-counter", "name": "Hate Counter", "description": "This dataset is built from Twitter and contains 1290 hate tweet and counterspeech reply pairs. After the annotation process, the dataset consists of 558 unique hate tweets from 548 user and 1290 counterspeech replies from 1239 users."}, {"id": "brain-tumor-mri-dataset", "name": "Brain Tumor MRI Dataset", "description": "This dataset is a combination of the following three datasets : figshare, SARTAJ dataset and Br35H"}, {"id": "udis-d-unsupervised-deep-image-stitching-dataset", "name": "UDIS-D (Unsupervised Deep Image Stitching Dataset)", "description": "UDIS-D is a large image dataset for image stitching or image registration. It contains different overlap rates, varying degrees of parallax, and variable scenes such as indoor, outdoor, night, dark, snow, and zooming."}, {"id": "urlb-unsupervised-reinforcement-learning-benchmark", "name": "URLB (Unsupervised Reinforcement Learning Benchmark)", "description": "URLB consists of two phases: reward-free pre-training and downstream task adaptation with extrinsic rewards. Building on the DeepMind Control Suite, it provides twelve continuous control tasks from three domains for evaluation."}, {"id": "mustard", "name": "MUStARD++", "description": "MUStARD++ is a multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions. It can be used for the task of detecting the emotion in a sarcastic statement."}, {"id": "blended-skill-talk", "name": "Blended Skill Talk", "description": "To analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes. "}, {"id": "am-2k-animal-matting-2k", "name": "AM-2k (Animal Matting - 2k)", "description": "AM-2k contains 2,000 high-resolution natural animal images from 20 categories along with manually labeled alpha mattes."}, {"id": "tum-vie-tum-stereo-visual-inertial-event-dataset", "name": "TUM-VIE (TUM Stereo Visual-Inertial Event Dataset)", "description": "TUM-VIE is an event camera dataset for developing 3D perception and navigation algorithms. It contains handheld and head-mounted sequences in indoor and outdoor environments with rapid motion during sports and high dynamic range. TUM-VIE includes challenging sequences where state-of-the art VIO fails or results in large drift. Hence, it can help to push the boundary on event-based visual-inertial algorithms."}, {"id": "czech-restaurant-information", "name": "Czech restaurant information", "description": "Czech restaurant information is a dataset for NLG in task-oriented spoken dialogue systems with Czech as the target language. It originated as a translation of the English San Francisco Restaurants dataset by Wen et al. (2015)."}, {"id": "sustainbench", "name": "SustainBench", "description": "SustainBench is a collection of 15 benchmark tasks across 7 sustainable development goals (SDGs), including tasks related to economic development, agriculture, health, education, water and sanitation, climate action, and life on land. The goals for SustainBench are to:"}, {"id": "eve-end-to-end-video-based-eye-tracking", "name": "EVE (End-to-end Video-based Eye-tracking)", "description": "EVE (End-to-end Video-based Eye-tracking) is a dataset for eye-tracking. It is collected from 54 participants and consists of 4 camera views, over 12 million frames and 1327 unique visual stimuli (images, video, text), adding up to approximately 105 hours of video data in total."}, {"id": "home-household-multimodal-environment", "name": "HoME (Household Multimodal Environment)", "description": "HoME (Household Multimodal Environment) is a multimodal environment for artificial agents to learn from vision, audio, semantics, physics, and interaction with objects and other agents, all within a realistic context. HoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning, and more."}, {"id": "3threedworld", "name": "3ThreeDWorld", "description": "TDW is a 3D virtual world simulation platform, utilizing state-of-the-art video game engine technology. A TDW simulation consists of two components: a) the Build, a compiled executable running on the Unity3D Engine, which is responsible for image rendering, audio synthesis and physics simulations; and b) the Controller, an external Python interface to communicate with the build."}, {"id": "raf-db-real-world-affective-faces", "name": "RAF-DB (Real-world Affective Faces)", "description": "The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. It contains 29672 facial images tagged with basic or compound expressions by 40 independent taggers. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc."}, {"id": "set5", "name": "Set5", "description": "The Set5 dataset is a dataset consisting of 5 images (\u201cbaby\u201d, \u201cbird\u201d, \u201cbutterfly\u201d, \u201chead\u201d, \u201cwoman\u201d) commonly used for testing performance of Image Super-Resolution models."}, {"id": "escape", "name": "eSCAPE", "description": "Consists of millions of entries in which the MT element of the training triplets has been obtained by translating the source side of publicly-available parallel corpora, and using the target side as an artificial human post-edit. Translations are obtained both with phrase-based and neural models."}, {"id": "visual-question-answering-v2-0-vqa-v2-0", "name": "Visual Question Answering v2.0 (VQA v2.0)", "description": "Visual Question Answering (VQA) v2.0 is a dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer. It is the second version of the VQA dataset."}, {"id": "laion-5b", "name": "LAION-5B", "description": "LAION 5B is a large-scale dataset for research purposes consisting of 5,85B CLIP-filtered image-text pairs. 2,3B contain English language, 2,2B samples from 100+ other languages and 1B samples have texts that do not allow a certain language assignment (e.g. names ). Additionally, we provide several nearest neighbor indices, an improved web interface for exploration & subset creation as well as detection scores for watermark and NSFW."}, {"id": "svamp-simple-variations-on-arithmetic-math-word-problems", "name": "SVAMP (Simple Variations on Arithmetic Math word Problems)", "description": "A challenge set for elementary-level Math Word Problems (MWP). An MWP consists of a short Natural Language narrative that describes a state of the world and poses a question about some unknown quantities."}, {"id": "fsod-few-shot-object-detection-dataset", "name": "FSOD (Few-Shot Object Detection Dataset)", "description": "Few-Shot Object Detection Dataset (FSOD) is a high-diverse dataset specifically designed for few-shot object detection and intrinsically designed to evaluate thegenerality of a model on novel categories."}, {"id": "dvqa-data-visualizations-via-question-answering", "name": "DVQA (Data Visualizations via Question Answering)", "description": "DVQA is a synthetic question-answering dataset on images of bar-charts."}, {"id": "how2sign-a-large-scale-multimodal-dataset-for-continuous-american-sign-language", "name": "How2Sign (A Large-scale Multimodal Dataset for Continuous American Sign Language)", "description": "The How2Sign is a multimodal and multiview continuous American Sign Language (ASL) dataset consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio enabling detailed 3D pose estimation."}, {"id": "kuzushiji-49", "name": "Kuzushiji-49", "description": "Kuzushiji-49 is an MNIST-like dataset that has 49 classes (28x28 grayscale, 270,912 images) from 48 Hiragana characters and one Hiragana iteration mark."}, {"id": "mpiigaze", "name": "MPIIGaze", "description": "MPIIGaze is a dataset for appearance-based gaze estimation in the wild. It contains 213,659 images collected from 15 participants during natural everyday laptop use over more than three months. It has a large variability in appearance and illumination."}, {"id": "figment", "name": "Figment", "description": "A dataset for fine-grained entity typing of knowledge graph entities built from Freebase.  It can be used to evaluate entity representations and also mention-level entity typing."}, {"id": "openbmat-open-broadcast-media-audio-from-tv", "name": "OpenBMAT (Open Broadcast Media Audio from TV)", "description": "Open Broadcast Media Audio from TV (OpenBMAT) is an open, annotated dataset for the task of music detection that contains over 27 hours of TV broadcast audio from 4 countries distributed over 1647 one-minute long excerpts. It is designed to encompass several essential features for any music detection dataset and is the first one to include annotations about the loudness of music in relation to other simultaneous non-music sounds. OpenBMAT has been cross-annotated by 3 annotators obtaining high inter-annotator agreement percentages, which validates the annotation methodology and ensures the annotations reliability."}, {"id": "sun-rgb-d", "name": "SUN RGB-D", "description": "The SUN RGBD dataset contains 10335 real RGB-D images of room scenes. Each RGB image has a corresponding depth and segmentation map. As many as 700 object categories are labeled. The training and testing sets contain 5285 and 5050 images, respectively."}, {"id": "dramaqa", "name": "DramaQA", "description": "The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story. The dataset is built upon the TV drama \"Another Miss Oh\" and it contains 17,983 QA pairs from 23,928 various length video clips, with each QA pair belonging to one of four difficulty levels."}, {"id": "buildingnet", "name": "BuildingNet", "description": "BuildingNet is a large-scale dataset of 3D building models whose exteriors are consistently labeled. The dataset consists on 513K annotated mesh primitives, grouped into 292K semantic part components across 2K building models. The dataset covers several building categories, such as houses, churches, skyscrapers, town halls, libraries, and castles."}, {"id": "oxford-102-flower-102-category-flower-dataset", "name": "Oxford 102 Flower (102 Category Flower Dataset)", "description": "Oxford 102 Flower is an image classification dataset consisting of 102 flower categories. The flowers chosen to be flower commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images."}, {"id": "dense-depth-estimation-on-synthetic-events", "name": "DENSE (Depth Estimation oN Synthetic Events)", "description": "DENSE (Depth Estimation oN Synthetic Events) is a new dataset with synthetic events and perfect ground truth."}, {"id": "interaction-dataset", "name": "INTERACTION Dataset", "description": "The INTERACTION dataset contains naturalistic motions of various traffic participants in a variety of highly interactive driving scenarios from different countries. The dataset can serve for many behavior-related research areas, such as "}, {"id": "mm-whs-2017", "name": "MM-WHS 2017", "description": "The MM-WHS 2017 dataset is a dataset for multi-modality whole heart segmentation. It provides 20 labeled and 40 unlabeled CT volumes, as well as 20 labeled and 40 unlabeled MR volumes. In total there are 120 multi-modality cardiac images acquired in a real clinical environment."}, {"id": "i2b2-de-identification-dataset-informatics-for-integrating-biology-and-the-bedside-i2b2-project-de-identification-dataset", "name": "i2b2 De-identification Dataset (Informatics for Integrating Biology and the Bedside (i2b2) Project \u2014 De-identification Dataset)", "description": "This dataset contains 1304 de-identified longitudinal medical records describing 296 patients."}, {"id": "kodf-korean-deepfake-detection-dataset", "name": "KoDF (Korean DeepFake Detection Dataset)", "description": "The Korean DeepFake Detection Dataset (KoDF) is a large-scale collection of synthesized and real videos focused on Korean subjects, used for the task of deepfake detection."}, {"id": "semaine", "name": "SEMAINE", "description": "The SEMAINE videos dataset contains spontaneous data capturing the audiovisual interaction between a human and an operator undertaking the role of an avatar with four personalities: Poppy (happy), Obadiah (gloomy), Spike (angry) and Prudence (pragmatic). The audiovisual sequences have been recorded at a video rate of 25 fps (352 x 288 pixels). The dataset consists of audiovisual interaction between a human and an operator undertaking the role of an agent (Sensitive Artificial Agent). SEMAINE video clips have been annotated with couples of epistemic states such as agreement, interested, certain, concentration, and thoughtful with continuous rating (within the range [1,-1]) where -1 indicates most negative rating (i.e: No concentration at all) and +1 defines the highest (Most concentration). Twenty-four recording sessions are used in the Solid SAL scenario. Recordings are made of both the user and the operator, and there are usually four character interactions in each recording session, providing a total of 95 character interactions and 190 video clips."}, {"id": "davis-densely-annotated-video-segmentation", "name": "DAVIS (Densely Annotated VIdeo Segmentation)", "description": "The Densely Annotation Video Segmentation dataset (DAVIS) is a high quality and high resolution densely annotated video segmentation dataset under two resolutions, 480p and 1080p. There are 50 video sequences with 3455 densely annotated frames in pixel level. 30 videos with 2079 frames are for training and 20 videos with 1376 frames are for validation."}, {"id": "mqtt-iot-ids2020", "name": "MQTT-IoT-IDS2020", "description": "Message Queuing Telemetry Transport (MQTT) protocol is one of the most used standards used in Internet of Things (IoT) machine to machine communication. The increase in the number of available IoT devices and used protocols reinforce the need for new and robust Intrusion Detection Systems (IDS). However, building IoT IDS requires the availability of datasets to process, train and evaluate these models. "}, {"id": "oscd-onera-satellite-change-detection", "name": "OSCD (Onera Satellite Change Detection)", "description": "The Onera Satellite Change Detection dataset addresses the issue of detecting changes between satellite images from different dates."}, {"id": "woodscape", "name": "WoodScape", "description": "Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of its prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. WoodScape is an extensive fisheye automotive dataset named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images."}, {"id": "wiki-one", "name": "Wiki-One", "description": "This dataset is a Wikipedia dump, split by relations to perform Few-Shot Knowledge Graph Completion. "}, {"id": "ambiguous-hoi", "name": "Ambiguous-HOI", "description": "Ambiguous-HOI is a challenging dataset containing ambiguous human-object interaction images for HOI detection based on HICO-DET."}, {"id": "cdcp-cornell-erulemaking-corpus", "name": "CDCP (Cornell eRulemaking Corpus)", "description": "The Cornell eRulemaking Corpus \u2013 CDCP is an argument mining corpus annotated with argumentative structure information capturing the evaluability of arguments. The corpus consists of 731 user comments on Consumer Debt Collection Practices (CDCP) rule by the Consumer Financial Protection Bureau (CFPB); the resulting dataset contains 4931 elementary unit and 1221 support relation annotations. It is a resource for building argument mining systems that can not only extract arguments from unstructured text, but also identify what additional information is necessary for readers to understand and evaluate a given argument. Immediate applications include providing real-time feedback to commenters, specifying which types of support for which propositions can be added to construct better-formed arguments."}, {"id": "ltcc", "name": "LTCC", "description": "LTCC contains 17,119 person images of 152 identities, and each identity is captured by at least two cameras.  The dataset can be divided into two subsets: one cloth-change set where 91 persons appear with 416 different sets of outfits in 14,783 images, and one cloth-consistent subset containing the remaining 61 identities with 2,336 images without outfit changes. On average, there are 5 different clothes for each cloth-changing person, with the number of outfit changes ranging from 2 to 14."}, {"id": "uspto-190", "name": "USPTO-190", "description": "A chemical synthesis route dataset constructed from the USPTO reaction dataset (1976-Sep2016) and a list of commercially available building blocks from eMolecules  (~23.1M molecules).  After processing, the dataset has 299202 training routes, 65274 validation routes, 190 test routes, and the corresponding target molecules."}, {"id": "abcd-study-adolescent-brain-cognitive-development", "name": "ABCD Study (Adolescent Brain Cognitive Development)", "description": "The ABCD Study is a prospective longitudinal study starting at the ages of 9-10 and following participants for 10 years. The study includes a diverse sample of nearly 12,000 youth enrolled at 21 research sites across the country. It measures brain development (via structural, task functional, and resting state functional imaging), social, emotional, and cognitive development, mental health, substance use and attitudes, gender identity and sexual health, bio-specimens, as well as a variety of physical health, and environmental factors."}, {"id": "ctspine1k", "name": "CTSpine1K", "description": "CTSpine1K is a large-scale and comprehensive dataset for research in spinal image analysis. CTSpine1K is curated from the following four open sources, totalling 1,005 CT volumes (over 500,000 labeled slices and over 11,000 vertebrae) of diverse appearance variations."}, {"id": "youtube-driving", "name": "YouTube Driving", "description": "YouTube Driving Dataset contains a massive amount of real-world driving frames with various conditions, from different weather, different regions, to diverse scene types"}, {"id": "contractnli-contractnli-a-dataset-for-document-level-natural-language-inference-for-contracts", "name": "ContractNLI (ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts)", "description": "ContractNLI is a dataset for document-level natural language inference (NLI) on contracts whose goal is to automate/support a time-consuming procedure of contract review. In this task, a system is given a set of hypotheses (such as \u201cSome obligations of Agreement may survive termination.\u201d) and a contract, and it is asked to classify whether each hypothesis is entailed by, contradicting to or not mentioned by (neutral to) the contract as well as identifying evidence for the decision as spans in the contract."}, {"id": "biked", "name": "BIKED", "description": "BIKED is a dataset comprised of 4500 individually designed bicycle models sourced from hundreds of designers. BIKED enables a variety of data-driven design applications for bicycles and generally supports the development of data-driven design methods. The dataset is comprised of a variety of design information including assembly images, component images, numerical design parameters, and class labels."}, {"id": "ncd-natural-color-dataset", "name": "NCD (Natural-Color Dataset)", "description": "The Natural-Color Dataset (NCD) is an image colorization dataset where images are true to their colors. For example, a carrot will have an orange color in most images. Bananas will be either greenish or yellowish. It contains 723 images from the internet distributed in 20 categories. Each image has an object and a white background."}, {"id": "abstractive-text-summarization-from-il-post", "name": "Abstractive Text Summarization from Il Post", "description": "IlPost dataset, containing news articles taken from IlPost."}, {"id": "rhd-rendered-hand-pose", "name": "RHD (Rendered Hand Pose)", "description": "Rendered Hand Pose (RHD) is a dataset for hand pose estimation. It provides segmentation maps with 33 classes: three for each finger, palm, person, and background. The 3D kinematic model of the hand provides 21 keypoints per hand: 4 keypoints per finger and one keypoint close to the wrist."}, {"id": "xl-sum", "name": "XL-Sum", "description": "XL-Sum is a comprehensive and diverse dataset for abstractive summarization comprising 1 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation."}, {"id": "shapenetcore", "name": "ShapeNetCore", "description": "ShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+, a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore."}, {"id": "carla-car-learning-to-act", "name": "CARLA (Car Learning to Act)", "description": "CARLA (CAR Learning to Act) is an open simulator for urban driving, developed as an open-source layer over Unreal Engine 4. Technically, it operates similarly to, as an open source layer over Unreal Engine 4 that provides sensors in the form of RGB cameras (with customizable positions), ground truth depth maps, ground truth semantic segmentation maps with 12 semantic classes designed for driving (road, lane marking, traffic sign, sidewalk and so on), bounding boxes for dynamic objects in the environment, and measurements of the agent itself (vehicle location and orientation)."}, {"id": "massive", "name": "MASSIVE", "description": "MASSIVE is a parallel dataset of > 1M utterances across 51 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation. Utterances span 60 intents and include 55 slot types. MASSIVE was created by localizing the SLURP dataset, composed of general Intelligent Voice Assistant single-shot interactions."}, {"id": "copa-choice-of-plausible-alternatives", "name": "COPA (Choice of Plausible Alternatives)", "description": "The Choice Of Plausible Alternatives (COPA) evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. COPA consists of 1000 questions, split equally into development and test sets of 500 questions each. Each question is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%."}, {"id": "rent3d-2", "name": "Rent3D++", "description": "Rent3D++ is an extension of the Rent3D floorplans + photos dataset.  The floorplans are annotated with room outline polygons, doors/windows as line segments, object-icons as axis-aligned bounding boxes, room-door-room connectivity graphs, and photo-room assignments. We have extracted rectified surface crops from architectural surfaces in photos, and these can drive interior texturing/material modeling tasks. This dataset can be used with our paper Plan2Scene to generate textured 3D mesh models of houses using floorplans and photos."}, {"id": "webchild", "name": "WebChild", "description": "One of the largest commonsense knowledge bases available, describing over 2 million disambiguated concepts and activities, connected by over 18 million assertions."}, {"id": "dmelodies", "name": "dMelodies", "description": "dMelodies is dataset of simple 2-bar melodies generated using 9 independent latent factors of variation where each data point represents a unique melody based on the following constraints: - Each melody will correspond to a unique scale (major, minor, blues, etc.). - Each melody plays the arpeggios using the standard I-IV-V-I cadence chord pattern. - Bar 1 plays the first 2 chords (6 notes), Bar 2 plays the second 2 chords (6 notes). - Each played note is an 8th note."}, {"id": "mit-bih-arrhythmia-database", "name": "MIT-BIH Arrhythmia Database", "description": "The MIT-BIH Arrhythmia Database contains 48 half-hour excerpts of two-channel ambulatory ECG recordings, obtained from 47 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979. Twenty-three recordings were chosen at random from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed population of inpatients (about 60%) and outpatients (about 40%) at Boston's Beth Israel Hospital; the remaining 25 recordings were selected from the same set to include less common but clinically significant arrhythmias that would not be well-represented in a small random sample."}, {"id": "multimnist", "name": "MultiMNIST", "description": "The MultiMNIST dataset is generated from MNIST. The training and tests are generated by overlaying a digit on top of another digit from the same set (training or test) but different class. Each digit is shifted up to 4 pixels in each direction resulting in a 36\u00d736 image. Considering a digit in a 28\u00d728 image is bounded in a 20\u00d720 box, two digits bounding boxes on average have 80% overlap. For each digit in the MNIST dataset 1,000 MultiMNIST examples are generated, so the training set size is 60M and the test set size is 10M."}, {"id": "aida-conll-yago", "name": "AIDA CoNLL-YAGO", "description": "AIDA CoNLL-YAGO contains assignments of entities to the mentions of named entities annotated for the original CoNLL 2003 entity recognition task. The entities are identified by YAGO2 entity name, by Wikipedia URL, or by Freebase mid. "}, {"id": "te141k", "name": "TE141K", "description": "A new text effects dataset with 141,081 text effect/glyph pairs in total. The dataset consists of 152 professionally designed text effects rendered on glyphs, including English letters, Chinese characters, and Arabic numerals. "}, {"id": "ward2icu", "name": "Ward2ICU", "description": "Ward2ICU is a vital signs dataset of inpatients from the general ward. It contains vital signs with class labels indicating patient transitions from the ward to intensive care units"}, {"id": "vlogs", "name": "Vlogs", "description": "Given a  video and its transcript, which human actions are visible in the video?"}, {"id": "cityuhk-x-bev", "name": "CityUHK-X-BEV", "description": "BEV Crowd-Counting dataset extended from CityUHK-X"}, {"id": "sun-seg-easy-unseen", "name": "SUN-SEG-Easy (Unseen)", "description": "The SUN-SEG dataset is a high-quality per-frame annotated VPS dataset, which includes 158,690 frames from the famous SUN dataset. It extends the labels with diverse types, i.e., object mask, boundary, scribble, polygon, and visual attribute. It also introduces the pathological information from the original SUN dataset, including pathological classification labels, location information, and shape information."}, {"id": "cog", "name": "COG", "description": "A configurable visual question and answer dataset (COG) to parallel experiments in humans and animals. COG is much simpler than the general problem of video analysis, yet it addresses many of the problems relating to visual and logical reasoning and memory -- problems that remain challenging for modern deep learning architectures. "}, {"id": "refuge-challenge-retinal-fundus-glaucoma-challenge", "name": "REFUGE Challenge (Retinal Fundus Glaucoma Challenge)", "description": "REFUGE Challenge provides a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one."}, {"id": "disfa-denver-intensity-of-spontaneous-facial-action", "name": "DISFA (Denver Intensity of Spontaneous Facial Action)", "description": "The Denver Intensity of Spontaneous Facial Action (DISFA) dataset consists of 27 videos of 4844 frames each, with 130,788 images in total. Action unit annotations are on different levels of intensity, which are ignored in the following experiments and action units are either set or unset. DISFA was selected from a wider range of databases popular in the field of facial expression recognition because of the high number of smiles, i.e. action unit 12. In detail, 30,792 have this action unit set, 82,176 images have some action unit(s) set and 48,612 images have no action unit(s) set at all."}, {"id": "nisp-nitk-iisc-multilingual-multi-accent-speaker-profiling", "name": "NISP (NITK-IISc Multilingual Multi-accent Speaker Profiling)", "description": "This dataset contains speech recordings along with speaker physical parameters (height, weight, shoulder size, age ) as well as regional information and linguistic information."}, {"id": "run-the-run-dataset", "name": "RUN (The RUN Dataset)", "description": "The RUN dataset  is based on OpenStreetMap (OSM). The map contains rich layers and an abundance of entities of different types. Each entity is complex and can contain (at least) four labels: name, type, is building=y/n, and house number. An entity can spread over several tiles. As the maps do not overlap, only very few entities are shared among them. The RUN dataset aligns NL navigation instructions to coordinates of their corresponding route on the OSM map."}, {"id": "voc-2012-the-pascal-visual-object-classes-challenge-2012", "name": "VOC 2012 (The PASCAL Visual Object Classes Challenge 2012)", "description": "see detailed use case on code implementation of the paper 'Tell Me Where To Look: Guided Attention Inference Networks'"}, {"id": "flickr-cropping-dataset", "name": "Flickr Cropping Dataset", "description": "The Flick Cropping Dataset consists of high quality cropping and pairwise ranking annotations used to evaluate the performance of automatic image cropping approaches."}, {"id": "mtl-aqa", "name": "MTL-AQA", "description": "A new multitask action quality assessment (AQA) dataset, the largest to date, comprising of more than 1600 diving samples; contains detailed annotations for  fine-grained action recognition, commentary generation, and estimating the AQA score. Videos from multiple angles provided wherever available."}, {"id": "imppres", "name": "IMPPRES", "description": "An IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of >25k semiautomatically generated sentence pairs illustrating well-studied pragmatic inference types."}, {"id": "some-like-it-hoax", "name": "Some Like it Hoax", "description": "Some Like it Hoax is a fake news detection dataset consisting of 15,500 Facebook posts and 909,236 users."}, {"id": "musiccaps", "name": "MusicCaps", "description": "MusicCaps is a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts. For each 10-second music clip, MusicCaps provides: "}, {"id": "swax-sense-wax-attack-dataset", "name": "SWAX (Sense Wax Attack dataset)", "description": "Comprised of real human and wax figure images and videos that endorse the problem of face spoofing detection. The dataset consists of more than 1800 face images and 110 videos of 55 people/waxworks, arranged in training, validation and test sets with a large range in expression, illumination and pose variations. "}, {"id": "sede-stack-exchange-data-explorer", "name": "SEDE (Stack Exchange Data Explorer)", "description": "SEDE is a dataset comprised of 12,023 complex and diverse SQL queries and their natural language titles and descriptions, written by real users of the Stack Exchange Data Explorer out of a natural interaction. These pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset. The goal of this dataset is to take a significant step towards evaluation of Text-to-SQL models in a real-world setting. Compared to other Text-to-SQL datasets, SEDE contains at least 10 times more SQL queries templates (queries after canonization and anonymization of values) than other datasets, and has the most diverse set of utterances and SQL queries (in terms of 3-grams) out of all single-domain datasets. SEDE introduces real-world challenges, such as under-specification, usage of parameters in queries, dates manipulation and more."}, {"id": "exams", "name": "EXAMS", "description": "A new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. Collects more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others. EXAMS offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. "}, {"id": "msd-million-song-dataset", "name": "MSD (Million Song Dataset)", "description": "The Million Song Dataset is a freely-available collection of audio features and metadata for a million contemporary popular music tracks."}, {"id": "lidirus-linguistic-diagnostic-for-russian", "name": "LiDiRus (Linguistic Diagnostic for Russian)", "description": "LiDiRus is a diagnostic dataset that covers a large volume of linguistic phenomena, while allowing you to evaluate information systems on a simple test of textual entailment recognition. See more details diagnostics."}, {"id": "microsoft-academic-graph", "name": "Microsoft Academic Graph", "description": "The Microsoft Academic Graph is a heterogeneous graph containing scientific publication records, citation relationships between those publications, as well as authors, institutions, journals, conferences, and fields of study."}, {"id": "avaspeech-smad-avaspeech-smad-a-strongly-labelled-speech-and-music-activity-detection-dataset-with-label-co-occurrence", "name": "AVASpeech-SMAD (AVASpeech-SMAD: A Strongly Labelled Speech and Music Activity Detection Dataset with Label Co-Occurrence)", "description": "We propose a dataset, AVASpeech-SMAD, to assist speech and music activity detection research. With frame-level music labels, the proposed dataset extends the existing AVASpeech dataset, which originally consists of 45 hours of audio and speech activity labels. To the best of our knowledge, the proposed AVASpeech-SMAD is the first open-source dataset that features strong polyphonic labels for both music and speech. The dataset was manually annotated and verified via an iterative cross-checking process. A simple automatic examination was also implemented to further improve the quality of the labels. Evaluation results from two state-of-the-art SMAD systems are also provided as a benchmark for future reference."}, {"id": "tobacco-3482", "name": "Tobacco-3482", "description": "The Tobacco-3482 dataset consists of document images belonging to 10 classes such as letter, form, email, resume, memo, etc. The dataset has 3482 images."}, {"id": "bcnb-early-breast-cancer-core-needle-biopsy-wsi", "name": "BCNB (Early Breast Cancer Core-Needle Biopsy WSI)", "description": "Breast cancer (BC) has become the greatest threat to women\u2019s health worldwide. Clinically, identification of axillary lymph node (ALN) metastasis and other tumor clinical characteristics such as ER, PR, and so on, are important for evaluating the prognosis and guiding the treatment for BC patients."}, {"id": "hvu-holistic-video-understanding", "name": "HVU (Holistic Video Understanding)", "description": "HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios."}, {"id": "mbpp-mostly-basic-python-programming", "name": "MBPP (Mostly Basic Python Programming)", "description": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry-level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases."}, {"id": "shipsear-shipsear-an-underwater-vessel-noise-database", "name": "ShipsEar (ShipsEar: An underwater vessel noise database)", "description": "This contribution presents a database of underwater sounds produced by vessels of various types. Besides sound recordings, the database contains details of the conditions for obtaining each recording: type of vessel, location of the recording equipment, weather conditions, etc. For its realization, a methodology for recording sounds and gathering additional information has been established, that will facilitate its use to the research community, and expanding the number of records in the database in the future."}, {"id": "kanface-kanface-dataset", "name": "KANFace (KANFace Dataset)", "description": "KANFace consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The dataset is manually annotated in terms of identity, exact age, gender and kinship."}, {"id": "imagenet-s-imagenet-semantic-segmentation", "name": "ImageNet-S (ImageNet Semantic Segmentation)", "description": "Powered by the ImageNet dataset, unsupervised learning on large-scale data has made significant advances for classification tasks. There are two major challenges to allowing such an attractive learning modality for segmentation tasks: i) a large-scale benchmark for assessing algorithms is missing; ii) unsupervised shape representation learning is difficult. We propose a new problem of large-scale unsupervised semantic segmentation (LUSS) with a newly created benchmark dataset to track the research progress. Based on the ImageNet dataset, we propose the ImageNet-S dataset with 1.2 million training images and 50k high-quality semantic segmentation annotations for evaluation. Our benchmark has a high data diversity and a clear task objective. We also present a simple yet effective baseline method that works surprisingly well for LUSS. In addition, we benchmark related un/weakly/fully supervised methods accordingly, identifying the challenges and possible directions of LUSS."}, {"id": "kamel-knowledge-analysis-with-multitoken-entities-in-language-models", "name": "KAMEL (Knowledge Analysis with Multitoken Entities in Language Models)", "description": "KAMEL comprises knowledge about 234 relations from Wikidata with a large training, validation, and test dataset. We make sure that all facts are also present in Wikipedia so that they have been seen during the pre-training procedure of the LMs we are probing. Most importantly we overcome the limitations of existing probing datasets by  (1) having a larger variety of knowledge graph relations,  (2) it contains single- and multi-token entities,  (3) we use relations with literals, and  (4) have alternative labels for entities.  (5) Furthermore, we created an evaluation procedure for higher cardinality relations, which was missing in previous works, and  (6) make sure that the dataset can be used for causal LMs."}, {"id": "bigdatasetgan", "name": "BigDatasetGAN", "description": "BigDatasetGAN is a dataset for pixel-wise ImageNet segmentation. It consists of large synthetic datasets from BigGAN & VQGAN."}, {"id": "brightkite", "name": "Brightkite", "description": "Brightkite was once a location-based social networking service provider where users shared their locations by checking-in. The friendship network was collected using their public API, and consists of 58,228 nodes and 214,078 edges. The network is originally directed but the collectors have constructed a network with undirected edges when there is a friendship in both ways. The collectors have also collected a total of 4,491,143 checkins of these users over the period of Apr. 2008 - Oct. 2010."}, {"id": "trec-covid", "name": "TREC-COVID", "description": "TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search."}, {"id": "wildtrack", "name": "Wildtrack", "description": "Wildtrack is a large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated frames for detection at a rate of 2 frames per second. This results in over 40 000 bounding boxes delimiting every person present in the area of interest, for a total of more than 300 individuals."}, {"id": "upiq-unified-photometric-image-quality", "name": "UPIQ (Unified Photometric Image Quality)", "description": "Contains over 4,000 images created by realigning and merging existing HDR and standard-dynamic-range (SDR) datasets."}, {"id": "medmnist-v2", "name": "MedMNIST v2", "description": "MedMNIST v2 is a large-scale MNIST-like collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with the corresponding classification labels, so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various data scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision and machine learning."}, {"id": "swords-stanford-word-substitution-benchmark", "name": "Swords (Stanford Word Substitution benchmark)", "description": "Swords (Standford Word Substitution) is a benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. Swords is composed of context, target word, and substitute triples (c, w, w'), each of which has a score that indicates the appropriateness of the substitute."}, {"id": "isic-2020-challenge-dataset-official-dataset-of-the-siim-isic-melanoma-classification-challenge-2020", "name": "ISIC 2020 Challenge Dataset (Official dataset of the SIIM-ISIC Melanoma Classification Challenge 2020)", "description": "The dataset contains 33,126 dermoscopic training images of unique benign and malignant skin lesions from over 2,000 patients. Each image is associated with one of these individuals using a unique patient identifier. All malignant diagnoses have been confirmed via histopathology, and benign diagnoses have been confirmed using either expert agreement, longitudinal follow-up, or histopathology. A thorough publication describing all features of this dataset is available in the form of a pre-print that has not yet undergone peer review."}, {"id": "sst-stanford-sentiment-treebank", "name": "SST (Stanford Sentiment Treebank)", "description": "The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges."}, {"id": "stylized-imagenet", "name": "Stylized ImageNet", "description": "The Stylized-ImageNet dataset is created by removing local texture cues in ImageNet while retaining global shape information on natural images via AdaIN style transfer. This nudges CNNs towards learning more about shapes and less about local textures."}, {"id": "vggface2", "name": "VGGFace2", "description": "The VGGFace2 dataset is made of around 3.31 million images divided into 9131 classes, each representing a different person identity. The dataset is divided into two splits, one for the training and one for test. The latter contains around 170000 images divided into 500 identities while all the other images belong to the remaining 8631 classes available for training. While constructing the datasets, the authors focused their efforts on reaching a very low label noise and a high pose and age diversity thus, making the VGGFace2 dataset a suitable choice to train state-of-the-art deep learning models on face-related tasks. The images of the training set have an average resolution of 137x180 pixels, with less than 1% at a resolution below 32 pixels (considering the shortest side)."}, {"id": "lrw-lip-reading-in-the-wild", "name": "LRW (Lip Reading in the Wild)", "description": "The Lip Reading in the Wild (LRW) dataset  a large-scale audio-visual database that contains 500 different words from over 1,000 speakers. Each utterance has 29 frames, whose boundary is centered around the target word. The database is divided into training, validation and test sets. The training set contains at least 800 utterances for each class while the validation and test sets contain 50 utterances."}, {"id": "google-speech-commands-musan", "name": "Google Speech Commands - Musan", "description": "This noisy speech test set is created from the Google Speech Commands v2 [1] and the Musan dataset[2]. "}, {"id": "biwi-kinect-head-pose", "name": "Biwi Kinect Head Pose", "description": "Biwi Kinect Head Pose is a challenging dataset mainly inspired by the automotive setup.  It is acquired with the Microsoft Kinect sensor, a structured IR light device. It contains about 15k frame, with RGB. (640 \u00d7 480) and depth maps (640 \u00d7 480). Twenty subjects have been involved in the recordings: four of them were recorded twice, for a total of 24 sequences. The ground truth of yaw, pitch and roll angles is reported together with the head center and the calibration matrix. "}, {"id": "obstacle-tower", "name": "Obstacle Tower", "description": "Obstacle Tower is a high fidelity, 3D, 3rd person, procedurally generated environment for reinforcement learning. An agent playing Obstacle Tower must learn to solve both low-level control and high-level planning problems in tandem while learning from pixels and a sparse reward signal. Unlike other benchmarks such as the Arcade Learning Environment, evaluation of agent performance in Obstacle Tower is based on an agent\u2019s ability to perform well on unseen instances of the environment."}, {"id": "mimic-cxr", "name": "MIMIC-CXR", "description": "MIMIC-CXR from Massachusetts Institute of Technology presents 371,920 chest X-rays associated with 227,943 imaging studies from 65,079 patients. The studies were performed at Beth Israel Deaconess Medical Center in Boston, MA."}, {"id": "iam-dataset-a-comprehensive-and-large-scale-dataset-for-integrated-argument-mining-tasks", "name": "IAM Dataset (A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks)", "description": "We introduce a large and comprehensive dataset to facilitate the study of several essential AM tasks in the debating system. In our work, we first review the existing subtasks (claim extraction, stance classification, evidence extraction), and then propose two integrated argument mining tasks: claim extraction with stance classification (CESC) and claim-evidence pair extraction (CEPE)."}, {"id": "1000-hours-american-english-colloquial-video-speech-data", "name": "1,000 Hours - American English Colloquial Video Speech Data", "description": "Description\uff1a 1,000 Hours - American English Colloquial Video Speech Data, collected from real website, covering multiple fields. Various attributes such as text content and speaker identity are annotated. This data set can be used for voiceprint recognition model training, construction of corpus for machine translation and algorithm research."}, {"id": "complexwebquestions", "name": "ComplexWebQuestions", "description": "ComplexWebQuestions is a dataset for answering complex questions that require reasoning over multiple web snippets. It contains a large set of complex questions in natural language, and can be used in multiple ways:"}, {"id": "audio-demo-files", "name": "Audio demo files", "description": "Audio files that supplement \"Treatise on Hearing: The Temporal Auditory Imaging Theory Inspired by Optics and Communication\"."}, {"id": "clicr", "name": "CliCR", "description": "CliCR is a new dataset for domain specific reading comprehension used to construct around 100,000 cloze queries from clinical case reports."}, {"id": "wikicatsum", "name": "WikiCatSum", "description": "WikiCatSum is a domain specific Multi-Document Summarisation (MDS) dataset. It assumes the summarisation task of generating Wikipedia lead sections for Wikipedia entities of a certain domain (e.g. Companies) from the set of documents cited in Wikipedia articles or returned by Google (using article titles as queries). The dataset includes three domains: Companies, Films, and Animals."}, {"id": "yoga-82", "name": "Yoga-82", "description": "Dataset for large-scale yoga pose recognition with 82 classes."}, {"id": "off-hard-parallel-smac-off-hard-parallel-20", "name": "Off_Hard_parallel (SMAC+_Off_Hard_parallel_20)", "description": "smac+ offensive hard scenario with 20 parallel episodic buffer."}, {"id": "openimages-v6", "name": "OpenImages-v6", "description": "OpenImages V6 is a large-scale dataset , consists of 9 million training images, 41,620 validation samples, and 125,456 test samples. It is a partially annotated dataset, with 9,600 trainable classes"}, {"id": "live-fb-lsvq-live-fb-large-scale-social-video-quality-lsvq-database", "name": "LIVE-FB LSVQ (LIVE-FB Large-Scale Social Video Quality (LSVQ) Database)", "description": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, \"in-the-wild\" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 real-world distorted videos and 117, 000 space-time localized video patches (\"v-patches\"), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process."}, {"id": "imdb-movie-reviews", "name": "IMDb Movie Reviews", "description": "The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data."}, {"id": "epillid", "name": "ePillID", "description": "ePillID is a benchmark for developing and evaluating computer vision models for pill identification. The ePillID benchmark is designed as a low-shot fine-grained benchmark, reflecting real-world challenges for developing image-based pill identification systems. The characteristics of the ePillID benchmark include: * Reference and consumer images: The reference images are taken with controlled lighting and backgrounds, and with professional equipment. The consumer images are taken with real-world settings including different lighting, backgrounds, and equipment. For most of the pills, one image per side (two images per pill type) is available from the NIH Pillbox dataset. * Low-shot and fine-grained setting: 13k images representing 9804 appearance classes (two sides for 4902 pill types). For most of the appearance classes, there exists only one reference image, making it a challenging low-shot recognition setting."}, {"id": "unite-the-people", "name": "Unite the People", "description": "Unite The People is a dataset for 3D body estimation. The images come from the Leeds Sports Pose dataset and its extended version, as well as the single person tagged people from the MPII Human Pose Dataset. The images are labeled with different types of annotations such as segmentation labels, pose or 3D."}, {"id": "counter", "name": "COUNTER", "description": "The COUNTER (COrpus of Urdu News TExt Reuse) corpus contains 600 source-derived document pairs collected from the field of journalism. It can be used to evaluate mono-lingual text reuse detection systems in general and specifically for Urdu language."}, {"id": "fe108", "name": "FE108", "description": "Large-scale single-object tracking dataset, containing 108 sequences with a total length of 1.5 hours.  FE108 provides ground truth annotations on both the frame and event domain.  The annotation frequency is up to 40Hz and 240Hz for the frame and event domains, respectively.  FE108 is the largest event-frame-based dataset for single object tracking, and also offers the highest annotation frequency in the event domain."}, {"id": "redweb-s", "name": "ReDWeb-S", "description": "ReDWeb-S is a large-scale challenging dataset for Salient Object Detection. It has totally 3179 images with various real-world scenes and high-quality depth maps. The dataset is split into a training set with 2179 RGB-D image pairs and a testing set with the remaining 1000 image pairs."}, {"id": "cityscapes-3d", "name": "Cityscapes 3D", "description": "Detecting vehicles and representing their position and orientation in the three dimensional space is a key technology for autonomous driving. Recently, methods for 3D vehicle detection solely based on monocular RGB images gained popularity. In order to facilitate this task as well as to compare and drive state-of-the-art methods, several new datasets and benchmarks have been published. Ground truth annotations of vehicles are usually obtained using lidar point clouds, which often induces errors due to imperfect calibration or synchronization between both sensors. To this end, we propose Cityscapes 3D, extending the original Cityscapes dataset with 3D bounding box annotations for all types of vehicles. In contrast to existing datasets, our 3D annotations were labeled using stereo RGB images only and capture all nine degrees of freedom. This leads to a pixel-accurate reprojection in the RGB image and a higher range of annotations compared to lidar-based approaches. In order to ease multitask learning, we provide a pairing of 2D instance segments with 3D bounding boxes. In addition, we complement the Cityscapes benchmark suite with 3D vehicle detection based on the new annotations as well as metrics presented in this work. Dataset and benchmark are available online."}, {"id": "pnt-parsing-time-normalizations", "name": "PNT (Parsing Time Normalizations)", "description": "The Parsing Time Normalizations (PNT) corpus in SCATE format allows the representation of a wider variety of time expressions than previous approaches. This corpus was release with SemEval 2018 Task 6."}, {"id": "oxford105k", "name": "Oxford105k", "description": "Oxford105k is the combination of the Oxford5k dataset and 99782 negative images crawled from Flickr using 145 most popular tags. This dataset is used to evaluate search performance for object retrieval (reported as mAP) on a large scale."}, {"id": "raddet-range-azimuth-doppler-based-radar-dataset", "name": "RADDet (Range-Azimuth-Doppler based Radar Dataset)", "description": "RADDet is a radar dataset that contains radar data in the form of Range-Azimuth-Doppler tensors along with the bounding boxes on the tensor for dynamic road users, category labels, and 2D bounding boxes on the Cartesian Bird-Eye-View range map. It is used to train and evaluate methods for object detection using automotive radars."}, {"id": "tinyface", "name": "TinyFace", "description": "TinyFace is a large scale face recognition benchmark to facilitate the investigation of natively LRFR (Low Resolution Face Recognition) at large scales (large gallery population sizes) in deep learning. The TinyFace dataset consists of 5,139 labelled facial identities given by 169,403 native LR face images (average 20\u00d716 pixels) designed for 1:N recognition test. All the LR faces in TinyFace are collected from public web data across a large variety of imaging scenarios, captured under uncontrolled viewing conditions in pose, illumination, occlusion and background."}, {"id": "deepscores", "name": "DeepScores", "description": "DeepScores contains high quality images of musical scores, partitioned into 300,000 sheets of written music that contain symbols of different shapes and sizes. For advancing the state-of-the-art in small objects recognition, and by placing the question of object recognition in the context of scene understanding."}, {"id": "big", "name": "BIG", "description": "A high-resolution semantic segmentation dataset with 50 validation and 100 test objects. Image resolution in BIG ranges from 2048\u00d71600 to 5000\u00d73600. Every image in the dataset has been carefully labeled by a professional while keeping the same guidelines as PASCAL VOC 2012 without the void region."}, {"id": "covid19-algeria-and-world-dataset", "name": "COVID19-Algeria-and-World-Dataset", "description": "A coronavirus dataset with 98 countries constructed from different reliable sources, where each row represents a country, and the columns represent geographic, climate, healthcare, economic, and demographic factors that may contribute to accelerate/slow the spread of the COVID-19. The assumptions for the different factors are as follows:"}, {"id": "wflw-wider-facial-landmarks-in-the-wild", "name": "WFLW (Wider Facial Landmarks in the Wild)", "description": "The Wider Facial Landmarks in the Wild or WFLW database contains 10000 faces (7500 for training and 2500 for testing) with 98 annotated landmarks. This database also features rich attribute annotations in terms of occlusion, head pose, make-up, illumination, blur and expressions."}, {"id": "the-boston-housing-dataset", "name": "The Boston Housing Dataset", "description": "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. However, these comparisons were primarily done outside of Delve and are thus somewhat suspect. The dataset is small in size with only 506 cases."}, {"id": "wikimatrix", "name": "WikiMatrix", "description": "WikiMatrix is a dataset of parallel sentences in the textual content of Wikipedia for all possible language pairs. The mined data consists of:"}, {"id": "realdof-single-image-defocus-deblurring", "name": "RealDOF (Single Image Defocus deblurring)", "description": "This dataset consists of 50 high resolution image pairs captured by dual-camera setup for single image defocus deblurring . Please note this is not a training set but  a benchmark assessment."}, {"id": "selfie2anime", "name": "selfie2anime", "description": "The selfie dataset contains 46,836 selfie images annotated with 36 different attributes. We only use photos of females as training data and test data. The size of the training dataset is 3400, and that of the test dataset is 100, with the image size of 256 x 256. For the anime dataset, we have firstly retrieved 69,926 animation character images from Anime-Planet1. Among those images, 27,023 face images are extracted by using an anime-face detector2. After selecting only female character images and removing monochrome images manually, we have collected two datasets of female anime face images, with the sizes of 3400 and 100 for training and test data respectively, which is the same numbers as the selfie dataset. Finally, all anime face images are resized to 256 x 256 by applying a CNN-based image super-resolution algorithm."}, {"id": "aml-robot-cutting-dataset", "name": "AML Robot Cutting Dataset", "description": "The AML Robot Cutting Dataset consists of approximately 1500 seconds of real data collected on Kinova Jaco 2 robot retrofitted with a custom end-effector fixture and dremel performing cutting tasks on wood specimens for 5 materials and 5 thicknesses."}, {"id": "viton-hd-high-resolution-viton-zalando-dataset", "name": "VITON-HD (High-Resolution VITON-Zalando Dataset)", "description": "VITON-HD dataset is a dataset for high-resolution (i.e.,  1024x768) virtual try-on of clothing items. Specifically, it consists of 13,679 frontal-view woman and top clothing image pairs."}, {"id": "gen1-detection-prophesee-gen1-automotive-detection-dataset", "name": "GEN1 Detection (Prophesee GEN1 Automotive Detection Dataset)", "description": "Prophesee\u2019s GEN1 Automotive Detection Dataset is the largest Event-Based Dataset to date."}, {"id": "jft-3b", "name": "JFT-3B", "description": "JFT-3B is an internal Google dataset and a larger version of the JFT-300M dataset. It consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic pipeline. In other words, the data and associated labels are noisy."}, {"id": "summe", "name": "SumMe", "description": "The SumMe dataset is a video summarization dataset consisting of 25 videos, each annotated with at least 15 human summaries (390 in total)."}, {"id": "pace-2016-feedback-vertex-set-pace-2016-track-b-feedback-vertex-set", "name": "PACE 2016 Feedback Vertex Set (PACE 2016 Track B, Feedback Vertex Set)", "description": "This is the dataset used in the PACE 2016 challenge, Track B, which was computing minimal Feedback Vertex Set. This competition focused on exact solutions, i.e. provably minimal feedback vertex sets (and no heuristic solutions). This should not be confused with the PACE 2022 challenge, which focused on directed feedback vertex set, and has its own entries on PapersWithCode (exact and heuristic)."}, {"id": "minihack", "name": "MiniHack", "description": "MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforcement Learning (RL). MiniHack includes a collection of example environments that can be used to test various capabilities of RL agents, as well as serve as building blocks for researchers wishing to develop their own environments. MiniHack's navigation tasks challenge the agent to reach the goal position by overcoming various difficulties on their way, such as fighting monsters in corridors, crossing a river by pushing boulders into it, navigating through complex, procedurally generated mazes, etc. MiniHack's skill acquisition tasks enable utilising the rich diversity of NetHack objects, monsters and dungeon features, and the interactions between them. The skill acquisition tasks feature a large action space (75 actions), where the actions are instantiated differently depending on which object they are acting on."}, {"id": "grailqa-strongly-generalizable-question-answering", "name": "GrailQA (Strongly Generalizable Question Answering)", "description": "GrailQA is a new large-scale, high-quality dataset for question answering on knowledge bases (KBQA) on Freebase with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: i.i.d., compositional, and zero-shot."}, {"id": "sarc", "name": "SARC", "description": "This dataset was designed for contextual investigations, with related works making considerable usage of said context. The dataset was constructed by scraping Reddit comments; with sarcastic entries being self-annotated by authors through the use of the \\s token, which indicates sarcastic intent on the website. Posts on Reddit are often in response to another comment; SARC incorporates this information through the addition of the parent comment and further child comments surrounding a post. "}, {"id": "egodexter", "name": "EgoDexter", "description": "The EgoDexter dataset provides both 2D and 3D pose annotations for 4 testing video sequences with 3190 frames. The videos are recorded with body-mounted camera from egocentric viewpoints and contain cluttered backgrounds, fast camera motion, and complex interactions with various objects. Fingertip positions were manually annotated for 1485 out of 3190 frames."}, {"id": "aces-a-translation-accuracy-challenge-set", "name": "ACES (A Translation Accuracy Challenge Set)", "description": "ACES a dataset consisting of 68 phenomena ranging from simple perturbations at the word/character level to more complex errors based on discourse and real-world knowledge. It can be used to evaluate a wide range of Machine Translation metrics."}, {"id": "gowalla", "name": "Gowalla", "description": "Gowalla is a location-based social networking website where users share their locations by checking-in. The friendship network is undirected and was collected using their public API, and consists of 196,591 nodes and 950,327 edges. We have collected a total of 6,442,890 check-ins of these users over the period of Feb. 2009 - Oct. 2010."}, {"id": "draw-1k-diverse-algebra-word-problem-set", "name": "DRAW-1K (Diverse Algebra Word Problem Set)", "description": "DRAW-1K is a dataset consisting of 1000 algebra word problems, semiautomatically annotated for the evaluation of automatic solvers. DRAW includes gold coefficient alignments that are necessary uniquely identify the derivation of an equation system."}, {"id": "tilde-model-corpus-tilde-multilingual-open-data-for-european-languages", "name": "Tilde MODEL Corpus (Tilde Multilingual Open Data for European Languages)", "description": "Tilde MODEL Corpus is a multilingual corpora for European languages \u2013 particularly focused on the smaller languages. The collected resources have been cleaned, aligned, and formatted into a corpora standard TMX format useable for developing new Language technology products and services."}, {"id": "abuseanalyzer-dataset", "name": "AbuseAnalyzer Dataset", "description": "The dataset contains 7,601 Gab posts classified on three different aspects: abuse presence or not, abuse severity and abuse target."}, {"id": "arcd", "name": "ARCD", "description": "Composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). "}, {"id": "quasar-s-question-answering-by-search-and-reading-stack-overflow", "name": "QUASAR-S (QUestion Answering by Search And Reading \u2013 Stack Overflow)", "description": "QUASAR-S is a large-scale dataset aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. It consists of 37,362 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The answer to each question is restricted to be another software entity, from an output vocabulary of 4874 entities."}, {"id": "pemsd7", "name": "PeMSD7", "description": "PeMSD7 is traffic data in District 7 of California consisting of the traffic speed of 228 sensors while the period is from May to June in 2012 (only weekdays) with a time interval of 5 minutes. This dataset is popular for benchmark the traffic forecasting models."}, {"id": "msu-video-super-resolution-benchmark-detail-restoration", "name": "MSU Video Super Resolution Benchmark: Detail Restoration", "description": "This is a dataset for a video super-resolution task. The dataset contains the most complex content for the restoration task: faces, text, QR-codes, car numbers, unpatterned textures, small details. Videos include different types of motion and different types of degradation: bicubic interpolation (BI) and Gaussian blurring and downsampling (BD). The resolution of all input video sequences is 480x320."}, {"id": "wikireading", "name": "WikiReading", "description": "WikiReading is a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs)."}, {"id": "a-wn18rr", "name": "A-WN18RR", "description": "This dataset is based on WN18RR and a pre-trained language-model-based KGE. The main task is to add the new knowledge that the pre-trained model didn't see in the previous training stage. The model can be downloaded from here."}, {"id": "olives-dataset-ophthalmic-labels-for-investigating-visual-eye-semantics", "name": "OLIVES Dataset (Ophthalmic Labels for Investigating Visual Eye Semantics)", "description": "Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. While the clinical labels, fundus images and OCT scans are instrumental measurements, the vectorized biomarkers are interpreted attributes from the other measurements. Clinical practitioners use all these data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between these relevant data modalities. Existing datasets are limited in that: (i) they view the problem as disease prediction without assessing biomarkers, and (ii) they do not consider the explicit relationship among all four data modalities over the treatment period. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitations. This is the first OCT and fundus dataset that includes clinical labels, biomarker labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 fundus eye images each with 49 OCT scans, and 16 biomarkers, along with 3 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. OLIVES dataset has advantages in other fields of machine learning research including self-supervised learning as it provides alternate augmentation schemes that are medically grounded."}, {"id": "movingfashion", "name": "MovingFashion", "description": "MovingFashion is a dataset for video-to-shop, the task of retrieving clothes which are worn in social media videos. MovingFashion is composed of 14,855 social videos, each one of them associated with e-commerce \"shop\" images where the corresponding clothing items are clearly portrayed."}, {"id": "docbank", "name": "DocBank", "description": "A benchmark dataset that contains 500K document pages with fine-grained token-level annotations for document layout analysis. DocBank is constructed using a simple yet effective way with weak supervision from the \\LaTeX{} documents available on the arXiv.com."}, {"id": "codrep", "name": "CodRep", "description": "Five curated datasets of one-liner commits from open-source projects. In total, they are composed of 58069 one-liner commits."}, {"id": "prostate-mri-segmentation-dataset", "name": "Prostate MRI Segmentation Dataset", "description": "This prostate MRI segmentation dataset is collected from six different data sources."}, {"id": "gda-gene-disease-associations-corpus", "name": "GDA (Gene-Disease Associations Corpus)", "description": "The gene-disease associations corpus contains 30,192 titles and abstracts from PubMed articles that have been automatically labelled for genes, diseases and gene-disease associations via distant supervision. The test set is comprised of 1000 of these examples. It is common to hold out a random 20% of the examples in the train set as a validation set."}, {"id": "epie", "name": "EPIE", "description": "Corpus containing 25206 sentences labelled with lexical instances of 717 idiomatic expressions. These spans also cover literal usages for the given set of idiomatic expressions. "}, {"id": "inaturalist", "name": "iNaturalist", "description": "The iNaturalist 2017 dataset (iNat) contains 675,170 training and validation images from 5,089 natural fine-grained categories. Those categories belong to 13 super-categories including Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal), and so on. The iNat dataset is highly imbalanced with dramatically different number of images per category. For example, the largest super-category \u201cPlantae (Plant)\u201d has 196,613 images from 2,101 categories; whereas the smallest super-category \u201cProtozoa\u201d only has 381 images from 4 categories."}, {"id": "unarxive", "name": "unarXive", "description": "A scholarly data set with publications\u2019 full-text, annotated in-text citations, and links to metadata."}, {"id": "cruw", "name": "CRUW", "description": "CRUW is a dataset for the radar object detection (ROD) task, which aims to classify and localize the objects in 3D purely from radar's radio frequency (RF) images. The CRUW dataset has a systematic annotation and evaluation system, which involves camera RGB images and radar RF images, collected in various driving scenarios."}, {"id": "ecthr-european-court-of-human-rights-cases", "name": "ECtHR (European Court of Human Rights Cases)", "description": "ECtHR is a dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. This dataset comprises 11k ECtHR cases and can be viewed as an enriched version of the ECtHR dataset of Chalkidis et al. (2019), which did not provide ground truth for alleged article violations (articles discussed) and rationales. It is released with silver rationales obtained from references in court decisions, and gold rationales provided by ECHR-experienced lawyers"}, {"id": "aircraft-context-dataset", "name": "Aircraft Context Dataset", "description": "The Aircraft Context Dataset, a composition of two inter-compatible large-scale and versatile image datasets focusing on manned aircraft and UAVs, is intended for training and evaluating classification, detection and segmentation models in aerial domains. Additionally, a set of relevant meta-parameters can be used to quantify dataset variability as well as the impact of environmental conditions on model performance."}, {"id": "iclabel", "name": "ICLabel", "description": "An Independent components (IC) dataset containing spatiotemporal measures for over 200,000 ICs from more than 6,000 EEG recordings."}, {"id": "vocalsound", "name": "VocalSound", "description": "VocalSound is a free dataset consisting of 21,024 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. The VocalSound dataset also contains meta-information such as speaker age, gender, native language, country, and health condition."}, {"id": "turkcorpus", "name": "TurkCorpus", "description": "TurkCorpus, a dataset with 2,359 original sentences from English Wikipedia, each with 8 manual reference simplifications. The dataset is divided into two subsets: 2,000 sentences for validation and 359 for testing of sentence simplification models."}, {"id": "drive-act", "name": "Drive&Act", "description": "The Drive&Act dataset is a state of the art multi modal benchmark for driver behavior recognition. The dataset includes 3D skeletons in addition to frame-wise hierarchical labels of 9.6 Million frames captured by 6 different views and 3 modalities (RGB, IR and depth)."}, {"id": "deic-benchmark-data-efficient-image-classification-benchmark", "name": "DEIC Benchmark (Data-Efficient Image Classification Benchmark)", "description": "DEIC is a benchmark for measuring the data efficiency of models in the context of image classification. It is composed of 6 datasets that contain a small number of training samples per class (i.e., 30 < x < 80). It covers multiple image domains (i.e., natural images, fine-grained recognition, medical images, remote sensing, handwriting recognition) and data types (i.e., RGB, grayscale, multi-spectral)."}, {"id": "bair-robot-pushing", "name": "BAIR Robot Pushing", "description": "Dataset of 64x64 images of a robot pushing objects on a table top. From Berkeley AI Research (BAIR)."}, {"id": "personaldialog", "name": "PersonalDialog", "description": "PersonalDialog is a large-scale multi-turn dialogue dataset containing various traits from a large number of speakers. The dataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers. Each utterance is associated with a speaker who is marked with traits like Age, Gender, Location, Interest Tags, etc. Several anonymization schemes are designed to protect the privacy of each speaker. "}, {"id": "endomapper", "name": "Endomapper", "description": "The Endomapper dataset is the first collection of complete endoscopy sequences acquired during regular medical practice, including slow and careful screening explorations, making secondary use of medical data. Its original purpose is to facilitate the development and evaluation of VSLAM (Visual Simultaneous Localization and Mapping) methods in real endoscopy data. The first release of the dataset is composed of 50 sequences with a total of more than 13 hours of video. It is also the first endoscopic dataset that includes both the computed geometric and photometric endoscope calibration as well as the original calibration videos. Meta-data and annotations associated to the dataset varies from anatomical landmark and description of the procedure labeling, tools segmentation masks, COLMAP 3D reconstructions, simulated sequences with groundtruth and meta-data related to special cases, such as sequences from the same patient. This information will improve the research in endoscopic VSLAM, as well as other research lines, and create new research lines."}, {"id": "veremi", "name": "VeReMi", "description": "The Vehicular Reference Misbehavior (VeReMi) dataset, is a dataset for the evaluation of misbehavior detection mechanisms for VANETs (vehicular networks). This dataset consists of message logs of on-board units, including a labelled ground truth, generated from a simulation environment. The dataset includes malicious messages intended to trigger incorrect application behavior, which is what misbehavior detection mechanisms aim to prevent. The initial dataset contains a number of simple attacks: the idea of this dataset release is not just to provide a baseline for the comparison of detection mechanisms, but also to serve as a starting point for more complex attacks."}, {"id": "amharic-english-parallel-corpus-for-machine-translation", "name": "Amharic - English Parallel Corpus for Machine Translation", "description": "Amharic - English Parallel Corpus for Machine Translation contains 33,955 sentence pairs extracted text from such news platforms as Ethiopian Press Agency1, Fana Broadcasting Corporate2, and Walta Information Center3. As the data we used is from different sources, it includes various domains such as religious (Bible and Quran), politics, economics, sports, news, among others."}, {"id": "wic-words-in-context", "name": "WiC (Words in Context)", "description": "WiC is a benchmark for the evaluation of context-sensitive word embeddings. WiC is framed as a binary classification task. Each instance in WiC has a target word w, either a verb or a noun, for which two contexts are provided. Each of these contexts triggers a specific meaning of w. The task is to identify if the occurrences of w in the two contexts correspond to the same meaning or not. In fact, the dataset can also be viewed as an application of Word Sense Disambiguation in practise."}, {"id": "panoptop31k", "name": "PanopTOP31K", "description": "Starting from the Panoptic Dataset, we use the PanopTOP framework to generate the PanopTOP31K dataset, consisting of 31K images from 23 different subjects recorded from diverse and challenging viewpoints, also including the top-view."}, {"id": "oxford-robotcar-dataset", "name": "Oxford RobotCar Dataset", "description": "The Oxford RobotCar Dataset contains over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks."}, {"id": "moviegraphs", "name": "MovieGraphs", "description": "Provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions."}, {"id": "mechanical-mnist-crack-path", "name": "Mechanical MNIST Crack Path", "description": "The Mechanical MNIST Crack Path dataset contains Finite Element simulation results from phase-field models of quasi-static brittle fracture in heterogeneous material domains subjected to prescribed loading and boundary conditions. For all samples, the material domain is a square with a side length of $1$. There is an initial crack of fixed length ($0.25$) on the left edge of each domain. The bottom edge of the domain is fixed in $x$ (horizontal) and $y$ (vertical), the right edge of the domain is fixed in $x$ and free in $y$, and the left edge is free in both $x$ and $y$. The top edge is free in $x$, and in $y$ it is displaced such that, at each step, the displacement increases linearly from zero at the top right corner to the maximum displacement on the top left corner. Maximum displacement starts at $0.0$ and increases to $0.02$ by increments of $0.0001$ ($200$ simulation steps in total). The heterogeneous material distribution is obtained by adding rigid circular inclusions to the domain using the Fashion MNIST bitmaps as the reference location for the center of the inclusions. Specifically, each center point location is generated randomly inside a square region defined by the corresponding Fashion MNIST pixel when the pixel has an intensity value higher than $10$. In addition, a minimum center-to-center distance limit of $0.0525$ is applied while generating these center points for each sample. The values of Young\u2019s Modulus $(E)$, Fracture Toughness $(G_f)$, and Failure Strength $(f_t)$ near each inclusion are increased with respect to the background domain by a variable rigidity ratio $r$. The background value for $E$ is $210000$, the background value for $G_f$ is $2.7$, and the background value for $f_t$ is $2445.42$. The rigidity ratio throughout the domain depends on position with respect to all inclusion centers such that the closer a point is to the inclusion center the higher the rigidity ratio will be. We note that the full algorithm for constructing the heterogeneous material property distribution is included in the simulations scripts shared on GitHub. The following information is included in our dataset: (1) A rigidity ratio array to capture heterogeneous material distribution reported over a uniform $64\\times64$ grid, (2) the damage field at the final level of applied displacement reported over a uniform $256\\times256$ grid, and (3) the force-displacement curves for each simulation. All simulations are conducted with the FEniCS computing platform (https://fenicsproject.org). The code to reproduce these simulations is hosted on GitHub (https://github.com/saeedmhz/phase-field)."}, {"id": "darktrack2021", "name": "DarkTrack2021", "description": "DarkTrack2021 is a challenging nighttime UAV tracking benchmark, which contains 110 challenging sequences with over 100 K frames in total."}, {"id": "kaist-multispectral-pedestrian-detection-benchmark", "name": "KAIST Multispectral Pedestrian Detection Benchmark", "description": "KAIST Multispectral Pedestrian Dataset"}, {"id": "pascal3d", "name": "PASCAL3D+", "description": "The Pascal3D+ multi-view dataset consists of images in the wild, i.e., images of object categories exhibiting high variability, captured under uncontrolled settings, in cluttered scenes and under many different poses. Pascal3D+ contains 12 categories of rigid objects selected from the PASCAL VOC 2012 dataset. These objects are annotated with pose information (azimuth, elevation and distance to camera). Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset."}, {"id": "nuscenes-c", "name": "nuScenes-C", "description": "nuScenes-C is an evaluation benchmark heading toward robust and reliable 3D perception in autonomous driving. With it, we probe the robustness of 3D detectors and segmentors under out-of-distribution (OoD) scenarios against corruptions that occur in the real-world environment. Specifically, we consider natural corruptions happen in the following cases:"}, {"id": "parapat-parallel-corpus-of-patents-abstracts", "name": "ParaPat (Parallel Corpus of Patents Abstracts)", "description": "A parallel corpus from the open access Google Patents dataset in 74 language pairs, comprising more than 68 million sentences and 800 million tokens. Sentences were automatically aligned using the Hunalign algorithm for the largest 22 language pairs, while the others were abstract (i.e. paragraph) aligned."}, {"id": "hirid", "name": "HiRID", "description": "HiRID is a freely accessible critical care dataset containing data relating to almost 34 thousand patient admissions to the Department of Intensive Care Medicine of the Bern University Hospital, Switzerland (ICU), an interdisciplinary 60-bed unit admitting >6,500 patients per year. The ICU offers the full range of modern interdisciplinary intensive care medicine for adult patients. The dataset was developed in cooperation between the Swiss Federal Institute of Technology (ETH) Z\u00fcrich, Switzerland and the ICU."}, {"id": "memexqa", "name": "MemexQA", "description": "A large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers."}, {"id": "touchdown-dataset", "name": "Touchdown Dataset", "description": "Touchdown is a corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments. The task is to follow instruction to a goal position and there find a hidden object, Touchdown the bear."}, {"id": "medical-segmentation-decathlon", "name": "Medical Segmentation Decathlon", "description": "The Medical Segmentation Decathlon is a collection of medical image segmentation datasets. It contains a total of 2,633 three-dimensional images collected across multiple anatomies of interest, multiple modalities and multiple sources. Specifically, it contains data for the following body organs or parts: Brain, Heart, Liver, Hippocampus, Prostate, Lung, Pancreas, Hepatic Vessel, Spleen and Colon."}, {"id": "d3d-hoi", "name": "D3D-HOI", "description": "D3D-HOI is a dataset of monocular videos with ground truth annotations of 3D object pose, shape and part motion during human-object interactions. The dataset consists of several common articulated objects captured from diverse real-world scenes and camera viewpoints. Each manipulated object (e.g., microwave oven) is represented with a matching 3D parametric model. This data allows researchers to evaluate the reconstruction quality of articulated objects and establish a benchmark for this challenging task."}, {"id": "tickettalk", "name": "TicketTalk", "description": "A movie ticketing dialog dataset with 23,789 annotated conversations. The movie ticketing conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns. In qualitative human evaluations, model-generated responses trained on just 10,000 TicketTalk dialogs were rated to \"make sense\" 86.5 percent of the time, almost the same as human responses in the same contexts."}, {"id": "charades-ego", "name": "Charades-Ego", "description": "Contains 68,536 activity instances in 68.8 hours of first and third-person video, making it one of the largest and most diverse egocentric datasets available. Charades-Ego furthermore shares activity classes, scripts, and methodology with the Charades dataset, that consist of additional 82.3 hours of third-person video with 66,500 activity instances. "}, {"id": "long-term-visual-localization", "name": "Long-term visual localization", "description": "Long-term visual localization provides a benchmark datasets aimed at evaluating 6 DoF pose estimation accuracy over large appearance variations caused by changes in seasonal (summer, winter, spring, etc.) and illumination (dawn, day, sunset, night) conditions. Each dataset consists of a set of reference images, together with their corresponding ground truth poses, and a set of query images."}, {"id": "real-rain-dataset", "name": "Real Rain Dataset", "description": "A large-scale dataset of ~29.5K rain/rain-free image pairs that covers a wide range of natural rain scenes."}, {"id": "dl-hard-deep-learning-hard", "name": "DL-HARD (Deep Learning Hard)", "description": "Deep Learning Hard  (DL-HARD) is an annotated dataset designed to more effectively evaluate neural ranking models on complex topics. It builds on TREC Deep Learning (DL) questions extensively annotated with query intent categories, answer types, wikified entities, topic categories, and result type metadata from a leading web search engine."}, {"id": "referring-expressions-for-davis-2016-2017", "name": "Referring Expressions for DAVIS 2016 & 2017", "description": "Our task is to localize and provide a pixel-level mask of an object on all video frames given a language referring expression obtained either by looking at the first frame only or the full video. To validate our approach we employ two popular video object segmentation datasets, DAVIS16 [38] and DAVIS17 [42]. These two datasets introduce various challenges, containing videos with single or multiple salient objects, crowded scenes, similar looking instances, occlusions, camera view changes, fast motion, etc."}, {"id": "road-road-the-road-event-awareness-dataset-for-autonomous-driving", "name": "ROAD (ROAD: The ROad event Awareness Dataset for Autonomous Driving)", "description": "ROAD is designed to test an autonomous vehicle's ability to detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from the Oxford RobotCar Dataset, annotated with bounding boxes showing the location in the image plane of each road event."}, {"id": "mmdialog", "name": "MMDialog", "description": "MMDialog is a large-scale multi-turn dialogue dataset containing multi-modal open-domain conversations derived from real human-human chat content in social media. MMDialog contains 1.08M dialogue sessions and 1.53M associated images. On average, one dialogue session has 2.59 images, which can be located anywhere at any conversation turn."}, {"id": "chaosnli", "name": "ChaosNLI", "description": "Chaos NLI is a Natural Language Inference (NLI) dataset with 100 annotations per example (for a total of 464,500 annotations) for some existing data points in the development sets of SNLI, MNLI, and Abductive NLI. The dataset provides additional labels for NLI annotations that reflect the distribution of human annotators, instead of picking the majority label as the gold standard label."}, {"id": "ontonotes-4-0-ontonotes-release-4-0", "name": "OntoNotes 4.0 (OntoNotes Release 4.0)", "description": "OntoNotes Release 4.0 contains the content of earlier releases -- OntoNotes Release 1.0 LDC2007T21, OntoNotes Release 2.0 LDC2008T04 and OntoNotes Release 3.0 LDC2009T24 -- and adds newswire, broadcast news, broadcast conversation and web data in English and Chinese and newswire data in Arabic. This cumulative publication consists of 2.4 million words as follows: 300k words of Arabic newswire 250k words of Chinese newswire, 250k words of Chinese broadcast news, 150k words of Chinese broadcast conversation and 150k words of Chinese web text and 600k words of English newswire, 200k word of English broadcast news, 200k words of English broadcast conversation and 300k words of English web text."}, {"id": "mafl-multi-attribute-facial-landmark", "name": "MAFL (Multi-Attribute Facial Landmark)", "description": "The MAFL dataset contains manually annotated facial landmark locations for 19,000 training and 1,000 test images."}, {"id": "multisports", "name": "MultiSports", "description": "Spatio-temporal action detection is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as MultiSports. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action detection by proposing three criteria: (1) multi-person scenes and motion dependent identification, (2) with well-defined boundaries, (3) relatively fine-grained classes of high complexity. Based on these guidelines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting 3200 video clips, and annotating 37701 action instances with 902k bounding boxes. Our dataset is characterized with important properties of high diversity, dense annotation, and high quality. Our MultiSports, with its realistic setting and detailed annotations, exposes the intrinsic challenges of spatio-temporal action detection. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action detection in the future."}, {"id": "unitopatho", "name": "UNITOPATHO", "description": "Histopathological characterization of colorectal polyps allows to tailor patients' management and follow up with the ultimate aim of avoiding or promptly detecting an invasive carcinoma. Colorectal polyps characterization relies on the histological analysis of tissue samples to determine the polyps malignancy and dysplasia grade. Deep neural networks achieve outstanding accuracy in medical patterns recognition, however they require large sets of annotated training images. We introduce UniToPatho, an annotated dataset of 9536 hematoxylin and eosin stained patches extracted from 292 whole-slide images, meant for training deep neural networks for colorectal polyps classification and adenomas grading. The slides are acquired through a Hamamatsu Nanozoomer S210 scanner at 20\u00d7 magnification (0.4415 \u03bcm/px)"}, {"id": "labr-large-scale-arabic-book-reviews", "name": "LABR (Large-Scale Arabic Book Reviews)", "description": "LABR is a large sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. "}, {"id": "n-digit-mnist", "name": "N-Digit MNIST", "description": "N-Digit MNIST is a multi-digit MNIST-like dataset."}, {"id": "tacred-the-tac-relation-extraction-dataset", "name": "TACRED (The TAC Relation Extraction Dataset)", "description": "TACRED is a large-scale relation extraction dataset with 106,264 examples built over newswire and web text from the corpus used in the yearly TAC Knowledge Base Population (TAC KBP) challenges. Examples in TACRED cover 41 relation types as used in the TAC KBP challenges (e.g., per:schools_attended and org:members) or are labeled as no_relation if no defined relation is held. These examples are created by combining available human annotations from the TAC KBP challenges and crowdsourcing."}, {"id": "scisummnet", "name": "ScisummNet", "description": "Large-scale manually-annotated corpus for 1,000 scientific papers (on computational linguistics) for automatic summarization. Summaries for each paper are constructed from the papers that cite that paper and from that paper's abstract."}, {"id": "t-less", "name": "T-LESS", "description": "T-LESS is a dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects."}, {"id": "pastis-panoptic-segmentation-of-satellite-image-times-series", "name": "PASTIS (Panoptic Segmentation of satellite image TImes Series)", "description": "PASTIS is a benchmark dataset for panoptic and semantic segmentation of agricultural parcels from satellite image time series. It is composed of 2433 one square kilometer-patches in the French metropolitan territory for which sequences of satellite observations are assembled into a four-dimensional spatio-temporal tensor. The dataset contains both semantic and instance annotations, assigning to each pixel a semantic label and an instance id. There is an official 5 fold split provided in the dataset's metadata."}, {"id": "agqa-action-genome-question-answering", "name": "AGQA (Action Genome Question Answering)", "description": "Action Genome Question Answering (AGQA) is a benchmark for compositional spatio-temporal reasoning. AGQA contains 192M unbalanced question answer pairs for 9.6K videos. It also contains a balanced subset of 3.9M question answer pairs, 3 orders of magnitude larger than existing benchmarks, that minimizes bias by balancing the answer distributions and types of question structures. "}, {"id": "texas-48-32-20-fixed-splits", "name": "Texas (48%/32%/20% fixed splits)", "description": "Node classification on Texas with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "answersumm", "name": "AnswerSumm", "description": "AnswerSumm is a dataset of 4,631 CQA threads for answer summarization, curated by professional linguists."}, {"id": "sysu-mm01-c", "name": "SYSU-MM01-C", "description": "SYSU-MM01-C is an evaluation set that consists of algorithmically generated corruptions applied to the SYSU-MM01 test-set.  These corruptions consist of Noise: Gaussian, shot, impulse, and speckle; Blur: defocus, frosted glass, motion, zoom, and Gaussian; Weather: snow, frost, fog, brightness, spatter, and rain; Digital: contrast, elastic, pixel, JPEG compression, and saturate. Each corruption has five severity levels, resulting in 100 distinct corruptions."}, {"id": "eth3d", "name": "ETH3D", "description": "ETHD is a multi-view stereo benchmark / 3D reconstruction benchmark that covers a variety of indoor and outdoor scenes. Ground truth geometry has been obtained using a high-precision laser scanner. A DSLR camera as well as a synchronized multi-camera rig with varying field-of-view was used to capture images."}, {"id": "speech-commands", "name": "Speech Commands", "description": "Speech Commands is an audio dataset of spoken words designed to help train and evaluate keyword spotting systems ."}, {"id": "whamr-wham-with-synthetic-reverberated-sources", "name": "WHAMR! (WHAM! with synthetic reverberated sources)", "description": "WHAMR! is a dataset for noisy and reverberant speech separation. It extends WHAM! by introducing synthetic reverberation to the speech sources in addition to the existing noise. Room impulse responses were generated and convolved using pyroomacoustics. Reverberation times were chosen to approximate domestic and classroom environments (expected to be similar to the restaurants and coffee shops where the WHAM! noise was collected), and further classified as high, medium, and low reverberation based on a qualitative assessment of the mixture\u2019s noise recording."}, {"id": "dr-eye-ve", "name": "DR(eye)VE", "description": "DR(eye)VE is a large dataset of driving scenes for which eye-tracking annotations are available. This dataset features more than 500,000 registered frames, matching ego-centric views (from glasses worn by drivers) and car-centric views (from roof-mounted camera), further enriched by other sensors measurements."}, {"id": "apolloscape", "name": "ApolloScape", "description": "ApolloScape is a large dataset consisting of over 140,000 video frames (73 street scene videos) from various locations in China under varying weather conditions. Pixel-wise semantic annotation of the recorded data is provided in 2D, with point-wise semantic annotation in 3D for 28 classes. In addition, the dataset contains lane marking annotations in 2D."}, {"id": "malaria-dataset", "name": "Malaria Dataset", "description": "The dataset contains a total of 27,558 cell images with equal instances of parasitized and uninfected cells."}, {"id": "ikala", "name": "iKala", "description": "The iKala dataset is a singing voice separation dataset that comprises of 252 30-second excerpts sampled from 206 iKala songs (plus 100 hidden excerpts reserved for MIREX data mining contest). The music accompaniment and the singing voice are recorded at the left and right channels respectively. Additionally, the human-labeled pitch contours and timestamped lyrics are provided."}, {"id": "simpledbpediaqa", "name": "SimpleDBpediaQA", "description": "A new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia. "}, {"id": "cuhk02-cuhk-person-re-identification-dataset", "name": "CUHK02 (CUHK Person Re-identification Dataset)", "description": "CUHK02 is a dataset for person re-identification. It contains 1,816 identities from two disjoint camera views. Each identity has two samples per camera view making a total of 7,264 images. It is used for Person Re-identification."}, {"id": "vqg-visual-question-generation", "name": "VQG (Visual Question Generation)", "description": "VQG is a collection of datasets for visual question generation. VQG questions were collected by crowdsourcing the task on Amazon Mechanical Turk (AMT). The authors provided details on the prompt and the specific instructions for all the crowdsourcing tasks in this paper in the supplementary material. The prompt was successful at capturing nonliteral questions. Images were taken from the MSCOCO dataset."}, {"id": "opendialkg", "name": "OpenDialKG", "description": "OpenDialKG contains utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. "}, {"id": "scribblesup-pascal-scribble-dataset", "name": "ScribbleSup (PASCAL-Scribble Dataset)", "description": "The PASCAL-Scribble Dataset is an extension of the PASCAL dataset with scribble annotations for semantic segmentation. The annotations follow two different protocols. In the first protocol, the PASCAL VOC 2012 set is annotated, with 20 object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated, including 10,582 images in the training set and 1,449 images in the validation set. In the second protocol, the 59 object/stuff categories and one background category involved in the PASCAL-CONTEXT dataset are used. Besides the 20 object categories in the first protocol, there are 39 extra categories (snow, tree, ...) included. This protocol is followed to annotate the PASCAL-CONTEXT dataset. 4,998 images in the training set have been annotated."}, {"id": "tncr-dataset-table-net-detection-and-classification-dataset", "name": "TNCR Dataset (Table Net Detection and Classification Dataset)", "description": "We present TNCR, a new table dataset with varying image quality collected from free open source websites. TNCR dataset can be used for table detection in scanned document images and their classification into 5 different classes."}, {"id": "n-cars", "name": "N-CARS", "description": "A large real-world event-based dataset for object classification."}, {"id": "yahoo-answers", "name": "Yahoo! Answers", "description": "The Yahoo! Answers topic classification dataset is constructed using 10 largest main categories. Each class contains 140,000 training samples and 6,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 60,000 in this dataset. From all the answers and other meta-information, we only used the best answer content and the main category information."}, {"id": "airport", "name": "Airport", "description": "The Airport dataset is a dataset for person re-identification which consists of 39,902 images and 9,651 identities across six cameras."}, {"id": "bucc-building-and-using-comparable-corpora", "name": "BUCC (Building and Using Comparable Corpora)", "description": "The BUCC mining task is a shared task on parallel sentence extraction from two monolingual corpora with a subset of them assumed to be parallel, and that has been available since 2016. For each language pair, the shared task provides a monolingual corpus for each language and a gold mapping list containing true translation pairs. These pairs are the ground truth. The task is to construct a list of translation pairs from the monolingual corpora. The constructed list is compared to the ground truth, and evaluated in terms of the F1 measure."}, {"id": "race-reading-comprehension-dataset-from-examinations", "name": "RACE (ReAding Comprehension dataset from Examinations)", "description": "The ReAding Comprehension dataset from Examinations (RACE) dataset is a machine reading comprehension dataset consisting of 27,933 passages and 97,867 questions from English exams, targeting Chinese students aged 12-18. RACE consists of two subsets, RACE-M and RACE-H, from middle school and high school exams, respectively. RACE-M has 28,293 questions and RACE-H has 69,574. Each question is associated with 4 candidate answers, one of which is correct. The data generation process of RACE differs from most machine reading comprehension datasets - instead of generating questions and answers by heuristics or crowd-sourcing, questions in RACE are specifically designed for testing human reading skills, and are created by domain experts."}, {"id": "svt-street-view-text-dataset", "name": "SVT (Street View Text Dataset)", "description": "The Street View Text (SVT) dataset was harvested from Google Street View. Image text in this data exhibits high variability and often has low resolution. In dealing with outdoor street level imagery, we note two characteristics. (1) Image text often comes from business signage and (2) business names are easily available through geographic business searches. These factors make the SVT set uniquely suited for word spotting in the wild: given a street view image, the goal is to identify words from nearby businesses."}, {"id": "york-urban-line-segment-database", "name": "York Urban Line Segment Database", "description": "The York Urban Line Segment Database is a compilation of 102 images (45 indoor, 57 outdoor) of urban environments consisting mostly of scenes from the campus of York University and downtown Toronto, Canada. The images are 640 x 480 in size and have been taken with a calibrated Panasonic Lumix DMC-LC80 digital camera."}, {"id": "icubworld", "name": "ICubWorld", "description": "iCubWorld datasets are collections of images recording the visual experience of iCub while observing objects in its typical environment, a laboratory or an office. The acquisition setting is devised to allow a natural human-robot interaction, where a teacher verbally provides the label of the object of interest and shows it to the robot, by holding it in the hand; the iCub can either track the object while the teacher moves it, or take it in its hand."}, {"id": "atariari-atari-annotated-ram-interface", "name": "AtariARI (Atari Annotated RAM Interface)", "description": "The AtariARI (Atari Annotated RAM Interface) is an environment for representation learning. The Atari Arcade Learning Environment (ALE) does not explicitly expose any ground truth state information. However, ALE does expose the RAM state (128 bytes per timestep) which are used by the game programmer to store important state information such as the location of sprites, the state of the clock, or the current room the agent is in. To extract these variables, the dataset creators consulted commented disassemblies (or source code) of Atari 2600 games which were made available by Engelhardt and Jentzsch and CPUWIZ. The dataset creators were able to find and verify important state variables for a total of 22 games. Once this information was acquired, combining it with the ALE interface produced a wrapper that can automatically output a state label for every example frame generated from the game. The dataset creators make this available with an easy-to-use gym wrapper, which returns this information with no change to existing code using gym interfaces."}, {"id": "bigpatent", "name": "BigPatent", "description": "Consists of 1.3 million records of U.S. patent documents along with human written abstractive summaries."}, {"id": "122-people-passenger-behavior-recognition-data", "name": "122 People - Passenger Behavior Recognition Data", "description": "Description\uff1a 122 People - Passenger Behavior Recognition Data. The data includes multiple age groups, multiple time periods and multiple races (Caucasian, Black, Indian). The passenger behaviors include passenger normal behavior, passenger abnormal behavior(passenger carsick behavior, passenger sleepy behavior, passenger lost items behavior). In terms of device, binocular cameras of RGB and infrared channels were applied. This data can be used for tasks such as passenger behavior analysis."}, {"id": "eth-sfm-eth-structure-from-motion", "name": "ETH SfM (ETH Structure-from-Motion)", "description": "The ETH SfM (structure-from-motion) dataset is a dataset for 3D Reconstruction. The benchmark investigates how different methods perform in terms of building a 3D model from a set of available 2D images."}, {"id": "cuhk-image-cropping", "name": "CUHK Image Cropping", "description": "CUHK Image Cropping  is a dataset for image cropping. The photos are of varying aesthetic quality and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is manually cropped by three expert photographers (graduate students in art whose primary medium is photography) to form three training sets. There are 1,000 photos in the dataset."}, {"id": "cvl-traffic-signs-dataset", "name": "CVL Traffic Signs Dataset", "description": "A video dataset for recognising traffic signs hosted with the first IEEE Video and Image Processing (VIP) Cup within the IEEE Signal Processing Society."}, {"id": "ibims-1-independent-benchmark-images-and-matched-scans-v1", "name": "IBims-1 (Independent benchmark images and matched scans v1)", "description": "iBims-1 (independent Benchmark images and matched scans - version 1) is a new high-quality RGB-D dataset, especially designed for testing single-image depth estimation (SIDE) methods. A customized acquisition setup, composed of a digital single-lens reflex (DSLR) camera and a high-precision laser scanner was used to acquire high-resolution images and highly accurate depth maps of diverse indoors scenarios."}, {"id": "xd-violence", "name": "XD-Violence", "description": "XD-Violence is a large-scale audio-visual dataset for violence detection in videos."}, {"id": "iot-inspector", "name": "IoT Inspector", "description": "IoT Inspector is a large dataset of labeled network traffic from smart home devices from within real-world home networks. It is used to conduct data-driven smart home research. An open source tool with the same name has been used to collect data from 44,956 smart home devices across 13 categories and 53 vendors."}, {"id": "polsf", "name": "PolSF", "description": "Collects five open polarimetric SAR images, which are images of the San Francisco area. These five images come from different satellites at different times, which has great scientific research value. "}, {"id": "the-little-prince-the-little-prince-corpus", "name": "The Little Prince (The Little Prince Corpus)", "description": "This corpus is an annotation of the novel The Little Prince by Antoine de Saint-Exup\u00e9ry, published in 1943. We were inspired by the UNL project to include this novel, so that different groups could compare representations on the same text."}, {"id": "polusa", "name": "POLUSA", "description": "A dataset that represents the online media landscape as perceived by an average US news consumer. The dataset contains 0.9M articles covering policy topics published between Jan. 2017 and Aug. 2019 by 18 news outlets representing the political spectrum. Each outlet is labeled by its political leaning derived using a systematic aggregation of eight data sources. The news dataset is balanced with respect to publication date and outlet popularity. POLUSA enables studying a variety of subjects, e.g., media effects and political partisanship."}, {"id": "objects365", "name": "Objects365", "description": "Objects365 is a large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. "}, {"id": "modelnet40-c-modelnet-c", "name": "ModelNet40-C (ModelNet-C)", "description": "ModelNet40-C is a comprehensive dataset to benchmark the corruption robustness of 3D point cloud recognition."}, {"id": "awa2-animals-with-attributes-2", "name": "AwA2 (Animals with Attributes 2)", "description": "Animals with Attributes 2 (AwA2) is a dataset for benchmarking transfer-learning algorithms, such as attribute base classification and zero-shot learning. AwA2 is a drop-in replacement of original Animals with Attributes (AwA) dataset, with more images released for each category. Specifically, AwA2 consists of in total 37322 images distributed in 50 animal categories. The AwA2 also provides a category-attribute matrix, which contains an 85-dim attribute vector (e.g., color, stripe, furry, size, and habitat) for each category."}, {"id": "trecvid-avs16-iacc-3", "name": "TRECVID-AVS16 (IACC.3)", "description": "Internet Archive videos (IACC.3) under Creative Commons licenses. The test video collection for TRECVID-AVS2016-TRECVID-AVS2018 contains 335,944 web video clips (600hr)."}, {"id": "fc100-fewshot-cifar100", "name": "FC100 (Fewshot-CIFAR100)", "description": "The FC100 dataset (Fewshot-CIFAR100) is a newly split dataset based on CIFAR-100 for few-shot learning. It contains 20 high-level categories which are divided into 12, 4, 4 categories for training, validation and test. There are 60, 20, 20 low-level classes in the corresponding split containing 600 images of size 32 \u00d7 32 per class. Smaller image size makes it more challenging for few-shot learning."}, {"id": "ntu-rgb-d", "name": "NTU RGB+D", "description": "NTU RGB+D is a large-scale dataset for RGB-D human action recognition. It involves 56,880 samples of 60 action classes collected from 40 subjects. The actions can be generally divided into three categories: 40 daily actions (e.g., drinking, eating, reading), nine health-related actions (e.g., sneezing, staggering, falling down), and 11 mutual actions (e.g., punching, kicking, hugging). These actions take place under 17 different scene conditions corresponding to 17 video sequences (i.e., S001\u2013S017). The actions were captured using three cameras with different horizontal imaging viewpoints, namely, \u221245\u2218,0\u2218, and +45\u2218. Multi-modality information is provided for action characterization, including depth maps, 3D skeleton joint position, RGB frames, and infrared sequences. The performance evaluation is performed by a cross-subject test that split the 40 subjects into training and test groups, and by a cross-view test that employed one camera (+45\u2218) for testing, and the other two cameras for training."}, {"id": "reddit-binary", "name": "REDDIT-BINARY", "description": "REDDIT-BINARY consists of graphs corresponding to online discussions on Reddit. In each graph, nodes represent users, and there is an edge between them if at least one of them respond to the other\u2019s comment. There are four popular subreddits, namely, IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are two question/answer based subreddits, and TrollXChromosomes and atheism are two discussion-based subreddits. A graph is labeled according to whether it belongs to a question/answer-based community or a discussion-based community."}, {"id": "pathtrack", "name": "PathTrack", "description": "PathTrack is a dataset for person tracking which contains more than 15,000 person trajectories in 720 sequences."}, {"id": "morph", "name": "MORPH", "description": "MORPH is a facial age estimation dataset, which contains 55,134 facial images of 13,617 subjects ranging from 16 to 77 years old."}, {"id": "sviro-synthetic-vehicle-interior-rear-seat-occupancy-dataset", "name": "SVIRO (Synthetic Vehicle Interior Rear Seat Occupancy Dataset)", "description": "Contains bounding boxes for object detection, instance segmentation masks, keypoints for pose estimation and depth images for each synthetic scenery as well as images for each individual seat for classification. "}, {"id": "imagenet-a", "name": "ImageNet-A", "description": "The ImageNet-A dataset consists of real-world, unmodified, and naturally occurring examples that are misclassified by ResNet models."}, {"id": "vihsd-vietnamese-hate-speech-detection-dataset", "name": "ViHSD (Vietnamese Hate Speech Detection Dataset)", "description": "This dataset contains 33,400 annotated comments used for hate speech detection on social network sites. Label: CLEAN (non hate), OFFENSIVE and HATE"}, {"id": "ljspeech-the-lj-speech-dataset", "name": "LJSpeech (The LJ Speech Dataset)", "description": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours. The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain."}, {"id": "set11", "name": "Set11", "description": "Set11 is a dataset of 11 grayscale images. It is a dataset used for image reconstruction and image compression."}, {"id": "tcc", "name": "TCC", "description": "The largest and most realistic dataset available for TCC. It consists of 600 real-world videos recorded with a high-resolution mobile phone camera shooting 1824 x 1368 sized pictures. The length of these videos ranges from 3 to 17 frames (7.3 on average, the median is 7.0 and mode is 8.5). Ground truth information is present only for the last frame in each video (i.e., the shot frame), and was collected using a gray surface calibration target."}, {"id": "rlv-reinforcement-learning-with-videos", "name": "RLV (Reinforcement Learning with Videos)", "description": "We provide video observations of humans performing two simple tasks  in natural environments.  The tasks are pushing and drawer opening."}, {"id": "armbench", "name": "ARMBench", "description": "ARMBench is a large-scale, object-centric benchmark dataset for robotic manipulation in the context of a warehouse. ARMBench contains images, videos, and metadata that corresponds to 235K+ pick-and-place activities on 190K+ unique objects. The data is captured at different stages of manipulation, i.e., pre-pick, during transfer, and after placement."}, {"id": "enron-email-dataset", "name": "Enron Email Dataset", "description": "This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron, organized into folders. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation."}, {"id": "kitti-masks", "name": "KITTI-Masks", "description": "This Dataset consists of 2120 sequences of binary masks of pedestrians. The sequence length varies between 2-710. For details, we refer to our paper. It is based on the original KITTI Segmentation challenge which can be found at https://www.vision.rwth-aachen.de/page/mots "}, {"id": "writingprompts", "name": "WritingPrompts", "description": "WritingPrompts is a large dataset of 300K human-written stories paired with writing prompts from an online forum. "}, {"id": "argoverse", "name": "Argoverse", "description": "Argoverse is a tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami. Each scenario is a sequence of frames sampled at 10 HZ. Each sequence has an interesting object called \u201cagent\u201d, and the task is to predict the future locations of agents in a 3 seconds future horizon. The sequences are split into training, validation and test sets, which have 205,942, 39,472 and 78,143 sequences respectively. These splits have no geographical overlap."}, {"id": "teach-task-driven-embodied-agents-that-chat", "name": "TEACh (Task-driven Embodied Agents that Chat)", "description": "Robots operating in human spaces must be able to engage in natural language interaction with people, both understanding and executing instructions, and using conversation to resolve ambiguity and recover from mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human--human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\", asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models' abilities in dialogue understanding, language grounding, and task execution."}, {"id": "lapa", "name": "LaPa", "description": "A large-scale Landmark guided face Parsing dataset (LaPa) for face parsing. It consists of more than 22,000 facial images with abundant variations in expression, pose and occlusion, and each image of LaPa is provided with a 11-category pixel-level label map and 106-point landmarks."}, {"id": "seeds", "name": "seeds", "description": "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin."}, {"id": "ncbi-disease", "name": "NCBI Disease", "description": "The NCBI Disease corpus consists of 793 PubMed abstracts, which are separated into training (593), development (100) and test (100) subsets. The NCBI Disease corpus is annotated with disease mentions, using concept identifiers from either MeSH or OMIM."}, {"id": "atypict", "name": "AtyPict", "description": "AtyPict is a dataset of atypical sketch content designed for atypical sketch content detection tasks."}, {"id": "jobstack", "name": "JobStack", "description": "JobStack is a new corpus for de-identification of personal data in job vacancies on Stackoverflow.  De-identification is the task of detecting privacy-related entities in text, such as person names, emails and contact data."}, {"id": "imagenet", "name": "ImageNet", "description": "The ImageNet dataset contains 14,197,122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a benchmark in image classification and object detection. The publicly released dataset contains a set of manually annotated training images. A set of test images is also released, with the manual annotations withheld. ILSVRC annotations fall into one of two categories: (1) image-level annotation of a binary label for the presence or absence of an object class in the image, e.g., \u201cthere are cars in this image\u201d but \u201cthere are no tigers,\u201d and (2) object-level annotation of a tight bounding box and class label around an object instance in the image, e.g., \u201cthere is a screwdriver centered at position (20,25) with width of 50 pixels and height of 30 pixels\u201d. The ImageNet project does not own the copyright of the images, therefore only thumbnails and URLs of images are provided."}, {"id": "si-score-synthetic-interventions-on-scenes-for-robustness-evaluation", "name": "SI-SCORE (Synthetic Interventions on Scenes for Robustness Evaluation)", "description": "A synthetic dataset uses for a systematic analysis across common factors of variation."}, {"id": "2wikimultihopqa", "name": "2WikiMultiHopQA", "description": "Uses structured and unstructured data. The dataset introduces the evidence information containing a reasoning path for multi-hop questions."}, {"id": "jester-gesture-recognition", "name": "Jester (Gesture Recognition)", "description": "Jester Gesture Recognition dataset includes 148,092 labeled video clips of humans performing basic, pre-defined hand gestures in front of a laptop camera or webcam. It is designed for training machine learning models to recognize human hand gestures like sliding two fingers down, swiping left or right and drumming fingers."}, {"id": "stardata", "name": "StarData", "description": "StarData is a StarCraft: Brood War replay dataset, with 65,646 games. The full dataset after compression is 365 GB, 1535 million frames, and 496 million player actions. The entire frame data was dumped out at 8 frames per second."}, {"id": "tr-news", "name": "TR-News", "description": "This dataset is collected from various global and local news sources. Toponyms are manually annotated in the articles with the corresponding entries from GeoNames. In total, the dataset consists of 118 articles."}, {"id": "standardized-project-gutenberg-corpus", "name": "Standardized Project Gutenberg Corpus", "description": "The Standardized Project Gutenberg Corpus (SPGC) is an open science approach to a curated version of the complete PG data containing more than 50,000 books and more than 3\u00d7109 word-tokens."}, {"id": "videoattentiontarget", "name": "VideoAttentionTarget", "description": "A dataset with fully annotated attention targets in video for attention target estimation."}, {"id": "ucf-qnrf", "name": "UCF-QNRF", "description": "The UCF-QNRF dataset is a crowd counting dataset and it contains large diversity both in scenes, as well as in background types. It consists of 1535 images high-resolution images from Flickr, Web Search and Hajj footage. The number of people (i.e., the count) varies from 50 to 12,000 across images."}, {"id": "dips", "name": "DIPS", "description": "Contains biases but is two orders of magnitude larger than those used previously. "}, {"id": "mlb-dataset", "name": "MLB Dataset", "description": "A new dataset on the baseball domain."}, {"id": "coco-qa", "name": "COCO-QA", "description": "COCO-QA is a dataset for visual question answering. It consists of:"}, {"id": "nlc2cmd", "name": "NLC2CMD", "description": "The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line. Participants were tasked with building models that can transform descriptions of command line tasks in English to their Bash syntax."}, {"id": "umdfaces", "name": "UMDFaces", "description": "UMDFaces is a face dataset divided into two parts:"}, {"id": "ppm-100", "name": "PPM-100", "description": "PPM is a portrait matting benchmark with the following characteristics:"}, {"id": "inria-horse", "name": "INRIA-Horse", "description": "The INRIA-Horse dataset consists of 170 horse images and 170 images without horses. All horses in all images are annotated with a bounding-box. The main challenges it offers are clutter, intra-class shape variability, and scale changes. The horses are mostly unoccluded, taken from approximately the side viewpoint, and face the same direction."}, {"id": "cer-smart-metering-project-electricity-customer-behaviour-trial-cer-smart-metering-project-electricity-customer-behaviour-trial-2009-2010-irish-social-science-data-archive-sn-0012-00", "name": "CER Smart Metering Project - Electricity Customer Behaviour Trial (CER Smart Metering Project - Electricity Customer Behaviour Trial, 2009-2010. Irish Social Science Data Archive. SN: 0012-00)", "description": "The CER initiated the Smart Metering Project in 2007 with the purpose of undertaking trials to assess the performance of Smart Meters, their impact on consumers\u2019 energy consumption and the economic case for a wider national rollout. It is a collaborative energy industry-wide project managed by the CER and actively involving energy industry participants including the Sustainable Energy Authority of Ireland (SEAI), the Department of Communications, Energy and Natural Resources (DCENR), ESB Networks, Bord G\u00e1is Networks, Electric Ireland, Bord G\u00e1is Energy and other energy suppliers."}, {"id": "off-superhard-parallel-smac-off-superhard-parallel-20", "name": "Off_Superhard_parallel (SMAC+_Off_Superhard_parallel_20)", "description": "smac+ offensive scenario with 20 parallel episodic buffer."}, {"id": "a-ava-actor-identified-spatiotemporal-action-detection", "name": "A-AVA (Actor-identified Spatiotemporal Action Detection)", "description": "A new Actor-identified A-AVA dataset based on the existing AVA dataset and the TAO dataset, by assigning the unique actor identity and actions to each actor."}, {"id": "300w-300-faces-in-the-wild", "name": "300W (300 Faces-In-The-Wild)", "description": "The 300-W is a face dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as \u201cparty\u201d, \u201cconference\u201d, \u201cprotests\u201d, \u201cfootball\u201d and \u201ccelebrities\u201d. Compared to the rest of in-the-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common \u201cneutral\u201d or \u201csmile\u201d, such as \u201csurprise\u201d or \u201cscream\u201d. Images were annotated with the 68-point mark-up using a semi-automatic methodology. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases. Many images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with [3, 7] faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 \u00d7 292) pixels."}, {"id": "ehr-rel", "name": "EHR-Rel", "description": "EHR-RelB is a benchmark dataset for biomedical concept relatedness, consisting of 3630 concept pairs sampled from electronic health records (EHRs). EHR-RelA is a smaller dataset of 111 concept pairs, which are mainly unrelated."}, {"id": "arid-autonomous-robot-indoor-dataset", "name": "ARID (Autonomous Robot Indoor Dataset)", "description": "ARID is a large-scale, multi-view object dataset collected with an RGB-D camera mounted on a mobile robot."}, {"id": "uc-merced-land-use-dataset", "name": "UC Merced Land Use Dataset", "description": "This is a 21 class land use image dataset meant for research purposes."}, {"id": "nytwit", "name": "NYTWIT", "description": "A collection of over 2,500 novel English words published in the New York Times between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). "}, {"id": "dis5k-dichotomous-image-segmentation-dis-dataset", "name": "DIS5K (Dichotomous Image Segmentation (DIS) Dataset)", "description": "To build the highly accurate Dichotomous Image Segmentation dataset (DIS5K), we first manually collected over 12,000 images from Flickr1 based on our pre-designed keywords. Then, we obtained 5,470 images of 22 groups and 225 categories from the 12,000 images according to the structural complexities of the objects. Each image is then manually labeled with pixel-wise accuracy using GIMP. The labeled targets in DIS5K mainly focus on the \u201cobjects of the images defined by the pre-designed keywords (categories)\u201d regardless of their characteristics e.g., salient, common, camouflaged, meticulous, etc. The average per-image labeling time is \u223c30 minutes and some images cost up to 10 hours."}, {"id": "kuairec", "name": "KuaiRec", "description": "KuaiRec is a real-world dataset collected from the recommendation logs of the video-sharing mobile app Kuaishou. For now, it is the first dataset that contains a fully observed user-item interaction matrix. For the term \u201cfully observed\u201d, we mean there are almost no missing values in the user-item matrix, i.e., each user has viewed each video and then left feedback."}, {"id": "dimo-dataset-of-industrial-metal-objects", "name": "DIMO (Dataset of Industrial Metal Objects)", "description": "The Industrial Metal Objects dataset is a diverse dataset of industrial metal objects. These objects are symmetric, textureless and highly reflective, leading to challenging conditions not captured in existing datasets. The dataset contains both real-world and synthetic multi-view RGB images with 6D object pose labels."}, {"id": "children-s-song-dataset", "name": "Children's Song Dataset", "description": "Children's Song Dataset is open source dataset for singing voice research. This dataset contains 50 Korean and 50 English songs sung by one Korean female professional pop singer. Each song is recorded in two separate keys resulting in a total of 200 audio recordings. Each audio recording is paired with a MIDI transcription and lyrics annotations in both grapheme-level and phoneme-level."}, {"id": "fb15k-237", "name": "FB15k-237", "description": "FB15k-237 is a link prediction dataset created from FB15k. While FB15k consists of 1,345 relations, 14,951 entities, and 592,213 triples, many triples are inverses that cause leakage from the training to testing and validation splits. FB15k-237 was created by Toutanova and Chen (2015) to ensure that the testing and evaluation datasets do not have inverse relation test leakage. In summary, FB15k-237 dataset contains 310,079 triples with 14,505 entities and 237 relation types."}, {"id": "music-avqa", "name": "MUSIC-AVQA", "description": "The large-scale MUSIC-AVQA dataset of musical performance contains 45,867 question-answer pairs, distributed in 9,288 videos for over 150 hours. All QA pairs types are divided into 3 modal scenarios, which contain 9 question types and 33 question templates. Finally, as an open-ended problem of our AVQA tasks, all 42 kinds of answers constitute a set for selection."}, {"id": "pybullet", "name": "PyBullet", "description": "PyBullet is an easy to use Python module for physics simulation, robotics and deep reinforcement learning based on the Bullet Physics SDK. With PyBullet you can load articulated bodies from URDF, SDF and other file formats. PyBullet provides forward dynamics simulation, inverse dynamics computation, forward and inverse kinematics and collision detection and ray intersection queries. Aside from physics simulation, PyBullet supports to rendering, with a CPU renderer and OpenGL visualization and support for virtual reality headsets."}, {"id": "videomatting108", "name": "VideoMatting108", "description": "VideoMatting108 is a large-scale video matting and trimap generation dataset with 80 training and 28 validation foreground video clips with ground-truth alpha mattes."}, {"id": "cmrc-chinese-machine-reading-comprehension-2018", "name": "CMRC (Chinese Machine Reading Comprehension 2018)", "description": "CMRC is a dataset is annotated by human experts with near 20,000 questions as well as a challenging set which is composed of the questions that need reasoning over multiple clues."}, {"id": "pandaset", "name": "PandaSet", "description": "PandaSet is a dataset produced by a complete, high-precision autonomous vehicle sensor kit with a no-cost commercial license. The dataset was collected using one 360x360 mechanical spinning LiDAR, one forward-facing, long-range LiDRAR, and 6 cameras. The datasets contains more than 100 scenes, each of which is 8 seconds long, and provides 28 types of labels for object classification and 37 types of annotations for semantic segmentation."}, {"id": "manysstubs4j", "name": "ManySStuBs4J", "description": "The ManySStuBs4J corpus is a collection of simple fixes to Java bugs, designed for evaluating program repair techniques. We collect all bug-fixing changes using the SZZ heuristic, and then filter these to obtain a data set of small bug fix changes. These are single statement fixes, classified where possible into one of 16 syntactic templates which we call SStuBs. The dataset contains simple statement bugs mined from open-source Java projects hosted in GitHub. There are two variants of the dataset. One mined from the 100 Java Maven Projects and one mined from the top 1000 Java Projects."}, {"id": "cvc-clinicdb", "name": "CVC-ClinicDB", "description": "CVC-ClinicDB is an open-access dataset of 612 images with a resolution of 384\u00d7288 from 31 colonoscopy sequences.It is used for medical image segmentation, in particular polyp detection in colonoscopy videos."}, {"id": "ontogum", "name": "OntoGUM", "description": "OntoGUM is an OntoNotes-like coreference dataset converted from GUM, an English corpus covering 12 genres using deterministic rules."}, {"id": "occluded-dukemtmc", "name": "Occluded-DukeMTMC", "description": "Occluded-DukeMTMC contains 15,618 training images, 17,661 gallery images, and 2,210 occluded query images. The experiment results on Occluded-DukeMTMC will demonstrate the superiority of our method in Occluded Person Re-ID problems, let alone that our method does not need any manually cropping procedure as pre-process."}, {"id": "air-quality-index-air-quality-index-prediction-probelm", "name": "Air Quality Index (Air Quality Index prediction probelm)", "description": "The AQI dataset is collected from 12 observing stations around Beijing from year 2013 to 2017. The data is accessible at The University of California, Irvine (UCI) Machine Learning Repository."}, {"id": "kubric", "name": "Kubric", "description": "Kubric is a data generation pipeline for creating semi-realistic synthetic multi-object videos with rich annotations such as instance segmentation masks, depth maps, and optical flow."}, {"id": "e-snli-ve", "name": "e-SNLI-VE", "description": "e-SNLI-VE is a large VL (vision-language) dataset with NLEs (natural language explanations) with over 430k instances for which the explanations rely on the image content. It has been built by merging the explanations from e-SNLI and the image-sentence pairs from SNLI-VE."}, {"id": "pomo", "name": "PoMo", "description": "PoMo consists of more than 231K sentences with post-modifiers and associated facts extracted from Wikidata for around 57K unique entities."}, {"id": "wiki-40b", "name": "Wiki-40B", "description": "A new multilingual language model benchmark that is composed of 40+ languages spanning several scripts and linguistic families containing round 40 billion characters and aimed to accelerate the research of multilingual modeling."}, {"id": "anetac-arabic-named-entity-transliteration-and-classification", "name": "ANETAC (Arabic Named Entity Transliteration and Classification)", "description": "An English-Arabic named entity transliteration and classification dataset built from freely available parallel translation corpora. The dataset contains 79,924 instances, each instance is a triplet (e, a, c), where e is the English named entity, a is its Arabic transliteration and c is its class that can be either a Person, a Location, or an Organization. The ANETAC dataset is mainly aimed for the researchers that are working on Arabic named entity transliteration, but it can also be used for named entity classification purposes."}, {"id": "arxivpapers", "name": "ArxivPapers", "description": "The ArxivPapers dataset is an unlabelled collection of over 104K papers related to machine learning and published on arXiv.org between 2007\u20132020. The dataset includes around 94K papers (for which LaTeX source code is available) in a structured form in which paper is split into a title, abstract, sections, paragraphs and references. Additionally, the dataset contains over 277K tables extracted from the LaTeX papers."}, {"id": "sen12ms", "name": "SEN12MS", "description": "A dataset consisting of 180,662 triplets of dual-pol synthetic aperture radar (SAR) image patches, multi-spectral Sentinel-2 image patches, and MODIS land cover maps."}, {"id": "samanantar", "name": "Samanantar", "description": "Samanantar is the largest publicly available parallel corpora collection for Indic languages: Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, Telugu. The corpus has 49.6M sentence pairs between English to Indian Languages."}, {"id": "birdsong", "name": "BirdSong", "description": "The BirdSong dataset consists of audio recordings of bird songs at the H. J. Andrews (HJA) Experimental Forest, using unattended microphones. The goal of the dataset is to provide data to automatically identify the species of bird responsible for each utterance in these recordings. The dataset contains 548 10-seconds audio recordings."}, {"id": "vegfru", "name": "VegFru", "description": "VegFru is a domain-specific dataset for fine-grained visual categorization. VegFru categorizes vegetables and fruits according to their eating characteristics, and each image contains at least one edible part of vegetables or fruits with the same cooking usage. Particularly, all the images are labelled hierarchically. The current version covers vegetables and fruits of 25 upper-level categories and 292 subordinate classes. And it contains more than 160,000 images in total and at least 200 images for each subordinate class."}, {"id": "truthfulqa", "name": "TruthfulQA", "description": "TruthfulQA is a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. The authors crafted questions that some humans would answer falsely due to a false belief or misconception."}, {"id": "cityscapes", "name": "Cityscapes", "description": "Cityscapes is a large-scale database which focuses on semantic understanding of urban street scenes. It provides semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces, humans, vehicles, constructions, objects, nature, sky, and void). The dataset consists of around 5000 fine annotated images and 20000 coarse annotated ones. Data was captured in 50 cities during several months, daytimes, and good weather conditions. It was originally recorded as video so the frames were manually selected to have the following features: large number of dynamic objects, varying scene layout, and varying background."}, {"id": "msu-video-frame-interpolation", "name": "MSU Video Frame Interpolation", "description": "This is a dataset for video frame interpolation task. The dataset contains the 1920\u00d71080 videos in 240 FPS for videos captured with iPhone 11 and in 120 FPS for gaming content captured with OBS."}, {"id": "coco-tasks", "name": "COCO-Tasks", "description": "Comprises about 40,000 images where the most suitable objects for 14 tasks have been annotated."}, {"id": "spartqa-spatial-reasoning-on-textual-question-answering-2", "name": "SPARTQA - (SPAtial Reasoning on Textual Question Answering.)", "description": "We take advantage of the ground truth of NLVR images, design CFGs to generate stories, and use spatial reasoning rules to ask and answer spatial reasoning questions. This automatically generated data is called SpaRTQA.   https://aclanthology.org/2021.naacl-main.364/"}, {"id": "trec-10-trec-10-question-classification", "name": "TREC-10 (TREC-10 Question Classification)", "description": "A question type classification dataset with 6 classes for questions about a person, location, numeric information, etc. The test split has 500 questions, and the training split has 5452 questions."}, {"id": "icbhi-respiratory-sound-database-the-respiratory-sound-database-icbhi-2017-challenge", "name": "ICBHI Respiratory Sound Database (The Respiratory Sound database -  ICBHI 2017 Challenge)", "description": "The Respiratory Sound database was originally compiled to support the scientific challenge organized at Int. Conf. on Biomedical Health Informatics - ICBHI 2017."}, {"id": "eld-extreme-low-light-denoising-dataset", "name": "ELD (Extreme Low-light Denoising dataset)", "description": "Extreme low-light denoising (ELD) dataset that covers 10 indoor scenes and 4 camera devices from multiple brands (SonyA7S2, NikonD850, CanonEOS70D, CanonEOS700D). It has three levels (800, 1600, 3200) and two low light factors(100, 200) for noisy images, resulting in 240 (3\u00d72\u00d710\u00d74) raw image pairs in total."}, {"id": "mlqa-multilingual-question-answering", "name": "MLQA (MultiLingual Question Answering)", "description": "MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average."}, {"id": "brnocompspeed", "name": "BrnoCompSpeed", "description": "The dataset contains 21 full-HD videos, each around 1 hr long, captured at six different locations. Vehicles in the videos (20 865 instances in total) are annotated with the precise speed measurements from optical gates using LiDAR and verified with several reference GPS tracks. The dataset is available for download and it contains the videos and metadata (calibration, lengths of features in image, annotations, and so on) for future comparison and evaluation."}, {"id": "record", "name": "ReCoRD", "description": "Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) is a large-scale reading comprehension dataset which requires commonsense reasoning. ReCoRD consists of queries automatically generated from CNN/Daily Mail news articles; the answer to each query is a text span from a summarizing passage of the corresponding news. The goal of ReCoRD is to evaluate a machine's ability of commonsense reasoning in reading comprehension. ReCoRD is pronounced as [\u02c8r\u025bk\u0259rd]."}, {"id": "deepmimo", "name": "DeepMIMO", "description": "DeepMIMO is a generic dataset for mmWave/massive MIMO channels. The DeepMIMO dataset generation framework has two important features. First, the DeepMIMO channels are constructed based on accurate ray-tracing data obtained from Remcom Wireless InSite. The DeepMIMO channels, therefore, capture the dependence on the environment geometry/materials and transmitter/receiver locations, which is essential for several machine learning applications. Second, the DeepMIMO dataset is generic/parameterized as the researcher can adjust a set of system and channel parameters to tailor the generated DeepMIMO dataset for the target machine learning application. The DeepMIMO dataset can then be completely defined by the (i) the adopted ray-tracing scenario and (ii) the set of parameters, which enables the accurate definition and reproduction of the dataset."}, {"id": "disc21-dataset-for-isc-2021", "name": "DISC21 (Dataset for ISC 2021)", "description": "DISC21 is a benchmark for large-scale image similarity detection. This benchmark is used for the Image Similarity Challenge at NeurIPS'21 (ISC2021). The goal is to determine whether a query image is a modified copy of any image in a reference corpus of size 1~million. The benchmark features a variety of image transformations such as automated transformations, hand-crafted image edits and machine-learning based manipulations. This mimics real-life cases appearing in social media, for example for integrity-related problems dealing with misinformation and objectionable content. The strength of the image manipulations, and therefore the difficulty of the benchmark, is calibrated according to the performance of a set of baseline approaches. Both the query and reference set contain a majority of ``distractor'' images that do not match, which corresponds to a real-life needle-in-haystack setting, and the evaluation metric reflects that."}, {"id": "criteo-live-traffic-data-sample-of-30-days-of-criteo-live-traffic-data", "name": "Criteo live traffic data (Sample of 30 days of Criteo live traffic data)", "description": "Content of this dataset This dataset includes following files:"}, {"id": "aslg-pc12-english-asl-gloss-parallel-corpus-2012", "name": "ASLG-PC12 (English-ASL Gloss Parallel Corpus 2012)", "description": "An artificial corpus built using grammatical dependencies rules due to the lack of resources for Sign Language."}, {"id": "okutama-action", "name": "Okutama-Action", "description": "A new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors."}, {"id": "nuscenes", "name": "nuScenes", "description": "The nuScenes dataset is a large-scale autonomous driving dataset. The dataset has 3D bounding boxes for 1000 scenes collected in Boston and Singapore. Each scene is 20 seconds long and annotated at 2Hz. This results in a total of 28130 samples for training, 6019 samples for validation and 6008 samples for testing. The dataset has the full autonomous vehicle data suite: 32-beam LiDAR, 6 cameras and radars with complete 360\u00b0 coverage. The 3D object detection challenge evaluates the performance on 10 classes: cars, trucks, buses, trailers, construction vehicles, pedestrians, motorcycles, bicycles, traffic cones and barriers."}, {"id": "indicglue-indic-general-language-understanding-evaluation-benchmark", "name": "IndicGLUE (Indic General Language Understanding Evaluation Benchmark)", "description": "We now introduce IndicGLUE, the Indic General Language Understanding Evaluation Benchmark, which is a collection of various NLP tasks as de- scribed below. The goal is to provide an evaluation benchmark for natural language understanding ca- pabilities of NLP models on diverse tasks and mul- tiple Indian languages."}, {"id": "viquae", "name": "ViQuAE", "description": "ViQuAE is a dataset for KVQAE (Knowledge-based Visual Question Answering about named Entities), a task which consists in answering questions about named entities grounded in a visual context  using a Knowledge Base. It is the first KVQAE dataset to cover a wide range of entity types (e.g. persons, landmarks, and products). We argue that KVQAE is a clear, well-defined task that can be evaluated easily, making it suitable to track the progress of multimodal entity representation\u2019s quality. Multimodal entity representation is a central issue that will allow to make human-machine interactions more natural. For example, while watching a movie, one might wonder \u2018\u2018Where did I already see this actress?\u2019\u2019 or \u2018\u2018Did she ever win an Oscar?\u2019\u2019"}, {"id": "icb-image-compression-benchmark", "name": "ICB (Image Compression Benchmark)", "description": "A carefully chosen set of high-resolution high-precision natural images suited for compression algorithm evaluation."}, {"id": "rwf-2000", "name": "RWF-2000", "description": "A database with 2,000 videos captured by surveillance cameras in real-world scenes. "}, {"id": "stanford-schema2qa-dataset", "name": "Stanford Schema2QA Dataset", "description": "Schema2QA is the first large question answering dataset over real-world Schema.org data. It covers 6 common domains: restaurants, hotels, people, movies, books, and music, based on crawled Schema.org metadata from 6 different websites (Yelp, Hyatt, LinkedIn, IMDb, Goodreads, and last.fm.). In total, there are over 2,000,000 examples for training, consisting of both augmented human paraphrase data and high-quality synthetic data generated by Genie. All questions are annotated with executable virtual assistant programming language ThingTalk."}, {"id": "fce-first-certificate-in-english", "name": "FCE (First Certificate in English)", "description": "The Cambridge Learner Corpus First Certificate in English (CLC FCE) dataset consists of short texts, written by learners of English as an additional language in response to exam prompts eliciting free-text answers and assessing mastery of the upper-intermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. The full dataset consists of 323,192 sentences. The publicly released subset of the dataset, named FCE-public, consists of 33,673 sentences split into test and training sets of 2,720 and 30,953 sentences, respectively."}, {"id": "multiface", "name": "Multiface", "description": "Multiface consists of high quality recordings of the faces of 13 identities, each captured in a multi-view capture stage performing various facial expressions. An average of 12,200 (v1 scripts) to 23,000 (v2 scripts) frames per subject with capture rate at 30 fps. Each frame includes roughly 40 (v1) to 160 (v2) different camera views under uniform illumination, yielding a total dataset size of 65TB. "}, {"id": "144810-images-multi-class-fashion-item-detection-data", "name": "144,810 Images Multi-class Fashion Item Detection Data", "description": "Description\uff1a 144,810 Images Multi-class Fashion Item Detection Data. In this dataset, 19,968 images of male and 124,842 images of female were included. The Fashion Items were divided into 4 parts based on the season (spring, autumn, summer and winter). In terms of annotation, rectangular bounding boxes were adopted to annotate fashion items. The data can be used for tasks such as fashion items detection, fashion recommendation and other tasks."}, {"id": "uit-vinewsqa", "name": "UIT-ViNewsQA", "description": "UIT-ViNewsQA is a new corpus for the Vietnamese language to evaluate healthcare reading comprehension models. The corpus comprises 22,057 human-generated question-answer pairs. Crowd-workers create the questions and their answers based on a collection of over 4,416 online Vietnamese healthcare news articles, where the answers comprise spans extracted from the corresponding articles. "}, {"id": "eur-lex-sum", "name": "EUR-Lex-Sum", "description": "EUR-Lex-Sum is a dataset for cross-lingual summarization. It is based on manually curated document summaries of legal acts from the European Union law platform. Documents and their respective summaries exist as crosslingual paragraph-aligned data in several of the 24 official European languages, enabling access to various cross-lingual and lower-resourced summarization setups. The dataset contains up to 1,500 document/summary pairs per language, including a subset of 375 cross-lingually aligned legal acts with texts available in all 24 languages."}, {"id": "cats-and-dogs", "name": "Cats and Dogs", "description": "A large set of images of cats and dogs. "}, {"id": "conditionalqa", "name": "ConditionalQA", "description": "ConditionalQA is a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply."}, {"id": "tcg-traffic-control-gesture", "name": "TCG (Traffic Control Gesture)", "description": "The TCG dataset is used to evaluate Traffic Control Gesture recognition for autonomous driving. The dataset is based on 3D body skeleton input to perform traffic control gesture classification on every time step. The dataset consists of 250 sequences from several actors, ranging from 16 to 90 seconds per sequence."}, {"id": "quva-repetition", "name": "QUVA Repetition", "description": "QUVA Repetition dataset consists of 100 videos displaying a wide variety of repetitive video dynamics, including swimming, stirring, cutting, combing and music-making. All videos have been annotated with individual cycle bounds and a total repetition count."}, {"id": "uzwordnet-the-uzbek-wordnet", "name": "UzWordnet (The Uzbek Wordnet)", "description": "UzWordnet is a lexical-semantic database, or a \u201cword-net\u201d, for the (Northern) Uzbek language (native: O\u2019zbek till) compatible with Princeton Wordnet. By providing it open source (see License), we aim to motivate, support, and increase the application of database and knowledge graphs principles and techniques to the study of computational aspects of the (Northern) Uzbek language and, more generally, the usability of Uzbek within IT applications and the Internet."}, {"id": "naamapadam", "name": "Naamapadam", "description": "Naamapadam is a Named Entity Recognition (NER) dataset for the 11 major Indian languages from two language families. In each language, it contains more than 400k sentences annotated with a total of at least 100k entities from three standard entity categories (Person, Location and Organization) for 9 out of the 11 languages. The training dataset has been automatically created from the Samanantar parallel corpus by projecting automatically tagged entities from an English sentence to the corresponding Indian language sentence."}, {"id": "ldv-large-scale-diverse-video", "name": "LDV (Large-scale Diverse Video)", "description": "LDV is a dataset for video enhancement. It contains 240 videos with diverse categories of content, different kinds of motion and various frame-rates."}, {"id": "flue-french-language-understanding-evaluation", "name": "FLUE (French Language Understanding Evaluation)", "description": "FLUE is a French Language Understanding Evaluation benchmark. It consists of 5 tasks: Text Classification, Paraphrasing, Natural Language Inference, Constituency Parsing and Part-of-Speech Tagging, and Word Sense Disambiguation."}, {"id": "toplogo-10", "name": "TopLogo-10", "description": "Collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context."}, {"id": "hateful-users-on-twitter", "name": "Hateful Users on Twitter", "description": "This is a Twitter dataset of 100,386 users along with up to 200 tweets from their timelines with a random-walk-based crawler on the retweet graph, with a subsample of 4,972 which is manually annotated as hateful or not through crowdsourcing. The dataset can be used to examine the difference between user activity patterns, the content disseminated between hateful and normal users, and network centrality measurements in the sampled graph."}, {"id": "egogesture", "name": "EgoGesture", "description": "The EgoGesture dataset contains 2,081 RGB-D videos, 24,161 gesture samples and 2,953,224 frames from 50 distinct subjects."}, {"id": "activities-activities-dataset", "name": "Activities (Activities dataset)", "description": "Contains ten synthetic time series with five days of high activity and two days of low activity. Each series has 3584 samples."}, {"id": "nes-mdb-nintendo-entertainment-system-music-database", "name": "NES-MDB (Nintendo Entertainment System Music Database)", "description": "The Nintendo Entertainment System Music Database (NES-MDB) is a dataset intended for building automatic music composition systems for the NES audio synthesizer. It consists of  5278 songs from the soundtracks of 397 NES games. The dataset represents 296 unique composers, and the songs contain more than two million notes combined. It has file format options for MIDI, score and NLM (NES Language Modeling)."}, {"id": "biwi", "name": "BIWI", "description": "The dataset contains over 15K images of 20 people (6 females and 14 males - 4 people were recorded twice). For each frame, a depth image, the corresponding rgb image (both 640x480 pixels), and the annotation is provided. The head pose range covers about +-75 degrees yaw and +-60 degrees pitch. Ground truth is provided in the form of the 3D location of the head and its rotation."}, {"id": "gyafc-grammarlys-yahoo-answers-formality-corpus", "name": "GYAFC (Grammarly\u2019s Yahoo Answers Formality Corpus)", "description": "Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC) is the largest dataset for any style containing a total of 110K informal / formal sentence pairs."}, {"id": "wikitablequestions", "name": "WikiTableQuestions", "description": "WikiTableQuestions is a question answering dataset over semi-structured tables. It is comprised of question-answer pairs on HTML tables, and was constructed by selecting data tables from Wikipedia that contained at least 8 rows and 5 columns. Amazon Mechanical Turk workers were then tasked with writing trivia questions about each table. WikiTableQuestions contains 22,033 questions. The questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. Compared to previous datasets on knowledge bases it covers nearly 4,000 unique column headers, containing far more relations than closed domain datasets and datasets for querying knowledge bases. Its questions cover a wide range of domains, requiring operations such as table lookup, aggregation, superlatives (argmax, argmin), arithmetic operations, joins and unions."}, {"id": "ehe-elderly-home-exercise", "name": "EHE (Elderly Home Exercise)", "description": "Human Action Evaluation (HAE) has rarely been applied to real-world disease monitoring, the EHE dataset aims to gather sample data to validate effective HAE methods that could then be expanded on a larger validation scale. EHE consists of several actions from morning exercises that patients complete daily in the elderly home. The EHE dataset contained 869 action repetitions performed by 25 older people. Six exercises were collected for the EHE dataset via Kinect v2."}, {"id": "sqa-sequentialqa", "name": "SQA (SequentialQA)", "description": "The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total."}, {"id": "ttstroke-21-me21-ttstroke-21-for-mediaeval-2021", "name": "TTStroke-21 ME21 (TTStroke-21 for MediaEval 2021)", "description": "This task offers researchers an opportunity to test their fine-grained classification methods for detecting and recognizing strokes in table tennis videos. (The low inter-class variability makes the task more difficult than with usual general datasets like UCF-101.) The task offers two subtasks:"}, {"id": "wiqa-what-if-question-answering", "name": "WIQA (What-If Question Answering)", "description": "The WIQA dataset V1 has 39705 questions containing a perturbation and a possible effect in the context of a paragraph. The dataset is split into 29808 train questions, 6894 dev questions and 3003 test questions."}, {"id": "crisismmd", "name": "CrisisMMD", "description": "CrisisMMD is a large multi-modal dataset collected from Twitter during different natural disasters. It consists of several thousands of manually annotated tweets and images collected during seven major natural disasters including earthquakes, hurricanes, wildfires, and floods that happened in the year 2017 across different parts of the World. The provided datasets include three types of annotations."}, {"id": "trip-tiered-reasoning-for-intuitive-physics", "name": "TRIP (Tiered Reasoning for Intuitive Physics)", "description": "Tiered Reasoning for Intuitive Physics (TRIP) is a novel commonsense reasoning dataset with dense annotations that enable multi-tiered evaluation of machines\u2019 reasoning process. TRIP serves as a benchmark for physical commonsense reasoning that provides traces of reasoning for an end task of plausibility prediction. The dataset consists of human-authored stories describing sequences of concrete physical actions. Given two stories composed of individually plausible sentences and only differing by one sentence (i.e., Sentence 5), the proposed task is to determine which story is more plausible. To understand stories like these and make such a prediction, one must have knowledge of verb causality and precondition, and rules of intuitive physics."}, {"id": "tsu-toyota-smarthome-untrimmed", "name": "TSU (Toyota Smarthome Untrimmed)", "description": "Toyota Smarthome Untrimmed (TSU) is a dataset for activity detection in long untrimmed videos. The dataset contains 536 videos with an average duration of 21 mins. Since this dataset is based on the same footage video as Toyota Smarthome Trimmed version, it features the same challenges and introduces additional ones. The dataset is annotated with 51 activities."}, {"id": "movielens", "name": "MovieLens", "description": "The MovieLens datasets, first released in 1998, describe people\u2019s expressed preferences for movies. These preferences take the form of tuples, each the result of a person expressing a preference (a 0-5 star rating) for a movie at a particular time. These preferences were entered by way of the MovieLens web site1 \u2014 a recommender system that asks its users to give movie ratings in order to receive personalized movie recommendations."}, {"id": "carl-context-adaptive-rl", "name": "CARL (Context Adaptive RL)", "description": "CARL (context adaptive RL) provides highly configurable contextual extensions to several well-known RL environments. It's designed to test your agent's generalization capabilities in all scenarios where intra-task generalization is important."}, {"id": "prw-person-re-identification-in-the-wild", "name": "PRW (Person Re-identification in the Wild)", "description": "PRW is a large-scale dataset for end-to-end pedestrian detection and person recognition in raw video frames. PRW is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities."}, {"id": "wikiqa-wikipedia-open-domain-question-answering", "name": "WikiQA (Wikipedia open-domain Question Answering)", "description": "The WikiQA corpus is a publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. In order to reflect the true information need of general users, Bing query logs were used as the question source. Each question is linked to a Wikipedia page that potentially has the answer. Because the summary section of a Wikipedia page provides the basic and usually most important information about the topic, sentences in this section were used as the candidate answers. The corpus includes 3,047 questions and 29,258 sentences, where 1,473 sentences were labeled as answer sentences to their corresponding questions."}, {"id": "mocha", "name": "MOCHA", "description": "Contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. "}, {"id": "kuzushiji-mnist", "name": "Kuzushiji-MNIST", "description": "Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images). Since MNIST restricts us to 10 classes, the authors chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST. Kuzushiji is a Japanese cursive writing style."}, {"id": "dense-fog-dense", "name": "Dense Fog (DENSE)", "description": "We introduce an object detection dataset in challenging adverse weather conditions covering 12000 samples in real-world driving scenes and 1500 samples in controlled weather conditions within a fog chamber. The dataset includes different weather conditions like fog, snow, and rain and was acquired by over 10,000 km of driving in northern Europe. The driven route with cities along the road is shown on the right. In total, 100k Objekts were labeled with accurate 2D and 3D bounding boxes. The main contributions of this dataset are: - We provide a proving ground for a broad range of algorithms covering signal enhancement, domain adaptation, object detection, or multi-modal sensor fusion, focusing on the learning of robust redundancies between sensors, especially if they fail asymmetrically in different weather conditions. - The dataset was created with the initial intention to showcase methods, which learn of robust redundancies between the sensor and enable a raw data sensor fusion in case of asymmetric sensor failure induced through adverse weather effects. - In our case we departed from proposal level fusion and applied an adaptive fusion driven by measurement entropy enabling the detection also in case of unknown adverse weather effects. This method outperforms other reference fusion methods, which even drop in below single image methods. - Please check out our paper for more information."}, {"id": "mmact", "name": "MMAct", "description": "MMAct is a large-scale dataset for multi/cross modal action understanding. This dataset has been recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. "}, {"id": "mpi-inf-3dhp", "name": "MPI-INF-3DHP", "description": "MPI-INF-3DHP is a 3D human body pose estimation dataset consisting of both constrained indoor and complex outdoor scenes. It records 8 actors performing 8 activities from 14 camera views. It consists on >1.3M frames captured from the 14 cameras."}, {"id": "rrs-restoration-200k-for-response-selection", "name": "RRS (Restoration-200k for Response Selection)", "description": "Ranking test set contains the high-quality responses that selected by some baselines, and their correlation with the conversation context are carefully annotated by 8 professional annotators (the average annotation scores are saved for ranking). For ranking test set, the metrics should be NDCG@3 and NDCG@5, since the correlation scores are provided. More details are available in the Appendix of the paper."}, {"id": "brats21-rsna-asnr-miccai-brain-tumor-segmentation-brats-challenge-2021", "name": "BRATS21 (RSNA-ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2021)", "description": "The RSNA-ASNR-MICCAI BraTS 2021 challenge utilizes multi-institutional pre-operative baseline multi-parametric magnetic resonance imaging (mpMRI) scans, and focuses on the evaluation of state-of-the-art methods for (Task 1) the segmentation of intrinsically heterogeneous brain glioblastoma sub-regions in mpMRI scans. Furthemore, this BraTS 2021 challenge also focuses on the evaluation of (Task 2) classification methods to predict the MGMT promoter methylation status."}, {"id": "fashionpedia", "name": "Fashionpedia", "description": "Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology. "}, {"id": "chikusei-dataset-airborne-hyperspectral-data-taken-over-chikusei", "name": "Chikusei Dataset (Airborne hyperspectral data taken over Chikusei)", "description": "The airborne hyperspectral dataset was taken by Headwall Hyperspec-VNIR-C imaging sensor over agricultural and urban areas in Chikusei, Ibaraki, Japan, on July 29, 2014 between the times 9:56 to 10:53 UTC+9. The central point of the scene is located at coordinates: 36.294946N, 140.008380E. The hyperspectral dataset has 128 bands in the spectral range from 363 nm to 1018 nm. The scene consists of 2517x2335 pixels and the ground sampling distance was 2.5 m. Ground truth of 19 classes was collected via a field survey and visual inspection using high-resolution color images obtained by Canon EOS 5D Mark II together with the hyperspectral data. The hyperspectral data and ground truth were made available to the scientific community in the ENVI and MATLAB formats at http://park.itc.u-tokyo.ac.jp/sal/hyperdata. More details of the experiment are presented in the technical report given below."}, {"id": "pubmed-48-32-20-fixed-splits", "name": "PubMed (48%/32%/20% fixed splits)", "description": "Node classification on PubMed with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "perlex", "name": "Perlex", "description": "Persian dataset for relation extraction, which is an expert-translated version of the \"Semeval-2010-Task-8\" dataset. "}, {"id": "datasets-for-3d-shape-reconstruction-from-2d-microscopy-images", "name": "Datasets for 3D shape reconstruction from 2D microscopy images", "description": "Two single cell datsets for 3D shape reconstruction from 2D microscopy images used for our three previous publication\u2019s, together with the respective model predictions."}, {"id": "metashift", "name": "MetaShift", "description": "MetaShift is a collection of 12,868 sets of natural images across 410 classes. It can be used to benchmark and evaluate how robust machine learning models are to data shifts."}, {"id": "meglass", "name": "MeGlass", "description": "MeGlass is an eyeglass dataset originally designed for eyeglass face recognition evaluation. All the face images are selected and cleaned from MegaFace. Each identity has at least two face images with eyeglass and two face images without eyeglass. It contains 47,817 images from 1,710 different identities."}, {"id": "cqr-contextual-query-rewrite", "name": "CQR (Contextual Query Rewrite)", "description": "CQR is an extension to the Stanford Dialogue Corpus. It contains crowd-sourced rewrites to facilitate research in dialogue state tracking using natural language as the interface."}, {"id": "claimbuster", "name": "ClaimBuster", "description": "Consist of 23,533 statements extracted from all U.S. general election presidential debates and annotated by human coders. The ClaimBuster dataset can be leveraged in building computational methods to identify claims that are worth fact-checking from the myriad of sources of digital or traditional media. "}, {"id": "rt-gene", "name": "RT-GENE", "description": "Presents a diverse eye-gaze dataset."}, {"id": "mila-simulated-floods", "name": "Mila Simulated Floods", "description": "Mila Simulated Floods Dataset is a 1.5 square km virtual world using the Unity3D game engine including urban, suburban and rural areas."}, {"id": "ccd-car-crash-dataset", "name": "CCD (Car Crash Dataset)", "description": "Car Crash Dataset (CCD) is collected for traffic accident analysis. It contains real traffic accident videos captured by dashcam mounted on driving vehicles, which is critical to developing safety-guaranteed self-driving systems. CCD is distinguished from existing datasets for diversified accident annotations, including environmental attributes (day/night, snowy/rainy/good weather conditions), whether ego-vehicles involved, accident participants, and accident reason descriptions."}, {"id": "ssp-3d-sports-shape-and-pose-3d", "name": "SSP-3D (Sports Shape and Pose 3D)", "description": "SSP-3D is an evaluation dataset consisting of 311 images of sportspersons in tight-fitted clothes, with a variety of body shapes and poses. The images were collected from the Sports-1M dataset. SSP-3D is intended for use as a benchmark for body shape prediction methods. Pseudo-ground-truth 3D shape labels (using the SMPL body model) were obtained via multi-frame optimisation with shape consistency between frames, as described here."}, {"id": "ednet", "name": "EdNet", "description": "A large-scale hierarchical dataset of diverse student activities collected by Santa, a multi-platform self-study solution equipped with artificial intelligence tutoring system. EdNet contains 131,441,538 interactions from 784,309 students collected over more than 2 years, which is the largest among the ITS datasets released to the public so far."}, {"id": "freiburg-spatial-relations", "name": "Freiburg Spatial Relations", "description": "The Freiburg Spatial Relations dataset features 546 scenes each containing two out of 25 household objects. The depicted spatial relations can roughly be described as on top, on top on the corner, inside, inside and inclined, next to, and inclined. The dataset contains the 25 object models as textured .obj and .dae files, a low resolution .dae version for visualization in rviz, a scene description file containing the translation and rotation of the objects for each scene, a file with labels for each scene, the 15 splits used for cross validation, and a bash script to convert the models to pointclouds."}, {"id": "abalone", "name": "Abalone", "description": "Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem."}, {"id": "nfcorpus", "name": "NFCorpus", "description": "NFCorpus is a full-text English retrieval data set for Medical Information Retrieval. It contains a total of 3,244 natural language queries (written in non-technical English, harvested from the NutritionFacts.org site) with 169,756 automatically extracted relevance judgments for 9,964 medical documents (written in a complex terminology-heavy language), mostly from PubMed."}, {"id": "europeana-newspapers", "name": "Europeana Newspapers", "description": "Europeana Newspapers consists of four datasets with 100 pages each for the languages Dutch, French, German (including Austrian) as part of the Europeana Newspapers project is expected to contribute to the further development and improvement of named entity recognition systems with a focus on historical content."}, {"id": "taskmaster-2", "name": "Taskmaster-2", "description": "The Taskmaster-2 dataset consists of 17,289 dialogs in seven domains: restaurants (3276), food ordering (1050), movies (3047), hotels (2355), flights (2481), music (1602), and sports (3478)."}, {"id": "ttpla-transmission-towers-and-power-lines-ttpla", "name": "TTPLA (Transmission Towers and Power Lines (TTPLA))", "description": "TTPLA is a public dataset which is a collection of aerial images on Transmission Towers (TTs) and Power Lines (PLs). It can be used for detection and segmentation of transmission towers and power lines. It consists of 1,100 images with the resolution of 3,840\u00d72,160 pixels, as well as manually labelled 8,987 instances of TTs and PLs."}, {"id": "pubtabnet", "name": "PubTabNet", "description": "PubTabNet is a large dataset for image-based table recognition, containing 568k+ images of tabular data annotated with the corresponding HTML representation of the tables. The table images are extracted from the scientific publications included in the PubMed Central Open Access Subset (commercial use collection). Table regions are identified by matching the PDF format and the XML format of the articles in the PubMed Central Open Access Subset. More details are available in our paper \"Image-based table recognition: data, model, and evaluation\"."}, {"id": "singa-pura-singapore-polyphonic-urban-audio", "name": "SINGA:PURA (SINGApore: Polyphonic URban Audio)", "description": "This repository contains the SINGA:PURA dataset, a strongly-labelled polyphonic urban sound dataset with spatiotemporal context. The data were collected via a number of recording units deployed across Singapore as a part of a wireless acoustic sensor network. These recordings were made as part of a project to identify and mitigate noise sources in Singapore, but also possess a wider applicability to sound event detection, classification, and localization. The taxonomy we used for the labels in this dataset has been designed to be compatible with other existing datasets for urban sound tagging while also able to capture sound events unique to the Singaporean context. Please refer to our conference paper published in APSIPA 2021 (which is found in this repository as the file \"APSIPA.pdf\") or download the readme (\"Readme.md\") for more details regarding the data collection, annotation, and processing methodologies for the creation of the dataset."}, {"id": "contentwise-impressions", "name": "ContentWise Impressions", "description": "The ContentWise Impressions dataset is a collection of implicit interactions and impressions of movies and TV series from an Over-The-Top media service, which delivers its media contents over the Internet. The dataset is distinguished from other already available multimedia recommendation datasets by the availability of impressions, i.e., the recommendations shown to the user, its size, and by being open-source. The items in the dataset represent the multimedia content that the service provided to the users and are represented by an anonymized numerical identifier. The items refer to television and cinema products belonging to four mutually exclusive categories: movies, movies and clips in series, TV movies or shows, and episodes of TV series. The interactions represent the actions performed by users on items in the service and are associated with the timestamp when it occurred. Interactions contain the identifier of the impressions, except in those cases where the recommendations came from a row added by the service provider. The interactions are categorized in four different types: views, detail, ratings, and purchases. The impressions refer to the recommended items that were presented to the user and are identified by their series. Impressions consist of a numerical identifier, the list position on the screen, the length of the recommendation list, and an ordered list of recommended series identifiers, where the most relevant item is in the first position."}, {"id": "pathvqa", "name": "PathVQA", "description": "PathVQA consists of 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness."}, {"id": "aspec-asian-scientific-paper-excerpt-corpus", "name": "ASPEC (Asian Scientific Paper Excerpt Corpus)", "description": "ASPEC, Asian Scientific Paper Excerpt Corpus, is constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). It consists of a Japanese-English paper abstract corpus of 3M parallel sentences (ASPEC-JE) and a Japanese-Chinese paper excerpt corpus of 680K parallel sentences (ASPEC-JC). This corpus is one of the achievements of the Japanese-Chinese machine translation project which was run in Japan from 2006 to 2010."}, {"id": "ijb-a-iarpa-janus-benchmark-a", "name": "IJB-A (IARPA Janus Benchmark A)", "description": "The IARPA Janus Benchmark A (IJB-A) database is developed with the aim to augment more challenges to the face recognition task by collecting facial images with a wide variations in pose, illumination, expression, resolution and occlusion. IJB-A is constructed by collecting 5,712 images and 2,085 videos from 500 identities, with an average of 11.4 images and 4.2 videos per identity."}, {"id": "chemdisgene", "name": "ChemDisGene", "description": "ChemDisGene, a new dataset for training and evaluating multi-class multi-label biomedical relation extraction models."}, {"id": "cremi", "name": "CREMI", "description": "MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images."}, {"id": "dispscenes", "name": "DispScenes", "description": "The DispScenes dataset was created to address the specific problem of disparate image matching. The image pairs in all the datasets exhibit high levels of variation in illumination and viewpoint and also contain instances of occlusion. The DispScenes dataset provides manual ground truth keypoint correspondences for all images."}, {"id": "riteyes", "name": "RITEyes", "description": "Deep neural networks for video based eye tracking have demonstrated resilience to noisy environments, stray reflections and low resolution. However, to train these networks, a large number of manually annotated images are required. To alleviate the cumbersome process of manual labeling, computer graphics rendering is employed to automatically generate a large corpus of annotated eye images under various conditions. In this work, we introduce RIT-Eyes, a novel synthetic eye image generation platform which improves upon previous work by adding features such as retinal retro-reflection, realistic blinks, an active deformable iris and an aspherical cornea. We add various external influences which potentially degrade eye tracking such as corrective eye-wear with varying refractive indices. To demonstrate the utility of RIT-Eyes, we generate and publicly share a large dataset of images with a variety of eye poses and viewing conditions."}, {"id": "sherliic", "name": "SherLIiC", "description": "SherLIiC is a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types."}, {"id": "activitynet-captions", "name": "ActivityNet Captions", "description": "The ActivityNet Captions dataset is built on ActivityNet v1.3 which includes 20k YouTube untrimmed videos with 100k caption annotations. The videos are 120 seconds long on average. Most of the videos contain over 3 annotated events with corresponding start/end time and human-written sentences, which contain 13.5 words on average. The number of videos in train/validation/test split is 10024/4926/5044, respectively."}, {"id": "multi-modal-celeba-hq", "name": "Multi-Modal CelebA-HQ", "description": "Multi-Modal-CelebA-HQ is a large-scale face image dataset that has 30,000 high-resolution face images selected from the CelebA dataset by following CelebA-HQ. Each image has high-quality segmentation mask, sketch, descriptive text, and image with transparent background."}, {"id": "dibco-2019", "name": "DIBCO 2019", "description": "DIBCO 2019 is the international Competition on Document Image Binarization organized in conjunction with the ICDAR 2019 conference. The general objective of the contest is to identify current advances in document image binarization of machine-printed and handwritten document images using performance evaluation measures that are motivated by document image analysis and recognition requirements."}, {"id": "sensereid", "name": "SenseReID", "description": "SenseReID is a person re-identification dataset for evaluating ReID models. It is captured from real surveillance cameras and the person bounding boxes are obtained from state-of-the-art detection algorithm. The dataset contains 1,717 identities in total."}, {"id": "bace-scaffold-scaffold-split-of-bace-dataset", "name": "BACE(scaffold) (Scaffold split of BACE  dataset)", "description": "MoleculeNet is a benchmark specially designed for testing machine learning methods of molecular properties. As we aim to facilitate the development of molecular machine learning method, this work curates a number of dataset collections, creates a suite of software that implements many known featurizations and previously proposed algorithms. All methods and datasets are integrated as parts of the open source DeepChem package(MIT license). MoleculeNet is built upon multiple public databases. The full collection currently includes over 700,000 compounds tested on a range of different properties. We test the performances of various machine learning models with different featurizations on the datasets(detailed descriptions here), with all results reported in AUC-ROC, AUC-PRC, RMSE and MAE scores. For users, please cite: Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande, MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017."}, {"id": "arcene", "name": "Arcene", "description": "ARCENE was obtained by merging three mass-spectrometry datasets to obtain enough training and test data for a benchmark. The original features indicate the abundance of proteins in human sera having a given mass value. Based on those features one must separate cancer patients from healthy patients. We added a number of distractor feature called 'probes' having no predictive power. The order of the features and patterns were randomized."}, {"id": "covid-19-twitter-chatter-dataset", "name": "COVID-19 Twitter Chatter Dataset", "description": "A large-scale curated dataset of over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st to April 4th at the time of writing."}, {"id": "isic-2017-task-3", "name": "ISIC 2017 Task 3", "description": "The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 3 challenge dataset for lesion classification contains 2,000 images for training including 374 melanoma, 254 seborrheic keratosis and the remainder as benign nevi (1372)."}, {"id": "echonet-dynamic", "name": "Echonet-Dynamic", "description": "Echocardiography, or cardiac ultrasound, is the most widely used and readily available imaging modality to assess cardiac function and structure. Combining portable instrumentation, rapid image acquisition, high temporal resolution, and without the risks of ionizing radiation, echocardiography is one of the most frequently utilized imaging studies in the United States and serves as the backbone of cardiovascular imaging. For diseases ranging from heart failure to valvular heart diseases, echocardiography is both necessary and suf\ufb01cient to diagnose many cardiovascular diseases. In addition to our deep learning model, we introduce a new large video dataset of echocardiograms for computer vision research. The EchoNet-Dynamic database includes 10,030 labeled echocardiogram videos and human expert annotations (measurements, tracings, and calculations) to provide a baseline to study cardiac motion and chamber sizes."}, {"id": "leaf-qa", "name": "LEAF-QA", "description": "LEAF-QA, a comprehensive dataset of 250,000 densely annotated figures/charts, constructed from real-world open data sources, along with ~2 million question-answer (QA) pairs querying the structure and semantics of these charts. LEAF-QA highlights the problem of multimodal QA, which is notably different from conventional visual QA (VQA), and has recently gained interest in the community. Furthermore, LEAF-QA is significantly more complex than previous attempts at chart QA, viz. FigureQA and DVQA, which present only limited variations in chart data. LEAF-QA being constructed from real-world sources, requires a novel architecture to enable question answering."}, {"id": "cowese-corpus-web-salud-espanol", "name": "CoWeSe (Corpus Web Salud Espanol)", "description": "CoWeSe is a Spanish biomedical corpus consisting of 4.5GB (about 750M tokens) of clean plain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains executed in 2020."}, {"id": "eorssd-extended-optical-remote-sensing-saliency-detection", "name": "EORSSD (Extended Optical Remote Sensing Saliency Detection)", "description": "The Extended Optical Remote Sensing Saliency Detection (EORSSD) dataset is an extension of the ORSSD dataset. This new dataset is larger and more varied than the original. It contains 2,000 images and corresponding pixel-wise ground truth, which includes many semantically meaningful but challenging images."}, {"id": "artbench-10-32x32", "name": "ArtBench-10 (32x32)", "description": "We introduce ArtBench-10, the first class-balanced, high-quality, cleanly annotated, and standardized dataset for benchmarking artwork generation. It comprises 60,000 images of artwork from 10 distinctive artistic styles, with 5,000 training images and 1,000 testing images per style. ArtBench-10 has several advantages over previous artwork datasets. Firstly, it is class-balanced while most previous artwork datasets suffer from the long tail class distributions. Secondly, the images are of high quality with clean annotations. Thirdly, ArtBench-10 is created with standardized data collection, annotation, filtering, and preprocessing procedures. We provide three versions of the dataset with different resolutions (32\u00d732, 256\u00d7256, and original image size), formatted in a way that is easy to be incorporated by popular machine learning frameworks."}, {"id": "kanhope-kannada-hope-speech-dataset", "name": "KanHope (Kannada Hope speech dataset)", "description": "KanHope is a code mixed hope speech dataset for equality, diversity, and inclusion in Kannada, an under-resourced Dravidian language. The dataset consists of 6,176 user-generated comments in code mixed Kannada crawled from YouTube and manually labelled as bearing hope speech or not-hope speech."}, {"id": "dsprites-disentanglement-testing-sprites-dataset", "name": "dSprites (Disentanglement testing Sprites dataset)", "description": "dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite."}, {"id": "afromnist", "name": "AfroMNIST", "description": "A set of synthetic MNIST-style datasets for four orthographies used in Afro-Asiatic and Niger-Congo languages: Ge`ez (Ethiopic), Vai, Osmanya, and N'Ko. These datasets serve as \"drop-in\" replacements for MNIST. "}, {"id": "scut-ctw1500", "name": "SCUT-CTW1500", "description": "The SCUT-CTW1500 dataset contains 1,500 images: 1,000 for training and 500 for testing. In particular, it provides 10,751 cropped text instance images, including 3,530 with curved text. The images are manually harvested from the Internet, image libraries such as Google Open-Image, or phone cameras. The dataset contains a lot of horizontal and multi-oriented text."}, {"id": "celeba-celebfaces-attributes-dataset", "name": "CelebA (CelebFaces Attributes Dataset)", "description": "CelebFaces Attributes dataset contains 202,599 face images of the size 178\u00d7218 from 10,177 celebrities, each annotated with 40 binary labels indicating facial attributes like hair color, gender and age."}, {"id": "taodescribe", "name": "TaoDescribe", "description": "The TaoDescribe dataset contains 2,129,187 product titles and descriptions in Chinese."}, {"id": "plotqa", "name": "PlotQA", "description": "PlotQA is a VQA dataset with 28.9 million question-answer pairs grounded over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates. Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice this is an unrealistic assumption because many questions require reasoning and thus have real valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real world plots by introducing PlotQA. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary."}, {"id": "multireqa", "name": "MultiReQA", "description": "MultiReQA is a cross-domain evaluation for retrieval question answering models. Retrieval question answering (ReQA) is the task of retrieving a sentence-level answer to a question from an open corpus. MultiReQA is a new multi-domain ReQA evaluation suite composed of eight retrieval QA tasks drawn from publicly available QA datasets from the MRQA shared task. MultiReQA contains the sentence boundary annotation from eight publicly available QA datasets including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, BioASQ, RelationExtraction, and TextbookQA. Five of these datasets, including SearchQA, TriviaQA, HotpotQA, NaturalQuestions, SQuAD, contain both training and test data, and three, in cluding BioASQ, RelationExtraction, TextbookQA, contain only the test data."}, {"id": "pdes-some-pde-solutions", "name": "PDEs (Some PDE solutions)", "description": "In this dataset, you will find solutions of the following partial differential equations: - Burgers - Kortweg-de-Vries -Newell-Whitehead - Kuramoto-Sivashinsky You will find more info about how these were generated in the supplementary material of the paper: https://arxiv.org/abs/2106.11936"}, {"id": "exvo2022-icml-exvo-2022-workshop-competition-data", "name": "ExVo2022 (ICML ExVo 2022 Workshop & Competition Data)", "description": "Baseline code for the three tracks of ExVo 2022 competition."}, {"id": "aadb", "name": "AADB", "description": "Contains aesthetic scores and meaningful attributes assigned to each image by multiple human raters. "}, {"id": "topo-boundary", "name": "Topo-boundary", "description": "Topo-boundary is a new benchmark dataset, named \\textit{Topo-boundary}, for off-line topological road-boundary detection. The dataset contains 21,556 1000 X 1000-sized 4-channel aerial images. Each image is provided with 8 training labels for different sub-tasks."}, {"id": "qmar-quality-of-movement-assessment-for-rehabilitation", "name": "QMAR (Quality of Movement Assessment for Rehabilitation)", "description": "QMAR is an RGB multi-view Quality of Human Movement Assessment  dataset."}, {"id": "ovad-benchmark-open-vocabulary-attribute-detection", "name": "OVAD benchmark (Open-Vocabulary Attribute Detection)", "description": "Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set covering 117 attribute classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's value by studying the attribute detection performance of several foundation models."}, {"id": "smartcity", "name": "SmartCity", "description": "SmartCity consists of 50 images in total collected from ten city scenes including office entrance, sidewalk, atrium, shopping mall etc.. Unlike the existing crowd counting datasets with images of hundreds/thousands of pedestrians and nearly all the images being taken outdoors, SmartCity has few pedestrians in images and consists of both outdoor and indoor scenes: the average number of pedestrians is only 7.4 with minimum being 1 and maximum being 14."}, {"id": "atpchecker-automated-third-party-library-privacy-compliance-checker", "name": "ATPChecker (Automated Third-party library Privacy compliance Checker)", "description": "A novel dataset for identifying privacy policy compliance of Android third-party libraries."}, {"id": "craigslistbargains", "name": "CraigslistBargains", "description": "A richer dataset based on real items on Craigslist."}, {"id": "abcd-action-based-conversations-dataset", "name": "ABCD (Action-Based Conversations Dataset)", "description": "Action-Based Conversations Dataset (ABCD) is a goal-oriented dialogue fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. The dataset is proposed to study customer service dialogue systems in more realistic settings."}, {"id": "open-images-v4", "name": "Open Images V4", "description": "Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images) are provided. The images often show complex scenes with several objects (8 annotated objects per image on average). Visual relationships between them are annotated, which support visual relationship detection, an emerging task that requires structured reasoning."}, {"id": "dibco-and-h-dibco-handwritten-document-image-binarization-competition-dibco", "name": "DIBCO and H_DIBCO ((Handwritten) Document Image Binarization Competition (DIBCO))", "description": "The contest of binarization using a popular document database was organized called as Document Image Binarization Contest (DIBCO) from 2009 to 2019, except for 2015."}, {"id": "airsim-stereo-synthetic-dataset", "name": "AirSim Stereo Synthetic Dataset", "description": "Synthetic Dataset created in AirSim"}, {"id": "once-one-million-scenes", "name": "ONCE (One Million Scenes)", "description": "ONCE (One millioN sCenEs) is a dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than other 3D autonomous driving datasets available like nuScenes and Waymo, and it is collected across a range of different areas, periods and weather conditions. "}, {"id": "grid-dataset", "name": "GRID Dataset", "description": "The QMUL underGround Re-IDentification (GRID) dataset contains 250 pedestrian image pairs. Each pair contains two images of the same individual seen from different camera views. All images are captured from 8 disjoint camera views installed in a busy underground station. The figures beside show a snapshot of each of the camera views of the station and sample images in the dataset. The dataset is challenging due to variations of pose, colours, lighting changes; as well as poor image quality caused by low spatial resolution."}, {"id": "shift15m", "name": "SHIFT15M", "description": "SHIFT15M is a dataset that can be used to properly evaluate models in situations where the distribution of data changes between training and testing.  The SHIFT15M dataset has several good properties: (i) Multiobjective. Each instance in the dataset has several numerical values that can be used as target variables. (ii) Large-scale. The SHIFT15M dataset consists of 15million fashion images. (iii) Coverage of types of dataset shifts. SHIFT15M contains multiple dataset shift problem settings (e.g., covariate shift or target shift). SHIFT15M also enables the performance evaluation of the model under various magnitudes of dataset shifts by switching the magnitude."}, {"id": "audioset", "name": "AudioSet", "description": "Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set."}, {"id": "deeplesion", "name": "DeepLesion", "description": "The National Institutes of Health\u2019s Clinical Center has made a large-scale dataset of CT images publicly available to help the scientific community improve detection accuracy of lesions. While most publicly available medical image datasets have less than a thousand lesions, this dataset, named DeepLesion, has over 32,000 annotated lesions (220GB) identified on CT images. DeepLesion, a dataset with 32,735 lesions in 32,120 CT slices from 10,594 studies of 4,427 unique patients. There are a variety of lesion types in this dataset, such as lung nodules, liver tumors, enlarged lymph nodes, and so on. It has the potential to be used in various medical image applications"}, {"id": "hs-sod-hyperspectral-salient-object-detection-dataset", "name": "HS-SOD (HyperSpectral Salient Object Detection Dataset)", "description": "HS-SOD is a hyperspectral salient object detection dataset with a collection of 60 hyperspectral images with their respective ground-truth binary images and representative rendered colour images (sRGB)."}, {"id": "ccpd-chinese-city-parking-dataset", "name": "CCPD (Chinese City Parking Dataset)", "description": "The Chinese City Parking Dataset (CCPD) is a dataset for license plate detection and recognition. It contains over 250k unique car images, with license plate location annotations."}, {"id": "mirrored-human", "name": "Mirrored-Human", "description": "Mirrored-Human is a dataset for 3D pose estimation from a single view. It covers a large variety of human subjects, poses and backgrounds. The images are collected from the internet and consists of people in front of mirrors, were both the person and the reflected image are visible. Actions cover dancing, fitness, mirror installation, swing practice"}, {"id": "mobility-flow", "name": "Mobility Flow", "description": "This is a multiscale dynamic human mobility flow dataset across the United States, with data starting from January 1st, 2019. By analyzing millions of anonymous mobile phone users\u2019 visit trajectories to various places provided by SafeGraph, the daily and weekly dynamic origin-to-destination (O-D) population flows are computed, aggregated, and inferred at three geographic scales: census tract, county, and state."}, {"id": "ip102", "name": "IP102", "description": "IP102 contains more than 75,000 images belonging to 102 categories, which exhibit a natural long-tailed distribution."}, {"id": "wmt-2016", "name": "WMT 2016", "description": "WMT 2016 is a collection of datasets used in shared tasks of the First Conference on Machine Translation. The conference builds on ten previous Workshops on statistical Machine Translation."}, {"id": "iiw-intrinsic-images-in-the-wild", "name": "IIW (Intrinsic Images in the Wild)", "description": "Intrinsic Images in the Wild is a large scale, public dataset for intrinsic image decompositions of real-world scenes selected from the OpenSurfaces dataset. Each image is annotated with crowdsourced pairwise comparisons of material properties. "}, {"id": "vqa-cp", "name": "VQA-CP", "description": "The VQA-CP dataset was constructed by reorganizing VQA v2 such that the correlation between the question type and correct answer differs in the training and test splits. For example, the most common answer to questions starting with What sport\u2026 is tennis in the training set, but skiing in the test set. A model that guesses an answer primarily from the question will perform poorly."}, {"id": "orl-our-database-of-faces", "name": "ORL (Our Database of Faces)", "description": "The ORL Database of Faces contains 400 images from 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). The size of each image is 92x112 pixels, with 256 grey levels per pixel."}, {"id": "supermat", "name": "SuperMat", "description": "A growing number of papers are published in the area of superconducting materials science. However, novel text and data mining (TDM) processes are still needed to efficiently access and exploit this accumulated knowledge, paving the way towards data-driven materials design. Herein, we present SuperMat (Superconductor Materials), an annotated corpus of linked data derived from scientific publications on superconductors, which comprises 142 articles, 16052 entities, and 1398 links that are characterised into six categories: the names, classes, and properties of materials; links to their respective superconducting critical temperature (Tc); and parametric conditions such as applied pressure or measurement methods. The construction of SuperMat resulted from a fruitful collaboration between computer scientists and material scientists, and its high quality is ensured through validation by domain experts. The quality of the annotation guidelines was ensured by satisfactory Inter Annotator Agreement (IAA) between the annotators and the domain experts."}, {"id": "red-miniimagenet-80-label-noise", "name": "Red MiniImageNet 80% label noise", "description": "Part of the Controlled Noisy Web Labels Dataset."}, {"id": "cornell-48-32-20-fixed-splits", "name": "Cornell (48%/32%/20% fixed splits)", "description": "Node classification on Cornell with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "extremeweather", "name": "ExtremeWeather", "description": "Encourages machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change."}, {"id": "ubc3v-dataset", "name": "UBC3V Dataset", "description": "~6 million synthetic depth frames for pose estimation from multiple cameras."}, {"id": "tempeval-3-tempeval-3-events-times-and-temporal-relations", "name": "TempEval-3 (TempEval-3: events, times, and temporal relations)", "description": "Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems \u2013 in each task and in general."}, {"id": "spaces", "name": "Spaces", "description": "We introduce our new dataset, Spaces, to provide a more challenging shared dataset for future view synthesis research. Spaces consists of 100 indoor and outdoor scenes, captured using a 16-camera rig.  For each scene, we captured image sets at 5-10 slightly different rig positions (within \u223c10cm of each other).  This jittering of the rig position provides a flexible dataset for view synthesis, as we can mix views from different rig positions for the same scene during training.  We calibrated the intrinsics and the relative pose of the rig cameras using a standard structure from motion approach, using the nominal rig layout as a prior.  We corrected exposure differences .  For our main experiments we undistort the images and downsample them to a resolution of 800 \u00d7 480. We use 90 scenes from the dataset for training and hold out 10 for evaluation."}, {"id": "european-flood-2013-dataset", "name": "European Flood 2013 Dataset", "description": "This dataset consists of 3,710 flood images, annotated by domain experts regarding their relevance with respect to three tasks (determining the flooded area, inundation depth, water pollution)."}, {"id": "promptspeech", "name": "PromptSpeech", "description": "PromptSpeech is a dataset that consists of speech and the corresponding prompts. We synthesize speech with 5 different style factors (gender, pitch, speaking speed, volume, and emotion) from a commercial TTS API. The emotion factor has 5 categories and the gender factor has 2 categories."}, {"id": "assembly101", "name": "Assembly101", "description": "Assembly101 is a new procedural activity dataset featuring 4321 videos of people assembling and disassembling 101 \"take-apart\" toy vehicles. Participants work without fixed instructions, and the sequences feature rich and natural variations in action ordering, mistakes, and corrections. Assembly101 is the first multi-view action dataset, with simultaneous static (8) and egocentric (4) recordings. Sequences are annotated with more than 100K coarse and 1M fine-grained action segments, and 18M 3D hand poses. We benchmark on three action understanding tasks: recognition, anticipation and temporal segmentation. Additionally, we propose a novel task of detecting mistakes. The unique recording format and rich set of annotations allow us to investigate generalization to new toys, cross-view transfer, long-tailed distributions, and pose vs. appearance. We envision that Assembly101 will serve as a new challenge to investigate various activity understanding problems."}, {"id": "vidor", "name": "VidOR", "description": "VidOR (Video Object Relation) dataset contains 10,000 videos (98.6 hours) from YFCC100M collection together with a large amount of fine-grained annotations for relation understanding. In particular, 80 categories of objects are annotated with bounding-box trajectory to indicate their spatio-temporal location in the videos; and 50 categories of relation predicates are annotated among all pairs of annotated objects with starting and ending frame index. This results in around 50,000 object and 380,000 relation instances annotated. To use the dataset for model development, the dataset is split into 7,000 videos for training, 835 videos for validation, and 2,165 videos for testing."}, {"id": "isic-2019", "name": "ISIC 2019", "description": "The goal for ISIC 2019 is classify dermoscopic images among nine different diagnostic categories.25,331 images are available for training across 8 different categories. Two tasks will be available for participation: 1) classify dermoscopic images without meta-data, and 2) classify images with additional available meta-data."}, {"id": "jhu-crowd-2", "name": "JHU-CROWD++", "description": "JHU-CROWD++ is A large-scale unconstrained crowd counting dataset with 4,372 images and 1.51 million annotations. This dataset is collected under a variety of diverse scenarios and environmental conditions. In addition, the dataset provides comparatively richer set of annotations like dots, approximate bounding boxes, blur levels, etc."}, {"id": "tapaco", "name": "TaPaCo", "description": "TaPaCo is a freely available paraphrase corpus for 73 languages extracted from the Tatoeba database."}, {"id": "eisen-funcat", "name": "Eisen Funcat", "description": "Hierarchical-multilabel classification dataset for functional genomics"}, {"id": "asset-corpus", "name": "ASSET Corpus", "description": "A crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations."}, {"id": "rsblur", "name": "RSBlur", "description": "The RSBlur dataset provides pairs of real and synthetic blurred images with ground truth sharp images. The dataset enables the evaluation of deblurring methods and blur synthesis methods on real-world blurred images.  Training, validation, and test sets consist of 8,878, 1,120, and 3,360 blurred images, respectively."}, {"id": "musescore", "name": "MuseScore", "description": "The MuseScore dataset is a collection of 344,166 audio and MIDI pairs downloaded from MuseScore website. The audio is usually synthesized by the MuseScore synthesizer. The audio clips have diverse musical genres and are about two mins long on average."}, {"id": "neuralnews", "name": "NeuralNews", "description": "NeuralNews is a dataset for machine-generated news detection. It consists of human-generated and machine-generated articles. The human-generated articles are extracted from the GoodNews dataset, which is extracted from the New York Times. It contains 4 types of articles:"}, {"id": "human-activity-recognition", "name": "Human Activity Recognition", "description": "We provide six different datasets with diverse range of activities"}, {"id": "scanobjectnn", "name": "ScanObjectNN", "description": "ScanObjectNN is a newly published real-world dataset comprising of 2902 3D objects in 15 categories. It is a challenging point cloud classification datasets due to the background, missing parts and deformations."}, {"id": "dwie-deutsche-welle-corpus-for-information-extraction", "name": "DWIE (Deutsche Welle corpus for Information Extraction)", "description": "The 'Deutsche Welle corpus for Information Extraction' (DWIE) is a multi-task dataset that combines four main Information Extraction (IE) annotation sub-tasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document."}, {"id": "aff-wild2", "name": "Aff-Wild2", "description": "Aff-Wild2 is an extension of the Aff-Wild dataset for affect recognition. It approximately doubles the number of included video frames and the number of subjects; thus, improving the variability of the included behaviors and of the involved persons. "}, {"id": "2021-hotel-id", "name": "2021 Hotel-ID", "description": "2021 Hotel-ID is a dataset for hotel recognition to help raise awareness of human trafficking and generate novel approaches. The dataset consists of hotel room images that have been crowd-sourced and uploaded through the TraffickCam mobile application."}, {"id": "glas-gland-segmentation-in-colon-histology-images-challenge", "name": "GlaS (Gland Segmentation in Colon Histology Images Challenge)", "description": "The dataset used in this challenge consists of 165 images derived from 16 H&E stained histological sections of stage T3 or T42 colorectal adenocarcinoma. Each section belongs to a different patient, and sections were processed in the laboratory on different occasions. Thus, the dataset exhibits high inter-subject variability in both stain distribution and tissue architecture. The digitization of these histological sections into whole-slide images (WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a pixel resolution of 0.465\u00b5m."}, {"id": "28-ghz-wireless-channel-dataset", "name": "28 Ghz wireless channel dataset", "description": "Our dataset which consists of multiple indoor and outdoor experiments for up to 30 m gNB-UE link. In each experiment, we fixed the location of the gNB and move the UE with an increment of roughly one degrees. The table above specifies the direction of user movement with respect to gNB-UE link, distance resolution, and the number of user locations for which we conduct channel measurements. Outdoor 30 m data also contains blockage between 3.9 m to 4.8 m. At each location, we scan the transmission beam and collect data for each beam. By doing so, we can get the full OFDM channels for different locations along the moving trajectory with all the beam angles. Moreover, we use 240 kHz subcarrier spacing, which is consistent with the 5G NR numerology at FR2, so the data we collect will be a true reflection of what a 5G UE will see."}, {"id": "dcase-2013", "name": "DCASE 2013", "description": "DCASE 2013 is a dataset for sound event detection. It consists of audio-only recordings where individual sound events are prominent in an acoustic scene."}, {"id": "vot2018", "name": "VOT2018", "description": "VOT2018 is a dataset for visual object tracking. It consists of 60 challenging videos collected from real-life datasets."}, {"id": "textzoom", "name": "TextZoom", "description": "TextZoom is a super-resolution dataset that consists of paired Low Resolution \u2013 High Resolution scene text images. The images are captured by cameras with different focal length in the wild."}, {"id": "cqadupstack", "name": "CQADupStack", "description": "CQADupStack is a benchmark dataset for community question-answering research. It contains threads from twelve StackExchange subforums, annotated with duplicate question information. Pre-defined training and test splits are provided, both for retrieval and classification experiments, to ensure maximum comparability between different studies using the set. Furthermore, it comes with a script to manipulate the data in various ways."}, {"id": "ogb-open-graph-benchmark", "name": "OGB (Open Graph Benchmark)", "description": "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. OGB datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can be evaluated using the OGB Evaluator in a unified manner. OGB is a community-driven initiative in active development."}, {"id": "tablebank", "name": "TableBank", "description": "To address the need for a standard open domain table benchmark dataset, the author propose a novel weak supervision approach to automatically create the TableBank, which is orders of magnitude larger than existing human labeled datasets for table analysis. Distinct from traditional weakly supervised training set, our approach can obtain not only large scale but also high quality training data."}, {"id": "coached-conversational-preference-elicitation", "name": "Coached Conversational Preference Elicitation", "description": "Coached Conversational Preference Elicitation is a dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language. It was collected using a Wizard-of-Oz methodology between two paid crowd-workers, where one worker plays the role of an 'assistant', while the other plays the role of a 'user'."}, {"id": "twitch-gamers", "name": "twitch-gamers", "description": "node classification on twitch-gamers"}, {"id": "so2sat-lcz42", "name": "So2Sat LCZ42", "description": "So2Sat LCZ42 consists of local climate zone (LCZ) labels of about half a million Sentinel-1 and Sentinel-2 image patches in 42 urban agglomerations (plus 10 additional smaller areas) across the globe. This dataset was labeled by 15 domain experts following a carefully designed labeling work flow and evaluation process over a period of six months. "}, {"id": "higgs-data-set", "name": "HIGGS Data Set", "description": "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set."}, {"id": "redwood-3dscan", "name": "redwood-3dscan", "description": "A dataset of more than ten thousand 3D scans of real objects. "}, {"id": "ucla-aerial-event-dataset", "name": "UCLA Aerial Event Dataset", "description": "The UCLA Aerial Event Dataest has been captured by a low-cost hex-rotor with a GoPro camera, which is able to eliminate the high frequency vibration of the camera and hold in air autonomously through a GPS and a barometer. It can also fly 20 \u223c 90m above the ground and stays 5 minutes in air. "}, {"id": "kuairand", "name": "KuaiRand", "description": "KuaiRand is an unbiased sequential recommendation dataset collected from the recommendation logs of the video-sharing mobile app, Kuaishou (\u5feb\u624b). It is the first recommendation dataset with millions of intervened interactions of randomly exposed items inserted in the standard recommendation feeds!"}, {"id": "walt-watch-and-learn-timelapse-images", "name": "WALT (Watch and Learn TimeLapse Images)", "description": "We introduce a new dataset, Watch and Learn Time-lapse (WALT), consisting of multiple (4K and 1080p) cameras capturing urban environments over a year."}, {"id": "acl-arc", "name": "ACL ARC", "description": "ACL Anthology Reference Corpus (ACL ARC) is a collection of 10,920 academic papers from the ACL Anthology. ACL ARC is cleaned to remove:"}, {"id": "timebankpt-portuguese-timebank", "name": "TimeBankPT (Portuguese TimeBank)", "description": "TimeBankPT is a corpus of Portuguese text with annotations about time. The annotation scheme used is similar to TimeML. TimeBankPT is the result of adapting the English corpus used in the first TempEval challenge to the Portuguese language."}, {"id": "robocup", "name": "RoboCup", "description": "RoboCup is an initiative in which research groups compete by enabling their robots to play football matches. Playing football requires solving several challenging tasks, such as vision, motion, and team coordination. Framing the research efforts onto football attracts public interest (and potential research funding) in robotics, which may otherwise be less entertaining to non-experts."}, {"id": "mot16-multiple-object-tracking-2016", "name": "MOT16 (Multiple Object Tracking 2016)", "description": "The MOT16 dataset is a dataset for multiple object tracking. It a collection of existing and new data (part of the sources are from and ), containing 14 challenging real-world videos of both static scenes and moving scenes, 7 for training and 7 for testing. It is a large-scale dataset, composed of totally 110407 bounding boxes in training set and 182326 bounding boxes in test set. All video sequences are annotated under strict standards, their ground-truths are highly accurate, making the evaluation meaningful."}, {"id": "mlrsnet", "name": "MLRSNet", "description": "MLRSNet is a a multi-label high spatial resolution remote sensing dataset for semantic scene understanding. It provides different perspectives of the world captured from satellites. That is, it is composed of high spatial resolution optical satellite images. MLRSNet contains 109,161 remote sensing images that are annotated into 46 categories, and the number of sample images in a category varies from 1,500 to 3,000. The images have a fixed size of 256\u00d7256 pixels with various pixel resolutions (~10m to 0.1m). Moreover, each image in the dataset is tagged with several of 60 predefined class labels, and the number of labels associated with each image varies from 1 to 13. The dataset can be used for multi-label based image classification, multi-label based image retrieval, and image segmentation."}, {"id": "pubmed-60-20-20-random-splits", "name": "PubMed (60%/20%/20% random splits)", "description": "Node classification on PubMed with 60%/20%/20% random splits for training/validation/test."}, {"id": "wikievents", "name": "WikiEvents", "description": "WikiEvents is a document-level event extraction benchmark dataset which includes complete event and coreference annotation."}, {"id": "cadsketchnet", "name": "CADSketchNet", "description": "CADSketchNet is an annotated collection of sketches of 3D CAD models."}, {"id": "4dfab", "name": "4DFAB", "description": "4DFAB is a large scale database of dynamic high-resolution 3D faces which consists of recordings of 180 subjects captured in four different sessions spanning over a five-year period (2012 - 2017), resulting in a total of over 1,800,000 3D meshes. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour."}, {"id": "agedb", "name": "AgeDB", "description": "AgeDB contains 16, 488 images of various famous people, such as actors/actresses, writers, scientists, politicians, etc. Every image is annotated with respect to the identity, age and gender attribute. There exist a total of 568 distinct subjects. The average number of images per subject is 29. The minimum and maximum age is 1 and 101, respectively. The average age range for each subject is 50.3 years."}, {"id": "vizwiz-vizwiz-vqa", "name": "VizWiz (VizWiz-VQA)", "description": "The VizWiz-VQA dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question. The proposed challenge addresses the following two tasks for this dataset: predict the answer to a visual question and (2) predict whether a visual question cannot be answered."}, {"id": "acl-title-and-abstract-dataset", "name": "ACL Title and Abstract Dataset", "description": "This dataset gathers 10,874 title and abstract pairs from the ACL Anthology Network (until 2016)."}, {"id": "indonli", "name": "IndoNLI", "description": "IndoNLI is the first human-elicited NLI dataset for Indonesian consisting of nearly 18K sentence pairs annotated by crowd workers and experts."}, {"id": "cluttered-omniglot", "name": "Cluttered Omniglot", "description": "Dataset for one-shot segmentation."}, {"id": "shoev2", "name": "ShoeV2", "description": "ShoeV2 is a dataset of 2,000 photos and 6648 sketches of shoes. The dataset is designed for fine-grained sketch-based image retrieval."}, {"id": "searchqa", "name": "SearchQA", "description": "SearchQA was built using an in-production, commercial search engine. It closely reflects the full pipeline of a (hypothetical) general question-answering system, which consists of information retrieval and answer synthesis. "}, {"id": "adl-piano-midi", "name": "ADL Piano MIDI", "description": "The ADL Piano MIDI is a dataset of 11,086 piano pieces from different genres. This dataset is based on the Lakh MIDI dataset, which is a collection on 45,129 unique MIDI files that have been matched to entries in the Million Song Dataset. Most pieces in the Lakh MIDI dataset have multiple instruments, so for each file the authors of ADL Piano MIDI dataset extracted only the tracks with instruments from the \"Piano Family\" (MIDI program numbers 1-8). This process generated a total of 9,021 unique piano MIDI files. Theses 9,021 files were then combined with other approximately 2,065 files scraped from publicly-available sources on the internet. All the files in the final collection were de-duped according to their MD5 checksum."}, {"id": "erato", "name": "ERATO", "description": "ERATO is a large-scale multi-modal dataset for Pairwise Emotional Relationship Recognition (PERR). It has 31,182 video clips, lasting about 203 video hours. Different from the existing datasets, ERATO contains interaction-centric videos with multi-shots, varied video length, and multiple modalities including visual, audio and text"}, {"id": "django", "name": "Django", "description": "The Django dataset is a dataset for code generation comprising of 16000 training, 1000 development and 1805 test annotations. Each data point consists of a line of Python code together with a manually created natural language description."}, {"id": "eurocity-persons", "name": "EuroCity Persons", "description": "The EuroCity Persons dataset provides a large number of highly diverse, accurate and detailed annotations of pedestrians, cyclists and other riders in urban traffic scenes. The images for this dataset were collected on-board a moving vehicle in 31 cities of 12 European countries. With over 238,200 person instances manually labeled in over 47,300 images, EuroCity Persons is nearly one order of magnitude larger than person datasets used previously for benchmarking. The dataset furthermore contains a large number of person orientation annotations (over 211,200)."}, {"id": "fes-fisheye-evaluation-suite", "name": "FES (Fisheye Evaluation Suite)", "description": "FES is an indoor dataset that can be used for evaluation of deep learning approaches. It consists of 301 top-view fisheye images from an indoor scene. Annotations include bounding boxes and instance segmentation masks for 6 classes."}, {"id": "tll-totally-looks-like", "name": "TLL (Totally-Looks-Like)", "description": "Contains 6016 image-pairs from the wild, shedding light upon a rich and diverse set of criteria employed by human beings."}, {"id": "bongard-hoi", "name": "Bongard-HOI", "description": "Bongard-HOI testifies to which extent your few-shot visual learner can quickly induce the true HOI concept from a handful of images and perform reasoning with it. Further, the learner is also expected to transfer the learned few-shot skills to novel HOI concepts compositionally."}, {"id": "whos-waldo", "name": "Who\u2019s Waldo", "description": "Who's Waldo is a dataset of 270K image\u2013caption pairs, depicting interactions of people, that is automatically mined from Wikimedia Commons. It is a benchmark dataset for person-centric visual grounding, the problem of linking between people named in a caption and people pictured in an image."}, {"id": "vfr-wild", "name": "VFR-Wild", "description": "325 word images intended for font recognition, whose fonts are included in VFR-447 (and VFR-2420)."}, {"id": "banglawriting", "name": "BanglaWriting", "description": "The BanglaWriting dataset contains single-page handwritings of 260 individuals of different personalities and ages. Each page includes bounding-boxes that bounds each word, along with the unicode representation of the writing. This dataset contains 21,234 words and 32,787 characters in total. Moreover, this dataset includes 5,470 unique words of Bangla vocabulary. Apart from the usual words, the dataset comprises 261 comprehensible overwriting and 450 incomprehensible overwriting. All of the bounding boxes and word labels are manually-generated. The dataset can be used for complex optical character/word recognition, writer identification, and handwritten word segmentation. Furthermore, this dataset is suitable for extracting age-based and gender-based variation of handwriting."}, {"id": "bosphorussign22k", "name": "BosphorusSign22k", "description": "BosphorusSign22k is a benchmark dataset for vision-based user-independent isolated Sign Language Recognition (SLR). The dataset is based on the BosphorusSign (Camgoz et al., 2016c) corpus which was collected with the purpose of helping both linguistic and computer science communities. It contains isolated videos of Turkish Sign Language glosses from three different domains: Health, finance and commonly used everyday signs. Videos in this dataset were performed by six native signers, which makes this dataset valuable for user independent sign language studies."}, {"id": "look", "name": "LOOK", "description": "LOOK is a large-scale dataset for eye contact detection in the wild, which focuses on diverse and unconstrained scenarios for real-world generalization. The dataset focuses on real-world scenarios for autonomous vehicles with no control over the environment or the distance of pedestrians"}, {"id": "div2k", "name": "DIV2K", "description": "DIV2K is a popular single-image super-resolution dataset which contains 1,000 images with different scenes and is splitted to 800 for training, 100 for validation and 100 for testing. It was collected for NTIRE2017 and NTIRE2018 Super-Resolution Challenges in order to encourage research on image super-resolution with more realistic degradation. This dataset contains low resolution images with different types of degradations. Apart from the standard bicubic downsampling, several types of degradations are considered in synthesizing low resolution images for different tracks of the challenges. Track 2 of NTIRE 2017 contains low resolution images with unknown x4 downscaling. Track 2 and track 4 of NTIRE 2018 correspond to realistic mild \u00d74 and realistic wild \u00d74 adverse conditions, respectively. Low-resolution images under realistic mild x4 setting suffer from motion blur, Poisson noise and pixel shifting. Degradations under realistic wild x4 setting are further extended to be of different levels from image to image."}, {"id": "sp-10k", "name": "SP-10K", "description": "A large-scale evaluation set that provides human ratings for the plausibility of 10,000 SP pairs over five SP relations, covering 2,500 most frequent verbs, nouns, and adjectives in American English."}, {"id": "google-refexp", "name": "Google Refexp", "description": "A new large-scale dataset for referring expressions, based on MS-COCO."}, {"id": "mnist-8m-infinite-mnist", "name": "MNIST-8M (Infinite MNIST)", "description": "MNIST8M is derived from the MNIST dataset by applying random deformations and translations to the dataset."}, {"id": "decagon-bio-decagon", "name": "Decagon (Bio-decagon)", "description": "Bio-decagon is a dataset for polypharmacy side effect identification problem framed as a multirelational link prediction problem in a two-layer multimodal graph/network of two node types: drugs and proteins. Protein-protein interaction network describes relationships between proteins. Drug-drug interaction network contains 964 different types of edges (one for each side effect type) and describes which drug pairs lead to which side effects. Lastly, drug-protein links describe the proteins targeted by a given drug."}, {"id": "tunizi", "name": "TUNIZI", "description": "A sentiment analysis Tunisian Arabizi Dataset, collected from social networks, preprocessed for analytical studies and annotated manually by Tunisian native speakers."}, {"id": "grab", "name": "GRAB", "description": "GRAB is a dataset of full-body motions interacting and grasping 3D objects. It contains accurate finger and facial motions as well as the contact between the objects and body. It contains 5 male and 5 female participants and 4 different motion intents. The GRAB dataset also contains binary contact maps between the body and objects."}, {"id": "nkl", "name": "NKL", "description": "NKL (short for NanKai Lines) is a dataset for semantic line detection. Semantic lines are meaningful line structures that outline the conceptual structure of natural images.  The NKL dataset contains 5,000 images of various scenes. Each of these images is annotated by multiple skilled human annotators. The dataset is split into training and validation subsets. There are 4,000 images in the training set and 1,000 in the validation set."}, {"id": "apollocar3d", "name": "ApolloCar3D", "description": "ApolloCar3DT is a dataset that contains 5,277 driving images and over 60K car instances, where each car is fitted with an industry-grade 3D CAD model with absolute model size and semantically labelled keypoints. This dataset is above 20 times larger than PASCAL3D+ and KITTI, the current state-of-the-art. "}, {"id": "physionet-challenge-2012", "name": "PhysioNet Challenge 2012", "description": "The PhysioNet Challenge 2012 dataset is publicly available and contains the de-identified records of 8000 patients in Intensive Care Units (ICU). Each record consists of roughly 48 hours of multivariate time series data with up to 37 features recorded at various times from the patients during their stay such as respiratory rate, glucose etc."}, {"id": "raindrop", "name": "Raindrop", "description": "Raindrop is a set of image pairs, where each pair contains exactly the same background scene, yet one is degraded by raindrops and the other one is free from raindrops. To obtain this, the images are captured through two pieces of exactly the same glass: one sprayed with water, and the other is left clean. The dataset consists of 1,119 pairs of images, with various background scenes and raindrops. They were captured with a Sony A6000 and a Canon EOS 60."}, {"id": "mumin", "name": "MuMiN", "description": "MuMiN is a misinformation graph dataset containing rich social media data (tweets, replies, users, images, articles, hashtags), spanning 21 million tweets belonging to 26 thousand Twitter threads, each of which have been semantically linked to 13 thousand fact-checked claims across dozens of topics, events and domains, in 41 different languages, spanning more than a decade."}, {"id": "musdb18-hq", "name": "MUSDB18-HQ", "description": "MUSDB18-HQ is a high-quality version of the MUSDB18 music tracks dataset. The high-quality dataset consists of the same 150 songs, but instead of MP4 files (compressed with Advanced Audio Coding encoder at 256kbps, with bandwidth limited to 16kHz), the songs are provided as raw WAV files."}, {"id": "houseexpo", "name": "HouseExpo", "description": "A large-scale indoor layout dataset containing 35,357 2D floor plans including 252,550 rooms in total."}, {"id": "wham-wsj0-hipster-ambient-mixtures", "name": "WHAM! (WSJ0 Hipster Ambient Mixtures)", "description": "The WSJ0 Hipster Ambient Mixtures (WHAM!) dataset pairs each two-speaker mixture in the wsj0-2mix dataset with a unique noise background scene. It has an extension called WHAMR! that adds artificial reverberation to the speech signals in addition to the background noise."}, {"id": "gamma-challenge", "name": "GAMMA Challenge", "description": "GAMMA releases the world's first multi-modal dataset for glaucoma grading, which was provided by the Sun Yat-sen Ophthalmic Center of Sun Yat-sen University in Guangzhou, China. The dataset consists of 2D fundus images and 3D optical coherence tomography (OCT) images of 300 patients. The dataset was annotated with glaucoma grade in every sample, and macular fovea coordinates as well as optic disc/cup segmentation mask in the fundus image. "}, {"id": "esxnli", "name": "esXNLI", "description": "esXNLI is a bilingual NLI dataset. It comprises 2,490 examples from 5 different genres that were originally annotated in Spanish, and translated into English by professional translators. It serves as a counterpoint to XNLI, which was originally annotated in English and translated into 14 other languages, including Spanish. The dataset was conceived to be used in conjunction with the XNLI development set to analyse the effect of translation in cross-lingual transfer learning."}, {"id": "modern-office-31", "name": "Modern Office-31", "description": "Modern Office-31 is a refurbished version of the commonly used Office-31 dataset. Modern Office-31 rectifies many of the annotation errors and low quality images in the Amazon domain of the original Office-31 dataset. Additionally, this dataset adds another synthetic domain based on the Adaptiope dataset."}, {"id": "vgg-sound", "name": "VGG-Sound", "description": "Consists of more than 210k videos for 310 audio classes."}, {"id": "rtmv", "name": "RTMV", "description": "RTMV is a large-scale synthetic dataset for novel view synthesis consisting of \u223c300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 \u00d7 1600 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis, thus providing a large unified benchmark for both training and evaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of our dataset exhibit challenging variations in camera views, lighting, shape, materials, and textures."}, {"id": "wikinldb", "name": "WikiNLDB", "description": "WikiNLDB is a novel dataset for training Natural Language Databases (NLDBs) which is generated by transforming structured data from Wikidata into natural language facts and queries."}, {"id": "opt-object-pose-tracking", "name": "OPT (Object Pose Tracking)", "description": "Accurately tracking the six degree-of-freedom pose of an object in real scenes is an important task in computer vision and augmented reality with numerous applications. Although a variety of algorithms for this task have been proposed, it remains difficult to evaluate existing methods in the literature as oftentimes different sequences are used and no large benchmark datasets close to real-world scenarios are available. In this paper, we present a large object pose tracking benchmark dataset consisting of RGB-D video sequences of 2D and 3D targets with ground-truth information. The videos are recorded under various lighting conditions, different motion patterns and speeds with the help of a programmable robotic arm. We present extensive quantitative evaluation results of the state-of-the-art methods on this benchmark dataset and discuss the potential research directions in this field."}, {"id": "linux-linux-program-dependence-graphs", "name": "Linux (Linux Program Dependence Graphs)", "description": "The LINUX dataset consists of 48,747 Program Dependence Graphs (PDG) generated from the Linux kernel. Each graph represents a function, where a node represents one statement and an edge represents the dependency between the two statements"}, {"id": "3d-front", "name": "3D-FRONT", "description": "3D-FRONT (3D Furnished Rooms with layOuts and semaNTics) is large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility. From layout semantics down to texture details of individual objects, the dataset is freely available to the academic community and beyond. "}, {"id": "hhoi", "name": "HHOI", "description": "A new RGB-D video dataset, i.e., UCLA Human-Human-Object Interaction (HHOI) dataset, which includes 3 types of human-human interactions, i.e., shake hands, high-five, pull up, and 2 types of human-object-human interactions, i.e., throw and catch, and hand over a cup. On average, there are 23.6 instances per interaction performed by totally 8 actors recorded from various views. Each interaction lasts 2-7 seconds presented at 10-15 fps."}, {"id": "a2dre-extension-of-a2d-sentences-where-trivial-cases-where-filtered", "name": "A2Dre+ (Extension of A2D sentences where trivial cases where filtered)", "description": "A2Dre is a subset from the A2D test set including $433$~\\textit{non-trivial} REs. Due to its highly unbalanced distribution across the $7$~semantic categories we select the $4$~major categories \\textsl{appearance, location, motion and static}. The four categories have in common that in most cases, for a given referent, a RE can be provided that expresses a certain category, and one that does not. We use these categories to augment A2Dre with additional REs, which vary according to the presence or absence of each of them. Specifically, based on our categorization of the original REs, for each RE~$re$ and category~$C$, we produce an additional RE~$re'$ by modifying $re$ slightly such that it does (or does not) express~$C$.  For example, for the last RE in  Figure~\\ref{fig:a2d-images}, i.e. \\emph{girl in yellow dress standing near the woman}, which could be categorized as \\textit{appearance}, \\textit{location}, no \\textit{motion} and \\textit{static}, we produce new REs for each category: \\emph{girl standing near the woman} (no \\textit{appearance}), \\emph{girl in yellow dress standing} (no \\textit{location}), \\emph{girl in yellow dress walking} (\\textit{motion}) and \\emph{girl in yellow dress near the woman} (no \\textit{static}).  We do not apply this procedure for \\textsl{category}, since it is expressed in almost all REs, and its removal may be difficult in many cases.  We name this extended dataset as A2Dre+."}, {"id": "diabetic-retinopathy-detection-dataset", "name": "Diabetic Retinopathy Detection Dataset", "description": "A large scale of retina image dataset."}, {"id": "gsm8k", "name": "GSM8K", "description": "GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers. The dataset is segmented into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ \u2212 \u00d7\u00f7) to reach the final answer. A bright middle school student should be able to solve every problem. It can be used for multi-step mathematical reasoning."}, {"id": "deepweeds", "name": "DeepWeeds", "description": "The DeepWeeds dataset consists of 17,509 images capturing eight different weed species native to Australia in situ with neighbouring flora."}, {"id": "open-relation-modeling", "name": "Open Relation Modeling", "description": "Given two entities, generating a coherent sentence describing the relation between them."}, {"id": "qmnist", "name": "QMNIST", "description": "The exact pre-processing steps used to construct the MNIST dataset have long been lost. This leaves us with no reliable way to associate its characters with the ID of the writer and little hope to recover the full MNIST testing set that had 60K images but was never released. The official MNIST testing set only contains 10K randomly sampled images and is often considered too small to provide meaningful confidence intervals. The QMNIST dataset was generated from the original data found in the NIST Special Database 19 with the goal to match the MNIST preprocessing as closely as possible. QMNIST is licensed under the BSD-style license."}, {"id": "animal-animal-10n", "name": "ANIMAL (ANIMAL-10N)", "description": "10 classes with 50, 000 training and 5, 000 testing images. Please note that, in ANIMAL10N, noisy labels were injected naturally by human mistakes, where its noise rate was estimated at 8%."}, {"id": "a-dataset-of-multispectral-potato-plants-images", "name": "A Dataset of Multispectral Potato Plants Images", "description": "The dataset contains aerial agricultural images of a potato field with manual labels of healthy and stressed plant regions. The images were collected with a Parrot Sequoia multispectral camera carried by a 3DR Solo drone flying at an altitude of 3 meters. The dataset consists of RGB images with a resolution of 750\u00d7750 pixels, and spectral monochrome red, green, red-edge, and near-infrared images with a resolution of 416\u00d7416 pixels, and XML files with annotated bounding boxes of healthy and stressed potato crop."}, {"id": "nlvr-natural-language-visual-reasoningnatural-language-for-visual-reasoning", "name": "NLVR (Natural Language Visual Reasoningnatural language for visual reasoning)", "description": "NLVR contains 92,244 pairs of human-written English sentences grounded in synthetic images. Because the images are synthetically generated, this dataset can be used for semantic parsing."}, {"id": "brno-urban-dataset", "name": "Brno-Urban-Dataset", "description": "This self-driving dataset collected in Brno, Czech Republic contains data from four WUXGA cameras, two 3D LiDARs, inertial measurement unit, infrared camera and especially differential RTK GNSS receiver with centimetre accuracy."}, {"id": "covid-19-fake-news-dataset-covid19-fake-news-detection-in-english", "name": "COVID-19 Fake News Dataset (COVID19 Fake News Detection in English)", "description": "Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news and rumors are rampant on social media. Believing in rumors can cause significant harm. This is further exacerbated at the time of a pandemic. To tackle this, we curate and release a manually annotated dataset of 10,700 social media posts and articles of real and fake news on COVID-19. We benchmark the annotated dataset with four machine learning baselines - Decision Tree, Logistic Regression , Gradient Boost , and Support Vector Machine (SVM). We obtain the best performance of 93.46\\% F1-score with SVM."}, {"id": "citeseer-48-32-20-fixed-splits", "name": "Citeseer (48%/32%/20% fixed splits)", "description": "Node classification on Citeseer with the fixed 48%/32%/20% splits provided by Geom-GCN."}, {"id": "scienceqa-science-question-answering", "name": "ScienceQA (Science Question Answering)", "description": "Science Question Answering (ScienceQA) is a new benchmark that consists of 21,208 multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations. Out of the questions in ScienceQA, 10,332 (48.7%) have an image context, 10,220 (48.2%) have a text context, and 6,532 (30.8%) have both. Most questions are annotated with grounded lectures (83.9%) and detailed explanations (90.5%). The lecture and explanation provide general external knowledge and specific reasons, respectively, for arriving at the correct answer. To the best of our knowledge, ScienceQA is the first large-scale multimodal dataset that annotates lectures and explanations for the answers."}, {"id": "iris", "name": "iris", "description": "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gasp\u00e9 Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\"."}, {"id": "mtst-mobile-turkish-scene-text", "name": "MTST (Mobile Turkish Scene Text)", "description": "The Mobile Turkish Scene Text (MTST 200) dataset consists of 200 indoor and outdoor Turkish scene text images."}, {"id": "ashrae-energy-prediction-iii", "name": "ASHRAE energy prediction III", "description": "Assessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower-cost financing."}, {"id": "iquad-interactive-question-answering-dataset", "name": "IQUAD (Interactive Question Answering Dataset)", "description": "IQUAD is a dataset for Visual Question Answering in interactive environments. It is built upon AI2-THOR, a simulated photo-realistic environment of configurable indoor scenes with interactive object. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration."}, {"id": "sdd", "name": "SDD", "description": "SDD dataset contains a variety of indoor and outdoor scenes, designed for Image Defocus Deblurring. There are 50 indoor scenes and 65 outdoor scenes in the training set, and 11 indoor scenes and 24 outdoor scenes in the testing set. "}, {"id": "sketch", "name": "Sketch", "description": "The Sketch dataset contains over 20,000 sketches evenly distributed over 250 object categories."}, {"id": "codex-medium", "name": "CoDEx Medium", "description": "CoDEx comprises a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. "}, {"id": "ui5k-mobile-app-user-interface-dataset", "name": "UI5k (Mobile App User Interface Dataset)", "description": "This dataset contains 54,987 UI screenshots and the metadata from 7,748 Android applications belonging to 25 application categories"}, {"id": "lshtc", "name": "LSHTC", "description": "LSHTC is a dataset for large-scale text classification. The data used in the LSHTC challenges originates from two popular sources: the DBpedia and the ODP (Open Directory Project) directory, also known as DMOZ. DBpedia instances were selected from the english, non-regional Extended Abstracts provided by the DBpedia site. The DMOZ instances consist of either Content vectors, Description vectors or both. A Content vectors is obtained by directly indexing the web page using standard indexing chain (preprocessing, stemming/lemmatization, stop-word removal). "}, {"id": "fsdkaggle2019", "name": "FSDKaggle2019", "description": "FSDKaggle2019 is an audio dataset containing 29,266 audio files annotated with 80 labels of the AudioSet Ontology. FSDKaggle2019 has been used for the DCASE Challenge 2019 Task 2, which was run as a Kaggle competition titled Freesound Audio Tagging 2019. The dataset allows development and evaluation of machine listening methods in conditions of label noise, minimal supervision, and real-world acoustic mismatch. FSDKaggle2019 consists of two train sets and one test set. One train set and the test set consists of manually-labeled data from Freesound, while the other train set consists of noisily labeled web audio data from Flickr videos taken from the YFCC dataset. The curated train set consists of manually labeled data from FSD: 4970 total clips with a total duration of 10.5 hours.  The noisy train set has 19,815 clips with a total duration of 80 hours. The test set has 4481 clips with a total duration of 12.9 hours."}, {"id": "adam-adam-automatic-detection-challenge-on-age-related-macular-degeneration", "name": "ADAM (Adam: automatic detection challenge on age-related macular degeneration)", "description": "ADAM is organized as a half day Challenge, a Satellite Event of the ISBI 2020 conference in Iowa City, Iowa, USA."}, {"id": "mmid-massively-multilingual-image-dataset", "name": "MMID (Massively Multilingual Image Dataset)", "description": "A large-scale multilingual corpus of images, each labeled with the word it represents. The dataset includes approximately 10,000 words in each of 100 languages."}, {"id": "crows-pairs", "name": "CrowS-Pairs", "description": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups."}, {"id": "highd-dataseth-the-highway-drone-dataset-naturalistic-trajectories-of-110-500-vehicles-recorded-at-german-highways", "name": "highD Dataseth (The Highway Drone Dataset Naturalistic Trajectories of 110 500 Vehicles Recorded at German Highways)", "description": "The highD dataset is a new dataset of naturalistic vehicle trajectories recorded on German highways. Using a drone, typical limitations of established traffic data collection methods such as occlusions are overcome by the aerial perspective. Traffic was recorded at six different locations and includes more than 110 500 vehicles. Each vehicle's trajectory, including vehicle type, size and manoeuvres, is automatically extracted. Using state-of-the-art computer vision algorithms, the positioning error is typically less than ten centimeters. Although the dataset was created for the safety validation of highly automated vehicles, it is also suitable for many other tasks such as the analysis of traffic patterns or the parameterization of driver models."}, {"id": "idrid-indian-diabetic-retinopathy-image-dataset", "name": "IDRiD (Indian Diabetic Retinopathy Image Dataset)", "description": "Indian Diabetic Retinopathy Image Dataset (IDRiD) dataset consists of typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. This dataset also provides information on the disease severity of diabetic retinopathy and diabetic macular edema for each image. This dataset is perfect for the development and evaluation of image analysis algorithms for early detection of diabetic retinopathy."}, {"id": "automating-dynamic-consent", "name": "Automating Dynamic Consent", "description": "This dataset is used to evaluate a predictive consent model for users\u2019 information shared in social media. In this task, the goal is to predict whether the users will give their consent to share that data with different hypothetical audiences within a medical context. The dataset is built from information the users posted on Facebook and their consent answers about each piece of information."}, {"id": "dpcspell-bangla-sec-corpus", "name": "DPCSpell-Bangla-SEC-Corpus", "description": "MIT licenseDPCSpell-Bangla-SEC-Corpus is a large-scale parallel corpus for Bangla spelling error correction."}, {"id": "feverous-fact-extraction-and-verification-over-unstructured-and-structured-information", "name": "FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information)", "description": "FEVEROUS (Fact Extraction and VERification Over Unstructured and Structured information) is a fact verification dataset which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict."}, {"id": "sportsett", "name": "SportSett", "description": "This resource is designed to allow for research into Natural Language Generation.  In particular, with neural data-to-text approaches although it is not limited to these."}, {"id": "mame-museum-art-medium-dataset", "name": "MAMe (Museum Art Medium dataset)", "description": "The MAMe dataset contains images of high-resolution and variable shape of artworks from 3 different museums:"}, {"id": "iconqa-icon-question-answering", "name": "IconQA (Icon Question Answering)", "description": "Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images in the daily-life context. Icon question answering (IconQA) is a benchmark which aims to highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning in real-world diagram word problems. For this benchmark, a large-scale IconQA dataset is built that consists of three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. Compared to existing VQA benchmarks, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning."}, {"id": "isaid", "name": "iSAID", "description": "iSAID contains 655,451 object instances for 15 categories across 2,806 high-resolution images. The images of iSAID is the same as the DOTA-v1.0 dataset, which are manily collected from the Google Earth, some are taken by satellite JL-1, the others are taken by satellite GF-2 of the China Centre for Resources Satellite Data and Application."}, {"id": "c-z", "name": "C&Z", "description": "One of the first datasets (if not the first) to highlight the importance of bias and diversity in the community, which started a revolution afterwards. Introduced in 2014 as integral part of a thesis of Master of Science [1,2] at Carnegie Mellon and City University of Hong Kong. It was later expanded by adding synthetic images generated by a GAN architecture at ETH Z\u00fcrich (in HDCGAN by Curt\u00f3 et al. 2017). Being then not only the pioneer of talking about the importance of balanced datasets for learning and vision but also for being the first GAN augmented dataset of faces. "}, {"id": "arxiv-gr-qc-general-relativity-and-quantum-cosmology-collaboration-network", "name": "Arxiv GR-QC (General Relativity and Quantum Cosmology collaboration network)", "description": "Arxiv GR-QC (General Relativity and Quantum Cosmology) collaboration network is from the e-print arXiv and covers scientific collaborations between authors papers submitted to General Relativity and Quantum Cosmology category. If an author i co-authored a paper with author j, the graph contains a undirected edge from i to j. If the paper is co-authored by k authors this generates a completely connected (sub)graph on k nodes."}, {"id": "unimib-shar", "name": "UniMiB SHAR", "description": "Includes 11,771 samples of both human activities and falls performed by 30 subjects of ages ranging from 18 to 60 years. Samples are divided in 17 fine grained classes grouped in two coarse grained classes: one containing samples of 9 types of activities of daily living (ADL) and the other containing samples of 8 types of falls. The dataset has been stored to include all the information useful to select samples according to different criteria, such as the type of ADL, the age, the gender, and so on. "}, {"id": "value-video-and-language-understanding-evaluation", "name": "VALUE (Video-And-Language Understanding Evaluation)", "description": "VALUE is a Video-And-Language Understanding Evaluation benchmark to test models that are generalizable to diverse tasks, domains, and datasets. It is an assemblage of 11 VidL (video-and-language) datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. "}, {"id": "yud-additional-vanishing-point-labels-for-the-york-urban-database", "name": "YUD+ (Additional Vanishing Point Labels for the York Urban Database)", "description": "YUD+ is a dataset containing additional Vanishing Point Labels for the York Urban Database."}, {"id": "omics-open-mind-indoor-common-sense", "name": "OMICS (Open Mind Indoor Common Sense)", "description": "OMICS is an extensive collection of knowledge for indoor service robots gathered from internet users. Currently, it contains 48 tables capturing different sorts of knowledge. Each tuple of the Help table maps a user desire to a task that may meet the desire (e.g., \u27e8 \u201cfeel thirsty\u201d, \u201cby offering drink\u201d \u27e9). Each tuple of the Tasks/Steps table decomposes a task into several steps (e.g., \u27e8 \u201cserve a drink\u201d, 0. \u201cget a glass\u201d, 1. \u201cget a bottle\u201d, 2. \u201cfill class from bottle\u201d, 3. \u201cgive class to person\u201d \u27e9). Given this, OMICS offers useful knowledge about hierarchism of naturalistic instructions, where a high-level user request (e.g., \u201cserve a drink\u201d) can be reduced to lower-level tasks (e.g., \u201cget a glass\u201d, \u22ef). Another feature of OMICS is that elements of any tuple in an OMICS table are semantically related according to a predefined template. This facilitates the semantic interpretation of the OMICS tuples."}, {"id": "flame-fire-luminosity-airborne-based-machine-learning-evaluation", "name": "FLAME (Fire Luminosity Airborne-based Machine learning Evaluation)", "description": "FLAME is a fire image dataset collected by drones during a prescribed burning piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. The captured videos and images are annotated and labeled frame-wise to help researchers easily apply their fire detection and modeling algorithms."}, {"id": "sku110k-r", "name": "SKU110K-R", "description": "SKU110K-R is a dataset relabeled with oriented bounding boxes based on SKU110K. It is focused on evaluating oriented and densely packed object detection."}, {"id": "raileye3d-dataset", "name": "RailEye3D Dataset", "description": "The RailEye3D dataset, a collection of train-platform scenarios for applications targeting passenger safety and automation of train dispatching, consists of 10 image sequences captured at 6 railway stations in Austria. Annotations for multi-object tracking are provided in both an unified format as well as the ground-truth format used in the MOTChallenge."}, {"id": "epilepsy-seizure-prediction", "name": "Epilepsy seizure prediction", "description": "The original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds."}, {"id": "imdb-binary", "name": "IMDB-BINARY", "description": "IMDB-BINARY is a movie collaboration dataset that consists of the ego-networks of 1,000 actors/actresses who played roles in movies in IMDB. In each graph, nodes represent actors/actress, and there is an edge between them if they appear in the same movie. These graphs are derived from the Action and Romance genres."}, {"id": "basil", "name": "BASIL", "description": "300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias."}, {"id": "frames-dataset", "name": "Frames Dataset", "description": "This dataset is dialog dataset collected in a Wizard-of-Oz fashion. Two humans talked to each other via a chat interface. One was playing the role of the user and the other one was playing the role of the conversational agent. The latter is called a wizard as a reference to the Wizard of Oz, the man behind the curtain. The wizards had access to a database of 250+ packages, each composed of a hotel and round-trip flights. The users were asked to find the best deal. This resulted in complex dialogues where a user would often consider different options, compare packages, and progressively build the description of her ideal trip."}, {"id": "ua-gec-ua-gec-grammatical-error-correction-and-fluency-corpus-for-the-ukrainian-language", "name": "UA-GEC (UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language)", "description": "UA-GEC: Grammatical Error Correction and Fluency Corpus for the Ukrainian Language"}, {"id": "domsev-dataset-of-multimodal-semantic-egocentric-video", "name": "DoMSEV (Dataset of Multimodal Semantic Egocentric Video)", "description": "The Dataset of Multimodal Semantic Egocentric Video (DoMSEV) contains 80-hours of multimodal (RGB-D, IMU, and GPS) data related to First-Person Videos with annotations for recorder profile, frame scene, activities, interaction, and attention."}, {"id": "vrd-visual-relationship-detection-dataset", "name": "VRD (Visual Relationship Detection dataset)", "description": "The Visual Relationship Dataset (VRD) contains 4000 images for training and 1000 for testing annotated with visual relationships. Bounding boxes are annotated with a label containing 100 unary predicates. These labels refer to animals, vehicles, clothes and generic objects. Pairs of bounding boxes are annotated with a label containing 70 binary predicates. These labels refer to actions, prepositions, spatial relations, comparatives or preposition phrases. The dataset has 37993 instances of visual relationships and 6672 types of relationships. 1877 instances of relationships occur only in the test set and they are used to evaluate the zero-shot learning scenario."}, {"id": "davis-2016", "name": "DAVIS 2016", "description": "DAVIS16 is a dataset for video object segmentation which consists of 50 videos in total (30 videos for training and 20 for testing). Per-frame pixel-wise annotations are offered."}, {"id": "break", "name": "BREAK", "description": "Break is a question understanding dataset, aimed at training models to reason over complex questions. It features 83,978 natural language questions, annotated with a new meaning representation, Question Decomposition Meaning Representation (QDMR). Each example has the natural question along with its QDMR representation. Break contains human composed questions, sampled from 10 leading question-answering benchmarks over text, images and databases. This dataset was created by a team of NLP researchers at Tel Aviv University and Allen Institute for AI."}, {"id": "widerperson", "name": "WiderPerson", "description": "WiderPerson contains a total of 13,382 images with 399,786 annotations, i.e., 29.87 annotations per image, which means this dataset contains dense pedestrians with various kinds of occlusions. Hence, pedestrians in the proposed dataset are extremely challenging due to large variations in the scenario and occlusion, which is suitable to evaluate pedestrian detectors in the wild."}, {"id": "nind-natural-image-noise-dataset", "name": "NIND (Natural Image Noise Dataset)", "description": "An open dataset of real photographs with real noise, from identical scenes captured with varying ISO values. Most images are taken with a Fujifilm X-T1 and XF18-55mm, other photographers are encouraged to contribute images for a more diverse crowdsourced effort."}, {"id": "taxinli", "name": "TaxiNLI", "description": "TaxiNLI is a dataset collected based on the principles and categorizations of the aforementioned taxonomy. A subset of examples are curated from MultiNLI (Williams et al., 2018) by sampling uniformly based on the entailment label and the domain. The dataset is annotated with finegrained category labels."}, {"id": "jamendo-corpus", "name": "Jamendo Corpus", "description": "The Jamendo Corpus is a voice detection dataset consisting of 93 songs with Creative Commons license from the Jamendo free music sharing website. Segments of each song are annotated as \u201cvoice\u201d (sung or spoken) or \u201cno-voice\u201d. The songs constitute a total of about 6 hours of music. The files are all from different artists and represent various genres from mainstream commercial music. The Jamendo audio files are coded in stereo Vorbis OGG 44.1kHz with 112KB/s bitrate. The original split contains 61, 16 and 16 songs in training, validation and testing set, respectively."}, {"id": "atue", "name": "ATUE", "description": "ATUE is an antibody study benchmark with four real-world supervised tasks covering therapeutic antibody engineering, B cell analysis, and antibody discovery."}, {"id": "amazon-men", "name": "Amazon Men", "description": "This datasets is a subset of the Amazon reviews dataset which contain Men related products"}, {"id": "mfr-ongoing-version-of-iccv-2021-masked-face-recognition-challenge-workshop-mfr", "name": "MFR (Ongoing version of ICCV-2021 Masked Face Recognition Challenge & Workshop(MFR))", "description": "During the COVID-19 coronavirus epidemic, almost everyone wears a facial mask, which poses a huge challenge to face recognition. Traditional face recognition systems may not effectively recognize the masked faces, but removing the mask for authentication will increase the risk of virus infection. Inspired by the COVID-19 pandemic response, the widespread requirement that people wear protective face masks in public places has driven a need to understand how face recognition technology deals with occluded faces, often with just the periocular area and above visible. "}, {"id": "cuad-contract-understanding-atticus-dataset", "name": "CUAD (Contract Understanding Atticus Dataset)", "description": "Contract Understanding Atticus Dataset (CUAD) is a dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review."}, {"id": "pg-19", "name": "PG-19", "description": "A new open-vocabulary language modelling benchmark derived from books."}, {"id": "istd", "name": "ISTD", "description": "The Image Shadow Triplets dataset (ISTD) is a dataset for shadow understanding that contains 1870 image triplets of shadow image, shadow mask, and shadow-free image."}, {"id": "crosswoz", "name": "CrossWOZ", "description": "CrossWOZ is the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides."}, {"id": "federated-stack-overflow", "name": "Federated Stack Overflow", "description": "This dataset is derived from the Stack Overflow Data hosted by kaggle.com and available to query through Kernels using the BigQuery API: https://www.kaggle.com/stackoverflow/stackoverflow"}, {"id": "n-caltech-101-neuromorphic-caltech101", "name": "N-Caltech 101 (Neuromorphic-Caltech101)", "description": "The Neuromorphic-Caltech101 (N-Caltech101) dataset is a spiking version of the original frame-based Caltech101 dataset. The original dataset contained both a \"Faces\" and \"Faces Easy\" class, with each consisting of different versions of the same images. The \"Faces\" class has been removed from N-Caltech101 to avoid confusion, leaving 100 object classes plus a background class. The N-Caltech101 dataset was captured by mounting the ATIS sensor on a motorized pan-tilt unit and having the sensor move while it views Caltech101 examples on an LCD monitor as shown in the video below. A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset."}, {"id": "dact-dataset-of-annotated-car-trajectories", "name": "DACT (Dataset of Annotated Car Trajectories)", "description": "DACT contains two subsets of annotated car trajectories data. The dataset contains 50 trajectories which cover about 13 hours of driving data. In DACT, we manually specified significant driving patterns by using an interactive framework. A significant driving pattern can be anything like a turn, speed-up, slow-down, etc. The annotation process consists of a crowd-sourcing task followed by comprehensive aggregation phases. The aggregation is done by two different strategies: Strict and Easy. For the first one, we used some strict constraints to aggregate crowd-sourcing results, while we used flexible constraints to generate the second subset of DACT."}, {"id": "dailydialog-2", "name": "DailyDialog++", "description": "Consists of (i) five relevant responses for each context and (ii) five adversarially crafted irrelevant responses for each context."}, {"id": "csl", "name": "CSL", "description": "CSL is a synthetic dataset introduced in Murphy et al. (2019) to test the expressivity of GNNs. In particular, graphs are isomorphic if they have the same degree and the task is to classify non-isomorphic graphs."}, {"id": "2devs", "name": "2devs", "description": "2devs is a publicly available dataset of fine-grained untangled code changes collected by recording the development sessions of two developers over the course of four months, and the corresponding manual clustering."}, {"id": "aic-ai-challenger", "name": "AIC (AI Challenger)", "description": "A large-scale dataset named AIC (AI Challenger) with three sub-datasets, human keypoint detection (HKD), large-scale attribute dataset (LAD) and image Chinese captioning (ICC)."}, {"id": "penn94", "name": "Penn94", "description": "Node classification on Penn94"}, {"id": "allegro-reviews", "name": "Allegro Reviews", "description": "A comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. "}, {"id": "cape-clothed-auto-person-encoding", "name": "CAPE (Clothed Auto Person Encoding)", "description": "The CAPE dataset is a 3D dynamic dataset of clothed humans, featuring:"}, {"id": "genia", "name": "GENIA", "description": "The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology."}, {"id": "vindr-cxr", "name": "VinDr-CXR", "description": "VinDr-CXR is an open large-scale dataset of chest X-rays with radiologist\u2019s annotations. It's bult from more than 100,000 raw images in DICOM format that were retrospectively collected from the Hospital 108 and the Hanoi Medical University Hospital, two of the largest hospitals in Vietnam. The published dataset consists of 18,000 postero-anterior (PA) view CXR scans that come with both the localization of critical findings and the classification of common thoracic diseases. These images were annotated by a group of 17 radiologists with at least 8 years of experience for the presence of 22 critical findings (local labels) and 6 diagnoses (global labels); each finding is localized with a bounding box. The local and global labels correspond to the \u201cFindings\u201d and \u201cImpressions\u201d sections, respectively, of a standard radiology report.  "}, {"id": "youtube-100m-youtube-100m", "name": "YouTube-100M (YouTube-100m)", "description": "The YouTube-100M data set consists of 100 million YouTube videos: 70M training videos, 10M evaluation videos, and 20M validation videos. Videos average 4.6 minutes each for a total of 5.4M training hours. Each of these videos is labeled with 1 or more topic identifiers from a set of 30,871 labels. There are an average of around 5 labels per video. The labels are assigned automatically based on a combination of metadata (title, description, comments, etc.), context, and image content for each video. The labels apply to the entire video and range from very generic (e.g. \u201cSong\u201d) to very specific (e.g. \u201cCormorant\u201d). Being machine generated, the labels are not 100% accurate and of the 30K labels, some are clearly acoustically relevant (\u201cTrumpet\u201d) and others are less so (\u201cWeb Page\u201d). Videos often bear annotations with multiple degrees of specificity. For example, videos labeled with \u201cTrumpet\u201d are often labeled \u201cEntertainment\u201d as well, although no hierarchy is enforced."}, {"id": "si-score", "name": "SI-Score", "description": "SI-SCORE is a synthetic dataset for the analysis of robustness to object location, rotation and size. It consists of images that vary only for factors like object size and object location."}, {"id": "tts-portuguese-corpus", "name": "TTS-Portuguese Corpus", "description": "The dataset has 10.5 hours from a single speaker."}, {"id": "ett-electricity-transformer-temperature", "name": "ETT (Electricity Transformer Temperature)", "description": "The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment. This dataset consists of 2 years data from two separated counties in China. To explore the granularity on the Long sequence time-series forecasting (LSTF) problem, different subsets are created, {ETTh1, ETTh2} for 1-hour-level and ETTm1 for 15-minutes-level. Each data point consists of the target value \u201doil temperature\u201d and 6 power load features. The train/val/test is 12/4/4 months."}, {"id": "2d-3d-match-dataset", "name": "2D-3D Match Dataset", "description": "2D-3D Match Dataset is a new dataset of 2D-3D correspondences by leveraging the availability of several 3D datasets from RGB-D scans. Specifically, the data from SceneNN and 3DMatch are used. The training dataset consists of 110 RGB-D scans, of which 56 scenes are from SceneNN and 54 scenes are from 3DMatch. The 2D-3D correspondence data is generated as follows. Given a 3D point which is randomly sampled from a 3D point cloud, a set of 3D patches from different scanning views are extracted. To find a 2D-3D correspondence, for each 3D patch, its 3D position is re-projected into all RGB-D frames for which the point lies in the camera frustum, taking occlusion into account. The corresponding local 2D patches around the re-projected point are extracted. In total, around 1.4 millions 2D-3D correspondences are collected."}, {"id": "chest-x-ray-images-chest-x-ray-images-for-pneumonia-detection", "name": "Chest X-ray images (chest X-ray images for pneumonia detection)", "description": "Chest X-ray images for pneumonia detection."}, {"id": "bigearthnet", "name": "BigEarthNet", "description": "BigEarthNet consists of 590,326 Sentinel-2 image patches, each of which is a section of i) 120x120 pixels for 10m bands; ii) 60x60 pixels for 20m bands; and iii) 20x20 pixels for 60m bands. "}, {"id": "adveta", "name": "ADVETA", "description": "ADVErsarial Table perturbAtion (ADVETA) is a robustness evaluation benchmark featuring natural and realistic ATPs. It is based on three mainstream Text-to-SQL datasets, Spider, WikiSQL and WTQ."}, {"id": "frgc-morphs", "name": "FRGC-Morphs", "description": "FRGC-Morphs is a dataset of morphed faces selected from the publicly available FRGC dataset [1]."}, {"id": "deezer-europe", "name": "Deezer-Europe", "description": "Node classification on Deezer Europe with 50%/25%/25% random splits for training/validation/test."}, {"id": "fm-iqa-freestyle-multilingual-image-question-answering", "name": "FM-IQA (Freestyle Multilingual Image Question Answering)", "description": "FM-IQA is a question-answering dataset containing over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations."}, {"id": "abstractreasoning", "name": "AbstractReasoning", "description": "AbstractReasoning is a dataset for abstract reasoning, where the goal is to infer the correct answer from the context panels based on abstract reasoning."}, {"id": "uavdt-unmanned-aerial-vehicle-benchmark-object-detection-and-tracking", "name": "UAVDT (Unmanned Aerial Vehicle Benchmark Object Detection and Tracking)", "description": "UAVDT is a large scale challenging UAV Detection and Tracking benchmark (i.e., about 80, 000 representative frames from 10 hours raw videos) for 3 important fundamental tasks, i.e., object DETection (DET), Single Object Tracking (SOT) and Multiple Object Tracking (MOT)."}, {"id": "sun-attribute", "name": "SUN Attribute", "description": "The SUN Attribute dataset consists of 14,340 images from 717 scene categories, and each category is annotated with a taxonomy of 102 discriminate attributes. The dataset can be used for high-level scene understanding and fine-grained scene recognition."}, {"id": "pa-hmdb51-privacy-annotated-hmdb51", "name": "PA-HMDB51 (Privacy Annotated HMDB51)", "description": "The Privacy Annotated HMDB51 (PA-HMDB51) dataset is a video-based dataset for evaluating pirvacy protection in visual action recognition algorithms. The dataset contains both target task labels (action) and selected privacy attributes (skin color, face, gender, nudity, and relationship) annotated on a per-frame basis."}, {"id": "cspubsum", "name": "CSPubSum", "description": "CSPubSum is a dataset for summarisation of computer science publications, created by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. "}, {"id": "hdd-honda-research-institute-driving-dataset", "name": "HDD (Honda Research Institute Driving Dataset)", "description": "Honda Research Institute Driving Dataset (HDD) is a dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors."}, {"id": "ctc-coco-text-captioned", "name": "CTC (COCO-Text Captioned)", "description": "A dataset that allows exploration of cross-modal retrieval where images contain scene-text instances. "}, {"id": "molweni", "name": "Molweni", "description": "A machine reading comprehension (MRC) dataset with discourse structure built over multiparty dialog. Molweni's source samples from the Ubuntu Chat Corpus, including 10,000 dialogs comprising 88,303 utterances. "}, {"id": "2-pm-vessel-dataset", "name": "2-PM Vessel Dataset", "description": "2-PM Vessel is an open-source volumetric brain vasculature dataset obtained with two-photon microscopy at Focused Ultrasound Lab, at Sunnybrook Research Institute (affiliated with University of Toronto by Dr. Alison Burgess, Charissa Poon and Marc Santos. The dataset contains a total of 12 volumetric stacks consisting of images of mouse brain vasculature and tumour vasculature."}, {"id": "bentham-bentham-project", "name": "Bentham (Bentham project)", "description": "Bentham manuscripts refers to a large set of documents that were written by the renowned English philosopher and reformer Jeremy Bentham (1748-1832). Volunteers of the Transcribe Bentham initiative transcribed this collection. Currently, >6 000 documents or > 25 000 pages have been transcribed using this public web platform. For our experiments, we used the BenthamR0 dataset a part of the Bentham manuscripts."}, {"id": "roadtracer", "name": "RoadTracer", "description": "RoadTracer is a dataset for extraction of road networks from aerial images. It consists of a large corpus of high-resolution satellite imagery and ground truth road network graphs covering the urban core of forty cities across six countries. For each city, the dataset covers a region of approximately 24 sq km around the city center. The satellite imagery is obtained from Google at 60 cm/pixel resolution, and the road network from OSM."}, {"id": "a2dre-subset-of-a2d-sentences-which-are-not-trivial", "name": "A2Dre (Subset of A2D Sentences which are not trivial)", "description": "We obtain A2Dre by selecting only instances that were labeled as non-trivial, which are 433 REs from 190 videos. We do not use the trivial cases as the analysis of such examples is not relevant, as referents can be described by using the category alone. Each annotator was presented with a RE, a video in which the target object was marked by a bounding box, and a set of questions paraphrasing our categories. A2Dre was annotated by 3 authors of the paper. Our final set of category annotations used for analysis was derived by means of majority voting: for each nontrivial RE, we kept all category labels which were assigned to the RE by at least two annotators."}, {"id": "clevr-humans", "name": "CLEVR-Humans", "description": "We collect  a  new  dataset  of  human-posed  free-form  natural  language  questions  about  CLEVR  images.    Many  of  these questions have out-of-vocabulary words and require reasoning skills that are absent from our model\u2019s repertoire"}, {"id": "oscar", "name": "OSCAR", "description": "OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture. The dataset used for training multilingual models such as BART incorporates 138 GB of text."}, {"id": "must-cinema", "name": "MuST-Cinema", "description": "MuST-Cinema is a Multilingual Speech-to-Subtitles corpus ideal for building subtitle-oriented machine and speech translation systems. It comprises audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations."}, {"id": "entityquestions", "name": "EntityQuestions", "description": "EntityQuestions is a dataset of simple, entity-rich questions based on facts from Wikidata (e.g., \"Where was Arve Furset born? \")."}, {"id": "lafan1-ubisoft-la-forge-animation-dataset", "name": "LaFAN1 (Ubisoft La Forge Animation Dataset)", "description": "Ubisoft La Forge Animation dataset and accompanying code for the SIGGRAPH 2020 paper Robust Motion In-betweening."}, {"id": "gumar-corpus", "name": "Gumar Corpus", "description": "A large-scale corpus of Gulf Arabic consisting of 110 million words from 1,200 forum novels."}, {"id": "urban100", "name": "Urban100", "description": "The Urban100 dataset contains 100 images of urban scenes. It commonly used as a test set to evaluate the performance of super-resolution models."}, {"id": "32vis", "name": "32vis", "description": "Dataset for the 32 years of IEEE VIS"}, {"id": "adni-alzheimer-s-disease-neuroimaging-initiative", "name": "ADNI (Alzheimer's Disease NeuroImaging Initiative)", "description": "Alzheimer's Disease Neuroimaging Initiative (ADNI) is a multisite study that aims to improve clinical trials for the prevention and treatment of Alzheimer\u2019s disease (AD).[1] This cooperative study combines expertise and funding from the private and public sector to study subjects with AD, as well as those who may develop AD and controls with no signs of cognitive impairment.[2] Researchers at 63 sites in the US and Canada track the progression of AD in the human brain with neuroimaging, biochemical, and genetic biological markers.[2][3] This knowledge helps to find better clinical trials for the prevention and treatment of AD. ADNI has made a global impact,[4]  firstly by developing a set of standardized protocols to allow the comparison of results from multiple centers,[4] and secondly by its data-sharing policy which makes available all at the data without embargo to qualified researchers worldwide.[5] To date, over 1000 scientific publications have used ADNI data.[6] A number of other initiatives related to AD and other diseases have been designed and implemented using ADNI as a model.[4] ADNI has been running since 2004 and is currently funded until 2021.[7]"}, {"id": "places-lt", "name": "Places-LT", "description": "Places-LT has an imbalanced training set with 62,500 images for 365 classes from Places-2. The class frequencies follow a natural power law distribution with a maximum number of 4,980 images per class and a minimum number of 5 images per class. The validation and testing sets are balanced and contain 20 and 100 images per class respectively."}, {"id": "sku110k", "name": "SKU110K", "description": "The Sku110k dataset provides 11,762 images with more than 1.7 million annotated bounding boxes captured in densely packed scenarios, including 8,233 images for training, 588 images for validation, and 2,941 images for testing. There are around 1,733,678 instances in total. The images are collected from thousands of supermarket stores and are of various scales, viewing angles, lighting conditions, and noise levels. All the images are resized into a resolution of one megapixel. Most of the instances in the dataset are tightly packed and typically of a certain orientation in the rage of [\u221215\u2218, 15\u2218]."}, {"id": "voiceprivacy-2020", "name": "VoicePrivacy 2020", "description": "VoicePrivacy 2020 is a dataset for developing anonymization solutions for speech technology. It is built from subsets of existing datasets such as: LibriSpeech, LibriTTS, VoxCeleb1, VoxCeleb2 and VCTK."}, {"id": "parkinson-s-pose-estimation-dataset", "name": "Parkinson's Pose Estimation Dataset", "description": "The data includes all movement trajectories extracted from the videos of Parkinson's assessments using Convolutional Pose Machines (CPM) as well as the confidence values from CPM. The dataset also includes ground truth ratings of parkinsonism and dyskinesia severity using the UDysRS, UPDRS, and CAPSIT."}, {"id": "uhrsd-ultra-high-resolution-saliency-detection-dataset", "name": "UHRSD (Ultra High-Resolution Saliency Detection Dataset)", "description": "Recent salient object detection (SOD) methods based on deep neural network have achieved remarkable performance. However, most of existing SOD models designed for low-resolution input perform poorly on high-resolution images due to the contradiction between the sampling depth and the receptive field size. Aiming at resolving this contradiction, we propose a novel one-stage framework called Pyramid Grafting Network (PGNet), using transformer and CNN backbone to extract features from different resolution images independently and then graft the features from transformer branch to CNN branch. An attention-based Cross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine broken detailed information more holistically, guided by different source feature during decoding process. Moreover, we design an Attention Guided Loss (AGL) to explicitly supervise the attention matrix generated by CMGM to help the network better interact with the attention from different models. We contribute a new Ultra-High-Resolution Saliency Detection dataset UHRSD, containing 5,920 images at 4K-8K resolutions. To our knowledge, it is the largest dataset in both quantity and resolution for high-resolution SOD task, which can be used for training and testing in future research. Sufficient experiments on UHRSD and widely-used SOD datasets demonstrate that our method achieves superior performance compared to the state-of-the-art methods."}, {"id": "magicdata-ramc", "name": "MagicData-RAMC", "description": "The MagicData-RAMC corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs in the dialogs are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided."}, {"id": "slurp-spoken-language-understanding-resource-package", "name": "SLURP (Spoken Language Understanding Resource Package)", "description": "A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets."}, {"id": "symphonynet", "name": "SymphonyNet", "description": "First large-scale symphony generation dataset."}, {"id": "hatemojicheck", "name": "HatemojiCheck", "description": "HatemojiCheck is a test suite for detecting emoji-based hate of 3,930 test cases covering seven functionalities of emoji-based hate and six identities."}, {"id": "arc-ukiyo-e-faces", "name": "ARC Ukiyo-e Faces", "description": "ARC Ukiyo-e Faces is a large-scale (>10k paintings, >20k faces) Ukiyo-e dataset with coherent semantic labels and geometric annotations through augmenting and organizing existing datasets with automatic detection."}, {"id": "cbc-complete-blood-count", "name": "CBC (Complete Blood Count)", "description": "The complete blood count (CBC) dataset contains 360 blood smear images along with their annotation files splitting into Training, Testing, and Validation sets. The training folder contains 300 images with annotations. The testing and validation folder both contain 60 images with annotations. We have done some modifications over the original dataset to prepare this CBC dataset where some of the image annotation files contain very low red blood cells (RBCs) than actual and one annotation file does not include any RBC at all although the cell smear image contains RBCs. So, we clear up all the fallacious files and split the dataset into three parts. Among the 360 smear images, 300 blood cell images with annotations are used as the training set first, and then the rest of the 60 images with annotations are used as the testing set. Due to the shortage of data, a subset of the training set is used to prepare the validation set which contains 60 images with annotations."}, {"id": "red-miniimagenet-20-label-noise", "name": "Red MiniImageNet 20% label noise", "description": "Part of the Controlled Noisy Web Labels Dataset."}, {"id": "ig-1b-targeted", "name": "IG-1B-Targeted", "description": "IG-1B-Targeted is an internal Facebook AI Research dataset that consists of 940 million public images with 1.5K hashtags matching with 1000 ImageNet1K synsets."}, {"id": "text-to-3d-house-model-text-to-3d-house-model", "name": "Text-to-3D House Model (Text--to--3D House Model)", "description": "The dataset contains 2,000 houses, 13,478 rooms and 873 (some rooms have same textures so this number is smaller than the total number of rooms.) texture images with corresponding natural language descriptions. These descriptions are firstly generated from some pre-defined templates and then refined by human workers. The average length of the description is 173.73 and there are 193 unique words. In our experiments, we use 1,600 pairs for training while 400 for testing in the building layout generation. For texture synthesis, we use 503 data for training and 370 data for testing."}, {"id": "mor-uav", "name": "MOR-UAV", "description": "A large-scale video dataset for MOR in aerial videos."}, {"id": "curvelanes", "name": "CurveLanes", "description": "CurveLanes is a new benchmark lane detection dataset with 150K lanes images for difficult scenarios such as curves and multi-lanes in traffic lane detection. It is collected in real urban and highway scenarios in multiple cities in China. It is the largest lane detection dataset so far and establishes a more challenging benchmark for the community."}, {"id": "color-feret", "name": "Color FERET", "description": "The color FERET database is a dataset for face recognition. It contains 11,338 color images of size 512\u00d7768 pixels captured in a semi-controlled environment with 13 different poses from 994 subjects."}, {"id": "lfw-labeled-faces-in-the-wild", "name": "LFW (Labeled Faces in the Wild)", "description": "The LFW dataset contains 13,233 images of faces collected from the web. This dataset consists of the 5749 identities with 1680 people with two or more images. In the standard LFW evaluation protocol the verification accuracies are reported on 6000 face pairs."}, {"id": "arsentd-lev", "name": "ArSentD-LEV", "description": "The Arabic Sentiment Twitter Dataset for the Levantine dialect (ArSenTD-LEV) is a dataset of 4,000 tweets with the following annotations: the overall sentiment of the tweet, the target to which the sentiment was expressed, how the sentiment was expressed, and the topic of the tweet. "}, {"id": "deep-pcb-deep-printed-circuit-board", "name": "Deep PCB (Deep Printed Circuit Board)", "description": "We use the axis-aligned bounding box with a class ID for each defect in the tested images. As illustrated in above, we annotate six common types of PCB defects: open, short, mousebite, spur, pin hole, and spurious copper. Since there are only a few defects in the real tested image, we manually arguement some artificial defects on each tested image according to the PCB defect patterns, which leads to around 3 to 12 defects in each 640 x 640 image. The number of PCB defects is shown in the following figure. We separate 1,000 images as a training set and the remains as a test set. Each annotated image owns an annotation file with the same filename, e.g.00041000_test.jpg, 00041000_temp.jpg, and 00041000.txt are the tested image, template image, and the corresponding annotation file. Each defect on the tested image is annotated as the format:x1,y1,x2,y2, type, where (x1,y1) and (x2,y2) is the top left and the bottom right corner of the bounding box of the defect. type is an integer ID that follows the matches: 0-background (not used), 1-open, 2-short, 3-mousebite, 4-spur, 5-copper, 6-pin-hole."}, {"id": "brats-2014", "name": "BraTS 2014", "description": "BRATS 2014 is a brain tumor segmentation dataset."}, {"id": "shapeworld", "name": "ShapeWorld", "description": "ShapeWorld is a new evaluation methodology and framework for multimodal deep learning models, with a focus on formal-semantic style generalization capabilities. In this framework, artificial data is automatically generated according to predefined specifications. This controlled data generation makes it possible to introduce previously unseen instance configurations during evaluation, which consequently require the system to recombine learned concepts in novel ways."}, {"id": "rf100-roboflow-100", "name": "RF100 (Roboflow 100)", "description": "The evaluation of object detection models is usually performed by optimizing a single metric, e.g. mAP, on a fixed set of datasets, e.g. Microsoft COCO and Pascal VOC. Due to image retrieval and annotation costs, these datasets consist largely of images found on the web and do not represent many real-life domains that are being modelled in practice, e.g. satellite, microscopic and gaming, making it difficult to assert the degree of generalization learned by the model."}, {"id": "ubofab19-svbrdf-database-bonn", "name": "UBOFAB19 (SVBRDF Database Bonn)", "description": "A database of several hundred high quality fabric material measurements, provided as carefully calibrated rectified HDR images, together with SVBRDF fits."}, {"id": "covid-fact", "name": "COVID-Fact", "description": "COVID-Fact is a FEVER-like dataset of  claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence."}, {"id": "orconvqa-open-retrieval-conversational-question-answering", "name": "ORConvQA (Open-Retrieval Conversational Question Answering)", "description": "Enhances QuAC by adapting it to an open-retrieval setting. It is an aggregation of three existing datasets: (1) the QuAC dataset that offers information-seeking conversations, (2) the CANARD dataset that consists of context-independent rewrites of QuAC questions, and (3) the Wikipedia corpus that serves as the knowledge source of answering questions."}, {"id": "aws-documentation", "name": "AWS Documentation", "description": "We present the AWS documentation corpus, an open-book QA dataset, which contains 25,175 documents along with 100 matched questions and answers. These questions are inspired by the author's interactions with real AWS customers and the questions they asked about AWS services. The data was anonymized and aggregated. All questions in the dataset have a valid, factual and unambiguous answer within the accompanying documents, we deliberately avoided questions that are ambiguous, incomprehensible, opinion-seeking, or not clearly a request for factual information. All questions, answers and accompanying documents in the dataset are annotated by authors. There are two types of answers: text and yes-no-none(YNN) answers. Text answers range from a few words to a full paragraph sourced from a continuous block of words in a document or from different locations within the same document. Every question in the dataset has a matched text answer. Yes-no-none(YNN) answers can be yes, no, or none depending on the type of question. For example the question: \u201cCan I stop a DB instance that has a read replica?\u201d has a clear yes or no answer but the question \u201cWhat is the maximum number of rows in a dataset in Amazon Forecast?\u201d is not a yes or no question and therefore has a \u201cNone\u201d as the YNN answer. 23 questions have \u2018Yes\u2019 YNN answers, 10 questions have \u2018No\u2019 YNN answers and 67 questions have \u2018None\u2019 YNN answers."}, {"id": "flyingthings3d", "name": "FlyingThings3D", "description": "FlyingThings3D is a synthetic dataset for optical flow, disparity and  scene flow estimation. It consists of everyday objects flying along randomized 3D trajectories. We generated about 25,000 stereo frames with ground truth data. Instead of focusing on a particular task (like KITTI) or enforcing strict naturalism (like Sintel), we rely on randomness and a large pool of rendering assets to generate orders of magnitude more data than any existing option, without running a risk of repetition or saturation."}, {"id": "lit-pcba-kat2a-kat2a-target-of-lit-pcba-dataset", "name": "LIT-PCBA(KAT2A) (KAT2A target of LIT-PCBA Dataset)", "description": "Comparative evaluation of virtual screening methods requires a rigorous benchmarking procedure on diverse, realistic, and unbiased data sets. Recent investigations from numerous research groups unambiguously demonstrate that artificially constructed ligand sets classically used by the community (e.g., DUD, DUD-E, MUV) are unfortunately biased by both obvious and hidden chemical biases, therefore overestimating the true accuracy of virtual screening methods. We herewith present a novel data set (LIT-PCBA) specifically designed for virtual screening and machine learning. LIT-PCBA relies on 149 dose\u2013response PubChem bioassays that were additionally processed to remove false positives and assay artifacts and keep active and inactive compounds within similar molecular property ranges. To ascertain that the data set is suited to both ligand-based and structure-based virtual screening, target sets were restricted to single protein targets for which at least one X-ray structure is available in complex with ligands of the same phenotype (e.g., inhibitor, inverse agonist) as that of the PubChem active compounds. Preliminary virtual screening on the 21 remaining target sets with state-of-the-art orthogonal methods (2D fingerprint similarity, 3D shape similarity, molecular docking) enabled us to select 15 target sets for which at least one of the three screening methods is able to enrich the top 1%-ranked compounds in true actives by at least a factor of 2. The corresponding ligand sets (training, validation) were finally unbiased by the recently described asymmetric validation embedding (AVE) procedure to afford the LIT-PCBA data set, consisting of 15 targets and 7844 confirmed active and 407,381 confirmed inactive compounds. The data set mimics experimental screening decks in terms of hit rate (ratio of active to inactive compounds) and potency distribution. It is available online at http://drugdesign.unistra.fr/LIT-PCBA for download and for benchmarking novel virtual screening methods, notably those relying on machine learning."}, {"id": "cuhk-sysu-cuhk-sysu-person-search-dataset", "name": "CUHK-SYSU (CUHK-SYSU Person Search Dataset)", "description": "The CUKL-SYSY dataset is a large scale benchmark for person search, containing 18,184 images and 8,432 identities. Different from previous re-id benchmarks, matching query persons with manually cropped pedestrians, this dataset is much closer to real application scenarios by searching person from whole images in the gallery."}, {"id": "qa-srl", "name": "QA-SRL", "description": "QA-SRL was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (\u201cWhere was someone educated?\u201d) whose answer is the argument (\u201cPrinceton\u201d). The authors collected about 19,000 question-answer pairs from 3,200 sentences."}, {"id": "italian-disinformation", "name": "Italian disinformation", "description": "This is a large-scale dataset of tweets associated to thousands of news articles published on Italian disinformation websites in the context of 2019 European elections."}, {"id": "object-discovery", "name": "Object Discovery", "description": "The Object Discovery dataset was collected by downloading images from Internet for airplane, car and horse. It is significantly larger and thus, diverse in terms of viewpoints, texture, color etc"}, {"id": "conll-2000", "name": "CoNLL-2000", "description": "CoNLL-2000 is a dataset for dividing text into syntactically related non-overlapping groups of words, so-called text chunking. "}, {"id": "medmcqa", "name": "MedMCQA", "description": "MedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions."}, {"id": "d-hazy", "name": "D-HAZY", "description": "The D-HAZY dataset is generated from NYU depth indoor image collection. D-HAZY contains depth map for each indoor hazy image. It contains 1400+ real images and corresponding depth maps used to synthesize hazy scenes based on Koschmieder\u2019s light propagation mode"}, {"id": "recipeqa", "name": "RecipeQA", "description": "RecipeQA is a dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images. Each question in RecipeQA involves multiple modalities such as titles, descriptions or images, and working towards an answer requires (i) joint understanding of images and text, (ii) capturing the temporal flow of events, and (iii) making sense of procedural knowledge."}, {"id": "chemprot", "name": "ChemProt", "description": "ChemProt consists of 1,820 PubMed abstracts with chemical-protein interactions annotated by domain experts and was used in the BioCreative VI text mining chemical-protein interactions shared task."}, {"id": "home-action-genome", "name": "Home Action Genome", "description": "Home Action Genome is a large-scale multi-view video database of indoor daily activities. Every activity is captured by synchronized multi-view cameras, including an egocentric view. There are 30 hours of vides with 70 classes of daily activities and 453 classes of atomic actions."}, {"id": "30mqa-30m-factoid-question-answer-corpus", "name": "30MQA (30M Factoid Question-Answer Corpus)", "description": "An enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions."}, {"id": "virtual-kitti", "name": "Virtual KITTI", "description": "Virtual KITTI is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation."}, {"id": "pointcloud-c", "name": "PointCloud-C", "description": "PointCloud-C is the very first test-suite for point cloud robustness analysis under corruptions."}, {"id": "ufpr-periocular", "name": "UFPR-Periocular", "description": "The UFPR-Periocular dataset has 16,830 images of both eyes (33,660 cropped images of each eye) from 1,122 subjects (2,244 classes)."}, {"id": "politifact", "name": "PolitiFact", "description": "Fact-checking (FC) articles which contains pairs (multimodal tweet and a FC-article) from politifact.com."}, {"id": "scannet", "name": "ScanNet", "description": "ScanNet is an instance-level indoor RGB-D dataset that includes both 2D and 3D data. It is a collection of labeled voxels rather than points or objects. Up to now, ScanNet v2, the newest version of ScanNet, has collected 1513 annotated scans with an approximate 90% surface coverage. In the semantic segmentation task, this dataset is marked in 20 classes of annotated 3D voxelized objects."}, {"id": "hrsod-high-resolution-salient-object-detection", "name": "HRSOD (High-Resolution Salient Object Detection)", "description": "There exist several datasets for saliency detection, but none of them is specifically designed for high-resolution salient object detection. High-Resolution Salient Object Detection (HRSOD) dataset, containing 1610 training images and 400 test images. The total 2010 images are collected from the website of Flickr with the license of all creative commons. Pixel-level ground truths are manually annotated by 40 subjects. The shortest edge of each image in HRSOD is more than 1200 pixels."}, {"id": "isic-2017-task-2", "name": "ISIC 2017 Task 2", "description": "The ISIC 2017 dataset was published by the International Skin Imaging Collaboration (ISIC) as a large-scale dataset of dermoscopy images. The Task 2 challenge dataset for lesion dermoscopic feature extraction contains the original lesion image, a corresponding superpixel mask, and superpixel-mapped expert annotations of the presence and absence of the following features: (a) network, (b) negative network, (c) streaks and (d) milia-like cysts."}, {"id": "iwslt-2017", "name": "IWSLT 2017", "description": "The IWSLT 2017 translation dataset."}, {"id": "opus-100", "name": "OPUS-100", "description": "A novel multilingual dataset with 100 languages."}]